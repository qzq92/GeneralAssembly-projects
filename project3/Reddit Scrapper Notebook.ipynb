{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Reddit APIs & Classification (Data scraping section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Reddit is a massive collection of forums where people share news and content or comment on other people’s posts. Reddit is broken up into more than a million communities known as “subreddits” and each of which covers a different topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "There are posts related to Pytorch and Tensorflow which could be wrongly posted in subreddits having similar content, and might require the moderators of the subreddits to clean it up occassionally to ensure content relevancy for viewers.\n",
    "\n",
    "In this project, I aim to develop classifier models and identify if a Naive Bayes classifer or another classifier model would be suitable in classifying if a reddit post belongs to the either Pytorch or Tensorflow subreddits which can be useful for the moderators in deciding which posts that require cleanup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "The growing AI industry and the availability of open-sourced frameworks online such as the Tensorflow and Pytorch, the top 2 common frameworks used for implementing various neural network architecture by AI researchers, enthusiasts and application engineers has resulted in lots of questions and answers, especially in Reddit. \n",
    "\n",
    "As such there might be posts on Reddit site related to the two frameworks that could be wrongly posted in wrong subreddits. In view of the increasing amount of posts, it would be beneficial for subreddit stakeholders to have a classifier that helps to classify related posts/comments which are posted incorrectly in various subreddits as part of their cleanup process.\n",
    "\n",
    "In this project, more than 900 unique reddit posts have been scraped from both Pytorch and Tensorflow subreddits respectively via Reddit's API, for the purpose of training classifier models which could potentially recommend Reddit moderators if a specific post should be moved to correct subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "This notebook focuses on scraping the posts of the subreddits of interest(Tensorflow and Pytorch) and saves the scraped data into csv files after removing duplicate entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries and define necessary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_query(url, req_headers, query_times = 4):\n",
    "    \"\"\"\n",
    "    Function that takes in url and number of queries which are used in determining the amount of queries\n",
    "    to be made from a site url for the purpose of collecting reddit posts. An additional request header\n",
    "    was required to prevent the issue of code 429 error caused by the use of common request header by various\n",
    "    client end points on the same page. \n",
    "    \n",
    "    Arguments:\n",
    "    @url: reddit url to be queried \n",
    "    @req_headers: Customised user agent for python \n",
    "    @query_times: number of queries to be made towards reddit url\n",
    "    \"\"\"\n",
    "    posts = []\n",
    "    after = None\n",
    "    for count in range(query_times):\n",
    "        print(\"Query count {}\".format(count))\n",
    "        if after == None:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = url + '?after=' + after\n",
    "            \n",
    "        # Change user agent identifier\n",
    "        req_headers[\"User-agent\"] = req_headers[\"User-agent\"] + str(count)\n",
    "        \n",
    "        res = requests.get(current_url, headers = req_headers)\n",
    "        # Check the status code before extending the number of posts\n",
    "        if res.status_code == 200:\n",
    "            print(\"Request sucessful\")\n",
    "            the_json = res.json()\n",
    "            posts.extend(the_json['data']['children'])\n",
    "        else:\n",
    "            print(\"Request failure\")\n",
    "            print(res.status_code)\n",
    "            break\n",
    "            \n",
    "        after = the_json['data']['after']\n",
    "        #Throttle accordingly based on 60 requests/min restrictions\n",
    "        sleep_duration = random.randint(2,10)\n",
    "        print(f\"Sleeping {sleep_duration}s\")\n",
    "        time.sleep(sleep_duration)\n",
    "    return posts\n",
    "\n",
    "\n",
    "\n",
    "def empty_string_proportion(df, col):\n",
    "    \"\"\"\n",
    "    Function that counts the proportion of empty string for a provided dataframe column\n",
    "    This function returns the scraped posts in the form of a python list.\n",
    "    \n",
    "    Arguments:\n",
    "    @df: pandas dataframe \n",
    "    @col: dataframe column name of interest in string\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        raise Exception(\"Sorry, no such column exists\")\n",
    "    else:\n",
    "        number_entries = df.shape[0]\n",
    "        empty_string_entries = df[df[col] == \"\"].shape[0]\n",
    "        proportion = empty_string_entries/number_entries\n",
    "        print(f\"Entries with empty string: {empty_string_entries} out of {number_entries}\")\n",
    "    return proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping subreddits of interest\n",
    "The variables name used are generic so as to cater scraping of subreddit content of any topic of interest without the need to rename when there is a need to change the topics to be scrap for project purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_url1 = \"https://www.reddit.com/r/pytorch.json\"\n",
    "subreddit_url2 = \"https://www.reddit.com/r/tensorflow.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_headers = {\"User-agent\": \"G.A_Proj3_QZQ\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query count 0\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 1\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 2\n",
      "Request sucessful\n",
      "Sleeping 8s\n",
      "Query count 3\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 4\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 5\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 6\n",
      "Request sucessful\n",
      "Sleeping 6s\n",
      "Query count 7\n",
      "Request sucessful\n",
      "Sleeping 8s\n",
      "Query count 8\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 9\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 10\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 11\n",
      "Request sucessful\n",
      "Sleeping 8s\n",
      "Query count 12\n",
      "Request sucessful\n",
      "Sleeping 3s\n",
      "Query count 13\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 14\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 15\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 16\n",
      "Request sucessful\n",
      "Sleeping 10s\n",
      "Query count 17\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 18\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 19\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 20\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 21\n",
      "Request sucessful\n",
      "Sleeping 6s\n",
      "Query count 22\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 23\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 24\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 25\n",
      "Request sucessful\n",
      "Sleeping 8s\n",
      "Query count 26\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 27\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 28\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 29\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 30\n",
      "Request sucessful\n",
      "Sleeping 8s\n",
      "Query count 31\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 32\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 33\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 34\n",
      "Request sucessful\n",
      "Sleeping 8s\n",
      "Query count 35\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 36\n",
      "Request sucessful\n",
      "Sleeping 8s\n",
      "Query count 37\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 38\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 39\n",
      "Request sucessful\n",
      "Sleeping 6s\n",
      "Query count 40\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 41\n",
      "Request sucessful\n",
      "Sleeping 10s\n",
      "Query count 42\n",
      "Request sucessful\n",
      "Sleeping 10s\n",
      "Query count 43\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 44\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 45\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 46\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 47\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 48\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 49\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 50\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 51\n",
      "Request sucessful\n",
      "Sleeping 3s\n",
      "Query count 52\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 53\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 54\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 55\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 56\n",
      "Request sucessful\n",
      "Sleeping 10s\n",
      "Query count 57\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 58\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 59\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 60\n",
      "Request sucessful\n",
      "Sleeping 10s\n",
      "Query count 61\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 62\n",
      "Request sucessful\n",
      "Sleeping 3s\n",
      "Query count 63\n",
      "Request sucessful\n",
      "Sleeping 6s\n",
      "Query count 64\n",
      "Request sucessful\n",
      "Sleeping 3s\n",
      "Query count 65\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 66\n",
      "Request sucessful\n",
      "Sleeping 6s\n",
      "Query count 67\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 68\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 69\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 70\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 71\n",
      "Request sucessful\n",
      "Sleeping 6s\n",
      "Query count 72\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 73\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 74\n",
      "Request sucessful\n",
      "Sleeping 6s\n",
      "Query count 75\n",
      "Request sucessful\n",
      "Sleeping 6s\n",
      "Query count 76\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 77\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 78\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 79\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 0\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 1\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 2\n",
      "Request sucessful\n",
      "Sleeping 10s\n",
      "Query count 3\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 4\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 5\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 6\n",
      "Request sucessful\n",
      "Sleeping 3s\n",
      "Query count 7\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 8\n",
      "Request sucessful\n",
      "Sleeping 3s\n",
      "Query count 9\n",
      "Request sucessful\n",
      "Sleeping 3s\n",
      "Query count 10\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 11\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 12\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 13\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 14\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 15\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 16\n",
      "Request sucessful\n",
      "Sleeping 10s\n",
      "Query count 17\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 18\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 19\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 20\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 21\n",
      "Request sucessful\n",
      "Sleeping 8s\n",
      "Query count 22\n",
      "Request sucessful\n",
      "Sleeping 6s\n",
      "Query count 23\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 24\n",
      "Request sucessful\n",
      "Sleeping 8s\n",
      "Query count 25\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 26\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 27\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 28\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 29\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 30\n",
      "Request sucessful\n",
      "Sleeping 3s\n",
      "Query count 31\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 32\n",
      "Request sucessful\n",
      "Sleeping 3s\n",
      "Query count 33\n",
      "Request sucessful\n",
      "Sleeping 10s\n",
      "Query count 34\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 35\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 36\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 37\n",
      "Request sucessful\n",
      "Sleeping 10s\n",
      "Query count 38\n",
      "Request sucessful\n",
      "Sleeping 3s\n",
      "Query count 39\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 40\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 41\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 42\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 43\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 44\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 45\n",
      "Request sucessful\n",
      "Sleeping 8s\n",
      "Query count 46\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 47\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 48\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 49\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 50\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 51\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 52\n",
      "Request sucessful\n",
      "Sleeping 6s\n",
      "Query count 53\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 54\n",
      "Request sucessful\n",
      "Sleeping 6s\n",
      "Query count 55\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 56\n",
      "Request sucessful\n",
      "Sleeping 10s\n",
      "Query count 57\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 58\n",
      "Request sucessful\n",
      "Sleeping 6s\n",
      "Query count 59\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 60\n",
      "Request sucessful\n",
      "Sleeping 5s\n",
      "Query count 61\n",
      "Request sucessful\n",
      "Sleeping 10s\n",
      "Query count 62\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 63\n",
      "Request sucessful\n",
      "Sleeping 3s\n",
      "Query count 64\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 65\n",
      "Request sucessful\n",
      "Sleeping 6s\n",
      "Query count 66\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 67\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 68\n",
      "Request sucessful\n",
      "Sleeping 3s\n",
      "Query count 69\n",
      "Request sucessful\n",
      "Sleeping 2s\n",
      "Query count 70\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 71\n",
      "Request sucessful\n",
      "Sleeping 7s\n",
      "Query count 72\n",
      "Request sucessful\n",
      "Sleeping 6s\n",
      "Query count 73\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 74\n",
      "Request sucessful\n",
      "Sleeping 3s\n",
      "Query count 75\n",
      "Request sucessful\n",
      "Sleeping 9s\n",
      "Query count 76\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 77\n",
      "Request sucessful\n",
      "Sleeping 4s\n",
      "Query count 78\n",
      "Request sucessful\n",
      "Sleeping 3s\n",
      "Query count 79\n",
      "Request sucessful\n",
      "Sleeping 10s\n"
     ]
    }
   ],
   "source": [
    "posts1 = reddit_query(subreddit_url1, req_headers, query_times = 80)\n",
    "posts2 = reddit_query(subreddit_url2, req_headers, query_times = 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the number of posts for each subreddit. Need collect lots of posts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts for subreddit1: 1988\n",
      "Number of posts for subreddit2: 1992\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of posts for subreddit1: {len(posts1)}\")       \n",
    "print(f\"Number of posts for subreddit2: {len(posts2)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of json structure of scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 't3',\n",
       " 'data': {'approved_at_utc': None,\n",
       "  'subreddit': 'pytorch',\n",
       "  'selftext': 'I have implemented ResNet-18 CNN from scatch in Python and PyTorch using CIFAR-10 dataset. You can see it [here](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet-18_CIFAR10-PyTorch.ipynb).\\n\\nLet me know your comments/feedbacks.',\n",
       "  'author_fullname': 't2_2mmql89p',\n",
       "  'saved': False,\n",
       "  'mod_reason_title': None,\n",
       "  'gilded': 0,\n",
       "  'clicked': False,\n",
       "  'title': 'ResNet-18 from scratch',\n",
       "  'link_flair_richtext': [],\n",
       "  'subreddit_name_prefixed': 'r/pytorch',\n",
       "  'hidden': False,\n",
       "  'pwls': 6,\n",
       "  'link_flair_css_class': None,\n",
       "  'downs': 0,\n",
       "  'thumbnail_height': None,\n",
       "  'top_awarded_type': None,\n",
       "  'hide_score': False,\n",
       "  'name': 't3_m12byo',\n",
       "  'quarantine': False,\n",
       "  'link_flair_text_color': 'dark',\n",
       "  'upvote_ratio': 0.76,\n",
       "  'author_flair_background_color': None,\n",
       "  'subreddit_type': 'public',\n",
       "  'ups': 9,\n",
       "  'total_awards_received': 0,\n",
       "  'media_embed': {},\n",
       "  'thumbnail_width': None,\n",
       "  'author_flair_template_id': None,\n",
       "  'is_original_content': False,\n",
       "  'user_reports': [],\n",
       "  'secure_media': None,\n",
       "  'is_reddit_media_domain': False,\n",
       "  'is_meta': False,\n",
       "  'category': None,\n",
       "  'secure_media_embed': {},\n",
       "  'link_flair_text': None,\n",
       "  'can_mod_post': False,\n",
       "  'score': 9,\n",
       "  'approved_by': None,\n",
       "  'author_premium': False,\n",
       "  'thumbnail': 'self',\n",
       "  'edited': False,\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_richtext': [],\n",
       "  'gildings': {},\n",
       "  'post_hint': 'self',\n",
       "  'content_categories': None,\n",
       "  'is_self': True,\n",
       "  'mod_note': None,\n",
       "  'created': 1615307794.0,\n",
       "  'link_flair_type': 'text',\n",
       "  'wls': 6,\n",
       "  'removed_by_category': None,\n",
       "  'banned_by': None,\n",
       "  'author_flair_type': 'text',\n",
       "  'domain': 'self.pytorch',\n",
       "  'allow_live_comments': False,\n",
       "  'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have implemented ResNet-18 CNN from scatch in Python and PyTorch using CIFAR-10 dataset. You can see it &lt;a href=\"https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet-18_CIFAR10-PyTorch.ipynb\"&gt;here&lt;/a&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;Let me know your comments/feedbacks.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;',\n",
       "  'likes': None,\n",
       "  'suggested_sort': None,\n",
       "  'banned_at_utc': None,\n",
       "  'view_count': None,\n",
       "  'archived': False,\n",
       "  'no_follow': True,\n",
       "  'is_crosspostable': False,\n",
       "  'pinned': False,\n",
       "  'over_18': False,\n",
       "  'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?auto=webp&amp;s=f2a8d2644c171302d258e04bb4f56341e0932f5c',\n",
       "      'width': 400,\n",
       "      'height': 400},\n",
       "     'resolutions': [{'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a712b927d57fefc453338d3cc567c9a6a5682180',\n",
       "       'width': 108,\n",
       "       'height': 108},\n",
       "      {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8fdbd8b838cc4c3070a600980ec621b534c468c2',\n",
       "       'width': 216,\n",
       "       'height': 216},\n",
       "      {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2541ecf1cf94955a97e415c8325f05533b49e8f9',\n",
       "       'width': 320,\n",
       "       'height': 320}],\n",
       "     'variants': {},\n",
       "     'id': 'SxmolEBMl5rw9UEn7Ftv1WN1nmytQ0dLmTjHbIRwo8E'}],\n",
       "   'enabled': False},\n",
       "  'all_awardings': [],\n",
       "  'awarders': [],\n",
       "  'media_only': False,\n",
       "  'can_gild': False,\n",
       "  'spoiler': False,\n",
       "  'locked': False,\n",
       "  'author_flair_text': None,\n",
       "  'treatment_tags': [],\n",
       "  'visited': False,\n",
       "  'removed_by': None,\n",
       "  'num_reports': None,\n",
       "  'distinguished': None,\n",
       "  'subreddit_id': 't5_3gbwg',\n",
       "  'mod_reason_by': None,\n",
       "  'removal_reason': None,\n",
       "  'link_flair_background_color': '',\n",
       "  'id': 'm12byo',\n",
       "  'is_robot_indexable': True,\n",
       "  'report_reasons': None,\n",
       "  'author': 'grid_world',\n",
       "  'discussion_type': None,\n",
       "  'num_comments': 0,\n",
       "  'send_replies': True,\n",
       "  'whitelist_status': 'all_ads',\n",
       "  'contest_mode': False,\n",
       "  'mod_reports': [],\n",
       "  'author_patreon_flair': False,\n",
       "  'author_flair_text_color': None,\n",
       "  'permalink': '/r/pytorch/comments/m12byo/resnet18_from_scratch/',\n",
       "  'parent_whitelist_status': 'all_ads',\n",
       "  'stickied': False,\n",
       "  'url': 'https://www.reddit.com/r/pytorch/comments/m12byo/resnet18_from_scratch/',\n",
       "  'subreddit_subscribers': 7135,\n",
       "  'created_utc': 1615278994.0,\n",
       "  'num_crossposts': 0,\n",
       "  'media': None,\n",
       "  'is_video': False}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_post1 = posts1[0]\n",
    "sample_post2 = posts2[0]\n",
    "sample_post1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'approved_at_utc': None,\n",
       " 'subreddit': 'pytorch',\n",
       " 'selftext': 'I have implemented ResNet-18 CNN from scatch in Python and PyTorch using CIFAR-10 dataset. You can see it [here](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet-18_CIFAR10-PyTorch.ipynb).\\n\\nLet me know your comments/feedbacks.',\n",
       " 'author_fullname': 't2_2mmql89p',\n",
       " 'saved': False,\n",
       " 'mod_reason_title': None,\n",
       " 'gilded': 0,\n",
       " 'clicked': False,\n",
       " 'title': 'ResNet-18 from scratch',\n",
       " 'link_flair_richtext': [],\n",
       " 'subreddit_name_prefixed': 'r/pytorch',\n",
       " 'hidden': False,\n",
       " 'pwls': 6,\n",
       " 'link_flair_css_class': None,\n",
       " 'downs': 0,\n",
       " 'thumbnail_height': None,\n",
       " 'top_awarded_type': None,\n",
       " 'hide_score': False,\n",
       " 'name': 't3_m12byo',\n",
       " 'quarantine': False,\n",
       " 'link_flair_text_color': 'dark',\n",
       " 'upvote_ratio': 0.76,\n",
       " 'author_flair_background_color': None,\n",
       " 'subreddit_type': 'public',\n",
       " 'ups': 9,\n",
       " 'total_awards_received': 0,\n",
       " 'media_embed': {},\n",
       " 'thumbnail_width': None,\n",
       " 'author_flair_template_id': None,\n",
       " 'is_original_content': False,\n",
       " 'user_reports': [],\n",
       " 'secure_media': None,\n",
       " 'is_reddit_media_domain': False,\n",
       " 'is_meta': False,\n",
       " 'category': None,\n",
       " 'secure_media_embed': {},\n",
       " 'link_flair_text': None,\n",
       " 'can_mod_post': False,\n",
       " 'score': 9,\n",
       " 'approved_by': None,\n",
       " 'author_premium': False,\n",
       " 'thumbnail': 'self',\n",
       " 'edited': False,\n",
       " 'author_flair_css_class': None,\n",
       " 'author_flair_richtext': [],\n",
       " 'gildings': {},\n",
       " 'post_hint': 'self',\n",
       " 'content_categories': None,\n",
       " 'is_self': True,\n",
       " 'mod_note': None,\n",
       " 'created': 1615307794.0,\n",
       " 'link_flair_type': 'text',\n",
       " 'wls': 6,\n",
       " 'removed_by_category': None,\n",
       " 'banned_by': None,\n",
       " 'author_flair_type': 'text',\n",
       " 'domain': 'self.pytorch',\n",
       " 'allow_live_comments': False,\n",
       " 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have implemented ResNet-18 CNN from scatch in Python and PyTorch using CIFAR-10 dataset. You can see it &lt;a href=\"https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet-18_CIFAR10-PyTorch.ipynb\"&gt;here&lt;/a&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;Let me know your comments/feedbacks.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;',\n",
       " 'likes': None,\n",
       " 'suggested_sort': None,\n",
       " 'banned_at_utc': None,\n",
       " 'view_count': None,\n",
       " 'archived': False,\n",
       " 'no_follow': True,\n",
       " 'is_crosspostable': False,\n",
       " 'pinned': False,\n",
       " 'over_18': False,\n",
       " 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?auto=webp&amp;s=f2a8d2644c171302d258e04bb4f56341e0932f5c',\n",
       "     'width': 400,\n",
       "     'height': 400},\n",
       "    'resolutions': [{'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a712b927d57fefc453338d3cc567c9a6a5682180',\n",
       "      'width': 108,\n",
       "      'height': 108},\n",
       "     {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8fdbd8b838cc4c3070a600980ec621b534c468c2',\n",
       "      'width': 216,\n",
       "      'height': 216},\n",
       "     {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2541ecf1cf94955a97e415c8325f05533b49e8f9',\n",
       "      'width': 320,\n",
       "      'height': 320}],\n",
       "    'variants': {},\n",
       "    'id': 'SxmolEBMl5rw9UEn7Ftv1WN1nmytQ0dLmTjHbIRwo8E'}],\n",
       "  'enabled': False},\n",
       " 'all_awardings': [],\n",
       " 'awarders': [],\n",
       " 'media_only': False,\n",
       " 'can_gild': False,\n",
       " 'spoiler': False,\n",
       " 'locked': False,\n",
       " 'author_flair_text': None,\n",
       " 'treatment_tags': [],\n",
       " 'visited': False,\n",
       " 'removed_by': None,\n",
       " 'num_reports': None,\n",
       " 'distinguished': None,\n",
       " 'subreddit_id': 't5_3gbwg',\n",
       " 'mod_reason_by': None,\n",
       " 'removal_reason': None,\n",
       " 'link_flair_background_color': '',\n",
       " 'id': 'm12byo',\n",
       " 'is_robot_indexable': True,\n",
       " 'report_reasons': None,\n",
       " 'author': 'grid_world',\n",
       " 'discussion_type': None,\n",
       " 'num_comments': 0,\n",
       " 'send_replies': True,\n",
       " 'whitelist_status': 'all_ads',\n",
       " 'contest_mode': False,\n",
       " 'mod_reports': [],\n",
       " 'author_patreon_flair': False,\n",
       " 'author_flair_text_color': None,\n",
       " 'permalink': '/r/pytorch/comments/m12byo/resnet18_from_scratch/',\n",
       " 'parent_whitelist_status': 'all_ads',\n",
       " 'stickied': False,\n",
       " 'url': 'https://www.reddit.com/r/pytorch/comments/m12byo/resnet18_from_scratch/',\n",
       " 'subreddit_subscribers': 7135,\n",
       " 'created_utc': 1615278994.0,\n",
       " 'num_crossposts': 0,\n",
       " 'media': None,\n",
       " 'is_video': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_post1['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'approved_at_utc': None,\n",
       " 'subreddit': 'tensorflow',\n",
       " 'selftext': \"You can discuss anything here that doesn't require it's own post\",\n",
       " 'author_fullname': 't2_yr9xa',\n",
       " 'saved': False,\n",
       " 'mod_reason_title': None,\n",
       " 'gilded': 0,\n",
       " 'clicked': False,\n",
       " 'title': 'The Official Feedback and Discussion Thread',\n",
       " 'link_flair_richtext': [],\n",
       " 'subreddit_name_prefixed': 'r/tensorflow',\n",
       " 'hidden': False,\n",
       " 'pwls': 6,\n",
       " 'link_flair_css_class': '',\n",
       " 'downs': 0,\n",
       " 'thumbnail_height': None,\n",
       " 'top_awarded_type': None,\n",
       " 'hide_score': False,\n",
       " 'name': 't3_fxzwdq',\n",
       " 'quarantine': False,\n",
       " 'link_flair_text_color': 'dark',\n",
       " 'upvote_ratio': 1.0,\n",
       " 'author_flair_background_color': None,\n",
       " 'subreddit_type': 'public',\n",
       " 'ups': 3,\n",
       " 'total_awards_received': 0,\n",
       " 'media_embed': {},\n",
       " 'thumbnail_width': None,\n",
       " 'author_flair_template_id': None,\n",
       " 'is_original_content': False,\n",
       " 'user_reports': [],\n",
       " 'secure_media': None,\n",
       " 'is_reddit_media_domain': False,\n",
       " 'is_meta': False,\n",
       " 'category': None,\n",
       " 'secure_media_embed': {},\n",
       " 'link_flair_text': 'Discussion',\n",
       " 'can_mod_post': False,\n",
       " 'score': 3,\n",
       " 'approved_by': None,\n",
       " 'author_premium': False,\n",
       " 'thumbnail': 'self',\n",
       " 'edited': False,\n",
       " 'author_flair_css_class': None,\n",
       " 'author_flair_richtext': [],\n",
       " 'gildings': {},\n",
       " 'content_categories': None,\n",
       " 'is_self': True,\n",
       " 'mod_note': None,\n",
       " 'created': 1586492549.0,\n",
       " 'link_flair_type': 'text',\n",
       " 'wls': 6,\n",
       " 'removed_by_category': None,\n",
       " 'banned_by': None,\n",
       " 'author_flair_type': 'text',\n",
       " 'domain': 'self.tensorflow',\n",
       " 'allow_live_comments': False,\n",
       " 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You can discuss anything here that doesn&amp;#39;t require it&amp;#39;s own post&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;',\n",
       " 'likes': None,\n",
       " 'suggested_sort': None,\n",
       " 'banned_at_utc': None,\n",
       " 'view_count': None,\n",
       " 'archived': True,\n",
       " 'no_follow': False,\n",
       " 'is_crosspostable': False,\n",
       " 'pinned': False,\n",
       " 'over_18': False,\n",
       " 'all_awardings': [],\n",
       " 'awarders': [],\n",
       " 'media_only': False,\n",
       " 'link_flair_template_id': '1db20040-afc0-11e7-b8c9-0e6c50199bd0',\n",
       " 'can_gild': False,\n",
       " 'spoiler': False,\n",
       " 'locked': False,\n",
       " 'author_flair_text': None,\n",
       " 'treatment_tags': [],\n",
       " 'visited': False,\n",
       " 'removed_by': None,\n",
       " 'num_reports': None,\n",
       " 'distinguished': 'moderator',\n",
       " 'subreddit_id': 't5_3alkk',\n",
       " 'mod_reason_by': None,\n",
       " 'removal_reason': None,\n",
       " 'link_flair_background_color': '',\n",
       " 'id': 'fxzwdq',\n",
       " 'is_robot_indexable': True,\n",
       " 'report_reasons': None,\n",
       " 'author': 'AkashMishra',\n",
       " 'discussion_type': None,\n",
       " 'num_comments': 2,\n",
       " 'send_replies': True,\n",
       " 'whitelist_status': 'all_ads',\n",
       " 'contest_mode': False,\n",
       " 'mod_reports': [],\n",
       " 'author_patreon_flair': False,\n",
       " 'author_flair_text_color': None,\n",
       " 'permalink': '/r/tensorflow/comments/fxzwdq/the_official_feedback_and_discussion_thread/',\n",
       " 'parent_whitelist_status': 'all_ads',\n",
       " 'stickied': True,\n",
       " 'url': 'https://www.reddit.com/r/tensorflow/comments/fxzwdq/the_official_feedback_and_discussion_thread/',\n",
       " 'subreddit_subscribers': 22217,\n",
       " 'created_utc': 1586463749.0,\n",
       " 'num_crossposts': 0,\n",
       " 'media': None,\n",
       " 'is_video': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_post2['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['approved_at_utc', 'subreddit', 'selftext', 'author_fullname', 'saved', 'mod_reason_title', 'gilded', 'clicked', 'title', 'link_flair_richtext', 'subreddit_name_prefixed', 'hidden', 'pwls', 'link_flair_css_class', 'downs', 'thumbnail_height', 'top_awarded_type', 'hide_score', 'name', 'quarantine', 'link_flair_text_color', 'upvote_ratio', 'author_flair_background_color', 'subreddit_type', 'ups', 'total_awards_received', 'media_embed', 'thumbnail_width', 'author_flair_template_id', 'is_original_content', 'user_reports', 'secure_media', 'is_reddit_media_domain', 'is_meta', 'category', 'secure_media_embed', 'link_flair_text', 'can_mod_post', 'score', 'approved_by', 'author_premium', 'thumbnail', 'edited', 'author_flair_css_class', 'author_flair_richtext', 'gildings', 'post_hint', 'content_categories', 'is_self', 'mod_note', 'created', 'link_flair_type', 'wls', 'removed_by_category', 'banned_by', 'author_flair_type', 'domain', 'allow_live_comments', 'selftext_html', 'likes', 'suggested_sort', 'banned_at_utc', 'view_count', 'archived', 'no_follow', 'is_crosspostable', 'pinned', 'over_18', 'preview', 'all_awardings', 'awarders', 'media_only', 'can_gild', 'spoiler', 'locked', 'author_flair_text', 'treatment_tags', 'visited', 'removed_by', 'num_reports', 'distinguished', 'subreddit_id', 'mod_reason_by', 'removal_reason', 'link_flair_background_color', 'id', 'is_robot_indexable', 'report_reasons', 'author', 'discussion_type', 'num_comments', 'send_replies', 'whitelist_status', 'contest_mode', 'mod_reports', 'author_patreon_flair', 'author_flair_text_color', 'permalink', 'parent_whitelist_status', 'stickied', 'url', 'subreddit_subscribers', 'created_utc', 'num_crossposts', 'media', 'is_video'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The keys would be our dataframe column names\n",
    "sample_post1['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pytorch'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check subreddit of the post(label)\n",
    "sample_post1['data']['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_list1 = [p['data'] for p in posts1]\n",
    "post_list2 = [p['data'] for p in posts2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1988, 114)\n",
      "(1992, 114)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame(post_list1)\n",
    "print(df1.shape)\n",
    "df2 = pd.DataFrame(post_list2)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_nodup = df1.drop_duplicates(subset=['selftext','title'])\n",
    "df2_nodup = df2.drop_duplicates(subset=['selftext','title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if the columns of NVIDIA subreddit is a result of additional 2 columns compared to AMD subreddit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poll_data'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(df1_nodup.columns)) - set(list(df2_nodup.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link_flair_template_id'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(df2_nodup.columns)) - set(list(df1_nodup.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check number of empty values in selftext columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with empty string: 230 out of 943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24390243902439024"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_string_proportion(df1_nodup, \"selftext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with empty string: 207 out of 918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22549019607843138"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_string_proportion(df2_nodup, \"selftext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataframes into their respective csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_nodup.to_csv('./pytorch_posts.csv')\n",
    "df2_nodup.to_csv('./tf_posts.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "373.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
