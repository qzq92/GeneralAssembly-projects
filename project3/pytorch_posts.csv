,approved_at_utc,subreddit,selftext,author_fullname,saved,mod_reason_title,gilded,clicked,title,link_flair_richtext,subreddit_name_prefixed,hidden,pwls,link_flair_css_class,downs,thumbnail_height,top_awarded_type,hide_score,name,quarantine,link_flair_text_color,upvote_ratio,author_flair_background_color,subreddit_type,ups,total_awards_received,media_embed,thumbnail_width,author_flair_template_id,is_original_content,user_reports,secure_media,is_reddit_media_domain,is_meta,category,secure_media_embed,link_flair_text,can_mod_post,score,approved_by,author_premium,thumbnail,edited,author_flair_css_class,author_flair_richtext,gildings,post_hint,content_categories,is_self,mod_note,created,link_flair_type,wls,removed_by_category,banned_by,author_flair_type,domain,allow_live_comments,selftext_html,likes,suggested_sort,banned_at_utc,view_count,archived,no_follow,is_crosspostable,pinned,over_18,preview,all_awardings,awarders,media_only,can_gild,spoiler,locked,author_flair_text,treatment_tags,visited,removed_by,num_reports,distinguished,subreddit_id,mod_reason_by,removal_reason,link_flair_background_color,id,is_robot_indexable,report_reasons,author,discussion_type,num_comments,send_replies,whitelist_status,contest_mode,mod_reports,author_patreon_flair,author_flair_text_color,permalink,parent_whitelist_status,stickied,url,subreddit_subscribers,created_utc,num_crossposts,media,is_video,url_overridden_by_dest,crosspost_parent_list,crosspost_parent,media_metadata,is_gallery,gallery_data,poll_data,author_cakeday
0,,pytorch,"I have implemented ResNet-18 CNN from scatch in Python and PyTorch using CIFAR-10 dataset. You can see it [here](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet-18_CIFAR10-PyTorch.ipynb).

Let me know your comments/feedbacks.",t2_2mmql89p,False,,0,False,ResNet-18 from scratch,[],r/pytorch,False,6,,0,,,False,t3_m12byo,False,dark,0.76,,public,9,0,{},,,False,[],,False,False,,{},,False,9,,False,self,False,,[],{},self,,True,,1615307794.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have implemented ResNet-18 CNN from scatch in Python and PyTorch using CIFAR-10 dataset. You can see it &lt;a href=""https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet-18_CIFAR10-PyTorch.ipynb""&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let me know your comments/feedbacks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?auto=webp&amp;s=f2a8d2644c171302d258e04bb4f56341e0932f5c', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a712b927d57fefc453338d3cc567c9a6a5682180', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8fdbd8b838cc4c3070a600980ec621b534c468c2', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2541ecf1cf94955a97e415c8325f05533b49e8f9', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'SxmolEBMl5rw9UEn7Ftv1WN1nmytQ0dLmTjHbIRwo8E'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,m12byo,True,,grid_world,,0,True,all_ads,False,[],False,,/r/pytorch/comments/m12byo/resnet18_from_scratch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/m12byo/resnet18_from_scratch/,7135,1615278994.0,0,,False,,,,,,,,
1,,pytorch,"I have trained [ResNet-18](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet-18_CIFAR10-PyTorch.ipynb) and [ResNet-34](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet_34_CIFAR10_PyTorch.ipynb)   from scratch using PyTorch on CIFAR-10 dataset. The validation  accuracy  I get for ResNet-18 is 84.01%, whereas for ResNet-34 is  82.43%. Is this  a sign of ResNet-34 overfitting as compared to  ResNet-18? Ideally,  ResNet-34 should achieve a higher validation  accuracy as compared to  ResNet-18.

Thoughts?",t2_2mmql89p,False,,0,False,ResNet-18 vs ResNet-34,[],r/pytorch,False,6,,0,,,False,t3_m1cftx,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1615342267.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have trained &lt;a href=""https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet-18_CIFAR10-PyTorch.ipynb""&gt;ResNet-18&lt;/a&gt; and &lt;a href=""https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet_34_CIFAR10_PyTorch.ipynb""&gt;ResNet-34&lt;/a&gt;   from scratch using PyTorch on CIFAR-10 dataset. The validation  accuracy  I get for ResNet-18 is 84.01%, whereas for ResNet-34 is  82.43%. Is this  a sign of ResNet-34 overfitting as compared to  ResNet-18? Ideally,  ResNet-34 should achieve a higher validation  accuracy as compared to  ResNet-18.&lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?auto=webp&amp;s=f2a8d2644c171302d258e04bb4f56341e0932f5c', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a712b927d57fefc453338d3cc567c9a6a5682180', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8fdbd8b838cc4c3070a600980ec621b534c468c2', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2541ecf1cf94955a97e415c8325f05533b49e8f9', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'SxmolEBMl5rw9UEn7Ftv1WN1nmytQ0dLmTjHbIRwo8E'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,m1cftx,True,,grid_world,,9,True,all_ads,False,[],False,,/r/pytorch/comments/m1cftx/resnet18_vs_resnet34/,all_ads,False,https://www.reddit.com/r/pytorch/comments/m1cftx/resnet18_vs_resnet34/,7135,1615313467.0,0,,False,,,,,,,,
2,,pytorch,"Open Neural Network Exchange (ONNX) is a powerful and open format built to represent machine learning models. The final outcome of training any machine learning or deep learning algorithm is a model file that represents the mapping of input data to output predictions in an efficient manner.

Read  more: [https://analyticsindiamag.com/converting-a-model-from-pytorch-to-tensorflow-guide-to-onnx/](https://analyticsindiamag.com/converting-a-model-from-pytorch-to-tensorflow-guide-to-onnx/)",t2_40d0zt4s,False,,0,False,Converting a model from Pytorch to Tensorflow: Guide to ONNX,[],r/pytorch,False,6,,0,,,False,t3_m0eby3,False,dark,0.77,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},self,,True,,1615233996.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Open Neural Network Exchange (ONNX) is a powerful and open format built to represent machine learning models. The final outcome of training any machine learning or deep learning algorithm is a model file that represents the mapping of input data to output predictions in an efficient manner.&lt;/p&gt;

&lt;p&gt;Read  more: &lt;a href=""https://analyticsindiamag.com/converting-a-model-from-pytorch-to-tensorflow-guide-to-onnx/""&gt;https://analyticsindiamag.com/converting-a-model-from-pytorch-to-tensorflow-guide-to-onnx/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/b3wcHCh1vyuDOJ2ZhnicD8BRGJl4-ggXE3nlWNQhAAk.jpg?auto=webp&amp;s=b6b9662774296c1ace6c21c71d74d18d6ec9f1bd', 'width': 1640, 'height': 924}, 'resolutions': [{'url': 'https://external-preview.redd.it/b3wcHCh1vyuDOJ2ZhnicD8BRGJl4-ggXE3nlWNQhAAk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=90766cbc4b97732d9ff2f50bf4d4de72c205f577', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/b3wcHCh1vyuDOJ2ZhnicD8BRGJl4-ggXE3nlWNQhAAk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=387fbd04ec08391130d43cd3ffd4d2ef1d7a1554', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/b3wcHCh1vyuDOJ2ZhnicD8BRGJl4-ggXE3nlWNQhAAk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f34d211dc68dd2f0b4cc8f2be4dab0c5c7221be', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/b3wcHCh1vyuDOJ2ZhnicD8BRGJl4-ggXE3nlWNQhAAk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1aa36311e3326aa1e972a76c3fa1d7c7fe32d336', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/b3wcHCh1vyuDOJ2ZhnicD8BRGJl4-ggXE3nlWNQhAAk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=effe39e93ee877daada6b97f1b0aad20a97c4592', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/b3wcHCh1vyuDOJ2ZhnicD8BRGJl4-ggXE3nlWNQhAAk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8046adbfafb368d868af71534e2e51991107ae44', 'width': 1080, 'height': 608}], 'variants': {}, 'id': 'n3T4I4dtOOPcNVA3EUHygJmlq7D5L3W3_vt08SvphxQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,m0eby3,True,,analyticsindiam,,0,True,all_ads,False,[],False,,/r/pytorch/comments/m0eby3/converting_a_model_from_pytorch_to_tensorflow/,all_ads,False,https://www.reddit.com/r/pytorch/comments/m0eby3/converting_a_model_from_pytorch_to_tensorflow/,7135,1615205196.0,0,,False,,,,,,,,
3,,pytorch,"Hi,

I'm considering a swap from TF 2.0 to PyTorch (because whole academia did so), but before doing such drastic actions I need to ensure that PyTorch can provide the same set of tools in some way.

I have found most features in tf2.0 to have a equivalent in pytorch, but I have not found anything that comes close to tf.data.Datasets. Specifically, I am looking for something that can eagerly prefetch and automatically batch my dataset. Is there such library for pytorch?",t2_6mbpx,False,,0,False,Is there a flexible Dataloader similar to tf.data.Datasets?,[],r/pytorch,False,6,,0,,,False,t3_m0anh5,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1615218006.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m considering a swap from TF 2.0 to PyTorch (because whole academia did so), but before doing such drastic actions I need to ensure that PyTorch can provide the same set of tools in some way.&lt;/p&gt;

&lt;p&gt;I have found most features in tf2.0 to have a equivalent in pytorch, but I have not found anything that comes close to tf.data.Datasets. Specifically, I am looking for something that can eagerly prefetch and automatically batch my dataset. Is there such library for pytorch?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,m0anh5,True,,Driiper,,2,True,all_ads,False,[],False,,/r/pytorch/comments/m0anh5/is_there_a_flexible_dataloader_similar_to/,all_ads,False,https://www.reddit.com/r/pytorch/comments/m0anh5/is_there_a_flexible_dataloader_similar_to/,7135,1615189206.0,0,,False,,,,,,,,
4,,pytorch,"I apologise in advanced for this newb question - i’ve just started with pytorch!

I’m trying to implement a multi-class text classifier using GloVe embeddings and Bi-LSTM.
I’ve downloaded the embeddings and processed them in to a dictionary in which the word is the key and value is an array of pre-trained weights.

I’m just confused about how i put it all together:

Do i feed the word vectors in to the LSTM for each sequence e.g 10 * word vectors each with 50 dimensions?

Do i implement the word embedding to work automatically as i pass words in to the LSTM?

I’m just a bit confused about how i feed a 50x1 vector of each word in the sequence in to the model.

Any help would be greatly appreciated! 

Thanks!",t2_9lo3x1bz,False,,0,False,How to implement pre trained embeddings?,[],r/pytorch,False,6,,0,,,False,t3_lzlwqo,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1615131768.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I apologise in advanced for this newb question - i’ve just started with pytorch!&lt;/p&gt;

&lt;p&gt;I’m trying to implement a multi-class text classifier using GloVe embeddings and Bi-LSTM.
I’ve downloaded the embeddings and processed them in to a dictionary in which the word is the key and value is an array of pre-trained weights.&lt;/p&gt;

&lt;p&gt;I’m just confused about how i put it all together:&lt;/p&gt;

&lt;p&gt;Do i feed the word vectors in to the LSTM for each sequence e.g 10 * word vectors each with 50 dimensions?&lt;/p&gt;

&lt;p&gt;Do i implement the word embedding to work automatically as i pass words in to the LSTM?&lt;/p&gt;

&lt;p&gt;I’m just a bit confused about how i feed a 50x1 vector of each word in the sequence in to the model.&lt;/p&gt;

&lt;p&gt;Any help would be greatly appreciated! &lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lzlwqo,True,,wherll,,3,True,all_ads,False,[],False,,/r/pytorch/comments/lzlwqo/how_to_implement_pre_trained_embeddings/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lzlwqo/how_to_implement_pre_trained_embeddings/,7135,1615102968.0,0,,False,,,,,,,,
5,,pytorch,"I'm trying to incorporate monads from PyMonad into my training function like so :

    monad = Maybe.apply(self.calculate_loss_accuracy).to_arguments(model(tweet, tweet_len), batch.Sentiment).value

I get the following error from this however :

    AttributeError: 'Tensor' object has no attribute 'is_nothing'

So from this, is it possible to add this 'is\_nothing' attribute or custom attributes in general to a tensor?",t2_3n6tle96,False,,0,False,Is it possible to add support of custom attributes to tensors?,[],r/pytorch,False,6,,0,,,False,t3_lze3w1,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1615102534.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to incorporate monads from PyMonad into my training function like so :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;monad = Maybe.apply(self.calculate_loss_accuracy).to_arguments(model(tweet, tweet_len), batch.Sentiment).value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I get the following error from this however :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;AttributeError: &amp;#39;Tensor&amp;#39; object has no attribute &amp;#39;is_nothing&amp;#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So from this, is it possible to add this &amp;#39;is_nothing&amp;#39; attribute or custom attributes in general to a tensor?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lze3w1,True,,Aloys1us_Bl00m,,3,True,all_ads,False,[],False,,/r/pytorch/comments/lze3w1/is_it_possible_to_add_support_of_custom/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lze3w1/is_it_possible_to_add_support_of_custom/,7135,1615073734.0,0,,False,,,,,,,,
6,,pytorch,,t2_a7i59xms,False,,0,False,Vision Transformer implemented from scratch,[],r/pytorch,False,6,,0,105.0,,False,t3_lykf5n,False,dark,0.84,,public,12,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ovB0ddFtzzA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Vision Transformer in PyTorch', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ovB0ddFtzzA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'mildlyoverfitted', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ovB0ddFtzzA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/mildlyoverfitted'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ovB0ddFtzzA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lykf5n', 'height': 200}",,False,12,,False,https://b.thumbs.redditmedia.com/A1tm1Zvcuo2jwTP-ScIxemISpHLslGrXvLp2S0c9wes.jpg,False,,[],{},rich:video,,False,,1615002984.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/2Q0fZwVh3PBrJJwb8W1CPqHWxbbgjJyqzSMAP5smFp4.jpg?auto=webp&amp;s=a9ff173506f7bef144d9d0c8784e34af4ffb124c', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/2Q0fZwVh3PBrJJwb8W1CPqHWxbbgjJyqzSMAP5smFp4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b251d335da80c25e4655de35c1633f777a4aab6f', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/2Q0fZwVh3PBrJJwb8W1CPqHWxbbgjJyqzSMAP5smFp4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3286becd7bd82853453239550bb66be6e8ec24be', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/2Q0fZwVh3PBrJJwb8W1CPqHWxbbgjJyqzSMAP5smFp4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=60ecb59ac07661e266ba91117e956dac4b8a7e6c', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'MkIPzywfZSr7bxcqgEj30Uezo82Cy0b5CnIMT57Xt8Q'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lykf5n,True,,mildlyoverfitted,,4,True,all_ads,False,[],False,,/r/pytorch/comments/lykf5n/vision_transformer_implemented_from_scratch/,all_ads,False,https://youtu.be/ovB0ddFtzzA,7135,1614974184.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Vision Transformer in PyTorch', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ovB0ddFtzzA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'mildlyoverfitted', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ovB0ddFtzzA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/mildlyoverfitted'}}",False,https://youtu.be/ovB0ddFtzzA,,,,,,,
7,,pytorch,,t2_7eslkpz,False,,0,False,PyTorch 1.8 released,[],r/pytorch,False,6,,0,140.0,,False,t3_lya2n2,False,dark,0.97,,public,34,0,{},140.0,,False,[],,False,False,,{},,False,34,,False,https://b.thumbs.redditmedia.com/VAVpPNBfFoyJ-gWaTi-CQkg6g7w_CeOt-x2jJyXb1Zo.jpg,False,,[],{},link,,False,,1614973427.0,text,6,,,text,pytorch.org,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?auto=webp&amp;s=d0da5dbb7aff983a5173aba7d2bdd36fa28abe08', 'width': 2500, 'height': 2500}, 'resolutions': [{'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fef05ec37ddf39032979380cc7acc403e05834be', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e1de8e408f33ef3c93425e30e0bb5afffd5c631', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89562dde9cea0af5ebaaba50a9437e484d732513', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d6125bcd02c2fed8d84dac49cc8d73dbe205d81', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=40d7ca8d88566f27054acb632514012b5026acc9', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=757464a70e69532931dcbf0fbd397b9d433cf205', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'Mg0HAz_PmYkYWkaGTVyhCEzp15PpLW_TWWqmn9VNVFs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lya2n2,True,,Marha01,,6,False,all_ads,False,[],False,,/r/pytorch/comments/lya2n2/pytorch_18_released/,all_ads,False,https://pytorch.org/blog/pytorch-1.8-released/,7135,1614944627.0,0,,False,https://pytorch.org/blog/pytorch-1.8-released/,,,,,,,
8,,pytorch,,t2_4dgro12d,False,,0,False,AI Show: What's new in Cognitive Search and PyTorch,[],r/pytorch,False,6,,0,68.0,,False,t3_lykjjs,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/QlTsY5ANWTdKY1t0-Qy6fOQ-4kBGTZ-OWEypAqLenxo.jpg,False,,[],{},,,False,,1615003322.0,text,6,,,text,self.Azure_AI_Cognitive,False,,,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lykjjs,True,,AysSomething,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lykjjs/ai_show_whats_new_in_cognitive_search_and_pytorch/,all_ads,False,/r/Azure_AI_Cognitive/comments/lyj1zy/ai_show_whats_new_in_cognitive_search_and_pytorch/,7135,1614974522.0,0,,False,/r/Azure_AI_Cognitive/comments/lyj1zy/ai_show_whats_new_in_cognitive_search_and_pytorch/,"[{'approved_at_utc': None, 'subreddit': 'Azure_AI_Cognitive', 'selftext': '&amp;#x200B;\n\n[AI Show Live](https://preview.redd.it/xcwyql0na9l61.png?width=1866&amp;format=png&amp;auto=webp&amp;s=b01d51d4438502d06cfa025aa665c8cb86f81112)\n\n[LearnTV](https://docs.microsoft.com/learn/tv/?WT.mc_id=aiml-0000-ayyonet)\n\n[YouTube](https://www.youtube.com/watch?v=3JO4ZntzhwU)', 'user_reports': [], 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""AI Show: What's new in Cognitive Search and PyTorch"", 'event_start': 1614970800.0, 'subreddit_name_prefixed': 'r/Azure_AI_Cognitive', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 68, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'xcwyql0na9l61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 53, 'x': 108, 'u': 'https://preview.redd.it/xcwyql0na9l61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0516d08c10f60c182e33358870ee64ef0f1a8e61'}, {'y': 106, 'x': 216, 'u': 'https://preview.redd.it/xcwyql0na9l61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd8967f63da3d34dfa9f6d256040f74706edbee2'}, {'y': 157, 'x': 320, 'u': 'https://preview.redd.it/xcwyql0na9l61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff63bc0ce5235883b904c8aac5551066821faedb'}, {'y': 315, 'x': 640, 'u': 'https://preview.redd.it/xcwyql0na9l61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=32e7468eb3e22bc39cde9874ed17ba4d7f5f93f7'}, {'y': 472, 'x': 960, 'u': 'https://preview.redd.it/xcwyql0na9l61.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d509e847d9395da92d5e43e3c0f6a3284a621c45'}, {'y': 531, 'x': 1080, 'u': 'https://preview.redd.it/xcwyql0na9l61.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b954ea88f89bc11c185e4b6b7e374462074bb4d0'}], 's': {'y': 919, 'x': 1866, 'u': 'https://preview.redd.it/xcwyql0na9l61.png?width=1866&amp;format=png&amp;auto=webp&amp;s=b01d51d4438502d06cfa025aa665c8cb86f81112'}, 'id': 'xcwyql0na9l61'}}, 'name': 't3_lyj1zy', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'author_fullname': 't2_4dgro12d', 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 1, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/QlTsY5ANWTdKY1t0-Qy6fOQ-4kBGTZ-OWEypAqLenxo.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1614999274.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.Azure_AI_Cognitive', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/xcwyql0na9l61.png?width=1866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b01d51d4438502d06cfa025aa665c8cb86f81112""&gt;AI Show Live&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://docs.microsoft.com/learn/tv/?WT.mc_id=aiml-0000-ayyonet""&gt;LearnTV&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=3JO4ZntzhwU""&gt;YouTube&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_richtext': [], 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_360bg0', 'event_end': 1614978000.0, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'event_is_live': False, 'id': 'lyj1zy', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'AysSomething', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/Azure_AI_Cognitive/comments/lyj1zy/ai_show_whats_new_in_cognitive_search_and_pytorch/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/Azure_AI_Cognitive/comments/lyj1zy/ai_show_whats_new_in_cognitive_search_and_pytorch/', 'subreddit_subscribers': 87, 'created_utc': 1614970474.0, 'num_crossposts': 4, 'media': None, 'is_video': False}]",t3_lyj1zy,,,,,
9,,pytorch,I have a requirement of training pipeline similar to Mixture of Experts (https://github.com/davidmrau/mixture-of-experts/blob/master/moe.py) but I want to train the Experts on a local loss for 1 epoch before predicting outputs from them (which would then be concatenated for the global loss of MoE). Can anyone suggest what’s the best way to set up this training pipeline?,t2_7ef47meo,False,,0,False,Local and Global loss,[],r/pytorch,False,6,,0,,,False,t3_ly1qee,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1614939774.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a requirement of training pipeline similar to Mixture of Experts (&lt;a href=""https://github.com/davidmrau/mixture-of-experts/blob/master/moe.py""&gt;https://github.com/davidmrau/mixture-of-experts/blob/master/moe.py&lt;/a&gt;) but I want to train the Experts on a local loss for 1 epoch before predicting outputs from them (which would then be concatenated for the global loss of MoE). Can anyone suggest what’s the best way to set up this training pipeline?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/OCqCJ2Dz1U8e4uGNk0sIOr9w617hhK5z0mRY8FPQ4XY.jpg?auto=webp&amp;s=1a40ab28bd9e8d536e5443ea6b3b7e0d331c334c', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/OCqCJ2Dz1U8e4uGNk0sIOr9w617hhK5z0mRY8FPQ4XY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=711756633e1bc720621ca717d7b5905315c53b83', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/OCqCJ2Dz1U8e4uGNk0sIOr9w617hhK5z0mRY8FPQ4XY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e8a213074f54c47f7048641f09bd1b11565dfcd7', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/OCqCJ2Dz1U8e4uGNk0sIOr9w617hhK5z0mRY8FPQ4XY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=187b04774f3f3352a99f078518f4a407a24a87b9', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'OHgJvgk2XP019e4mJzCR3aVKeOwrsMoA65jK2r8i_1Q'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ly1qee,True,,amateurAI,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ly1qee/local_and_global_loss/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ly1qee/local_and_global_loss/,7135,1614910974.0,0,,False,,,,,,,,
10,,pytorch,"This is probably an incredibly basic question, but I'm pretty new to torch and haven't come across a simple solution in the docs.

I have two 2D tensors `tokenized_text` and `translated_words`, and I'd like to swap certain rows in one with rows from the other. The tensors aren't guaranteed to be the same dimensions. The end result is then flattened to 1D and the padding values (x for x &lt; 5) are removed.

My first quick-and-dirty attempt (in order to test the rest of my code) involved converting the two tensors to lists, swapping the rows, flattening the result and building a new tensor from that. But that strikes me as super inefficient.

My updated version keeps everything as tensors by padding them so they have the same row length. See here:

        max_length = max(tokenized_text.size()[1], translated_words.size()[1])
            
        tokenized_text = torch.nn.ConstantPad1d((0, max_length - tokenized_text.size()[1]), tokenizer.pad_token_id)(tokenized_text)
        translated_words = torch.nn.ConstantPad1d((0, max_length - translated_words.size()[1]), tokenizer.pad_token_id)(translated_words)
            
        for n, word_index in enumerate(word_indexes):
            tokenized_text[word_index] = translated_words[n]
            
        # strip special chars
        tokenized_text = tokenized_text[tokenized_text &gt; 5]

Just wondering if there's a more efficient / more idiomatic way to accomplish this same task. Is there anything built in to the library?",t2_7ngwzjz,False,,0,False,Most efficient way to swap rows between 2D tensors?,[],r/pytorch,False,6,,0,,,False,t3_lxrlds,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1614911396.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is probably an incredibly basic question, but I&amp;#39;m pretty new to torch and haven&amp;#39;t come across a simple solution in the docs.&lt;/p&gt;

&lt;p&gt;I have two 2D tensors &lt;code&gt;tokenized_text&lt;/code&gt; and &lt;code&gt;translated_words&lt;/code&gt;, and I&amp;#39;d like to swap certain rows in one with rows from the other. The tensors aren&amp;#39;t guaranteed to be the same dimensions. The end result is then flattened to 1D and the padding values (x for x &amp;lt; 5) are removed.&lt;/p&gt;

&lt;p&gt;My first quick-and-dirty attempt (in order to test the rest of my code) involved converting the two tensors to lists, swapping the rows, flattening the result and building a new tensor from that. But that strikes me as super inefficient.&lt;/p&gt;

&lt;p&gt;My updated version keeps everything as tensors by padding them so they have the same row length. See here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    max_length = max(tokenized_text.size()[1], translated_words.size()[1])

    tokenized_text = torch.nn.ConstantPad1d((0, max_length - tokenized_text.size()[1]), tokenizer.pad_token_id)(tokenized_text)
    translated_words = torch.nn.ConstantPad1d((0, max_length - translated_words.size()[1]), tokenizer.pad_token_id)(translated_words)

    for n, word_index in enumerate(word_indexes):
        tokenized_text[word_index] = translated_words[n]

    # strip special chars
    tokenized_text = tokenized_text[tokenized_text &amp;gt; 5]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just wondering if there&amp;#39;s a more efficient / more idiomatic way to accomplish this same task. Is there anything built in to the library?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lxrlds,True,,porterhousegames,,4,True,all_ads,False,[],False,,/r/pytorch/comments/lxrlds/most_efficient_way_to_swap_rows_between_2d_tensors/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lxrlds/most_efficient_way_to_swap_rows_between_2d_tensors/,7135,1614882596.0,0,,False,,,,,,,,
11,,pytorch,,t2_4xhrybaz,False,,0,False,Pytorch Geometric or Pytorch DGL? Which one do you prefer?,[],r/pytorch,False,6,,0,,,False,t3_lxndny,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1614901369.0,text,6,,,text,self.pytorch,False,,,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lxndny,True,,banenvy,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lxndny/pytorch_geometric_or_pytorch_dgl_which_one_do_you/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lxndny/pytorch_geometric_or_pytorch_dgl_which_one_do_you/,7135,1614872569.0,0,,False,,,,,,,,
12,,pytorch,,t2_40d0zt4s,False,,0,False,PyTorch Geometric Temporal: What Is it &amp; Your InDepth Guide -,[],r/pytorch,False,6,,0,78.0,,False,t3_lxk0wl,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/9UGauASV0Y-kEBlOhzDKMahxw0qhDFomfwgq94sRpms.jpg,False,,[],{},link,,False,,1614891872.0,text,6,,,text,analyticsindiamag.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/NybbiRGg4Es2aM6NrAcweMxybonKtw4k3oepnmePtBA.jpg?auto=webp&amp;s=ed6274da68ed1f4ec0865963274e9f538c7fad41', 'width': 960, 'height': 540}, 'resolutions': [{'url': 'https://external-preview.redd.it/NybbiRGg4Es2aM6NrAcweMxybonKtw4k3oepnmePtBA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=21baf91a2ca8718529854cf3f4b67bdd969233ba', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/NybbiRGg4Es2aM6NrAcweMxybonKtw4k3oepnmePtBA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=99ab62f663af7a787c144e85f0c0c1962843e3bf', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/NybbiRGg4Es2aM6NrAcweMxybonKtw4k3oepnmePtBA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=877aad07930e457c8072203dcd5408363a91058b', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/NybbiRGg4Es2aM6NrAcweMxybonKtw4k3oepnmePtBA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=83e5f8a5eaa914df17b2a72f2d58ab93f8f03dff', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/NybbiRGg4Es2aM6NrAcweMxybonKtw4k3oepnmePtBA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=72ac178f823d146d65870a99c4e64fdf765acfc5', 'width': 960, 'height': 540}], 'variants': {}, 'id': 'QJW0b9kAjY_NIZVSOWYo3CT_8zs0o36kXd8K9BG5mUQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lxk0wl,True,,analyticsindiam,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lxk0wl/pytorch_geometric_temporal_what_is_it_your/,all_ads,False,https://analyticsindiamag.com/pytorch-geometric-temporal-what-is-it-your-indepth-guide/,7135,1614863072.0,0,,False,https://analyticsindiamag.com/pytorch-geometric-temporal-what-is-it-your-indepth-guide/,,,,,,,
13,,pytorch,"Hello,

I am trying to evaluate a pre-trained mobilenetv2 model from torchvision on the ImageNet training dataset using the official example ([https://github.com/pytorch/examples/blob/master/imagenet/main.py](https://github.com/pytorch/examples/blob/master/imagenet/main.py)).

To do so, I modify lines 235-237 to perform validation on the train loader instead of the val loader:

        if args.evaluate:
            validate(train_loader, model, criterion, args)
            return

Everything else is left untouched. The command I use to run is:

    python imagenet_train_example.py -a mobilenet_v2 -j 16 -b 1024 -e --pretrained /data/ImageNet

However, the results are much lower than expected:

&gt;Acc@1 2.926 Acc@5 15.079 Loss  11.795791

I was wondering if anyone knows why that might be? Am I doing something wrong?

Cheers!",t2_6ir8qhgt,False,,0,False,Low train accuracy using pretrained torchvision model,[],r/pytorch,False,6,,0,,,False,t3_lxjc07,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1614889517.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am trying to evaluate a pre-trained mobilenetv2 model from torchvision on the ImageNet training dataset using the official example (&lt;a href=""https://github.com/pytorch/examples/blob/master/imagenet/main.py""&gt;https://github.com/pytorch/examples/blob/master/imagenet/main.py&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;To do so, I modify lines 235-237 to perform validation on the train loader instead of the val loader:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    if args.evaluate:
        validate(train_loader, model, criterion, args)
        return
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Everything else is left untouched. The command I use to run is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python imagenet_train_example.py -a mobilenet_v2 -j 16 -b 1024 -e --pretrained /data/ImageNet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, the results are much lower than expected:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Acc@1 2.926 Acc@5 15.079 Loss  11.795791&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I was wondering if anyone knows why that might be? Am I doing something wrong?&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?auto=webp&amp;s=08c43430ff8793a25b767aa172468acb35be611a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4adad794ed3c0e2f76e2da992ae9eef8c074cae4', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5b3cb400fea60e0dfe87ebc5314f7db8016f8ab', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f149723b636ef3087022ddc1efa6ab53e3d4bc69', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_YPUxJ8AyYwgpO9r-tD7JUoQ-Nx8IlATLoEfmjW48iQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lxjc07,True,,MurlocXYZ,,5,True,all_ads,False,[],False,,/r/pytorch/comments/lxjc07/low_train_accuracy_using_pretrained_torchvision/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lxjc07/low_train_accuracy_using_pretrained_torchvision/,7135,1614860717.0,0,,False,,,,,,,,
14,,pytorch,Does anyone is having that problem too?,t2_1274wo,False,,0,False,Getting 403 when trying to download mnist from torchvision.datasets,[],r/pytorch,False,6,,0,,,False,t3_lxbwp3,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1614858484.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone is having that problem too?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lxbwp3,True,,RandomGaussian,,1,True,all_ads,False,[],False,,/r/pytorch/comments/lxbwp3/getting_403_when_trying_to_download_mnist_from/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lxbwp3/getting_403_when_trying_to_download_mnist_from/,7135,1614829684.0,0,,False,,,,,,,,
15,,pytorch,"I am a web developer, I don't know much about ML. I have been given a model that I'm supposed to integrate into my website but first I need to save it. It already has a save\_checkpoint method in the trainer file but I don't know how to use it. Please help.

&amp;#x200B;

I posted this on stackoverflow but nobody responded.

[https://stackoverflow.com/questions/66449973/how-to-use-the-save-checkpoint-method](https://stackoverflow.com/questions/66449973/how-to-use-the-save-checkpoint-method)",t2_6fwexyi9,False,,0,False,How to save model in pytorch?,[],r/pytorch,False,6,,0,,,False,t3_lxcouw,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1614861276.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a web developer, I don&amp;#39;t know much about ML. I have been given a model that I&amp;#39;m supposed to integrate into my website but first I need to save it. It already has a save_checkpoint method in the trainer file but I don&amp;#39;t know how to use it. Please help.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I posted this on stackoverflow but nobody responded.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://stackoverflow.com/questions/66449973/how-to-use-the-save-checkpoint-method""&gt;https://stackoverflow.com/questions/66449973/how-to-use-the-save-checkpoint-method&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lxcouw,True,,roymustang261,,3,True,all_ads,False,[],False,,/r/pytorch/comments/lxcouw/how_to_save_model_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lxcouw/how_to_save_model_in_pytorch/,7135,1614832476.0,0,,False,,,,,,,,
16,,pytorch,Even Knowing that Pytorch is so much flexible than tensorflow.,t2_6hhee0ag,False,,0,False,Why people alot use alot tensorflow instead of pytorch?,[],r/pytorch,False,6,,0,,,False,t3_lwy52r,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1614818264.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Even Knowing that Pytorch is so much flexible than tensorflow.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lwy52r,True,,Psycho-logical-being,,7,True,all_ads,False,[],False,,/r/pytorch/comments/lwy52r/why_people_alot_use_alot_tensorflow_instead_of/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lwy52r/why_people_alot_use_alot_tensorflow_instead_of/,7135,1614789464.0,0,,False,,,,,,,,
17,,pytorch,,t2_sswdj,False,,0,False,Getting Started with Distributed Machine Learning with PyTorch and Ray,[],r/pytorch,False,6,,0,70.0,,False,t3_lwcpwp,False,dark,0.93,,public,11,0,{},140.0,,False,[],,False,False,,{},,False,11,,False,https://b.thumbs.redditmedia.com/CiuGDImOoxc-xzEiBS7InGjKNDUj7wMla4dDy2nKDYo.jpg,False,,[],{},link,,False,,1614748500.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/aayYLAMlqEk4tGLXO83AKcUkoVdDBSCg_xRojMmAc6o.jpg?auto=webp&amp;s=3413573fdcf4537f9437efb406ecd2cdc76ccc38', 'width': 977, 'height': 489}, 'resolutions': [{'url': 'https://external-preview.redd.it/aayYLAMlqEk4tGLXO83AKcUkoVdDBSCg_xRojMmAc6o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=81b968d945091adc51ea8e5cf06ef8b326de1de2', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/aayYLAMlqEk4tGLXO83AKcUkoVdDBSCg_xRojMmAc6o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fedd0bc2f282c669a03eecd4b48fb16440534029', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/aayYLAMlqEk4tGLXO83AKcUkoVdDBSCg_xRojMmAc6o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=29109b4a1f4b3d29656b41a411deb3de470b97bd', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/aayYLAMlqEk4tGLXO83AKcUkoVdDBSCg_xRojMmAc6o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5bae0f15f2dbdf7516db4e82cf4bb79604ce35f', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/aayYLAMlqEk4tGLXO83AKcUkoVdDBSCg_xRojMmAc6o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e54a95b1c9c3a53fd24b92997d6d516eceecc259', 'width': 960, 'height': 480}], 'variants': {}, 'id': 'kgbZ2sDyEaH5FCkbnQaM4uATlM59LDdLE0AhV9DLtkQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lwcpwp,True,,mgalarny,,4,True,all_ads,False,[],False,,/r/pytorch/comments/lwcpwp/getting_started_with_distributed_machine_learning/,all_ads,False,https://medium.com/distributed-computing-with-ray/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-27175a1b4f25,7135,1614719700.0,0,,False,https://medium.com/distributed-computing-with-ray/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-27175a1b4f25,,,,,,,
18,,pytorch,,t2_766u1eio,False,,0,False,How to fix a SIGSEGV in pytorch when using distributed training (e.g. DDP)?,[],r/pytorch,False,6,,0,140.0,,False,t3_lwbb72,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://a.thumbs.redditmedia.com/u_dHFlbxQzP-5pxqrBaTgXjfrpwqILnkoZNwgWCph24.jpg,False,,[],{},link,,False,,1614744655.0,text,6,,,text,discuss.pytorch.org,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lwbb72,True,,No_Ad3397,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lwbb72/how_to_fix_a_sigsegv_in_pytorch_when_using/,all_ads,False,https://discuss.pytorch.org/t/how-to-fix-a-sigsegv-in-pytorch-when-using-distributed-training-e-g-ddp/113518,7135,1614715855.0,0,,False,https://discuss.pytorch.org/t/how-to-fix-a-sigsegv-in-pytorch-when-using-distributed-training-e-g-ddp/113518,,,,,,,
19,,pytorch,"Same data, same model
Their code: num_workers and pin memory has not been set. Gradients are not zeroed at before train (its grid search), gradients are calculated during validation. Just that all inputs are passed to the gpu. Has loads of redundant variables and print functions. Nothing related to threads specified.

My code: all of the above is done, but still the code runs slowly as compared to the their code. First I had a file system with all relevant code and functions segregated. Then I thought that might be taking up time, so I put it in one code, but it didn't change the time much.

Can't exactly understand why this is happening. Any tips?",t2_4xhrybaz,False,,0,False,"My partner made a shitty code in a jupyter and then exported it as .py, but it runs faster than my pytorch code.",[],r/pytorch,False,6,,0,,,False,t3_lw08z0,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1614686983.0,,[],{},,,True,,1614714690.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Same data, same model
Their code: num_workers and pin memory has not been set. Gradients are not zeroed at before train (its grid search), gradients are calculated during validation. Just that all inputs are passed to the gpu. Has loads of redundant variables and print functions. Nothing related to threads specified.&lt;/p&gt;

&lt;p&gt;My code: all of the above is done, but still the code runs slowly as compared to the their code. First I had a file system with all relevant code and functions segregated. Then I thought that might be taking up time, so I put it in one code, but it didn&amp;#39;t change the time much.&lt;/p&gt;

&lt;p&gt;Can&amp;#39;t exactly understand why this is happening. Any tips?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lw08z0,True,,banenvy,,5,True,all_ads,False,[],False,,/r/pytorch/comments/lw08z0/my_partner_made_a_shitty_code_in_a_jupyter_and/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lw08z0/my_partner_made_a_shitty_code_in_a_jupyter_and/,7135,1614685890.0,0,,False,,,,,,,,
20,,pytorch,,t2_766u1eio,False,,0,False,"How does one set the pytorch distributed hostname, port and GLOO_SOCKET_IFNAME so that DDP works?",[],r/pytorch,False,6,,0,140.0,,False,t3_lw1tkv,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/2vFch1Ff7QLTy54ouUuWdfTS0DIqWdh5mrHh381gWKA.jpg,False,,[],{},link,,False,,1614720116.0,text,6,,,text,discuss.pytorch.org,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lw1tkv,True,,No_Ad3397,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lw1tkv/how_does_one_set_the_pytorch_distributed_hostname/,all_ads,False,https://discuss.pytorch.org/t/how-does-one-set-the-pytorch-distributed-hostname-port-and-gloo-socket-ifname-so-that-ddp-works/113468,7135,1614691316.0,0,,False,https://discuss.pytorch.org/t/how-does-one-set-the-pytorch-distributed-hostname-port-and-gloo-socket-ifname-so-that-ddp-works/113468,,,,,,,
21,,pytorch,"Hey Guys, I wrote a sample code which implements [Fully Connected (FC) layer as Conv layer](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/Implementing_FC_as_conv_layer.py) in PyTorch. Let me know your thoughts. This is going to be used for optimized ""Sliding Windows object detection"" algorithm.",t2_2mmql89p,False,,0,False,Implementing FC layer as conv layer,[],r/pytorch,False,6,,0,,,False,t3_lvurr5,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1614691659.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey Guys, I wrote a sample code which implements &lt;a href=""https://github.com/arjun-majumdar/CNN_Classifications/blob/master/Implementing_FC_as_conv_layer.py""&gt;Fully Connected (FC) layer as Conv layer&lt;/a&gt; in PyTorch. Let me know your thoughts. This is going to be used for optimized &amp;quot;Sliding Windows object detection&amp;quot; algorithm.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?auto=webp&amp;s=f2a8d2644c171302d258e04bb4f56341e0932f5c', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a712b927d57fefc453338d3cc567c9a6a5682180', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8fdbd8b838cc4c3070a600980ec621b534c468c2', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2541ecf1cf94955a97e415c8325f05533b49e8f9', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'SxmolEBMl5rw9UEn7Ftv1WN1nmytQ0dLmTjHbIRwo8E'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lvurr5,True,,grid_world,,8,True,all_ads,False,[],False,,/r/pytorch/comments/lvurr5/implementing_fc_layer_as_conv_layer/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lvurr5/implementing_fc_layer_as_conv_layer/,7135,1614662859.0,0,,False,,,,,,,,
23,,pytorch,,t2_766u1eio,False,,0,False,How does one setup the set_sharing_strategy strategy for multiprocessing in Pytorch?,[],r/pytorch,False,6,,0,140.0,,False,t3_lvdze1,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://a.thumbs.redditmedia.com/T3ztDD_Pb-20WHtp2UfG1jBstkIBVj3fi2rVf09_3b4.jpg,False,,[],{},link,,False,,1614646140.0,text,6,,,text,stackoverflow.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lvdze1,True,,No_Ad3397,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lvdze1/how_does_one_setup_the_set_sharing_strategy/,all_ads,False,https://stackoverflow.com/questions/66426199/how-does-one-setup-the-set-sharing-strategy-strategy-for-multiprocessing-in-pyto,7135,1614617340.0,0,,False,https://stackoverflow.com/questions/66426199/how-does-one-setup-the-set-sharing-strategy-strategy-for-multiprocessing-in-pyto,,,,,,,
24,,pytorch,"&amp;#x200B;

https://preview.redd.it/xvdaj4vm98k61.png?width=1461&amp;format=png&amp;auto=webp&amp;s=67051689cf9252c8a6e1b5386e4c531685aa00be

I wrote a [C++ trainable semantic segmentation open source project](https://github.com/AllentDan/SegmentationCpp) supporting UNet, FPN, PAN, LinkNet, DeepLabV3 and DeepLabV3+ architectures.

The main features of this library are:

* High level API (just a line to create a neural network)
* 6 models architectures for binary and multi class segmentation (including legendary Unet)
* 7 available encoders
* All encoders have pre-trained weights for faster and better convergence
* 2x or more faster than pytorch cuda inferece, same speed for cpu. (Unet tested in gtx 2070s).

## 1. Create your first Segmentation model with Libtorch Segment

Segmentation model is just a LibTorch torch::nn::Module, which can be created as easy as:

    #include ""Segmentor.h""
    auto model = UNet(1, /*num of classes*/
                      ""resnet34"", /*encoder name, could be resnet50 or others*/
                      ""path to resnet34.pt""/*weight path pretrained on ImageNet, it is produced by torchscript*/
                      );

* see [table](#architectures) with available model architectures
* see [table](#encoders) with available encoders and their corresponding weights

## 2. Generate your own pretrained weights

All encoders have pretrained weights. Preparing your data the same way as during weights pre-training may give your better results (higher metric score and faster convergence). And you can also train only the decoder and segmentation head while freeze the backbone.

    import torch
    from torchvision import models
    
    # resnet50 for example
    model = models.resnet50(pretrained=True)
    model.eval()
    var=torch.ones((1,3,224,224))
    traced_script_module = torch.jit.trace(model, var)
    traced_script_module.save(""resnet50.pt"")

Congratulations! You are done! Now you can train your model with your favorite backbone and segmentation framework.

## 3. 💡 Examples

* Training model for person segmentation using images from PASCAL VOC Dataset. ""voc\_person\_seg"" dir contains 32 json labels and their corresponding jpeg images for training and 8 json labels with corresponding images for validation.

&amp;#x200B;

    Segmentor&lt;FPN&gt; segmentor;
    segmentor.Initialize(0/*gpu id, -1 for cpu*/,
                        512/*resize width*/,
                        512/*resize height*/,
                        {""background"",""person""}/*class name dict, background included*/,
                        ""resnet34""/*backbone name*/,
                        ""your path to resnet34.pt"");
    segmentor.Train(0.0003/*initial leaning rate*/,
                    300/*training epochs*/,
                    4/*batch size*/,
                    ""your path to voc_person_seg"",
                    "".jpg""/*image type*/,
                    ""your path to save segmentor.pt"");

* Predicting test. A segmentor.pt file is provided in the project. It is trained through a FPN with ResNet34 backbone for a few epochs. You can directly test the segmentation result through:

&amp;#x200B;

    cv::Mat image = cv::imread(""your path to voc_person_seg\\val\\2007_004000.jpg"");
    Segmentor&lt;FPN&gt; segmentor;
    segmentor.Initialize(0,512,512,{""background"",""person""},
                          ""resnet34"",""your path to resnet34.pt"");
    segmentor.LoadWeight(""segmentor.pt""/*the saved .pt path*/);
    segmentor.Predict(image,""person""/*class name for showing*/);

the predicted result shows as follow:

&amp;#x200B;

## 4. 🧑‍🚀 Train your own data

* Create your own dataset. Using [labelme](https://github.com/wkentaro/labelme) through ""pip install"" and label your images. Split the output json files and images into folders just like below:

&amp;#x200B;

    Dataset
    ├── train
    │   ├── xxx.json
    │   ├── xxx.jpg
    │   └......
    ├── val
    │   ├── xxxx.json
    │   ├── xxxx.jpg
    │   └......

* Training or testing. Just like the example of ""voc\_person\_seg"", replace ""voc\_person\_seg"" with your own dataset path.

## 📦 Models

## Architectures

* \[x\] Unet \[[paper](https://arxiv.org/abs/1505.04597)\]
* \[x\] FPN \[[paper](http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf)\]
* \[x\] PAN \[[paper](https://arxiv.org/abs/1805.10180)\]
* \[x\] LinkNet \[[paper](https://arxiv.org/abs/1707.03718)\]
* \[x\] DeepLabV3 \[[paper](https://arxiv.org/abs/1706.05587)\]
* \[x\] DeepLabV3+ \[[paper](https://arxiv.org/abs/1802.02611)\]
* \[ \] PSPNet \[[paper](https://arxiv.org/abs/1612.01105)\]

## Encoders

* \[x\] ResNet
* \[x\] ResNext
* \[ \] ResNest

The following is a list of supported encoders in the Libtorch Segment. All the encoders weights can be generated through torchvision except resnest. Select the appropriate family of encoders and click to expand the table and select a specific encoder and its pre-trained weights.

|Encoder|Encoder|Encoder|
|:-|:-|:-|
|Weights|Weights|Weights|
|Params, M|Params, M|Params, M|
|resnet18|resnext50\_32x4d|timm-resnest14d|
|imagenet|imagenet|imagenet|
|11M|22M|8M|
|resnet34|resnext101\_32x8d|timm-resnest26d|
|imagenet|imagenet|imagenet|
|21M|86M|15M|
|resnet50|timm-resnest50d|imagenet|
|imagenet|23M|25M|
|resnet101|timm-resnest101e|imagenet|
|imagenet|42M|46M|
|resnet152|timm-resnest200e|imagenet|
|imagenet|58M|68M|
|timm-resnest269e|imagenet|108M|
|timm-resnest50d\_4s2x40d|imagenet|28M|
|timm-resnest50d\_1s4x24d|imagenet|23M|

## 🛠 Installation

Windows:

Configure the environment for libtorch development. [Visual studio](https://allentdan.github.io/2020/12/16/pytorch%E9%83%A8%E7%BD%B2torchscript%E7%AF%87) and [Qt Creator](https://allentdan.github.io/2021/01/21/QT%20Creator%20+%20Opencv4.x%20+%20Libtorch1.7%E9%85%8D%E7%BD%AE/#more) are verified for libtorch1.7x release. Only chinese configuration blogs provided by now, english version ASAP.

Linux &amp;&amp; MacOS:

Follow the official pytorch c++ tutorials [here](https://pytorch.org/tutorials/advanced/cpp_export.html). It can be no more difficult than windows.

## 🤝 Thanks

This project is under developing. By now, these projects helps a lot.

* [official pytorch](https://github.com/pytorch/pytorch)
* [qubvel SMP](https://github.com/qubvel/segmentation_models.pytorch)
* [wkentaro labelme](https://github.com/wkentaro/labelme)
* [nlohmann json](https://github.com/nlohmann/json)

## 📝 Citing

    @misc{Chunyu:2021,
      Author = {Chunyu Dong},
      Title = {Libtorch Segment},
      Year = {2021},
      Publisher = {GitHub},
      Journal = {GitHub repository},
      Howpublished = {\url{https://github.com/AllentDan/SegmentationCpp}}
    }

## 🛡️ License

Project is distributed under [MIT License](https://github.com/qubvel/segmentation_models.pytorch/blob/master/LICENSE)",t2_887i8cel,False,,0,False,C++ trainable semantic segmentation models,[],r/pytorch,False,6,,0,,,False,t3_luh7ge,False,dark,1.0,,public,8,0,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},self,,True,,1614550941.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/xvdaj4vm98k61.png?width=1461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67051689cf9252c8a6e1b5386e4c531685aa00be""&gt;https://preview.redd.it/xvdaj4vm98k61.png?width=1461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67051689cf9252c8a6e1b5386e4c531685aa00be&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I wrote a &lt;a href=""https://github.com/AllentDan/SegmentationCpp""&gt;C++ trainable semantic segmentation open source project&lt;/a&gt; supporting UNet, FPN, PAN, LinkNet, DeepLabV3 and DeepLabV3+ architectures.&lt;/p&gt;

&lt;p&gt;The main features of this library are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;High level API (just a line to create a neural network)&lt;/li&gt;
&lt;li&gt;6 models architectures for binary and multi class segmentation (including legendary Unet)&lt;/li&gt;
&lt;li&gt;7 available encoders&lt;/li&gt;
&lt;li&gt;All encoders have pre-trained weights for faster and better convergence&lt;/li&gt;
&lt;li&gt;2x or more faster than pytorch cuda inferece, same speed for cpu. (Unet tested in gtx 2070s).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;1. Create your first Segmentation model with Libtorch Segment&lt;/h2&gt;

&lt;p&gt;Segmentation model is just a LibTorch torch::nn::Module, which can be created as easy as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;quot;Segmentor.h&amp;quot;
auto model = UNet(1, /*num of classes*/
                  &amp;quot;resnet34&amp;quot;, /*encoder name, could be resnet50 or others*/
                  &amp;quot;path to resnet34.pt&amp;quot;/*weight path pretrained on ImageNet, it is produced by torchscript*/
                  );
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;see &lt;a href=""#architectures""&gt;table&lt;/a&gt; with available model architectures&lt;/li&gt;
&lt;li&gt;see &lt;a href=""#encoders""&gt;table&lt;/a&gt; with available encoders and their corresponding weights&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;2. Generate your own pretrained weights&lt;/h2&gt;

&lt;p&gt;All encoders have pretrained weights. Preparing your data the same way as during weights pre-training may give your better results (higher metric score and faster convergence). And you can also train only the decoder and segmentation head while freeze the backbone.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import torch
from torchvision import models

# resnet50 for example
model = models.resnet50(pretrained=True)
model.eval()
var=torch.ones((1,3,224,224))
traced_script_module = torch.jit.trace(model, var)
traced_script_module.save(&amp;quot;resnet50.pt&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Congratulations! You are done! Now you can train your model with your favorite backbone and segmentation framework.&lt;/p&gt;

&lt;h2&gt;3. 💡 Examples&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Training model for person segmentation using images from PASCAL VOC Dataset. &amp;quot;voc_person_seg&amp;quot; dir contains 32 json labels and their corresponding jpeg images for training and 8 json labels with corresponding images for validation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Segmentor&amp;lt;FPN&amp;gt; segmentor;
segmentor.Initialize(0/*gpu id, -1 for cpu*/,
                    512/*resize width*/,
                    512/*resize height*/,
                    {&amp;quot;background&amp;quot;,&amp;quot;person&amp;quot;}/*class name dict, background included*/,
                    &amp;quot;resnet34&amp;quot;/*backbone name*/,
                    &amp;quot;your path to resnet34.pt&amp;quot;);
segmentor.Train(0.0003/*initial leaning rate*/,
                300/*training epochs*/,
                4/*batch size*/,
                &amp;quot;your path to voc_person_seg&amp;quot;,
                &amp;quot;.jpg&amp;quot;/*image type*/,
                &amp;quot;your path to save segmentor.pt&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Predicting test. A segmentor.pt file is provided in the project. It is trained through a FPN with ResNet34 backbone for a few epochs. You can directly test the segmentation result through:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cv::Mat image = cv::imread(&amp;quot;your path to voc_person_seg\\val\\2007_004000.jpg&amp;quot;);
Segmentor&amp;lt;FPN&amp;gt; segmentor;
segmentor.Initialize(0,512,512,{&amp;quot;background&amp;quot;,&amp;quot;person&amp;quot;},
                      &amp;quot;resnet34&amp;quot;,&amp;quot;your path to resnet34.pt&amp;quot;);
segmentor.LoadWeight(&amp;quot;segmentor.pt&amp;quot;/*the saved .pt path*/);
segmentor.Predict(image,&amp;quot;person&amp;quot;/*class name for showing*/);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the predicted result shows as follow:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;h2&gt;4. 🧑‍🚀 Train your own data&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Create your own dataset. Using &lt;a href=""https://github.com/wkentaro/labelme""&gt;labelme&lt;/a&gt; through &amp;quot;pip install&amp;quot; and label your images. Split the output json files and images into folders just like below:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Dataset
├── train
│   ├── xxx.json
│   ├── xxx.jpg
│   └......
├── val
│   ├── xxxx.json
│   ├── xxxx.jpg
│   └......
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Training or testing. Just like the example of &amp;quot;voc_person_seg&amp;quot;, replace &amp;quot;voc_person_seg&amp;quot; with your own dataset path.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;📦 Models&lt;/h2&gt;

&lt;h2&gt;Architectures&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;[x] Unet [&lt;a href=""https://arxiv.org/abs/1505.04597""&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;[x] FPN [&lt;a href=""http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf""&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;[x] PAN [&lt;a href=""https://arxiv.org/abs/1805.10180""&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;[x] LinkNet [&lt;a href=""https://arxiv.org/abs/1707.03718""&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;[x] DeepLabV3 [&lt;a href=""https://arxiv.org/abs/1706.05587""&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;[x] DeepLabV3+ [&lt;a href=""https://arxiv.org/abs/1802.02611""&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;[ ] PSPNet [&lt;a href=""https://arxiv.org/abs/1612.01105""&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Encoders&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;[x] ResNet&lt;/li&gt;
&lt;li&gt;[x] ResNext&lt;/li&gt;
&lt;li&gt;[ ] ResNest&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following is a list of supported encoders in the Libtorch Segment. All the encoders weights can be generated through torchvision except resnest. Select the appropriate family of encoders and click to expand the table and select a specific encoder and its pre-trained weights.&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;Encoder&lt;/th&gt;
&lt;th align=""left""&gt;Encoder&lt;/th&gt;
&lt;th align=""left""&gt;Encoder&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;Weights&lt;/td&gt;
&lt;td align=""left""&gt;Weights&lt;/td&gt;
&lt;td align=""left""&gt;Weights&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;Params, M&lt;/td&gt;
&lt;td align=""left""&gt;Params, M&lt;/td&gt;
&lt;td align=""left""&gt;Params, M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;resnet18&lt;/td&gt;
&lt;td align=""left""&gt;resnext50_32x4d&lt;/td&gt;
&lt;td align=""left""&gt;timm-resnest14d&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;11M&lt;/td&gt;
&lt;td align=""left""&gt;22M&lt;/td&gt;
&lt;td align=""left""&gt;8M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;resnet34&lt;/td&gt;
&lt;td align=""left""&gt;resnext101_32x8d&lt;/td&gt;
&lt;td align=""left""&gt;timm-resnest26d&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;21M&lt;/td&gt;
&lt;td align=""left""&gt;86M&lt;/td&gt;
&lt;td align=""left""&gt;15M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;resnet50&lt;/td&gt;
&lt;td align=""left""&gt;timm-resnest50d&lt;/td&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;td align=""left""&gt;23M&lt;/td&gt;
&lt;td align=""left""&gt;25M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;resnet101&lt;/td&gt;
&lt;td align=""left""&gt;timm-resnest101e&lt;/td&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;td align=""left""&gt;42M&lt;/td&gt;
&lt;td align=""left""&gt;46M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;resnet152&lt;/td&gt;
&lt;td align=""left""&gt;timm-resnest200e&lt;/td&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;td align=""left""&gt;58M&lt;/td&gt;
&lt;td align=""left""&gt;68M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;timm-resnest269e&lt;/td&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;td align=""left""&gt;108M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;timm-resnest50d_4s2x40d&lt;/td&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;td align=""left""&gt;28M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;timm-resnest50d_1s4x24d&lt;/td&gt;
&lt;td align=""left""&gt;imagenet&lt;/td&gt;
&lt;td align=""left""&gt;23M&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;🛠 Installation&lt;/h2&gt;

&lt;p&gt;Windows:&lt;/p&gt;

&lt;p&gt;Configure the environment for libtorch development. &lt;a href=""https://allentdan.github.io/2020/12/16/pytorch%E9%83%A8%E7%BD%B2torchscript%E7%AF%87""&gt;Visual studio&lt;/a&gt; and &lt;a href=""https://allentdan.github.io/2021/01/21/QT%20Creator%20+%20Opencv4.x%20+%20Libtorch1.7%E9%85%8D%E7%BD%AE/#more""&gt;Qt Creator&lt;/a&gt; are verified for libtorch1.7x release. Only chinese configuration blogs provided by now, english version ASAP.&lt;/p&gt;

&lt;p&gt;Linux &amp;amp;&amp;amp; MacOS:&lt;/p&gt;

&lt;p&gt;Follow the official pytorch c++ tutorials &lt;a href=""https://pytorch.org/tutorials/advanced/cpp_export.html""&gt;here&lt;/a&gt;. It can be no more difficult than windows.&lt;/p&gt;

&lt;h2&gt;🤝 Thanks&lt;/h2&gt;

&lt;p&gt;This project is under developing. By now, these projects helps a lot.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/pytorch/pytorch""&gt;official pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/qubvel/segmentation_models.pytorch""&gt;qubvel SMP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/wkentaro/labelme""&gt;wkentaro labelme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/nlohmann/json""&gt;nlohmann json&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;📝 Citing&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;@misc{Chunyu:2021,
  Author = {Chunyu Dong},
  Title = {Libtorch Segment},
  Year = {2021},
  Publisher = {GitHub},
  Journal = {GitHub repository},
  Howpublished = {\url{https://github.com/AllentDan/SegmentationCpp}}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;🛡️ License&lt;/h2&gt;

&lt;p&gt;Project is distributed under &lt;a href=""https://github.com/qubvel/segmentation_models.pytorch/blob/master/LICENSE""&gt;MIT License&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Oh4DejXMnC7Wv4vx1IM3rRMtJVJatk2aLCt31h1VkRQ.jpg?auto=webp&amp;s=c4842da9f5f87f7873f0fa2b3a5d066d29a4b982', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/Oh4DejXMnC7Wv4vx1IM3rRMtJVJatk2aLCt31h1VkRQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a95b796141a1a8edf97e22f597e101e5498f8f61', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/Oh4DejXMnC7Wv4vx1IM3rRMtJVJatk2aLCt31h1VkRQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6716402147edc9240400b1906a619e796bdd6d35', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/Oh4DejXMnC7Wv4vx1IM3rRMtJVJatk2aLCt31h1VkRQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5ef2cd10ba166f47bb6b9d9dc59a7ad38b1f3eee', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'IQ4apoS7UisxdHYP9shsgnD8MIWCArm-rkTPIFi_kSc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,luh7ge,True,,AllentDan,,2,True,all_ads,False,[],False,,/r/pytorch/comments/luh7ge/c_trainable_semantic_segmentation_models/,all_ads,False,https://www.reddit.com/r/pytorch/comments/luh7ge/c_trainable_semantic_segmentation_models/,7135,1614522141.0,0,,False,,,,"{'xvdaj4vm98k61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 22, 'x': 108, 'u': 'https://preview.redd.it/xvdaj4vm98k61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=39edff2540afdda0b9c9ed75edf591a1ce7148af'}, {'y': 44, 'x': 216, 'u': 'https://preview.redd.it/xvdaj4vm98k61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=28123ca950e92858c15566ac60602eeeb8a3cfa0'}, {'y': 65, 'x': 320, 'u': 'https://preview.redd.it/xvdaj4vm98k61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17a222407050674675d96908c3619b8f1fc200cd'}, {'y': 131, 'x': 640, 'u': 'https://preview.redd.it/xvdaj4vm98k61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1e4d664f79bd1d6c2a92a67874ff5db150e4a452'}, {'y': 197, 'x': 960, 'u': 'https://preview.redd.it/xvdaj4vm98k61.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=68e498ff834e9cfa26879c313d00cf184bb41868'}, {'y': 222, 'x': 1080, 'u': 'https://preview.redd.it/xvdaj4vm98k61.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f5f218255383e33cb6b059d52f5fb03fd37888c0'}], 's': {'y': 301, 'x': 1461, 'u': 'https://preview.redd.it/xvdaj4vm98k61.png?width=1461&amp;format=png&amp;auto=webp&amp;s=67051689cf9252c8a6e1b5386e4c531685aa00be'}, 'id': 'xvdaj4vm98k61'}}",,,,
25,,pytorch,"I have a 5 classes unbalanced dataset for classification. I'm using RandomWeightSampler to feed the dataloader to avoid the consequences due to unbalancing stuff and using CrossEntropy as a loss function on training.

As you know CrossEntropy can used with a weight, so should I pass a class weight to the loss function or the sampler which is create balanced batches for training is enough for handle the dataset balance tweak ?",t2_r8g9nqg,False,,0,False,Image Classification with Unbalanced Dataset,[],r/pytorch,False,6,,0,,,False,t3_ltzw6s,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1614496813.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a 5 classes unbalanced dataset for classification. I&amp;#39;m using RandomWeightSampler to feed the dataloader to avoid the consequences due to unbalancing stuff and using CrossEntropy as a loss function on training.&lt;/p&gt;

&lt;p&gt;As you know CrossEntropy can used with a weight, so should I pass a class weight to the loss function or the sampler which is create balanced batches for training is enough for handle the dataset balance tweak ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ltzw6s,True,,WhatsUpN,,10,True,all_ads,False,[],False,,/r/pytorch/comments/ltzw6s/image_classification_with_unbalanced_dataset/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ltzw6s/image_classification_with_unbalanced_dataset/,7135,1614468013.0,0,,False,,,,,,,,
26,,pytorch,"What reshape function can I use to convert this tensor Reshape = rank1.view(2, 2, 6)

tensor([[[ 0,  1,  2,  3,  4,  5],
             [ 6,  7,  8,  9, 10, 11]],
             [[12, 13, 14, 15, 16, 17],
             [18, 19, 20, 21, 22, 23]]])

To this ? 
expected = [
    [0, 1,  2,  3, 12, 13, 14, 15],
    [4, 5,  6,  7, 16, 17, 18, 19],
    [8, 9, 10, 11, 20, 21, 22, 23]]",t2_3ornuynn,False,,0,False,Reshaping Operations in Pytorch,[],r/pytorch,False,6,,0,,,False,t3_ltt8hm,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1614476658.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What reshape function can I use to convert this tensor Reshape = rank1.view(2, 2, 6)&lt;/p&gt;

&lt;p&gt;tensor([[[ 0,  1,  2,  3,  4,  5],
             [ 6,  7,  8,  9, 10, 11]],
             [[12, 13, 14, 15, 16, 17],
             [18, 19, 20, 21, 22, 23]]])&lt;/p&gt;

&lt;p&gt;To this ? 
expected = [
    [0, 1,  2,  3, 12, 13, 14, 15],
    [4, 5,  6,  7, 16, 17, 18, 19],
    [8, 9, 10, 11, 20, 21, 22, 23]]&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ltt8hm,True,,trenttaylow567,,10,True,all_ads,False,[],False,,/r/pytorch/comments/ltt8hm/reshaping_operations_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ltt8hm/reshaping_operations_in_pytorch/,7135,1614447858.0,0,,False,,,,,,,,
27,,pytorch,"I am not sure about [how to implement](https://gist.github.com/promach/ae0e48974ccf4bdee07c9d69148cf21b) `Ltrain(w+)` and  `Ltrain(w-)`  for [DARTS: Differentiable Architecture Search](https://arxiv.org/abs/1806.09055)

Could anyone advise ?

https://preview.redd.it/swqi3tl230k61.png?width=1920&amp;format=png&amp;auto=webp&amp;s=cafe8dacb197f4db8e56ec59e189103f741fe59f",t2_bpftl,False,,0,False,Questions about reproducing DARTS code implementation,[],r/pytorch,False,6,,0,,,False,t3_ltlo8f,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1614451883.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am not sure about &lt;a href=""https://gist.github.com/promach/ae0e48974ccf4bdee07c9d69148cf21b""&gt;how to implement&lt;/a&gt; &lt;code&gt;Ltrain(w+)&lt;/code&gt; and  &lt;code&gt;Ltrain(w-)&lt;/code&gt;  for &lt;a href=""https://arxiv.org/abs/1806.09055""&gt;DARTS: Differentiable Architecture Search&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Could anyone advise ?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/swqi3tl230k61.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cafe8dacb197f4db8e56ec59e189103f741fe59f""&gt;https://preview.redd.it/swqi3tl230k61.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cafe8dacb197f4db8e56ec59e189103f741fe59f&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?auto=webp&amp;s=079a7260ec149880c73263d64811698adb22760a', 'width': 1280, 'height': 640}, 'resolutions': [{'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5811c5bda5fece1040636a6af8702ba790f0fd4', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eee576fd4da7535eb53ceb88dd8b52f073048441', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=72872d880460efa723918c000adca0ed259cf775', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3545b9335d763c9da9c16bf7bf9a3f907dbd6f6', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d241ace0f1c07088fac3f8469dbad3b05d2d419', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9055f11bdc00beb0b3589e1cae5817d6070d83bc', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ltlo8f,True,,promach,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ltlo8f/questions_about_reproducing_darts_code/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ltlo8f/questions_about_reproducing_darts_code/,7135,1614423083.0,4,,False,,,,"{'swqi3tl230k61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 58, 'x': 108, 'u': 'https://preview.redd.it/swqi3tl230k61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2a8b1fd322e301d2cd51086bf3e7dc1b2066ea4'}, {'y': 116, 'x': 216, 'u': 'https://preview.redd.it/swqi3tl230k61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7066b6a53aec7481fa6bef3103c710c437df3224'}, {'y': 172, 'x': 320, 'u': 'https://preview.redd.it/swqi3tl230k61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0af2677494179be7a035bd6c7e75a265b3ed6085'}, {'y': 344, 'x': 640, 'u': 'https://preview.redd.it/swqi3tl230k61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=afa79eaf9ccc7b30f865cc554cff6ae92a09db28'}, {'y': 516, 'x': 960, 'u': 'https://preview.redd.it/swqi3tl230k61.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5937f59dac4ce92591eec5cb38552eeb76a0818'}, {'y': 580, 'x': 1080, 'u': 'https://preview.redd.it/swqi3tl230k61.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2157b41198b86437f00aebe38e71d0ea9573d011'}], 's': {'y': 1032, 'x': 1920, 'u': 'https://preview.redd.it/swqi3tl230k61.png?width=1920&amp;format=png&amp;auto=webp&amp;s=cafe8dacb197f4db8e56ec59e189103f741fe59f'}, 'id': 'swqi3tl230k61'}}",,,,
28,,pytorch,,t2_44mbtmjy,False,,0,False,Tom Cruise deepfake videos are all over the internet and passing the best deepfake detectors!,[],r/pytorch,False,6,,0,42.0,,False,t3_lt9lg4,False,dark,1.0,,public,8,0,{},140.0,,False,[],,False,False,,{},,False,8,,False,https://b.thumbs.redditmedia.com/wznT7cB7SlM6Ye_sJWF4lFYsuKvrDl6bXUgnWrY3qSI.jpg,False,,[],{},link,,False,,1614407323.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/CZcLat1G57HQ2laiL1yZp8HQry2k4Aveg6d2zUecVss.jpg?auto=webp&amp;s=4f24997ef2c142e60dc77448de49d1dc4b1eee89', 'width': 816, 'height': 246}, 'resolutions': [{'url': 'https://external-preview.redd.it/CZcLat1G57HQ2laiL1yZp8HQry2k4Aveg6d2zUecVss.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb18b9ff11a399264b6b501fe2d796f1842cf531', 'width': 108, 'height': 32}, {'url': 'https://external-preview.redd.it/CZcLat1G57HQ2laiL1yZp8HQry2k4Aveg6d2zUecVss.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=da2566eef17424fd29022c2bd0acf057bdb46a0e', 'width': 216, 'height': 65}, {'url': 'https://external-preview.redd.it/CZcLat1G57HQ2laiL1yZp8HQry2k4Aveg6d2zUecVss.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf85a599b289c4ca5c54a1bdb5cfe86f9639add4', 'width': 320, 'height': 96}, {'url': 'https://external-preview.redd.it/CZcLat1G57HQ2laiL1yZp8HQry2k4Aveg6d2zUecVss.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=acdd3d63aa80dcd914b7db35e68df4a506b56cb0', 'width': 640, 'height': 192}], 'variants': {}, 'id': 'Oy9XQnD0kbZKV8MluszAr5rc5pkygj6xIhZeXeFGkRs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lt9lg4,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lt9lg4/tom_cruise_deepfake_videos_are_all_over_the/,all_ads,False,/r/LatestInML/comments/lt94cb/tom_cruise_deepfake_videos_are_all_over_the/,7135,1614378523.0,0,,False,/r/LatestInML/comments/lt94cb/tom_cruise_deepfake_videos_are_all_over_the/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""Learn more about how this works: [link to paper and code](https://catalyzex.com/paper/arxiv:2005.05535)\n\nhttps://reddit.com/link/lt94cb/video/u5sfu79nawj61/player\n\n👇 Free extension to get code for ML papers (❤️' by Andrew Ng)  \nChrome: https://chrome.google.com/webstore/detail/find-code-for-research-pa/aikkeehnlfpamidigaffhfmgbkdeheil\n\nFirefox: https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex"", 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Tom Cruise deepfake videos are all over the internet and passing the best deepfake detectors!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 42, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'u5sfu79nawj61': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/lt94cb/asset/u5sfu79nawj61/DASHPlaylist.mpd?a=1618044117%2CMTBiYTE0YTJhZDcwMWE1OGJlZGUwMWFiOGQ5MTQxYjA5Y2U0ODY0NzYzY2ZlYzc1YWVmNTkzNGRhMDQxM2E1Mg%3D%3D&amp;v=1&amp;f=sd', 'x': 405, 'y': 720, 'hlsUrl': 'https://v.redd.it/link/lt94cb/asset/u5sfu79nawj61/HLSPlaylist.m3u8?a=1618044117%2CMjdjMjEyMzM4OGNmMGEzYzQ4ZTI4ODAwNjRiNTFjZWIwZGVkYjU4ZTkwOGY0MzEwZjJiMjFlMWQwMmRkYzU4Mw%3D%3D&amp;v=1&amp;f=sd', 'id': 'u5sfu79nawj61', 'isGif': False}}, 'name': 't3_lt94cb', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 48, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 48, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/wznT7cB7SlM6Ye_sJWF4lFYsuKvrDl6bXUgnWrY3qSI.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1614405976.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Learn more about how this works: &lt;a href=""https://catalyzex.com/paper/arxiv:2005.05535""&gt;link to paper and code&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/lt94cb/video/u5sfu79nawj61/player""&gt;https://reddit.com/link/lt94cb/video/u5sfu79nawj61/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;👇 Free extension to get code for ML papers (❤️&amp;#39; by Andrew Ng)&lt;br/&gt;\nChrome: &lt;a href=""https://chrome.google.com/webstore/detail/find-code-for-research-pa/aikkeehnlfpamidigaffhfmgbkdeheil""&gt;https://chrome.google.com/webstore/detail/find-code-for-research-pa/aikkeehnlfpamidigaffhfmgbkdeheil&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Firefox: &lt;a href=""https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex""&gt;https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/CZcLat1G57HQ2laiL1yZp8HQry2k4Aveg6d2zUecVss.jpg?auto=webp&amp;s=4f24997ef2c142e60dc77448de49d1dc4b1eee89', 'width': 816, 'height': 246}, 'resolutions': [{'url': 'https://external-preview.redd.it/CZcLat1G57HQ2laiL1yZp8HQry2k4Aveg6d2zUecVss.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb18b9ff11a399264b6b501fe2d796f1842cf531', 'width': 108, 'height': 32}, {'url': 'https://external-preview.redd.it/CZcLat1G57HQ2laiL1yZp8HQry2k4Aveg6d2zUecVss.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=da2566eef17424fd29022c2bd0acf057bdb46a0e', 'width': 216, 'height': 65}, {'url': 'https://external-preview.redd.it/CZcLat1G57HQ2laiL1yZp8HQry2k4Aveg6d2zUecVss.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf85a599b289c4ca5c54a1bdb5cfe86f9639add4', 'width': 320, 'height': 96}, {'url': 'https://external-preview.redd.it/CZcLat1G57HQ2laiL1yZp8HQry2k4Aveg6d2zUecVss.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=acdd3d63aa80dcd914b7db35e68df4a506b56cb0', 'width': 640, 'height': 192}], 'variants': {}, 'id': 'Oy9XQnD0kbZKV8MluszAr5rc5pkygj6xIhZeXeFGkRs'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'lt94cb', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/lt94cb/tom_cruise_deepfake_videos_are_all_over_the/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/lt94cb/tom_cruise_deepfake_videos_are_all_over_the/', 'subreddit_subscribers': 6676, 'created_utc': 1614377176.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_lt94cb,,,,,
29,,pytorch,"How can I use integer array indexing to get   elements from different columns? Lets say I wanted to grab 12,64,34; how can I get the elements?

  numbers = torch.tensor(\[\[2, 4, 8\], \[12, 16, 32\], \[64, 23, 3\], \[34, 56, 23\]\])",t2_409owhnf,False,,0,False,Element from a column,[],r/pytorch,False,6,,0,,,False,t3_lspv50,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1614342543.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How can I use integer array indexing to get   elements from different columns? Lets say I wanted to grab 12,64,34; how can I get the elements?&lt;/p&gt;

&lt;p&gt;numbers = torch.tensor([[2, 4, 8], [12, 16, 32], [64, 23, 3], [34, 56, 23]])&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lspv50,True,,destin95,,1,True,all_ads,False,[],False,,/r/pytorch/comments/lspv50/element_from_a_column/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lspv50/element_from_a_column/,7135,1614313743.0,0,,False,,,,,,,,
30,,pytorch,"Hi,

I built a model to predict a winning team in a game of dota. I wanted to evaluate each player (10 in total, each has 599 features encoded with 1 or 0) using the same criteria (player\_model). Then I wanted to predict the outcome of the match (match\_model) bases on the player evaluations of each team. (the commented variant includes an additional team evaluation)

The problem I have is that the model just favors (depending on the weight init) one outcome and does not converge at all.

I am not sure if the gradients are calculated correctly, when throwing the whole batch x player tensor in the player\_model like this: p = self.player\_model(x)

Or maybe something else screws it up...

Do you have any hints?

Thanks

Logs:

    torch.Size([64000, 10, 599]) torch.Size([64000, 2]) ---- Train set
    torch.Size([16000, 10, 599]) (16000, 2) ----- Test set
    accuracy_score: 0.5034375
    max_test_accuracy  0.5034375
    train 0.0006832404183223843
    eval  4.3321520090103147e-05
    [[   0 7945]
     [   0 8055]]
    accuracy_score: 0.5034375
    max_test_accuracy  0.5034375
    train 0.0006823675045743584
    eval  4.3321534991264343e-05
    [[   0 7945]
     [   0 8055]]
    accuracy_score: 0.5034375
    max_test_accuracy  0.5034375
    train 0.0006823821607977151
    eval  4.332022368907928e-05
    [[   0 7945]
     [   0 8055]]

Model:

        def __init__(self):
            super(Model, self).__init__()
    
            self.player_model = nn.Sequential(
                nn.Linear(feature_count, 128),
                nn.ReLU(),
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Linear(64, 32)
            )
    
            self.team_model = nn.Sequential(
                nn.Linear(32 * 5, 128),
                nn.ReLU(),
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Linear(64, 32)
            )
    
            self.match_model = nn.Sequential(
                nn.Linear(32 * 10, 8),
                nn.ReLU(),
                nn.Linear(8, 4),
                nn.ReLU(),
                nn.Linear(4, 2),
                nn.Softmax(dim=1)
            )
    
        def forward(self, x):
            p = self.player_model(x)
            # t1_in = p[:, :5, :].reshape(p.size(0), 5 * p.size(2))
            # t1 = self.team_model(t1_in)
    
            # t2_in = p[:, 5:, :].reshape(p.size(0), 5 * p.size(2))
            # t2 = self.team_model(t2_in)
    
            # m = torch.cat((t1, t2), 1)
            m = p.reshape(p.size(0), 10 * p.size(2))
    
            x = self.match_model(m)
    
            return x

Other

    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0004)",t2_3ngh3luk,False,,0,False,MLP Win Prediction Model does not converge,[],r/pytorch,False,6,,0,,,False,t3_lsigs0,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1614320170.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I built a model to predict a winning team in a game of dota. I wanted to evaluate each player (10 in total, each has 599 features encoded with 1 or 0) using the same criteria (player_model). Then I wanted to predict the outcome of the match (match_model) bases on the player evaluations of each team. (the commented variant includes an additional team evaluation)&lt;/p&gt;

&lt;p&gt;The problem I have is that the model just favors (depending on the weight init) one outcome and does not converge at all.&lt;/p&gt;

&lt;p&gt;I am not sure if the gradients are calculated correctly, when throwing the whole batch x player tensor in the player_model like this: p = self.player_model(x)&lt;/p&gt;

&lt;p&gt;Or maybe something else screws it up...&lt;/p&gt;

&lt;p&gt;Do you have any hints?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;

&lt;p&gt;Logs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;torch.Size([64000, 10, 599]) torch.Size([64000, 2]) ---- Train set
torch.Size([16000, 10, 599]) (16000, 2) ----- Test set
accuracy_score: 0.5034375
max_test_accuracy  0.5034375
train 0.0006832404183223843
eval  4.3321520090103147e-05
[[   0 7945]
 [   0 8055]]
accuracy_score: 0.5034375
max_test_accuracy  0.5034375
train 0.0006823675045743584
eval  4.3321534991264343e-05
[[   0 7945]
 [   0 8055]]
accuracy_score: 0.5034375
max_test_accuracy  0.5034375
train 0.0006823821607977151
eval  4.332022368907928e-05
[[   0 7945]
 [   0 8055]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    def __init__(self):
        super(Model, self).__init__()

        self.player_model = nn.Sequential(
            nn.Linear(feature_count, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32)
        )

        self.team_model = nn.Sequential(
            nn.Linear(32 * 5, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32)
        )

        self.match_model = nn.Sequential(
            nn.Linear(32 * 10, 8),
            nn.ReLU(),
            nn.Linear(8, 4),
            nn.ReLU(),
            nn.Linear(4, 2),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        p = self.player_model(x)
        # t1_in = p[:, :5, :].reshape(p.size(0), 5 * p.size(2))
        # t1 = self.team_model(t1_in)

        # t2_in = p[:, 5:, :].reshape(p.size(0), 5 * p.size(2))
        # t2 = self.team_model(t2_in)

        # m = torch.cat((t1, t2), 1)
        m = p.reshape(p.size(0), 10 * p.size(2))

        x = self.match_model(m)

        return x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Other&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0004)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lsigs0,True,,abholer0815,,2,True,all_ads,False,[],False,,/r/pytorch/comments/lsigs0/mlp_win_prediction_model_does_not_converge/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lsigs0/mlp_win_prediction_model_does_not_converge/,7135,1614291370.0,0,,False,,,,,,,,
31,,pytorch,"Hi,

I have an object detection dataset with RGB images and annotations in Json. I use a custom DataLoader class to read the images and the labels. One issue that I’m facing is that I would like to skip images when training my model if/when labels don’t contain certain objects.

For example, If one image doesn’t contain any target labels belonging to the class ‘Cars’, I would like to skip them. When parsing my Json annotation, I tried checking for labels that don’t contain the class ‘Cars’ and returned None. Subsequently, I used a collate function to filter the None but unfortunately, It is not working

    
    import torch
    from torch.utils.data.dataset import Dataset
    import json
    import os
    from PIL import Image
    from torchvision import transforms
    #import cv2
    import numpy as np
    general_classes = {
        # Cars
        ""Toyota Corolla"" : 0,
        ""VW Golf"" : 0,
        ""VW Beetle"" : 0,
    
        # Motor-cycles
        ""Harley Davidson"" : 1,
        ""Yamaha YZF-R6"" : 1,
    }
    
    car_classes={
    ""Toyota Corolla"" : 0,
    ""VW Golf"" : 0,
    ""VW Beetle"" : 0
    }
    
    def get_transform(train):
        transforms = []
        # converts the image, a PIL image, into a PyTorch Tensor
        transforms.append(T.ToTensor())
        if train:
            # during training, randomly flip the training images
            # and ground-truth for data augmentation
            transforms.append(T.RandomHorizontalFlip(0.5))
        return T.Compose(transforms)
    
    
    def my_collate(batch):
        batch = list(filter(lambda x: x is not None, batch))
        return torch.utils.data.dataloader.default_collate(batch)
    
    
    class FilteredDataset(Dataset):
        # The dataloader will skip the image and corresponding labels based on the dictionary 'car_classes'
        def __init__(self, data_dir, transforms):
            self.data_dir = data_dir
            img_folder_list = os.listdir(self.data_dir)
            self.transforms = transforms
    
            imgs_list = []
            json_list = []
            self.filter_count=0
            self.filtered_label_list=[]
    
            for img_path in img_folder_list:
                #img_full_path = self.data_dir + img_path
                img_full_path=os.path.join(self.data_dir,img_path)
                json_file = os.path.join(img_full_path, 'annotations-of-my-images.json')
                img_file = os.path.join(img_full_path, 'Image-Name.png')
    
                json_list.append(json_file)
                imgs_list.append(img_file)
            self.imgs = imgs_list
            self.annotations = json_list
            total_count=0
    
            for one_annotation in self.annotations:
                filtered_obj_id=[]
                with open(one_annotation) as f:
                    img_annotations = json.load(f)
    
                parts_list = img_annotations['regions']
                for part in parts_list:
                    current_obj_id = part['tags'][0] # bbox label 
                    check_obj_id = general_classes[current_obj_id]
                    if(check_obj_id==0):
                        subclass_id=car_classes[current_obj_id]
                        filtered_obj_id.append(subclass_id)
                        total_count=total_count+1
    
                if(len(filtered_obj_id)&gt;0):
                    self.filter_count=self.filter_count+1
                    self.filtered_label_list.append(one_annotation)
    
            print(""The total number of the objects in all images: "",total_count)
    
    
        # get one image and the bboxes,img_id, labels of parts, etc in the image as target.
        def __getitem__(self, idx):
    
            img_path = self.imgs[idx]
            image_id = torch.tensor([idx])
            
            with open(self.annotations[idx]) as f:
                img_annotations = json.load(f)
            parts_list = img_annotations['regions']
            obj_ids = []
            boxes = []
            for part in parts_list:
                obj_id = part['tags'][0]
                check_obj_id = general_classes[obj_id]
                if(check_obj_id==0):
                   obj_id=car_classes[obj_id]
                   obj_ids.append(obj_id)
                    #print(""---------------------------------------------------"")
                    
            if(len(obj_ids)&gt;0):
                img = Image.open(img_path).convert(""RGB"")
                labels = torch.as_tensor(obj_ids, dtype = torch.int64)
                target = {}
                target['labels'] = labels
                
                if self.transforms is not None:
                    img, target = self.transforms(img, target)
                    return img, target
            else:
                return None
    
    
        def __len__(self):
            return len(self.filtered_label_list)
    
    
    
    
    train_data_path = ""path-to-my-annotation""
    # Generators
    train_dataset = FilteredDataset(train_data_path,get_transform(train=True))
    print(""Total files in the train_dataset: "",len(train_dataset))
    #print(""The first instance in the train dataset : "",train_dataset[0])
    #training_generator = torch.utils.data.DataLoader(train_dataset)
    training_generator = torch.utils.data.DataLoader(train_dataset,collate_fn=my_collate)
    print(""\n\n Iterator in action! "")
    print(""---------------------------------------------------------"")
    count=0
    for img,target in training_generator:
        #print(""The img name : "",img[0])
        count=count+1
        print(""target name : "",target)
        print(""count : "",count)
        print(""**************************************************"")

However, I get the following error,

&amp;#x200B;

[Traceback that I get](https://preview.redd.it/hxsytz62wnj61.png?width=691&amp;format=png&amp;auto=webp&amp;s=633aa87dfa1d405cf853b93a1bab7b03e859d8b3)

&amp;#x200B;

Could anyone please suggest a way to skip the images that do not contain a particular categorical label?",t2_54wcclua,False,,0,False,How to skip the images in a custom dataset and deal with None values?,[],r/pytorch,False,6,,0,44.0,,False,t3_lsce7e,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/Ns6uupCoUwKqwVlkKGqf-nZtA-FCFyYGtu139dS7snc.jpg,False,,[],{},,,True,,1614304258.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have an object detection dataset with RGB images and annotations in Json. I use a custom DataLoader class to read the images and the labels. One issue that I’m facing is that I would like to skip images when training my model if/when labels don’t contain certain objects.&lt;/p&gt;

&lt;p&gt;For example, If one image doesn’t contain any target labels belonging to the class ‘Cars’, I would like to skip them. When parsing my Json annotation, I tried checking for labels that don’t contain the class ‘Cars’ and returned None. Subsequently, I used a collate function to filter the None but unfortunately, It is not working&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import torch
from torch.utils.data.dataset import Dataset
import json
import os
from PIL import Image
from torchvision import transforms
#import cv2
import numpy as np
general_classes = {
    # Cars
    &amp;quot;Toyota Corolla&amp;quot; : 0,
    &amp;quot;VW Golf&amp;quot; : 0,
    &amp;quot;VW Beetle&amp;quot; : 0,

    # Motor-cycles
    &amp;quot;Harley Davidson&amp;quot; : 1,
    &amp;quot;Yamaha YZF-R6&amp;quot; : 1,
}

car_classes={
&amp;quot;Toyota Corolla&amp;quot; : 0,
&amp;quot;VW Golf&amp;quot; : 0,
&amp;quot;VW Beetle&amp;quot; : 0
}

def get_transform(train):
    transforms = []
    # converts the image, a PIL image, into a PyTorch Tensor
    transforms.append(T.ToTensor())
    if train:
        # during training, randomly flip the training images
        # and ground-truth for data augmentation
        transforms.append(T.RandomHorizontalFlip(0.5))
    return T.Compose(transforms)


def my_collate(batch):
    batch = list(filter(lambda x: x is not None, batch))
    return torch.utils.data.dataloader.default_collate(batch)


class FilteredDataset(Dataset):
    # The dataloader will skip the image and corresponding labels based on the dictionary &amp;#39;car_classes&amp;#39;
    def __init__(self, data_dir, transforms):
        self.data_dir = data_dir
        img_folder_list = os.listdir(self.data_dir)
        self.transforms = transforms

        imgs_list = []
        json_list = []
        self.filter_count=0
        self.filtered_label_list=[]

        for img_path in img_folder_list:
            #img_full_path = self.data_dir + img_path
            img_full_path=os.path.join(self.data_dir,img_path)
            json_file = os.path.join(img_full_path, &amp;#39;annotations-of-my-images.json&amp;#39;)
            img_file = os.path.join(img_full_path, &amp;#39;Image-Name.png&amp;#39;)

            json_list.append(json_file)
            imgs_list.append(img_file)
        self.imgs = imgs_list
        self.annotations = json_list
        total_count=0

        for one_annotation in self.annotations:
            filtered_obj_id=[]
            with open(one_annotation) as f:
                img_annotations = json.load(f)

            parts_list = img_annotations[&amp;#39;regions&amp;#39;]
            for part in parts_list:
                current_obj_id = part[&amp;#39;tags&amp;#39;][0] # bbox label 
                check_obj_id = general_classes[current_obj_id]
                if(check_obj_id==0):
                    subclass_id=car_classes[current_obj_id]
                    filtered_obj_id.append(subclass_id)
                    total_count=total_count+1

            if(len(filtered_obj_id)&amp;gt;0):
                self.filter_count=self.filter_count+1
                self.filtered_label_list.append(one_annotation)

        print(&amp;quot;The total number of the objects in all images: &amp;quot;,total_count)


    # get one image and the bboxes,img_id, labels of parts, etc in the image as target.
    def __getitem__(self, idx):

        img_path = self.imgs[idx]
        image_id = torch.tensor([idx])

        with open(self.annotations[idx]) as f:
            img_annotations = json.load(f)
        parts_list = img_annotations[&amp;#39;regions&amp;#39;]
        obj_ids = []
        boxes = []
        for part in parts_list:
            obj_id = part[&amp;#39;tags&amp;#39;][0]
            check_obj_id = general_classes[obj_id]
            if(check_obj_id==0):
               obj_id=car_classes[obj_id]
               obj_ids.append(obj_id)
                #print(&amp;quot;---------------------------------------------------&amp;quot;)

        if(len(obj_ids)&amp;gt;0):
            img = Image.open(img_path).convert(&amp;quot;RGB&amp;quot;)
            labels = torch.as_tensor(obj_ids, dtype = torch.int64)
            target = {}
            target[&amp;#39;labels&amp;#39;] = labels

            if self.transforms is not None:
                img, target = self.transforms(img, target)
                return img, target
        else:
            return None


    def __len__(self):
        return len(self.filtered_label_list)




train_data_path = &amp;quot;path-to-my-annotation&amp;quot;
# Generators
train_dataset = FilteredDataset(train_data_path,get_transform(train=True))
print(&amp;quot;Total files in the train_dataset: &amp;quot;,len(train_dataset))
#print(&amp;quot;The first instance in the train dataset : &amp;quot;,train_dataset[0])
#training_generator = torch.utils.data.DataLoader(train_dataset)
training_generator = torch.utils.data.DataLoader(train_dataset,collate_fn=my_collate)
print(&amp;quot;\n\n Iterator in action! &amp;quot;)
print(&amp;quot;---------------------------------------------------------&amp;quot;)
count=0
for img,target in training_generator:
    #print(&amp;quot;The img name : &amp;quot;,img[0])
    count=count+1
    print(&amp;quot;target name : &amp;quot;,target)
    print(&amp;quot;count : &amp;quot;,count)
    print(&amp;quot;**************************************************&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, I get the following error,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/hxsytz62wnj61.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=633aa87dfa1d405cf853b93a1bab7b03e859d8b3""&gt;Traceback that I get&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Could anyone please suggest a way to skip the images that do not contain a particular categorical label?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lsce7e,True,,Conscious-Elk,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lsce7e/how_to_skip_the_images_in_a_custom_dataset_and/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lsce7e/how_to_skip_the_images_in_a_custom_dataset_and/,7135,1614275458.0,0,,False,,,,"{'hxsytz62wnj61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 34, 'x': 108, 'u': 'https://preview.redd.it/hxsytz62wnj61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=60959b6e5b88893f1b4027be51cd414a46c45c81'}, {'y': 68, 'x': 216, 'u': 'https://preview.redd.it/hxsytz62wnj61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=81e7c2a10cde8720f2276823b7ec6d075efb61c7'}, {'y': 100, 'x': 320, 'u': 'https://preview.redd.it/hxsytz62wnj61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=760cafdabae57413442b5b216ada72218983a9d6'}, {'y': 201, 'x': 640, 'u': 'https://preview.redd.it/hxsytz62wnj61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b198b2b130c8bab8e1bcbfea2c0ab7c71e0dcf0'}], 's': {'y': 218, 'x': 691, 'u': 'https://preview.redd.it/hxsytz62wnj61.png?width=691&amp;format=png&amp;auto=webp&amp;s=633aa87dfa1d405cf853b93a1bab7b03e859d8b3'}, 'id': 'hxsytz62wnj61'}}",,,,
32,,pytorch,"How do I get the rows 0 and 2  and a columns 1 and 4 from a tensor  below ?

b= torch.tensor(\[\[2, 6, 12, 18, 20\], \[3, 9, 12, 24, 15\], \[14, 15, 16, 19, 25\]\])",t2_409owhnf,False,,0,False,Slicing a tensor,[],r/pytorch,False,6,,0,,,False,t3_lrx0oq,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1614224244.0,,[],{},,,True,,1614252471.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do I get the rows 0 and 2  and a columns 1 and 4 from a tensor  below ?&lt;/p&gt;

&lt;p&gt;b= torch.tensor([[2, 6, 12, 18, 20], [3, 9, 12, 24, 15], [14, 15, 16, 19, 25]])&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lrx0oq,True,,destin95,,7,True,all_ads,False,[],False,,/r/pytorch/comments/lrx0oq/slicing_a_tensor/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lrx0oq/slicing_a_tensor/,7135,1614223671.0,0,,False,,,,,,,,
33,,pytorch,"Including torch/torch.h makes builds times unbearable, at least combined with Eigen, faiss, etc. 

The problem is, I really need torch::Tensor, as it is an enormously useful container type I'd like to use in many files.

Anybody know any good fix for this? I tried precompiled headers, but could not get it to work.",t2_qa2k7,False,,0,False,Faster builds using libtorch c++ (question),[],r/pytorch,False,6,,0,,,False,t3_lrlazm,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1614223993.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Including torch/torch.h makes builds times unbearable, at least combined with Eigen, faiss, etc. &lt;/p&gt;

&lt;p&gt;The problem is, I really need torch::Tensor, as it is an enormously useful container type I&amp;#39;d like to use in many files.&lt;/p&gt;

&lt;p&gt;Anybody know any good fix for this? I tried precompiled headers, but could not get it to work.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lrlazm,True,,terrrp,,1,False,all_ads,False,[],False,,/r/pytorch/comments/lrlazm/faster_builds_using_libtorch_c_question/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lrlazm/faster_builds_using_libtorch_c_question/,7135,1614195193.0,0,,False,,,,,,,,
34,,pytorch,"Hey everyone,

I'm just starting with pytorch and though a simple NN to take in an array of 0's and 1's and output the number of ones would be good just to see how everything works, I have this data (input, target):

data = ((\[0, 0, 1, 0, 1\], \[0, 0, 1, 0, 0, 0\]),(\[0, 1, 1, 0, 1\], \[0, 0, 0, 1, 0, 0\]),(\[1, 0, 1, 0, 0\], \[0, 0, 1, 0, 0, 0\]),(\[1, 1, 1, 1, 1\], \[0, 0, 0, 0, 0, 1\]),(\[1, 1, 1, 0, 1\], \[0, 0, 0, 0, 1, 0\]),(\[0, 0, 0, 0, 0\], \[1, 0, 0, 0, 0, 0\]),(\[1, 0, 1, 1, 0\], \[0, 0, 0, 1, 0, 0\]),(\[1, 1, 1, 1, 1\], \[0, 0, 0, 0, 0, 1\]),(\[1, 0, 0, 0, 1\], \[0, 0, 1, 0, 0, 0\]),)

could anyone please spin up a really quick network to get this working? I've tried but I'm struggling implementing it myself, would just like to see how it could be done, im struggling with the loss functions specifically!

&amp;#x200B;

any help is appreciated!

so I think I managed to get it working, it's very hacky (yes I know the data creation is terrible)

repo: [https://github.com/Torbet/Pytorch-Linear-Regression](https://github.com/Torbet/Pytorch-Linear-Regression)",t2_408liqov,False,,0,False,Beginner | Simple NN to find occurrences in array,[],r/pytorch,False,6,,0,,,False,t3_lrkn23,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,1614197653.0,,[],{},self,,True,,1614222287.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m just starting with pytorch and though a simple NN to take in an array of 0&amp;#39;s and 1&amp;#39;s and output the number of ones would be good just to see how everything works, I have this data (input, target):&lt;/p&gt;

&lt;p&gt;data = (([0, 0, 1, 0, 1], [0, 0, 1, 0, 0, 0]),([0, 1, 1, 0, 1], [0, 0, 0, 1, 0, 0]),([1, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0]),([1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 1]),([1, 1, 1, 0, 1], [0, 0, 0, 0, 1, 0]),([0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]),([1, 0, 1, 1, 0], [0, 0, 0, 1, 0, 0]),([1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 1]),([1, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0]),)&lt;/p&gt;

&lt;p&gt;could anyone please spin up a really quick network to get this working? I&amp;#39;ve tried but I&amp;#39;m struggling implementing it myself, would just like to see how it could be done, im struggling with the loss functions specifically!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;any help is appreciated!&lt;/p&gt;

&lt;p&gt;so I think I managed to get it working, it&amp;#39;s very hacky (yes I know the data creation is terrible)&lt;/p&gt;

&lt;p&gt;repo: &lt;a href=""https://github.com/Torbet/Pytorch-Linear-Regression""&gt;https://github.com/Torbet/Pytorch-Linear-Regression&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/yO-Dm4SH9VpsETyZh-KimlAPRegsho9amxbs6gvZLXs.jpg?auto=webp&amp;s=91ab78376403a513242e3471d2625e158368c3dd', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/yO-Dm4SH9VpsETyZh-KimlAPRegsho9amxbs6gvZLXs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2afbe81601e1c5fb8e625e80a8b60e39772fa128', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/yO-Dm4SH9VpsETyZh-KimlAPRegsho9amxbs6gvZLXs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab60dff6d1de373e03af25c3a78db4238ebc91c2', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/yO-Dm4SH9VpsETyZh-KimlAPRegsho9amxbs6gvZLXs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8e5f58d9c1ee56022ee52ce84bd2fcc8206df999', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'GYYlryL1KCHm0feo-c3UKQJ-sL8Y6iG9zmdTyI5UUlQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lrkn23,True,,GuyTorbet,,17,True,all_ads,False,[],False,,/r/pytorch/comments/lrkn23/beginner_simple_nn_to_find_occurrences_in_array/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lrkn23/beginner_simple_nn_to_find_occurrences_in_array/,7135,1614193487.0,0,,False,,,,,,,,
35,,pytorch,,t2_766u1eio,False,,0,False,Is the higher level library for meta-learning compatible with pytorch's distributed libraries?,[],r/pytorch,False,6,,0,140.0,,False,t3_lqpeie,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/eBBjc2VCGdJbJncsjjIMI8T6QwE0Qtl0l0Au6E87amQ.jpg,False,,[],{},link,,False,,1614133942.0,text,6,,,text,discuss.pytorch.org,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lqpeie,True,,No_Ad3397,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lqpeie/is_the_higher_level_library_for_metalearning/,all_ads,False,https://discuss.pytorch.org/t/is-the-higher-level-library-for-meta-learning-compatible-with-pytorchs-distributed-libraries/112753,7135,1614105142.0,0,,False,https://discuss.pytorch.org/t/is-the-higher-level-library-for-meta-learning-compatible-with-pytorchs-distributed-libraries/112753,,,,,,,
36,,pytorch,"I am training a RL model that has a heavy simulation component within the training loop. While training, I want to be able to ""step aside"" and run a couple of validation simulations on a separate data set.

Since the validation set should never update the weights, I was wondering if it's possible to freeze the PyTorch model, pass it to a background thread, and run the validation data set in the background.

I've come across the `torch.multiprocessing` and Python `multiprocessing`, but I am unsure which would be best suited to this use. Looking at the [PyTorch Multiprocessing Best Practices](https://pytorch.org/docs/stable/notes/multiprocessing.html#), it seems a direct handle to the current model is passed to the background thread. While this is good for asynchronous learning across threads, I don't want the model to update as the validation data is being run.

My first thought is to build a standard Python function to accept a deep copied PyTorch model and run that background dataset completely independently (using Python's native `multiprocessing` module). Does anyone have any recommendations on how to go about this?",t2_43i2kgo3,False,,0,False,"Train on main thread, validation on background thread?",[],r/pytorch,False,6,,0,,,False,t3_lqlheq,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1614124187.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am training a RL model that has a heavy simulation component within the training loop. While training, I want to be able to &amp;quot;step aside&amp;quot; and run a couple of validation simulations on a separate data set.&lt;/p&gt;

&lt;p&gt;Since the validation set should never update the weights, I was wondering if it&amp;#39;s possible to freeze the PyTorch model, pass it to a background thread, and run the validation data set in the background.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve come across the &lt;code&gt;torch.multiprocessing&lt;/code&gt; and Python &lt;code&gt;multiprocessing&lt;/code&gt;, but I am unsure which would be best suited to this use. Looking at the &lt;a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html#""&gt;PyTorch Multiprocessing Best Practices&lt;/a&gt;, it seems a direct handle to the current model is passed to the background thread. While this is good for asynchronous learning across threads, I don&amp;#39;t want the model to update as the validation data is being run.&lt;/p&gt;

&lt;p&gt;My first thought is to build a standard Python function to accept a deep copied PyTorch model and run that background dataset completely independently (using Python&amp;#39;s native &lt;code&gt;multiprocessing&lt;/code&gt; module). Does anyone have any recommendations on how to go about this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lqlheq,True,,GrandpaYeti,,4,True,all_ads,False,[],False,,/r/pytorch/comments/lqlheq/train_on_main_thread_validation_on_background/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lqlheq/train_on_main_thread_validation_on_background/,7135,1614095387.0,0,,False,,,,,,,,
37,,pytorch,"Specifically, how does it not create problems with the derivative of the function since a quantizing function is a step-like linear with grad=0. How is it possible to round decimals without messing with backpropagation?",t2_9c0pqtaw,False,,0,False,How does Quantize per tensor work in relation with gradient??,[],r/pytorch,False,6,,0,,,False,t3_lqi315,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1614114663.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Specifically, how does it not create problems with the derivative of the function since a quantizing function is a step-like linear with grad=0. How is it possible to round decimals without messing with backpropagation?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lqi315,True,,ChunkyFunkyFloater,,4,True,all_ads,False,[],False,,/r/pytorch/comments/lqi315/how_does_quantize_per_tensor_work_in_relation/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lqi315/how_does_quantize_per_tensor_work_in_relation/,7135,1614085863.0,0,,False,,,,,,,,
38,,pytorch,,t2_766u1eio,False,,0,False,"Why does my pytorch rpc workers deadlock, is it because I am using main as my master?",[],r/pytorch,False,6,,0,140.0,,False,t3_lq0gu7,False,dark,0.75,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://a.thumbs.redditmedia.com/tDHjCjKCBGn8zIFCBzQjpOZrAewJ6eYrXLAQnmFwp94.jpg,False,,[],{},link,,False,,1614058098.0,text,6,,,text,discuss.pytorch.org,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lq0gu7,True,,No_Ad3397,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lq0gu7/why_does_my_pytorch_rpc_workers_deadlock_is_it/,all_ads,False,https://discuss.pytorch.org/t/why-does-my-pytorch-rpc-workers-deadlock-is-it-because-i-am-using-main-as-my-master/112648,7135,1614029298.0,0,,False,https://discuss.pytorch.org/t/why-does-my-pytorch-rpc-workers-deadlock-is-it-because-i-am-using-main-as-my-master/112648,,,,,,,
39,,pytorch,"I'm trying to train the mask RCNN on custom data but I get Nans as loss values in the first step itself. 

{'loss\_classifier': tensor(nan, device='cuda:0', grad\_fn=&lt;NllLossBackward&gt;), 'loss\_box\_reg': tensor(nan, device='cuda:0', grad\_fn=&lt;DivBackward0&gt;), 'loss\_mask': tensor(-1.1146e+30, device='cuda:0',        grad\_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;), 'loss\_objectness': tensor(574.7335, device='cuda:0',        grad\_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;), 'loss\_rpn\_box\_reg': tensor(169.8945, device='cuda:0', grad\_fn=&lt;DivBackward0&gt;)}

The images have 3 channels and the mask input is of the dimension \[N,H,W\]. What can cause the loss to explode?",t2_2o7sl64r,False,,0,False,Nan LOSS while training Mask RCNN on custom data,[],r/pytorch,False,6,,0,,,False,t3_lplyvb,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1614022604.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to train the mask RCNN on custom data but I get Nans as loss values in the first step itself. &lt;/p&gt;

&lt;p&gt;{&amp;#39;loss_classifier&amp;#39;: tensor(nan, device=&amp;#39;cuda:0&amp;#39;, grad_fn=&amp;lt;NllLossBackward&amp;gt;), &amp;#39;loss_box_reg&amp;#39;: tensor(nan, device=&amp;#39;cuda:0&amp;#39;, grad_fn=&amp;lt;DivBackward0&amp;gt;), &amp;#39;loss_mask&amp;#39;: tensor(-1.1146e+30, device=&amp;#39;cuda:0&amp;#39;,        grad_fn=&amp;lt;BinaryCrossEntropyWithLogitsBackward&amp;gt;), &amp;#39;loss_objectness&amp;#39;: tensor(574.7335, device=&amp;#39;cuda:0&amp;#39;,        grad_fn=&amp;lt;BinaryCrossEntropyWithLogitsBackward&amp;gt;), &amp;#39;loss_rpn_box_reg&amp;#39;: tensor(169.8945, device=&amp;#39;cuda:0&amp;#39;, grad_fn=&amp;lt;DivBackward0&amp;gt;)}&lt;/p&gt;

&lt;p&gt;The images have 3 channels and the mask input is of the dimension [N,H,W]. What can cause the loss to explode?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lplyvb,True,,raptorAK27,,3,True,all_ads,False,[],False,,/r/pytorch/comments/lplyvb/nan_loss_while_training_mask_rcnn_on_custom_data/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lplyvb/nan_loss_while_training_mask_rcnn_on_custom_data/,7135,1613993804.0,0,,False,,,,,,,,
40,,pytorch,,t2_4xhrybaz,False,,0,False,Is nn.Parameter learnable?,[],r/pytorch,False,6,,0,,,False,t3_lpfwz2,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1613999951.0,text,6,,,text,self.pytorch,False,,,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lpfwz2,True,,banenvy,,2,True,all_ads,False,[],False,,/r/pytorch/comments/lpfwz2/is_nnparameter_learnable/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lpfwz2/is_nnparameter_learnable/,7135,1613971151.0,0,,False,,,,,,,,
41,,pytorch,,t2_a7i59xms,False,,0,False,torch.nn.Embedding explained (+ Character-level language model),[],r/pytorch,False,6,,0,105.0,,False,t3_lp6467,False,dark,0.84,,public,9,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/euwN5DHfLEo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'torch.nn.Embedding explained (+ Character-level language model)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/euwN5DHfLEo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'mildlyoverfitted', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/euwN5DHfLEo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYBSjwkGTK06NnDnFsOcR7g'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/euwN5DHfLEo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lp6467', 'height': 200}",,False,9,,False,https://b.thumbs.redditmedia.com/n66q4CktXCv8tp1WVn-M0N7mD2D0Nlsyrx0TY9jIzEw.jpg,False,,[],{},rich:video,,False,,1613968883.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/I0vrc84LNgZo1KtYAK2LHI0MXFJmW0WcQbxumGubQUE.jpg?auto=webp&amp;s=37752f4f5397554bf47b1da5702afd9db5e0fe8f', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/I0vrc84LNgZo1KtYAK2LHI0MXFJmW0WcQbxumGubQUE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=112df77fe2f62cc52d842e2863cba9379f62a590', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/I0vrc84LNgZo1KtYAK2LHI0MXFJmW0WcQbxumGubQUE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ecf4a35272bd76ffc004bcf7040aefced45ef6d', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/I0vrc84LNgZo1KtYAK2LHI0MXFJmW0WcQbxumGubQUE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=47b49ac9c4e11cd8cd001f36e7b75a75c4ee3dac', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'L-5Gq7rcAFZ3CdcfNdrFH4rvBBb64NLcElZJn6UQ5a0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lp6467,True,,mildlyoverfitted,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lp6467/torchnnembedding_explained_characterlevel/,all_ads,False,https://youtu.be/euwN5DHfLEo,7135,1613940083.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'torch.nn.Embedding explained (+ Character-level language model)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/euwN5DHfLEo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'mildlyoverfitted', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/euwN5DHfLEo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYBSjwkGTK06NnDnFsOcR7g'}}",False,https://youtu.be/euwN5DHfLEo,,,,,,,
42,,pytorch,"I have defined a simple autoencoder using DGL (PyTorch backend). The code looks like this:

`from dgl.nn import GraphConv`  
`class AEGCN(nn.Module):`  
 `def __init__(self, in_feats, hidden_size, num_classes):`  
 `super(AEGCN, self).__init__()`  
 `self.conv1 = GraphConv(in_feats, hidden_size)`  
 `self.conv2 = GraphConv(hidden_size, in_feats)`  
 `def forward(self, g, inputs):`  
 `h = self.conv1(g, inputs)`  
 `h = torch.relu(h)`  
 `h = self.conv2(g, h)`  
 `return h`  
`net = AEGCN(192, 20, 192)`

&amp;#x200B;

I have trained it. How, I intend to see the output of conv1. How do I do that? Also, if I add multiple layers in the encoder part, how do I get the output of those layers of just those layers?",t2_oj2leuw,False,,0,False,How do I visualize the output from the encoder in an autoencoder model?,[],r/pytorch,False,6,,0,,,False,t3_lpi7rk,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1614008270.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have defined a simple autoencoder using DGL (PyTorch backend). The code looks like this:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;from dgl.nn import GraphConv&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;class AEGCN(nn.Module):&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;def __init__(self, in_feats, hidden_size, num_classes):&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;super(AEGCN, self).__init__()&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;self.conv1 = GraphConv(in_feats, hidden_size)&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;self.conv2 = GraphConv(hidden_size, in_feats)&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;def forward(self, g, inputs):&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;h = self.conv1(g, inputs)&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;h = torch.relu(h)&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;h = self.conv2(g, h)&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;return h&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;net = AEGCN(192, 20, 192)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I have trained it. How, I intend to see the output of conv1. How do I do that? Also, if I add multiple layers in the encoder part, how do I get the output of those layers of just those layers?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lpi7rk,True,,l34df4rm3r,,3,True,all_ads,False,[],False,,/r/pytorch/comments/lpi7rk/how_do_i_visualize_the_output_from_the_encoder_in/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lpi7rk/how_do_i_visualize_the_output_from_the_encoder_in/,7135,1613979470.0,0,,False,,,,,,,,
43,,pytorch,"Hi, Im trying to implement visualizations from ZFNet paper, but i dont know how to do deconvolution.
Any advice? :)",t2_6jp3bf9q,False,,0,False,Deconvolution operation in PyTorch,[],r/pytorch,False,6,,0,,,False,t3_lpheu9,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1614005358.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, Im trying to implement visualizations from ZFNet paper, but i dont know how to do deconvolution.
Any advice? :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lpheu9,True,,PytonRzeczny,,6,True,all_ads,False,[],False,,/r/pytorch/comments/lpheu9/deconvolution_operation_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lpheu9/deconvolution_operation_in_pytorch/,7135,1613976558.0,0,,False,,,,,,,,
44,,pytorch,"Hello all. New to PyTorch and ML in general.

My end goal right now is to use PyTorch and RL specifically to calculate the movement of a robotic arm to a given target location. To take some small steps towards my end goal, I’m starting off with a single link arm in a 2D environment which will try and point towards a goal position.

To reiterate, I am brand new to machine learning but do have over a decade of programming experience.

I have followed this basic tutorial [here](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) to get the infamous CartPole model running from OpenAI. After getting that all running on my computer I began to try and modify the CartPole env source code from [here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) and the PyTorch example to get to my “2D Single link arm” environment.

I have gotten this modified environment to run as you can see below:

https://preview.redd.it/y2vu5vkqvvi61.png?width=2682&amp;format=png&amp;auto=webp&amp;s=b62e643654f8b61c4bc4eaa1a6465a30af234f00

&amp;#x200B;

https://preview.redd.it/4fbcktpn91j61.png?width=2696&amp;format=png&amp;auto=webp&amp;s=fed27980876ade082ceecfaa7d886401d750268a

I'm not sure that I am using the env state and reward calculations correctly as the model doesn't seen to converge on a good reward in training. I'm also not sure if I'm making the training loop correct, although the only thing I've change so far in the PyTorch tutorial is the `get_screen` function.

I would love some input on the code I've written as I'm not sure where to go from here.

I am attaching the code to the modified PyTorch example and Env.

armenv.py:

    import gym
    from gym import spaces
    from gym.utils import seeding
    import numpy as np
    
    MAX_STEPS=200
    STEPS_ON_GOAL_TO_FINISH=10
    
    class ArmEnv(gym.Env):
      """"""Arm Environment that follows gym interface""""""
      metadata = {
          'render.modes': ['human', 'rgb_array'],
          'video.frames_per_second': 50
      }
    
      def __init__(self):
        self.length = 1.0  # length of arm
        self.goal = [2.,2.]
        self.tau = 0.02  # seconds between state updates
        self.theta_adj = 2.0
    
        self.angle_difference_threshold = 0.5
    
        # The max and min values that can be observed
        high = np.array([np.pi/2, np.pi, 1.8, 2.1], dtype=np.float32)
        low = np.array([-np.pi/2, 0, -1.8, 1.1], dtype=np.float32)
    
        self.action_space = spaces.Discrete(3)
        self.observation_space = spaces.Box(low, high, dtype=np.float32)
    
        self.on_goal = 0
        self.current_step = 0
    
        self.seed()
        self.viewer = None
        self.state = None
    
      def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]
    
      def calc_angle_difference(self, theta):
        # Get state of arm
        arm_mag = self.length
        arm_vector = np.array([arm_mag * np.sin(theta), arm_mag * np.cos(theta)])
    
        # Get distance vector between the goal and end of arm
        distance_vector = np.array([self.goal[0] - arm_vector[0], self.goal[1] - arm_vector[1]])
        distance_mag = np.sqrt(distance_vector[0]**2 + distance_vector[1]**2)
    
        # Get the angle between this distance vector and the arm vector
        return np.arccos(np.dot(arm_vector, distance_vector)/(arm_mag*distance_mag), dtype=np.float32)
    
      def step(self, action):
        err_msg = ""%r (%s) invalid"" % (action, type(action))
        assert self.action_space.contains(action), err_msg
    
        self.current_step += 1
        theta, _, goalx, goaly = self.state
    
        # Adjust theta based on the chosen action
        # if action == 1:
        #   theta_adj = self.theta_adj
        # else:
        #   theta_adj = -self.theta_adj
        if action == 1:
          theta_adj = self.theta_adj
        elif action == 2:
          theta_adj = -self.theta_adj
        else:
          theta_adj = 0
    
        theta += theta_adj * self.tau
        theta = max(min(theta, np.pi/2), -np.pi/2)
      
        # Get the angle between this distance vector and the arm vector
        angle_difference = self.calc_angle_difference(theta)
    
        self.state = (theta, angle_difference, goalx, goaly)
    
        # delay_modifier = float(self.current_step / MAX_STEPS)
        # r = float(1 - angle_difference*2)
        # r = np.exp(-angle_difference, dtype=np.float32)
        r = np.exp(-angle_difference*3, dtype=np.float32)
        # r = np.exp(-angle_difference, dtype=np.float32) * delay_modifier
        # r = np.exp(-angle_difference, dtype=np.float32) * (1.0 - delay_modifier)
    
        if theta &gt;= np.pi/2 or theta &lt;= -np.pi/2:
          r = 0.0 # Baaaad boi
    
        if angle_difference &lt;= self.angle_difference_threshold and \
          angle_difference &gt;= -self.angle_difference_threshold:
          self.on_goal += 1
          r = 1.0 # Goooood boi
        else:
          self.on_goal = self.on_goal - 2 if self.on_goal &gt; 0 else 0
    
        done = bool(self.on_goal &gt;= STEPS_ON_GOAL_TO_FINISH or
          self.current_step &gt;= MAX_STEPS or
          theta &gt;= np.pi/2 or
          theta &lt;= -np.pi/2
        )
    
        print(self.current_step, np.array(self.state), r, self.on_goal, action)
        return np.array(self.state), float(r), done, {}
    
      def reset(self):
        self.goal = np.array([self.np_random.rand() * 3.6 - 1.8, self.np_random.rand() + 1.1])
        self.on_goal = 0
        self.current_step = 0
    
        new_theta = self.np_random.rand()*np.pi - np.pi/2
        new_angle_difference = self.calc_angle_difference(new_theta)
    
        self.state = np.array([new_theta, new_angle_difference, *self.goal], dtype=np.float32)
        return np.array(self.state)
    
      def render(self, mode='human'):
          screen_width = 600
          screen_height = 400
    
          world_width = self.length * 4
          scale = screen_width/world_width
          polewidth = 10.0
          polelen = scale * (self.length)
          goalwidth = 15.0
          goalheight = 15.0
    
          if self.viewer is None:
              from gym.envs.classic_control import rendering
              self.viewer = rendering.Viewer(screen_width, screen_height)
              l, r, t, b = -goalwidth / 2, goalwidth / 2, goalheight / 2, -goalheight / 2
              goal = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
              self.goaltrans = rendering.Transform(translation=(self.goal[0] * scale + screen_width / 2.0, self.goal[0] * scale))
              goal.add_attr(self.goaltrans)
              self.viewer.add_geom(goal)
              l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
              pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
              pole.set_color(.8, .6, .4)
              self.poletrans = rendering.Transform(translation=(screen_width / 2.0, 0))
              pole.add_attr(self.poletrans)
              self.viewer.add_geom(pole)
              self._pole_geom = pole
    
          if self.state is None:
              return None
    
          # Edit the pole polygon vertex
          pole = self._pole_geom
          l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
          pole.v = [(l, b), (l, t), (r, t), (r, b)]
    
          x = self.state
          self.goaltrans.set_translation(self.goal[0] * scale + screen_width / 2.0, self.goal[1] * scale)
          self.poletrans.set_rotation(-x[0])
    
          return self.viewer.render(return_rgb_array=mode == 'rgb_array')
    
      def close(self):
          if self.viewer:
              self.viewer.close()
              self.viewer = None

main.py:

    from armenv import ArmEnv
    import math
    import random
    import numpy as np
    import matplotlib
    import matplotlib.pyplot as plt
    from collections import namedtuple
    from itertools import count
    from PIL import Image
    
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    import torchvision.transforms as T
    
    env = ArmEnv()
    
    # set up matplotlib
    is_ipython = 'inline' in matplotlib.get_backend()
    if is_ipython:
        from IPython import display
    
    plt.ion()
    
    # if gpu is to be used
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
    
    Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))
    
    
    class ReplayMemory(object):
      def __init__(self, capacity):
          self.capacity = capacity
          self.memory = []
          self.position = 0
    
      def push(self, *args):
          """"""Saves a transition.""""""
          if len(self.memory) &lt; self.capacity:
              self.memory.append(None)
          self.memory[self.position] = Transition(*args)
          self.position = (self.position + 1) % self.capacity
    
      def sample(self, batch_size):
          return random.sample(self.memory, batch_size)
    
      def __len__(self):
          return len(self.memory)
    
    class DQN(nn.Module):
    
      def __init__(self, h, w, outputs):
        super(DQN, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)
        self.bn1 = nn.BatchNorm2d(16)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)
        self.bn2 = nn.BatchNorm2d(32)
        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)
        self.bn3 = nn.BatchNorm2d(32)
    
        # Number of Linear input connections depends on output of conv2d layers
        # and therefore the input image size, so compute it.
        def conv2d_size_out(size, kernel_size = 5, stride = 2):
            return (size - (kernel_size - 1) - 1) // stride  + 1
        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))
        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))
        linear_input_size = convw * convh * 32
        self.head = nn.Linear(linear_input_size, outputs)
    
      # Called with either one element to determine next action, or a batch
      # during optimization. Returns tensor([[left0exp,right0exp]...]).
      def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))
        return self.head(x.view(x.size(0), -1))
    
    resize = T.Compose([T.ToPILImage(),
      T.Resize(40, interpolation=Image.CUBIC),
      T.ToTensor()])
    
    def get_screen():
      # Returned screen requested by gym is 400x600x3, but is sometimes larger
      # such as 800x1200x3. Transpose it into torch order (CHW).
      screen = env.render(mode='rgb_array').transpose((2, 0, 1))
      screen = np.ascontiguousarray(screen, dtype=np.float32) / 255
      screen = torch.from_numpy(screen)
      # Resize, and add a batch dimension (BCHW)
      return resize(screen).unsqueeze(0).to(device)
    
    env.reset()
    plt.figure()
    plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),
               interpolation='none')
    plt.title('Example extracted screen')
    plt.show()
    
    BATCH_SIZE = 128
    GAMMA = 0.999
    EPS_START = 0.9
    EPS_END = 0.05
    EPS_DECAY = 200
    TARGET_UPDATE = 10
    
    # Get screen size so that we can initialize layers correctly based on shape
    # returned from AI gym. Typical dimensions at this point are close to 3x40x90
    # which is the result of a clamped and down-scaled render buffer in get_screen()
    init_screen = get_screen()
    _, _, screen_height, screen_width = init_screen.shape
    
    # Get number of actions from gym action space
    n_actions = env.action_space.n
    
    policy_net = DQN(screen_height, screen_width, n_actions).to(device)
    target_net = DQN(screen_height, screen_width, n_actions).to(device)
    target_net.load_state_dict(policy_net.state_dict())
    target_net.eval()
    
    optimizer = optim.RMSprop(policy_net.parameters())
    memory = ReplayMemory(10000)
    
    steps_done = 0
    
    def select_action(state):
      global steps_done
      sample = random.random()
      eps_threshold = EPS_END + (EPS_START - EPS_END) * \
        math.exp(-1. * steps_done / EPS_DECAY)
      steps_done += 1
      if sample &gt; eps_threshold:
        with torch.no_grad():
          # t.max(1) will return largest column value of each row.
          # second column on max result is index of where max element was
          # found, so we pick action with the larger expected reward.
          return policy_net(state).max(1)[1].view(1, 1)
      else:
        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)
    
    episode_rewards = []
    
    def plot_rewards():
      plt.figure(2)
      plt.clf()
      rewards_t = torch.tensor(episode_rewards, dtype=torch.float)
      plt.title('Training...')
      plt.xlabel('Episode')
      plt.ylabel('Reward')
      plt.plot(rewards_t.numpy())
      # Take 100 episode averages and plot them too
      if len(rewards_t) &gt;= 10:
        means = rewards_t.unfold(0, 10, 1).mean(1).view(-1)
        means = torch.cat((torch.zeros(9), means))
        plt.plot(means.numpy())
    
      plt.pause(0.001)  # pause a bit so that plots are updated
      if is_ipython:
        display.clear_output(wait=True)
        display.display(plt.gcf())
    
    def optimize_model():
      if len(memory) &lt; BATCH_SIZE:
        return
      transitions = memory.sample(BATCH_SIZE)
      # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for
      # detailed explanation). This converts batch-array of Transitions
      # to Transition of batch-arrays.
      batch = Transition(*zip(*transitions))
    
      # Compute a mask of non-final states and concatenate the batch elements
      # (a final state would've been the one after which simulation ended)
      non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,
                                              batch.next_state)), device=device, dtype=torch.bool)
      non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
      state_batch = torch.cat(batch.state)
      action_batch = torch.cat(batch.action)
      reward_batch = torch.cat(batch.reward)
    
      # Compute Q(s_t, a) - the model computes Q(s_t), then we select the
      # columns of actions taken. These are the actions which would've been taken
      # for each batch state according to policy_net
      state_action_values = policy_net(state_batch).gather(1, action_batch)
    
      # Compute V(s_{t+1}) for all next states.
      # Expected values of actions for non_final_next_states are computed based
      # on the ""older"" target_net; selecting their best reward with max(1)[0].
      # This is merged based on the mask, such that we'll have either the expected
      # state value or 0 in case the state was final.
      next_state_values = torch.zeros(BATCH_SIZE, device=device)
      next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()
      # Compute the expected Q values
      expected_state_action_values = (next_state_values * GAMMA) + reward_batch
    
      # Compute Huber loss
      loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))
    
      # Optimize the model
      optimizer.zero_grad()
      loss.backward()
      for param in policy_net.parameters():
        param.grad.data.clamp_(-1, 1)
      optimizer.step()
    
    num_episodes = 400
    for i_episode in range(num_episodes):
      # Initialize the environment and state
      env.reset()
      last_screen = get_screen()
      current_screen = get_screen()
      state = current_screen - last_screen
      for t in count():
        # Select and perform an action
        action = select_action(state)
        _, reward, done, _ = env.step(action.item())
        reward = torch.tensor([reward], device=device)
    
        # Observe new state
        last_screen = current_screen
        current_screen = get_screen()
        if not done:
          next_state = current_screen - last_screen
        else:
          next_state = None
    
        # Store the transition in memory
        memory.push(state, action, next_state, reward)
    
        # Move to the next state
        state = next_state
    
        # Perform one step of the optimization (on the target network)
        optimize_model()
        if done:
          episode_rewards.append(reward)
          plot_rewards()
          break
      # Update the target network, copying all weights and biases in DQN
      if i_episode % TARGET_UPDATE == 0:
        target_net.load_state_dict(policy_net.state_dict())
    
    print('Complete')
    env.render()
    env.close()
    plt.ioff()
    plt.show()",t2_ixly3,False,,0,False,Newcomer to PyTorch in need of help,[],r/pytorch,False,6,,0,91.0,,False,t3_lp4sv0,False,dark,0.6,,public,1,1,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/tF02KB8hgpBAyXcRUZaGlF0AMigU7N_u-Q0lM7FqcEM.jpg,1614001527.0,,[],{},self,,True,,1613965301.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all. New to PyTorch and ML in general.&lt;/p&gt;

&lt;p&gt;My end goal right now is to use PyTorch and RL specifically to calculate the movement of a robotic arm to a given target location. To take some small steps towards my end goal, I’m starting off with a single link arm in a 2D environment which will try and point towards a goal position.&lt;/p&gt;

&lt;p&gt;To reiterate, I am brand new to machine learning but do have over a decade of programming experience.&lt;/p&gt;

&lt;p&gt;I have followed this basic tutorial &lt;a href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html""&gt;here&lt;/a&gt; to get the infamous CartPole model running from OpenAI. After getting that all running on my computer I began to try and modify the CartPole env source code from &lt;a href=""https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py""&gt;here&lt;/a&gt; and the PyTorch example to get to my “2D Single link arm” environment.&lt;/p&gt;

&lt;p&gt;I have gotten this modified environment to run as you can see below:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/y2vu5vkqvvi61.png?width=2682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b62e643654f8b61c4bc4eaa1a6465a30af234f00""&gt;https://preview.redd.it/y2vu5vkqvvi61.png?width=2682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b62e643654f8b61c4bc4eaa1a6465a30af234f00&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/4fbcktpn91j61.png?width=2696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fed27980876ade082ceecfaa7d886401d750268a""&gt;https://preview.redd.it/4fbcktpn91j61.png?width=2696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fed27980876ade082ceecfaa7d886401d750268a&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m not sure that I am using the env state and reward calculations correctly as the model doesn&amp;#39;t seen to converge on a good reward in training. I&amp;#39;m also not sure if I&amp;#39;m making the training loop correct, although the only thing I&amp;#39;ve change so far in the PyTorch tutorial is the &lt;code&gt;get_screen&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;I would love some input on the code I&amp;#39;ve written as I&amp;#39;m not sure where to go from here.&lt;/p&gt;

&lt;p&gt;I am attaching the code to the modified PyTorch example and Env.&lt;/p&gt;

&lt;p&gt;armenv.py:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import gym
from gym import spaces
from gym.utils import seeding
import numpy as np

MAX_STEPS=200
STEPS_ON_GOAL_TO_FINISH=10

class ArmEnv(gym.Env):
  &amp;quot;&amp;quot;&amp;quot;Arm Environment that follows gym interface&amp;quot;&amp;quot;&amp;quot;
  metadata = {
      &amp;#39;render.modes&amp;#39;: [&amp;#39;human&amp;#39;, &amp;#39;rgb_array&amp;#39;],
      &amp;#39;video.frames_per_second&amp;#39;: 50
  }

  def __init__(self):
    self.length = 1.0  # length of arm
    self.goal = [2.,2.]
    self.tau = 0.02  # seconds between state updates
    self.theta_adj = 2.0

    self.angle_difference_threshold = 0.5

    # The max and min values that can be observed
    high = np.array([np.pi/2, np.pi, 1.8, 2.1], dtype=np.float32)
    low = np.array([-np.pi/2, 0, -1.8, 1.1], dtype=np.float32)

    self.action_space = spaces.Discrete(3)
    self.observation_space = spaces.Box(low, high, dtype=np.float32)

    self.on_goal = 0
    self.current_step = 0

    self.seed()
    self.viewer = None
    self.state = None

  def seed(self, seed=None):
    self.np_random, seed = seeding.np_random(seed)
    return [seed]

  def calc_angle_difference(self, theta):
    # Get state of arm
    arm_mag = self.length
    arm_vector = np.array([arm_mag * np.sin(theta), arm_mag * np.cos(theta)])

    # Get distance vector between the goal and end of arm
    distance_vector = np.array([self.goal[0] - arm_vector[0], self.goal[1] - arm_vector[1]])
    distance_mag = np.sqrt(distance_vector[0]**2 + distance_vector[1]**2)

    # Get the angle between this distance vector and the arm vector
    return np.arccos(np.dot(arm_vector, distance_vector)/(arm_mag*distance_mag), dtype=np.float32)

  def step(self, action):
    err_msg = &amp;quot;%r (%s) invalid&amp;quot; % (action, type(action))
    assert self.action_space.contains(action), err_msg

    self.current_step += 1
    theta, _, goalx, goaly = self.state

    # Adjust theta based on the chosen action
    # if action == 1:
    #   theta_adj = self.theta_adj
    # else:
    #   theta_adj = -self.theta_adj
    if action == 1:
      theta_adj = self.theta_adj
    elif action == 2:
      theta_adj = -self.theta_adj
    else:
      theta_adj = 0

    theta += theta_adj * self.tau
    theta = max(min(theta, np.pi/2), -np.pi/2)

    # Get the angle between this distance vector and the arm vector
    angle_difference = self.calc_angle_difference(theta)

    self.state = (theta, angle_difference, goalx, goaly)

    # delay_modifier = float(self.current_step / MAX_STEPS)
    # r = float(1 - angle_difference*2)
    # r = np.exp(-angle_difference, dtype=np.float32)
    r = np.exp(-angle_difference*3, dtype=np.float32)
    # r = np.exp(-angle_difference, dtype=np.float32) * delay_modifier
    # r = np.exp(-angle_difference, dtype=np.float32) * (1.0 - delay_modifier)

    if theta &amp;gt;= np.pi/2 or theta &amp;lt;= -np.pi/2:
      r = 0.0 # Baaaad boi

    if angle_difference &amp;lt;= self.angle_difference_threshold and \
      angle_difference &amp;gt;= -self.angle_difference_threshold:
      self.on_goal += 1
      r = 1.0 # Goooood boi
    else:
      self.on_goal = self.on_goal - 2 if self.on_goal &amp;gt; 0 else 0

    done = bool(self.on_goal &amp;gt;= STEPS_ON_GOAL_TO_FINISH or
      self.current_step &amp;gt;= MAX_STEPS or
      theta &amp;gt;= np.pi/2 or
      theta &amp;lt;= -np.pi/2
    )

    print(self.current_step, np.array(self.state), r, self.on_goal, action)
    return np.array(self.state), float(r), done, {}

  def reset(self):
    self.goal = np.array([self.np_random.rand() * 3.6 - 1.8, self.np_random.rand() + 1.1])
    self.on_goal = 0
    self.current_step = 0

    new_theta = self.np_random.rand()*np.pi - np.pi/2
    new_angle_difference = self.calc_angle_difference(new_theta)

    self.state = np.array([new_theta, new_angle_difference, *self.goal], dtype=np.float32)
    return np.array(self.state)

  def render(self, mode=&amp;#39;human&amp;#39;):
      screen_width = 600
      screen_height = 400

      world_width = self.length * 4
      scale = screen_width/world_width
      polewidth = 10.0
      polelen = scale * (self.length)
      goalwidth = 15.0
      goalheight = 15.0

      if self.viewer is None:
          from gym.envs.classic_control import rendering
          self.viewer = rendering.Viewer(screen_width, screen_height)
          l, r, t, b = -goalwidth / 2, goalwidth / 2, goalheight / 2, -goalheight / 2
          goal = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
          self.goaltrans = rendering.Transform(translation=(self.goal[0] * scale + screen_width / 2.0, self.goal[0] * scale))
          goal.add_attr(self.goaltrans)
          self.viewer.add_geom(goal)
          l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
          pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
          pole.set_color(.8, .6, .4)
          self.poletrans = rendering.Transform(translation=(screen_width / 2.0, 0))
          pole.add_attr(self.poletrans)
          self.viewer.add_geom(pole)
          self._pole_geom = pole

      if self.state is None:
          return None

      # Edit the pole polygon vertex
      pole = self._pole_geom
      l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
      pole.v = [(l, b), (l, t), (r, t), (r, b)]

      x = self.state
      self.goaltrans.set_translation(self.goal[0] * scale + screen_width / 2.0, self.goal[1] * scale)
      self.poletrans.set_rotation(-x[0])

      return self.viewer.render(return_rgb_array=mode == &amp;#39;rgb_array&amp;#39;)

  def close(self):
      if self.viewer:
          self.viewer.close()
          self.viewer = None
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;main.py:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from armenv import ArmEnv
import math
import random
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple
from itertools import count
from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms as T

env = ArmEnv()

# set up matplotlib
is_ipython = &amp;#39;inline&amp;#39; in matplotlib.get_backend()
if is_ipython:
    from IPython import display

plt.ion()

# if gpu is to be used
device = torch.device(&amp;quot;cuda&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;)

Transition = namedtuple(&amp;#39;Transition&amp;#39;, (&amp;#39;state&amp;#39;, &amp;#39;action&amp;#39;, &amp;#39;next_state&amp;#39;, &amp;#39;reward&amp;#39;))


class ReplayMemory(object):
  def __init__(self, capacity):
      self.capacity = capacity
      self.memory = []
      self.position = 0

  def push(self, *args):
      &amp;quot;&amp;quot;&amp;quot;Saves a transition.&amp;quot;&amp;quot;&amp;quot;
      if len(self.memory) &amp;lt; self.capacity:
          self.memory.append(None)
      self.memory[self.position] = Transition(*args)
      self.position = (self.position + 1) % self.capacity

  def sample(self, batch_size):
      return random.sample(self.memory, batch_size)

  def __len__(self):
      return len(self.memory)

class DQN(nn.Module):

  def __init__(self, h, w, outputs):
    super(DQN, self).__init__()
    self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)
    self.bn1 = nn.BatchNorm2d(16)
    self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)
    self.bn2 = nn.BatchNorm2d(32)
    self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)
    self.bn3 = nn.BatchNorm2d(32)

    # Number of Linear input connections depends on output of conv2d layers
    # and therefore the input image size, so compute it.
    def conv2d_size_out(size, kernel_size = 5, stride = 2):
        return (size - (kernel_size - 1) - 1) // stride  + 1
    convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))
    convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))
    linear_input_size = convw * convh * 32
    self.head = nn.Linear(linear_input_size, outputs)

  # Called with either one element to determine next action, or a batch
  # during optimization. Returns tensor([[left0exp,right0exp]...]).
  def forward(self, x):
    x = F.relu(self.bn1(self.conv1(x)))
    x = F.relu(self.bn2(self.conv2(x)))
    x = F.relu(self.bn3(self.conv3(x)))
    return self.head(x.view(x.size(0), -1))

resize = T.Compose([T.ToPILImage(),
  T.Resize(40, interpolation=Image.CUBIC),
  T.ToTensor()])

def get_screen():
  # Returned screen requested by gym is 400x600x3, but is sometimes larger
  # such as 800x1200x3. Transpose it into torch order (CHW).
  screen = env.render(mode=&amp;#39;rgb_array&amp;#39;).transpose((2, 0, 1))
  screen = np.ascontiguousarray(screen, dtype=np.float32) / 255
  screen = torch.from_numpy(screen)
  # Resize, and add a batch dimension (BCHW)
  return resize(screen).unsqueeze(0).to(device)

env.reset()
plt.figure()
plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),
           interpolation=&amp;#39;none&amp;#39;)
plt.title(&amp;#39;Example extracted screen&amp;#39;)
plt.show()

BATCH_SIZE = 128
GAMMA = 0.999
EPS_START = 0.9
EPS_END = 0.05
EPS_DECAY = 200
TARGET_UPDATE = 10

# Get screen size so that we can initialize layers correctly based on shape
# returned from AI gym. Typical dimensions at this point are close to 3x40x90
# which is the result of a clamped and down-scaled render buffer in get_screen()
init_screen = get_screen()
_, _, screen_height, screen_width = init_screen.shape

# Get number of actions from gym action space
n_actions = env.action_space.n

policy_net = DQN(screen_height, screen_width, n_actions).to(device)
target_net = DQN(screen_height, screen_width, n_actions).to(device)
target_net.load_state_dict(policy_net.state_dict())
target_net.eval()

optimizer = optim.RMSprop(policy_net.parameters())
memory = ReplayMemory(10000)

steps_done = 0

def select_action(state):
  global steps_done
  sample = random.random()
  eps_threshold = EPS_END + (EPS_START - EPS_END) * \
    math.exp(-1. * steps_done / EPS_DECAY)
  steps_done += 1
  if sample &amp;gt; eps_threshold:
    with torch.no_grad():
      # t.max(1) will return largest column value of each row.
      # second column on max result is index of where max element was
      # found, so we pick action with the larger expected reward.
      return policy_net(state).max(1)[1].view(1, 1)
  else:
    return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)

episode_rewards = []

def plot_rewards():
  plt.figure(2)
  plt.clf()
  rewards_t = torch.tensor(episode_rewards, dtype=torch.float)
  plt.title(&amp;#39;Training...&amp;#39;)
  plt.xlabel(&amp;#39;Episode&amp;#39;)
  plt.ylabel(&amp;#39;Reward&amp;#39;)
  plt.plot(rewards_t.numpy())
  # Take 100 episode averages and plot them too
  if len(rewards_t) &amp;gt;= 10:
    means = rewards_t.unfold(0, 10, 1).mean(1).view(-1)
    means = torch.cat((torch.zeros(9), means))
    plt.plot(means.numpy())

  plt.pause(0.001)  # pause a bit so that plots are updated
  if is_ipython:
    display.clear_output(wait=True)
    display.display(plt.gcf())

def optimize_model():
  if len(memory) &amp;lt; BATCH_SIZE:
    return
  transitions = memory.sample(BATCH_SIZE)
  # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for
  # detailed explanation). This converts batch-array of Transitions
  # to Transition of batch-arrays.
  batch = Transition(*zip(*transitions))

  # Compute a mask of non-final states and concatenate the batch elements
  # (a final state would&amp;#39;ve been the one after which simulation ended)
  non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,
                                          batch.next_state)), device=device, dtype=torch.bool)
  non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
  state_batch = torch.cat(batch.state)
  action_batch = torch.cat(batch.action)
  reward_batch = torch.cat(batch.reward)

  # Compute Q(s_t, a) - the model computes Q(s_t), then we select the
  # columns of actions taken. These are the actions which would&amp;#39;ve been taken
  # for each batch state according to policy_net
  state_action_values = policy_net(state_batch).gather(1, action_batch)

  # Compute V(s_{t+1}) for all next states.
  # Expected values of actions for non_final_next_states are computed based
  # on the &amp;quot;older&amp;quot; target_net; selecting their best reward with max(1)[0].
  # This is merged based on the mask, such that we&amp;#39;ll have either the expected
  # state value or 0 in case the state was final.
  next_state_values = torch.zeros(BATCH_SIZE, device=device)
  next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()
  # Compute the expected Q values
  expected_state_action_values = (next_state_values * GAMMA) + reward_batch

  # Compute Huber loss
  loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))

  # Optimize the model
  optimizer.zero_grad()
  loss.backward()
  for param in policy_net.parameters():
    param.grad.data.clamp_(-1, 1)
  optimizer.step()

num_episodes = 400
for i_episode in range(num_episodes):
  # Initialize the environment and state
  env.reset()
  last_screen = get_screen()
  current_screen = get_screen()
  state = current_screen - last_screen
  for t in count():
    # Select and perform an action
    action = select_action(state)
    _, reward, done, _ = env.step(action.item())
    reward = torch.tensor([reward], device=device)

    # Observe new state
    last_screen = current_screen
    current_screen = get_screen()
    if not done:
      next_state = current_screen - last_screen
    else:
      next_state = None

    # Store the transition in memory
    memory.push(state, action, next_state, reward)

    # Move to the next state
    state = next_state

    # Perform one step of the optimization (on the target network)
    optimize_model()
    if done:
      episode_rewards.append(reward)
      plot_rewards()
      break
  # Update the target network, copying all weights and biases in DQN
  if i_episode % TARGET_UPDATE == 0:
    target_net.load_state_dict(policy_net.state_dict())

print(&amp;#39;Complete&amp;#39;)
env.render()
env.close()
plt.ioff()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/G-8veUKwNQczaBKRS6AaXd-iEH0Dmgoq_7n9x8KJXws.jpg?auto=webp&amp;s=c0586d83536092d7a151861ef6b17290011d9e63', 'width': 640, 'height': 416}, 'resolutions': [{'url': 'https://external-preview.redd.it/G-8veUKwNQczaBKRS6AaXd-iEH0Dmgoq_7n9x8KJXws.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=435160129aa84a1229a1cb8fd712abcf45033760', 'width': 108, 'height': 70}, {'url': 'https://external-preview.redd.it/G-8veUKwNQczaBKRS6AaXd-iEH0Dmgoq_7n9x8KJXws.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b9c4fd6c963a8d7eaae1abfb2724414f974a7d05', 'width': 216, 'height': 140}, {'url': 'https://external-preview.redd.it/G-8veUKwNQczaBKRS6AaXd-iEH0Dmgoq_7n9x8KJXws.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=844ad59b6e03e732727c83fc101f9381b9d255d7', 'width': 320, 'height': 208}, {'url': 'https://external-preview.redd.it/G-8veUKwNQczaBKRS6AaXd-iEH0Dmgoq_7n9x8KJXws.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=35b7250dcef518dbbbe697cc4f780d83d7d8e6ff', 'width': 640, 'height': 416}], 'variants': {}, 'id': 'LBOjoxnn9jr8tHn1H0Ojg51O5HQdOBKZ71z_F2zT3BA'}], 'enabled': False}","[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 50, 'id': 'award_02d9ab2c-162e-4c01-8438-317a016ed3d9', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=16&amp;height=16&amp;auto=webp&amp;s=10034f3fdf8214c8377134bb60c5b832d4bbf588', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=32&amp;height=32&amp;auto=webp&amp;s=100f785bf261fa9452a5d82ee0ef0793369dbfa5', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=48&amp;height=48&amp;auto=webp&amp;s=b15d030fdfbbe4af4a5b34ab9dc90a174df40a23', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=64&amp;height=64&amp;auto=webp&amp;s=601c75be6ee30dc4b47a5c65d64dea9a185502a1', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=128&amp;height=128&amp;auto=webp&amp;s=540f36e65c0e2f1347fe32020e4a1565e3680437', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""I'm in this with you."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Take My Energy', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=16&amp;height=16&amp;auto=webp&amp;s=045db73f47a9513c44823d132b4c393ab9241b6a', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=32&amp;height=32&amp;auto=webp&amp;s=298a02e0edbb5b5e293087eeede63802cbe1d2c7', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=48&amp;height=48&amp;auto=webp&amp;s=7d06d606eb23dbcd6dbe39ee0e60588c5eb89065', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=64&amp;height=64&amp;auto=webp&amp;s=ecd9854b14104a36a210028c43420f0dababd96b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=128&amp;height=128&amp;auto=webp&amp;s=0d5d7b92c1d66aff435f2ad32e6330ca2b971f6d', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png'}]",[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lp4sv0,True,,DrArtimos,,9,True,all_ads,False,[],False,,/r/pytorch/comments/lp4sv0/newcomer_to_pytorch_in_need_of_help/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lp4sv0/newcomer_to_pytorch_in_need_of_help/,7135,1613936501.0,0,,False,,,,"{'4fbcktpn91j61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 69, 'x': 108, 'u': 'https://preview.redd.it/4fbcktpn91j61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a6d3310da43cdb199675636ff427d526574af8f2'}, {'y': 138, 'x': 216, 'u': 'https://preview.redd.it/4fbcktpn91j61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e9af1092482a4cfad025921a86cdbe161c9b38a9'}, {'y': 205, 'x': 320, 'u': 'https://preview.redd.it/4fbcktpn91j61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65a9c1d9ec2184998b25abf1f54d964c2ed64c34'}, {'y': 411, 'x': 640, 'u': 'https://preview.redd.it/4fbcktpn91j61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=604c1acd3a496b6846cc9c34d3e286cfd4740a9c'}, {'y': 617, 'x': 960, 'u': 'https://preview.redd.it/4fbcktpn91j61.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bfd8663989dd1774cdfafee771e29a3cfcb9eeb'}, {'y': 694, 'x': 1080, 'u': 'https://preview.redd.it/4fbcktpn91j61.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=197c051bb0d05d5d47611d111c9ff86913b29352'}], 's': {'y': 1734, 'x': 2696, 'u': 'https://preview.redd.it/4fbcktpn91j61.png?width=2696&amp;format=png&amp;auto=webp&amp;s=fed27980876ade082ceecfaa7d886401d750268a'}, 'id': '4fbcktpn91j61'}, 'y2vu5vkqvvi61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/y2vu5vkqvvi61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d98cc11058e99da4b71ca02b704cb0e34cc98db'}, {'y': 120, 'x': 216, 'u': 'https://preview.redd.it/y2vu5vkqvvi61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a0eaf850533009b54423936bce582d771400a79d'}, {'y': 178, 'x': 320, 'u': 'https://preview.redd.it/y2vu5vkqvvi61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2e482f20cbcff74515c897ab5b4d68be72f5c368'}, {'y': 356, 'x': 640, 'u': 'https://preview.redd.it/y2vu5vkqvvi61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=72c15e93a8c3bf2f08aa24d6998cf52cd6f44e49'}, {'y': 534, 'x': 960, 'u': 'https://preview.redd.it/y2vu5vkqvvi61.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=689c515422672d415d65e0fb1d51efb0d40469fc'}, {'y': 601, 'x': 1080, 'u': 'https://preview.redd.it/y2vu5vkqvvi61.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0670b0c6558963e65151b039dea0b59d65b025bb'}], 's': {'y': 1494, 'x': 2682, 'u': 'https://preview.redd.it/y2vu5vkqvvi61.png?width=2682&amp;format=png&amp;auto=webp&amp;s=b62e643654f8b61c4bc4eaa1a6465a30af234f00'}, 'id': 'y2vu5vkqvvi61'}}",,,,
45,,pytorch,,t2_766u1eio,False,,0,False,How does one implement parallel SGD with the pytorch autograd RPC library so that gradients can be received from different processes without errors?,[],r/pytorch,False,6,,0,140.0,,False,t3_lp4cok,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/s_zQksmi_sk2984DLS6wn3j1tEbMOgXXINm1zlHOXbU.jpg,False,,[],{},link,,False,,1613964011.0,text,6,,,text,stackoverflow.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lp4cok,True,,No_Ad3397,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lp4cok/how_does_one_implement_parallel_sgd_with_the/,all_ads,False,https://stackoverflow.com/questions/66306115/how-does-one-implement-parallel-sgd-with-the-pytorch-autograd-rpc-library-so-tha,7135,1613935211.0,0,,False,https://stackoverflow.com/questions/66306115/how-does-one-implement-parallel-sgd-with-the-pytorch-autograd-rpc-library-so-tha,,,,,,,
46,,pytorch,"In almost every text generation context, when a character or word is generated by the LSTM, it is fed back into the LSTM as input for the next character or word generation round. With pytorch LSTM, however, you input the whole sequence at once. How can you make text generation with pytorch then?",t2_2e1j2cvf,False,,0,False,How to feed output of LSTM into itself?,[],r/pytorch,False,6,,0,,,False,t3_loedsu,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1613877399.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In almost every text generation context, when a character or word is generated by the LSTM, it is fed back into the LSTM as input for the next character or word generation round. With pytorch LSTM, however, you input the whole sequence at once. How can you make text generation with pytorch then?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,loedsu,True,,gamechanger4r,,5,True,all_ads,False,[],False,,/r/pytorch/comments/loedsu/how_to_feed_output_of_lstm_into_itself/,all_ads,False,https://www.reddit.com/r/pytorch/comments/loedsu/how_to_feed_output_of_lstm_into_itself/,7135,1613848599.0,0,,False,,,,,,,,
47,,pytorch,"I originally asked a question about writing large amounts of data in r/learnpython and was directed towards HDF5 - my new question around that format I think might be better suited here - apologies if not.

I'm currently working on a project involving multichannel audio. The dataset I'm using needs to be processed into the desired target signals. This takes a database of 64 recordings at around 80GB and prodcues target data of around 5TB. I should also add that I'm taking the 64 recordings and splitting them into 30 second chunks, resulting in 1,280 recordings in total. The size of each chunk after processing is 1,440,000  x 93.

What's the best way of writing this to a HDF5 file, which means I can then utilise Dataloader to load data in for training in batches and allows me to access the data in such a way that I can index into the appropriate part of the data to be used for each training example. Is it best just to have one HDF5 file and have different subfiles within that, or should I split it down into smaller individual HDF5 files with each containing all the chunks from the parent recording, for example.

When it comes to training/test sets, I'm also intending for all the chunks taken from a parent recording to be in the same set. So parent recording ""A"" produces 20 smaller chunks, I wouldn't have those spread across both training and test sets. So being able to store and organise by parent recording is important.

Along with the target data I'm going to have input data where each example is 1,440,000 x 2 which also needs to be stored.

The example I've found so far for using dataloader with HDF5 is [here](https://towardsdatascience.com/hdf5-datasets-for-pytorch-631ff1d750f5)\- but with this method it loads the entire HDF5 file into memory, and I can't do that unless I just have a bunch of seperate HDF5 files for each 30 second clips worth of data. Which seems a pretty inefficient way of doing it and would make sorting them into test/training sets a bit more difficult.

I hope this all makes sense :-)",t2_2t7g6k8t,False,,0,False,Writing large amounts of generated data to HDF5 to later be used for training.,[],r/pytorch,False,6,,0,,,False,t3_lo5gm6,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1613821895.0,,[],{},self,,True,,1613850096.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I originally asked a question about writing large amounts of data in &lt;a href=""/r/learnpython""&gt;r/learnpython&lt;/a&gt; and was directed towards HDF5 - my new question around that format I think might be better suited here - apologies if not.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently working on a project involving multichannel audio. The dataset I&amp;#39;m using needs to be processed into the desired target signals. This takes a database of 64 recordings at around 80GB and prodcues target data of around 5TB. I should also add that I&amp;#39;m taking the 64 recordings and splitting them into 30 second chunks, resulting in 1,280 recordings in total. The size of each chunk after processing is 1,440,000  x 93.&lt;/p&gt;

&lt;p&gt;What&amp;#39;s the best way of writing this to a HDF5 file, which means I can then utilise Dataloader to load data in for training in batches and allows me to access the data in such a way that I can index into the appropriate part of the data to be used for each training example. Is it best just to have one HDF5 file and have different subfiles within that, or should I split it down into smaller individual HDF5 files with each containing all the chunks from the parent recording, for example.&lt;/p&gt;

&lt;p&gt;When it comes to training/test sets, I&amp;#39;m also intending for all the chunks taken from a parent recording to be in the same set. So parent recording &amp;quot;A&amp;quot; produces 20 smaller chunks, I wouldn&amp;#39;t have those spread across both training and test sets. So being able to store and organise by parent recording is important.&lt;/p&gt;

&lt;p&gt;Along with the target data I&amp;#39;m going to have input data where each example is 1,440,000 x 2 which also needs to be stored.&lt;/p&gt;

&lt;p&gt;The example I&amp;#39;ve found so far for using dataloader with HDF5 is &lt;a href=""https://towardsdatascience.com/hdf5-datasets-for-pytorch-631ff1d750f5""&gt;here&lt;/a&gt;- but with this method it loads the entire HDF5 file into memory, and I can&amp;#39;t do that unless I just have a bunch of seperate HDF5 files for each 30 second clips worth of data. Which seems a pretty inefficient way of doing it and would make sorting them into test/training sets a bit more difficult.&lt;/p&gt;

&lt;p&gt;I hope this all makes sense :-)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/6xKOcB7FtYREn87e7wLjzoYFEX2WbzeepVWvLq6AI-E.jpg?auto=webp&amp;s=d9b651a851dd26b4442befe6311d9c1a0bbab5f0', 'width': 1200, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/6xKOcB7FtYREn87e7wLjzoYFEX2WbzeepVWvLq6AI-E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12f1ea4dd492c5a73018b744f3d72f0e62904e20', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/6xKOcB7FtYREn87e7wLjzoYFEX2WbzeepVWvLq6AI-E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83c1a297124386645de04ee0e72daae611cafdee', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/6xKOcB7FtYREn87e7wLjzoYFEX2WbzeepVWvLq6AI-E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b0a551db8bba51e2d74a87dee609016a103aa99', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/6xKOcB7FtYREn87e7wLjzoYFEX2WbzeepVWvLq6AI-E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8927748d669af3e633c5ed5c6424bf703b0ebf1e', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/6xKOcB7FtYREn87e7wLjzoYFEX2WbzeepVWvLq6AI-E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8f3d97ea1843ce48c15d924719aa1229ad65d4bd', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/6xKOcB7FtYREn87e7wLjzoYFEX2WbzeepVWvLq6AI-E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0aae1d667207a578b792d9d8ce86050168915ee4', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'Acn7BFoKeF6HliL5QhFwgX6p7EYhmL23Pz1T8Qw1bJw'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lo5gm6,True,,Molem7b5,,5,True,all_ads,False,[],False,,/r/pytorch/comments/lo5gm6/writing_large_amounts_of_generated_data_to_hdf5/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lo5gm6/writing_large_amounts_of_generated_data_to_hdf5/,7135,1613821296.0,0,,False,,,,,,,,
48,,pytorch,,t2_766u1eio,False,,0,False,How to parallelize a training loop ever samples of a batch when CPU is only available in pytorch?,[],r/pytorch,False,6,,0,140.0,,False,t3_lmvp5k,False,dark,1.0,,public,9,0,{},140.0,,False,[],,False,False,,{},,False,9,,False,https://b.thumbs.redditmedia.com/5U7JmKKrje7ah4YLtwFXmyVcRDPHiJhWW7w-RN3Ao9k.jpg,False,,[],{},link,,False,,1613707161.0,text,6,,,text,stackoverflow.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lmvp5k,True,,No_Ad3397,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lmvp5k/how_to_parallelize_a_training_loop_ever_samples/,all_ads,False,https://stackoverflow.com/questions/66226135/how-to-parallelize-a-training-loop-ever-samples-of-a-batch-when-cpu-is-only-avai,7135,1613678361.0,0,,False,https://stackoverflow.com/questions/66226135/how-to-parallelize-a-training-loop-ever-samples-of-a-batch-when-cpu-is-only-avai,,,,,,,
49,,pytorch,,t2_766u1eio,False,,0,False,Why is mp.spawn spawning 4 processes when I only want 2?,[],r/pytorch,False,6,,0,140.0,,False,t3_lmx71y,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/8RKnD-J3XHgDFI1w9PSeYL-kim-0mZcIS0tc69i3zLo.jpg,False,,[],{},link,,False,,1613711050.0,text,6,,,text,discuss.pytorch.org,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lmx71y,True,,No_Ad3397,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lmx71y/why_is_mpspawn_spawning_4_processes_when_i_only/,all_ads,False,https://discuss.pytorch.org/t/why-is-mp-spawn-spawning-4-processes-when-i-only-want-2/112299,7135,1613682250.0,0,,False,https://discuss.pytorch.org/t/why-is-mp-spawn-spawning-4-processes-when-i-only-want-2/112299,,,,,,,
50,,pytorch,"I'm trying to use skorch GridSearchCV to fit a custom model that uses dgl.GATConv. I used the SliceDataset to split the dataset to x and y to run GridSearchCV. I added a custom collate() in iterator_train_collate as I did while training the model normally.

However on running gs.fit(x,y) every trial takes 0 seconds and the final best accuracy is nan.

What do I do to solve this issue?

net = NeuralNetClassifier(  
    module=GATconvClassifier,  
    module__in_dim=24,  
    module__num_classes=2,  
    module__residual=True,  
    module__activation=F.relu,     
    module__topkf=15,      
    max_epochs=100,  
    lr=0.001,  
    criterion=torch.nn.NLLLoss,  
    train_split=False,  
    iterator_train__collate_fn=collate,  
    iterator_valid__collate_fn=collate,  
   
)",t2_4xhrybaz,False,,0,False,HELP: Skorch GridSearchCV best accuracy is nan,[],r/pytorch,False,6,,0,,,False,t3_lmqaj9,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,1613665111.0,,[],{},,,True,,1613693630.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to use skorch GridSearchCV to fit a custom model that uses dgl.GATConv. I used the SliceDataset to split the dataset to x and y to run GridSearchCV. I added a custom collate() in iterator_train_collate as I did while training the model normally.&lt;/p&gt;

&lt;p&gt;However on running gs.fit(x,y) every trial takes 0 seconds and the final best accuracy is nan.&lt;/p&gt;

&lt;p&gt;What do I do to solve this issue?&lt;/p&gt;

&lt;p&gt;net = NeuralNetClassifier(&lt;br/&gt;
    module=GATconvClassifier,&lt;br/&gt;
    module&lt;strong&gt;in_dim=24,&lt;br/&gt;
    module&lt;/strong&gt;num&lt;em&gt;classes=2,&lt;br/&gt;
    module&lt;/em&gt;&lt;em&gt;residual=True,&lt;br/&gt;
    module&lt;/em&gt;&lt;em&gt;activation=F.relu,&lt;br/&gt;
    module&lt;/em&gt;&lt;em&gt;topkf=15,&lt;br/&gt;
    max_epochs=100,&lt;br/&gt;
    lr=0.001,&lt;br/&gt;
    criterion=torch.nn.NLLLoss,&lt;br/&gt;
    train_split=False,&lt;br/&gt;
    iterator_train&lt;/em&gt;&lt;em&gt;collate_fn=collate,&lt;br/&gt;
    iterator_valid&lt;/em&gt;_collate_fn=collate,  &lt;/p&gt;

&lt;p&gt;)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lmqaj9,True,,banenvy,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lmqaj9/help_skorch_gridsearchcv_best_accuracy_is_nan/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lmqaj9/help_skorch_gridsearchcv_best_accuracy_is_nan/,7135,1613664830.0,0,,False,,,,,,,,
51,,pytorch,"I need a collate_fn in my data loader for running a normal pytorch code.

Now I plan to use skorch's gridsearchcv() on the same model. But here we pass our dataset as input and label, and it uses the default Data Loader from pytorch. How do I make sure GridSearchCV uses my collate().",t2_4xhrybaz,False,,0,False,How to add collate_fn when using Skorch?,[],r/pytorch,False,6,,0,,,False,t3_lmpv04,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1613692555.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need a collate_fn in my data loader for running a normal pytorch code.&lt;/p&gt;

&lt;p&gt;Now I plan to use skorch&amp;#39;s gridsearchcv() on the same model. But here we pass our dataset as input and label, and it uses the default Data Loader from pytorch. How do I make sure GridSearchCV uses my collate().&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lmpv04,True,,banenvy,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lmpv04/how_to_add_collate_fn_when_using_skorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lmpv04/how_to_add_collate_fn_when_using_skorch/,7135,1613663755.0,0,,False,,,,,,,,
52,,pytorch,"https://github.com/theswgong/MoNet

This is the repository I need to use for my thesis. I have 4GB GPU memory. The first problem while running this was: 

Default device argument was set to 3. So I guess the original code was compiled using 4 graphics devices. So I set the value to 0.

Then the next problem was runtime error: CUDA out of memory. The code for ‘image’ tried to allocate 50 GB, when I have only 4. So I reduced the batch size.

Decreasing batch size introduced a new runtime error in the message block of ‘gmm_conv.py’ saying size of tensor a(some integer) must be equal to size of tensor b(25).

I could see that size of tensor b is the kernel size 25. And for batch size 1, size of tensor a was 1316 (which decreases with the decrease of batch size). So I pass kernel size as 1316, so that size of tensor a and tensor b matches. But then size of tensor a slightly increased and they mismatched and throw the same error again.

I am sorry for the long post. I don’t think I explained very clearly. Please overlook my clumsiness as I am just starting ML.",t2_7btw3gt1,False,,0,False,Can anyone help me to run this code?,[],r/pytorch,False,6,,0,,,False,t3_lmp0ko,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1613690252.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://github.com/theswgong/MoNet""&gt;https://github.com/theswgong/MoNet&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is the repository I need to use for my thesis. I have 4GB GPU memory. The first problem while running this was: &lt;/p&gt;

&lt;p&gt;Default device argument was set to 3. So I guess the original code was compiled using 4 graphics devices. So I set the value to 0.&lt;/p&gt;

&lt;p&gt;Then the next problem was runtime error: CUDA out of memory. The code for ‘image’ tried to allocate 50 GB, when I have only 4. So I reduced the batch size.&lt;/p&gt;

&lt;p&gt;Decreasing batch size introduced a new runtime error in the message block of ‘gmm_conv.py’ saying size of tensor a(some integer) must be equal to size of tensor b(25).&lt;/p&gt;

&lt;p&gt;I could see that size of tensor b is the kernel size 25. And for batch size 1, size of tensor a was 1316 (which decreases with the decrease of batch size). So I pass kernel size as 1316, so that size of tensor a and tensor b matches. But then size of tensor a slightly increased and they mismatched and throw the same error again.&lt;/p&gt;

&lt;p&gt;I am sorry for the long post. I don’t think I explained very clearly. Please overlook my clumsiness as I am just starting ML.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BGhvZ_ScryvYU4OOnMHn_2tBKXDIUbzD8HymOGCjnpM.jpg?auto=webp&amp;s=d341c9cd8eec700c26c76c90697210859bcf66c3', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/BGhvZ_ScryvYU4OOnMHn_2tBKXDIUbzD8HymOGCjnpM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d984151e151db74a638ae610a0e0b42d48ce984', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/BGhvZ_ScryvYU4OOnMHn_2tBKXDIUbzD8HymOGCjnpM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ae877af208428de373697017fa5472cd28b1fc64', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/BGhvZ_ScryvYU4OOnMHn_2tBKXDIUbzD8HymOGCjnpM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f69aa0afcf227e47f47f5052dd8c92ea7bbdcd0b', 'width': 320, 'height': 320}], 'variants': {}, 'id': '6w9hgpSn8i1I-Nk0962vNLnQMXoFQdsf8knQh51voWk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lmp0ko,True,,RaghadR,,13,True,all_ads,False,[],False,,/r/pytorch/comments/lmp0ko/can_anyone_help_me_to_run_this_code/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lmp0ko/can_anyone_help_me_to_run_this_code/,7135,1613661452.0,0,,False,,,,,,,,
53,,pytorch," Hi, I have a segmentation Unet based on a resnet that takes a around half a second to execute. I want to be able to use it on a live video feed but of course the execution takes much longer than each frame lasts. I imagine there's a way with multi threading to allow a smooth output albeit with a lag but I need some pointers to how/if this is possible?",t2_wethllt,False,,0,False,"Video processing for live video using resnet, processing takes longer than each frame lasts",[],r/pytorch,False,6,,0,,,False,t3_lm05gk,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1613612779.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I have a segmentation Unet based on a resnet that takes a around half a second to execute. I want to be able to use it on a live video feed but of course the execution takes much longer than each frame lasts. I imagine there&amp;#39;s a way with multi threading to allow a smooth output albeit with a lag but I need some pointers to how/if this is possible?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lm05gk,True,,OneCollar,,13,True,all_ads,False,[],False,,/r/pytorch/comments/lm05gk/video_processing_for_live_video_using_resnet/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lm05gk/video_processing_for_live_video_using_resnet/,7135,1613583979.0,0,,False,,,,,,,,
54,,pytorch,,t2_ae751h28,False,,0,False,What is the most suitable loss function for text summarization?,[],r/pytorch,False,6,,0,,,False,t3_llupzn,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1613598038.0,text,6,,,text,self.pytorch,False,,,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,llupzn,True,,blackpropagation,,5,True,all_ads,False,[],False,,/r/pytorch/comments/llupzn/what_is_the_most_suitable_loss_function_for_text/,all_ads,False,https://www.reddit.com/r/pytorch/comments/llupzn/what_is_the_most_suitable_loss_function_for_text/,7135,1613569238.0,0,,False,,,,,,,,
55,,pytorch,"Hi how's it going? I'm trying to build a model that takes in a 2D Matrix as a single sample and outputs the row index that's the best action by using softmax.

This is what I have so far:

&amp;#x200B;

'''

names = \['Bob','Henry','Mike','Phil'\]

max\_squat = \[300,400,200,100\]

max\_bench = \[200,100,225,100\]

max\_deadlift = \[600,400,300,225\]

strongest\_worker\_df = pd.DataFrame({'Name':names,'Max\_Squat':max\_squat,'Max\_Bench':max\_bench,'Max\_Deadlift':max\_deadlift})

'''

&amp;#x200B;

https://preview.redd.it/0plouuysp3i61.png?width=458&amp;format=png&amp;auto=webp&amp;s=4ee60d6539b4319419aa84dd24e47749f2ac2228

\`\`\`

class Policy(nn.Module):

def \_\_init\_\_(self):

super(Policy, self).\_\_init\_\_()

self.layer1 = torch.nn.Conv1d(in\_channels=4, out\_channels=4, kernel\_size=3, stride=1)

&amp;#x200B;

def forward(self, x):

x = self.layer1(x)

x = F.softmax(x,dim=1)

return x

&amp;#x200B;

def act(self, state):

state = state.float()

value = self.forward(state)

return value

&amp;#x200B;

policy = Policy()

result = policy.act(input\_torch)

\`\`\`

The result shape is torch.Size(\[1,4,1\]). How do I get this to output a column vector instead?

&amp;#x200B;

Also, is Conv1D with kernel size equal to number of features per row the most logical approach to this input state representation?",t2_893qljdw,False,,0,False,Need help passing in 2D Matrix to Conv1D layer and outputting a softmax probability,[],r/pytorch,False,6,,0,67.0,,False,t3_lm4fx8,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/0OXVYS9zfk-dR2ujg4FeG8Khu6AAJ39xK_XbCarA7sI.jpg,False,,[],{},,,True,,1613624125.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi how&amp;#39;s it going? I&amp;#39;m trying to build a model that takes in a 2D Matrix as a single sample and outputs the row index that&amp;#39;s the best action by using softmax.&lt;/p&gt;

&lt;p&gt;This is what I have so far:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/p&gt;

&lt;p&gt;names = [&amp;#39;Bob&amp;#39;,&amp;#39;Henry&amp;#39;,&amp;#39;Mike&amp;#39;,&amp;#39;Phil&amp;#39;]&lt;/p&gt;

&lt;p&gt;max_squat = [300,400,200,100]&lt;/p&gt;

&lt;p&gt;max_bench = [200,100,225,100]&lt;/p&gt;

&lt;p&gt;max_deadlift = [600,400,300,225]&lt;/p&gt;

&lt;p&gt;strongest_worker_df = pd.DataFrame({&amp;#39;Name&amp;#39;:names,&amp;#39;Max_Squat&amp;#39;:max_squat,&amp;#39;Max_Bench&amp;#39;:max_bench,&amp;#39;Max_Deadlift&amp;#39;:max_deadlift})&lt;/p&gt;

&lt;p&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/0plouuysp3i61.png?width=458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ee60d6539b4319419aa84dd24e47749f2ac2228""&gt;https://preview.redd.it/0plouuysp3i61.png?width=458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ee60d6539b4319419aa84dd24e47749f2ac2228&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;class Policy(nn.Module):&lt;/p&gt;

&lt;p&gt;def __init__(self):&lt;/p&gt;

&lt;p&gt;super(Policy, self).__init__()&lt;/p&gt;

&lt;p&gt;self.layer1 = torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, stride=1)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;def forward(self, x):&lt;/p&gt;

&lt;p&gt;x = self.layer1(x)&lt;/p&gt;

&lt;p&gt;x = F.softmax(x,dim=1)&lt;/p&gt;

&lt;p&gt;return x&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;def act(self, state):&lt;/p&gt;

&lt;p&gt;state = state.float()&lt;/p&gt;

&lt;p&gt;value = self.forward(state)&lt;/p&gt;

&lt;p&gt;return value&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;policy = Policy()&lt;/p&gt;

&lt;p&gt;result = policy.act(input_torch)&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;The result shape is torch.Size([1,4,1]). How do I get this to output a column vector instead?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Also, is Conv1D with kernel size equal to number of features per row the most logical approach to this input state representation?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lm4fx8,True,,LowPizza8626,,2,True,all_ads,False,[],False,,/r/pytorch/comments/lm4fx8/need_help_passing_in_2d_matrix_to_conv1d_layer/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lm4fx8/need_help_passing_in_2d_matrix_to_conv1d_layer/,7135,1613595325.0,0,,False,,,,"{'0plouuysp3i61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 52, 'x': 108, 'u': 'https://preview.redd.it/0plouuysp3i61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e955e3cc03aea9a5327649af0a5ee0b5d04317d7'}, {'y': 104, 'x': 216, 'u': 'https://preview.redd.it/0plouuysp3i61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe0015cecbdd878f0c4ac5d9dbc0df007f93ec84'}, {'y': 155, 'x': 320, 'u': 'https://preview.redd.it/0plouuysp3i61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a93bd66dbfcd04accf5cd8625f14d130e25eb280'}], 's': {'y': 222, 'x': 458, 'u': 'https://preview.redd.it/0plouuysp3i61.png?width=458&amp;format=png&amp;auto=webp&amp;s=4ee60d6539b4319419aa84dd24e47749f2ac2228'}, 'id': '0plouuysp3i61'}}",,,,
56,,pytorch,"&amp;#x200B;

[I cant think how to save the prediction values from torch.max  into a csv file has anyone tried this before, Cheers!](https://preview.redd.it/jkd0rr76a3i61.png?width=274&amp;format=png&amp;auto=webp&amp;s=eb71daa508d02c11d86bbc2144272b126b9b8c64)",t2_nvpfl,False,,0,False,Help saving prediction values as csv !,[],r/pytorch,False,6,,0,17.0,,False,t3_lm2iop,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/Vj6ucYVe2FClj2eM5OgiEls7m9z2UPpJZ_Ra_YZ2DkY.jpg,False,,[],{},,,True,,1613618978.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/jkd0rr76a3i61.png?width=274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb71daa508d02c11d86bbc2144272b126b9b8c64""&gt;I cant think how to save the prediction values from torch.max  into a csv file has anyone tried this before, Cheers!&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lm2iop,True,,AndrewClarkingson,,2,True,all_ads,False,[],False,,/r/pytorch/comments/lm2iop/help_saving_prediction_values_as_csv/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lm2iop/help_saving_prediction_values_as_csv/,7135,1613590178.0,0,,False,,,,"{'jkd0rr76a3i61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 13, 'x': 108, 'u': 'https://preview.redd.it/jkd0rr76a3i61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8af00dcbea2b81c261f72b321b89664e7b41981'}, {'y': 27, 'x': 216, 'u': 'https://preview.redd.it/jkd0rr76a3i61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=993a3fe6997a2d8358aa00d4f6061a4ebc6427b4'}], 's': {'y': 35, 'x': 274, 'u': 'https://preview.redd.it/jkd0rr76a3i61.png?width=274&amp;format=png&amp;auto=webp&amp;s=eb71daa508d02c11d86bbc2144272b126b9b8c64'}, 'id': 'jkd0rr76a3i61'}}",,,,
57,,pytorch,"I am following this code [https://github.com/bentrevett/pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq) for text summarization but the output is always blank padding, how should I proceed to resolve this? I tried to debug it multiple times but the output is always either blank padding or EOS character.",t2_1258babs,False,,0,False,Text summarization code giving only padding as output,[],r/pytorch,False,6,,0,,,False,t3_ll2hn8,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1613506085.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am following this code &lt;a href=""https://github.com/bentrevett/pytorch-seq2seq""&gt;https://github.com/bentrevett/pytorch-seq2seq&lt;/a&gt; for text summarization but the output is always blank padding, how should I proceed to resolve this? I tried to debug it multiple times but the output is always either blank padding or EOS character.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/PmJJXmhSvFvpyL4pGxhKoKRqZBwvsyBKsqOYsT-C8Ro.jpg?auto=webp&amp;s=ae87629c90b2c689338c7b3065e0247c92589bf1', 'width': 195, 'height': 195}, 'resolutions': [{'url': 'https://external-preview.redd.it/PmJJXmhSvFvpyL4pGxhKoKRqZBwvsyBKsqOYsT-C8Ro.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=234d293468a6e059afbb7a63e2029363bcb6275b', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'XEY8Qy4209yUStbZqT0keV3wev46ryqNO6WkXoAmLqQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ll2hn8,True,,abdullahkhilji,,7,True,all_ads,False,[],False,,/r/pytorch/comments/ll2hn8/text_summarization_code_giving_only_padding_as/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ll2hn8/text_summarization_code_giving_only_padding_as/,7135,1613477285.0,0,,False,,,,,,,,
58,,pytorch,,t2_766u1eio,False,,0,False,How to use multiprocessing in PyTorch?,[],r/pytorch,False,6,,0,140.0,,False,t3_ll4rov,False,dark,0.25,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/qQN5tWfIHVrQKmAR91JZv4zbhD5wGvAUvEigIshYaTE.jpg,False,,[],{},link,,False,,1613514066.0,text,6,,,text,stackoverflow.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ll4rov,True,,No_Ad3397,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ll4rov/how_to_use_multiprocessing_in_pytorch/,all_ads,False,https://stackoverflow.com/questions/56174874/how-to-use-multiprocessing-in-pytorch,7135,1613485266.0,0,,False,https://stackoverflow.com/questions/56174874/how-to-use-multiprocessing-in-pytorch,,,,,,,
59,,pytorch,"Both legacy companies and many tech companies doing commercial ML have pain points regarding:

- Moving to the cloud,
- Creating and managing ML pipelines,
- Scaling,
- Dealing with sensitive data at scale,
- And about a million other problems.

At the same time, if we want to be serious and actually have models touch real-life business problems and real people, we have to deal with the essentials like:

- acquiring &amp; cleaning large amounts of data;
- setting up tracking and versioning for experiments and model training runs;
- setting up the deployment and monitoring pipelines for the models that do get to production. 
- and we need to find a way to scale our ML operations to the needs of the business and/or users of our ML models.

This article gives you broad overview on the topic:

[What is MLOps](https://neptune.ai/blog/mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective&amp;utm_content=pytorch)",t2_5hfacnnv,False,,0,False,"[Overview] MLOps: What It Is, Why it Matters, and How To Implement it",[],r/pytorch,False,6,,0,,,False,t3_lkh7z1,False,dark,0.93,,public,11,0,{},,,False,[],,False,False,,{},,False,11,,False,self,False,,[],{},self,,True,,1613435658.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Both legacy companies and many tech companies doing commercial ML have pain points regarding:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Moving to the cloud,&lt;/li&gt;
&lt;li&gt;Creating and managing ML pipelines,&lt;/li&gt;
&lt;li&gt;Scaling,&lt;/li&gt;
&lt;li&gt;Dealing with sensitive data at scale,&lt;/li&gt;
&lt;li&gt;And about a million other problems.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At the same time, if we want to be serious and actually have models touch real-life business problems and real people, we have to deal with the essentials like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;acquiring &amp;amp; cleaning large amounts of data;&lt;/li&gt;
&lt;li&gt;setting up tracking and versioning for experiments and model training runs;&lt;/li&gt;
&lt;li&gt;setting up the deployment and monitoring pipelines for the models that do get to production. &lt;/li&gt;
&lt;li&gt;and we need to find a way to scale our ML operations to the needs of the business and/or users of our ML models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This article gives you broad overview on the topic:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://neptune.ai/blog/mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective?utm_source=reddit&amp;amp;utm_medium=post&amp;amp;utm_campaign=blog-mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective&amp;amp;utm_content=pytorch""&gt;What is MLOps&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/avB9zeexGy9-Ag-cOYKHGl_UtRdOWxXQkIutcIIdtvQ.jpg?auto=webp&amp;s=874615a9296bc300eb9acaf00c00fb97b729fa3f', 'width': 1920, 'height': 1377}, 'resolutions': [{'url': 'https://external-preview.redd.it/avB9zeexGy9-Ag-cOYKHGl_UtRdOWxXQkIutcIIdtvQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a9b2fd767ec6b7fc35f8d1ce19f887487ce65b27', 'width': 108, 'height': 77}, {'url': 'https://external-preview.redd.it/avB9zeexGy9-Ag-cOYKHGl_UtRdOWxXQkIutcIIdtvQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7deb11953582b676d7aaea352055378062f6df09', 'width': 216, 'height': 154}, {'url': 'https://external-preview.redd.it/avB9zeexGy9-Ag-cOYKHGl_UtRdOWxXQkIutcIIdtvQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=50c25964e0b5873fe8d044ec7d568af5b9e69daf', 'width': 320, 'height': 229}, {'url': 'https://external-preview.redd.it/avB9zeexGy9-Ag-cOYKHGl_UtRdOWxXQkIutcIIdtvQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4a80d48c989893db1019bd23fbe34cf6889cf6a2', 'width': 640, 'height': 459}, {'url': 'https://external-preview.redd.it/avB9zeexGy9-Ag-cOYKHGl_UtRdOWxXQkIutcIIdtvQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=26e901401e153f64203dc3232e4bf93f90212dc0', 'width': 960, 'height': 688}, {'url': 'https://external-preview.redd.it/avB9zeexGy9-Ag-cOYKHGl_UtRdOWxXQkIutcIIdtvQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=46230a655dedeeedf2435c3a899c08439a33d536', 'width': 1080, 'height': 774}], 'variants': {}, 'id': 'TJKZMEW5FoNNmDLcx7H8GdUQ_sfMR34xpl_gU9pPsDQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lkh7z1,True,,kk_ai,,3,True,all_ads,False,[],False,,/r/pytorch/comments/lkh7z1/overview_mlops_what_it_is_why_it_matters_and_how/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lkh7z1/overview_mlops_what_it_is_why_it_matters_and_how/,7135,1613406858.0,0,,False,,,,,,,,
60,,pytorch,"I found a way to find gradients from a model in PyTorch as shown here: [https://medium.com/datadriveninvestor/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4](https://medium.com/datadriveninvestor/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4)

I tried to recreate this at inference stage as follows:

pred, \_ = model(graph, node\_features)  
argmax\_Y = pred.max(dim=1)\[1\]  
best\_pred = pred\[0, argmax\_Y\]  
best\_pred.backward()  
saliency= node\_features.grad  
print(saliency)

However, the saliency is None

I am completely unaware about how to calculate gradients for graphs. How do I do it?Here, \`model\` is a few GATConv layers followed by a Linear layer for Graph classification.

I need these gradients to generate saliency maps.",t2_4xhrybaz,False,,0,False,HELP: How do I get the gradients for a GAT model in dgl??,[],r/pytorch,False,6,,0,,,False,t3_lkk6gm,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1613415835.0,,[],{},self,,True,,1613443846.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I found a way to find gradients from a model in PyTorch as shown here: &lt;a href=""https://medium.com/datadriveninvestor/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4""&gt;https://medium.com/datadriveninvestor/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I tried to recreate this at inference stage as follows:&lt;/p&gt;

&lt;p&gt;pred, _ = model(graph, node_features)&lt;br/&gt;
argmax_Y = pred.max(dim=1)[1]&lt;br/&gt;
best_pred = pred[0, argmax_Y]&lt;br/&gt;
best_pred.backward()&lt;br/&gt;
saliency= node_features.grad&lt;br/&gt;
print(saliency)&lt;/p&gt;

&lt;p&gt;However, the saliency is None&lt;/p&gt;

&lt;p&gt;I am completely unaware about how to calculate gradients for graphs. How do I do it?Here, `model` is a few GATConv layers followed by a Linear layer for Graph classification.&lt;/p&gt;

&lt;p&gt;I need these gradients to generate saliency maps.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5kuokHp19uwK1-OdcOYUuobCxgWE2VbRKUDcZCpquIA.jpg?auto=webp&amp;s=357bf4d9df15da3b01dd02b542aaffb0ac30d5ca', 'width': 720, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/5kuokHp19uwK1-OdcOYUuobCxgWE2VbRKUDcZCpquIA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eb49ce5aaa472676091e1be2aec2f8d81777ea9e', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/5kuokHp19uwK1-OdcOYUuobCxgWE2VbRKUDcZCpquIA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=da262b832e0e8001d98780d4aba04990bf42ed9f', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/5kuokHp19uwK1-OdcOYUuobCxgWE2VbRKUDcZCpquIA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b54bf88787e74376063454ad2c9b4074050de50', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/5kuokHp19uwK1-OdcOYUuobCxgWE2VbRKUDcZCpquIA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a998f222ff822d998bb65b532aa829ae96e0569f', 'width': 640, 'height': 320}], 'variants': {}, 'id': 'Ah9MKSRaMFZ2BaqgjsuAknWDATP2U2jfG0aB-P0q528'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lkk6gm,True,,banenvy,,2,True,all_ads,False,[],False,,/r/pytorch/comments/lkk6gm/help_how_do_i_get_the_gradients_for_a_gat_model/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lkk6gm/help_how_do_i_get_the_gradients_for_a_gat_model/,7135,1613415046.0,2,,False,,,,,,,,
61,,pytorch,"Hi I am new to using pytorch and cannot for the life of me think how to create a convolutional network for a 2d dataset(time-series) with 14 input channels and 3 possible outputs can anyone point me in the right direction or know of any similar projects? 

https://preview.redd.it/fcu2q3y0boh61.png?width=1029&amp;format=png&amp;auto=webp&amp;s=e3a9a6a8b74cf7a01f7790f69460bb217b42009c",t2_nvpfl,False,,0,False,Pytorch CNN help?,[],r/pytorch,False,6,,0,78.0,,False,t3_lkhwl8,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/QQlgVLGJ1w55Z9pOdu2ovNoXn4Kq6RclIJ3xXVYhVbM.jpg,False,,[],{},,,True,,1613437554.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi I am new to using pytorch and cannot for the life of me think how to create a convolutional network for a 2d dataset(time-series) with 14 input channels and 3 possible outputs can anyone point me in the right direction or know of any similar projects? &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/fcu2q3y0boh61.png?width=1029&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3a9a6a8b74cf7a01f7790f69460bb217b42009c""&gt;https://preview.redd.it/fcu2q3y0boh61.png?width=1029&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3a9a6a8b74cf7a01f7790f69460bb217b42009c&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lkhwl8,True,,AndrewClarkingson,,4,True,all_ads,False,[],False,,/r/pytorch/comments/lkhwl8/pytorch_cnn_help/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lkhwl8/pytorch_cnn_help/,7135,1613408754.0,0,,False,,,,"{'fcu2q3y0boh61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/fcu2q3y0boh61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f536c86271e63fcb21a286cc05a050125985ce60'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/fcu2q3y0boh61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aeadf8a26e6313dd7340a41a00d40953525bb9e2'}, {'y': 179, 'x': 320, 'u': 'https://preview.redd.it/fcu2q3y0boh61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=94c4172ba86bcbd779095fe0e55c658c437340c3'}, {'y': 358, 'x': 640, 'u': 'https://preview.redd.it/fcu2q3y0boh61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=94a6b2ab2fba4aba523cd45632aaa91630a7e26f'}, {'y': 538, 'x': 960, 'u': 'https://preview.redd.it/fcu2q3y0boh61.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4c58388073288b04b843db7b0ef867539d4742c4'}], 's': {'y': 577, 'x': 1029, 'u': 'https://preview.redd.it/fcu2q3y0boh61.png?width=1029&amp;format=png&amp;auto=webp&amp;s=e3a9a6a8b74cf7a01f7790f69460bb217b42009c'}, 'id': 'fcu2q3y0boh61'}}",,,,
62,,pytorch,,t2_a7i59xms,False,,0,False,Gradient with respect to input (Integrated gradients + FGSM attack),[],r/pytorch,False,6,,0,105.0,,False,t3_lkdfmh,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/5lFiZTSsp40?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Gradient with respect to input', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/5lFiZTSsp40?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'mildlyoverfitted', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/5lFiZTSsp40/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYBSjwkGTK06NnDnFsOcR7g'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/5lFiZTSsp40?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lkdfmh', 'height': 200}",,False,1,,False,https://a.thumbs.redditmedia.com/EFhOW5GLblmzLfG8B6CZE7wdC13L7fDL7kxd2gw6d40.jpg,False,,[],{},rich:video,,False,,1613423636.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/c_KKQVPMtd2FWdUHOiUz3zvyoLkSJvAstwS910rxzcc.jpg?auto=webp&amp;s=720d30a3c79cdafb95ac9825c7b5f24327d8d23f', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/c_KKQVPMtd2FWdUHOiUz3zvyoLkSJvAstwS910rxzcc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d3b65eb566d54739d72047fb4c76e1f96bde9af7', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/c_KKQVPMtd2FWdUHOiUz3zvyoLkSJvAstwS910rxzcc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a995df86d4c3526e46285b5cabc12ac3cb6d462', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/c_KKQVPMtd2FWdUHOiUz3zvyoLkSJvAstwS910rxzcc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dacdda315d09fa6334a6bd482bc84080d7017228', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'N8mSWv5J9VSTHyALbwXxSD4IM3W1ZMJE6092Lu2-dy0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lkdfmh,True,,mildlyoverfitted,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lkdfmh/gradient_with_respect_to_input_integrated/,all_ads,False,https://youtu.be/5lFiZTSsp40,7135,1613394836.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Gradient with respect to input', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/5lFiZTSsp40?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'mildlyoverfitted', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/5lFiZTSsp40/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYBSjwkGTK06NnDnFsOcR7g'}}",False,https://youtu.be/5lFiZTSsp40,,,,,,,
63,,pytorch,"Hello, I’m about to start learning pytorch soon but I’ve read it’s a lot like numpy. Do you need a deep understanding of numpy before using it? I already know how to do general purpose python and have used it for data science so I’m comfortable with the language, but just haven’t used numpy a lot. Is knowing how to build a neural network in numpy a prerequisite for learning pytorch? Or in general a deep understanding of numpy?",t2_5w4i5kd1,False,,0,False,Learn numpy before pytorch?,[],r/pytorch,False,6,,0,,,False,t3_ljfhjn,False,dark,0.93,,public,11,0,{},,,False,[],,False,False,,{},,False,11,,False,self,False,,[],{},,,True,,1613299218.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I’m about to start learning pytorch soon but I’ve read it’s a lot like numpy. Do you need a deep understanding of numpy before using it? I already know how to do general purpose python and have used it for data science so I’m comfortable with the language, but just haven’t used numpy a lot. Is knowing how to build a neural network in numpy a prerequisite for learning pytorch? Or in general a deep understanding of numpy?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ljfhjn,True,,veeeerain,,7,True,all_ads,False,[],False,,/r/pytorch/comments/ljfhjn/learn_numpy_before_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ljfhjn/learn_numpy_before_pytorch/,7135,1613270418.0,0,,False,,,,,,,,
64,,pytorch,,t2_75peu,False,,0,False,b44e8280 I am worlds him and the one - Pytorch Crypto Art,[],r/pytorch,False,6,,0,105.0,,False,t3_ljj6vs,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/b21Ftj908rw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'b44e8280 I am worlds him and the one', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/b21Ftj908rw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'DeepOrb Machine Learning Art and Music', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/b21Ftj908rw/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCxf5jeaNJRfBS90tYZMdpcQ'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/b21Ftj908rw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ljj6vs', 'height': 200}",,False,0,,False,https://b.thumbs.redditmedia.com/0_h90hk52ExWz8mNMtu8wxABLMnRKPjE2DJS4vBAoFA.jpg,False,,[],{},rich:video,,False,,1613313174.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/hJpqWePhNifSmxkSRg98b2_iXiiL3dv4iCbDeRDEqFI.jpg?auto=webp&amp;s=1c25ffb22a1feb41f4742e37040e0b91424f599f', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/hJpqWePhNifSmxkSRg98b2_iXiiL3dv4iCbDeRDEqFI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1395ed42f74ee2500618332e23a3cd3069eda193', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/hJpqWePhNifSmxkSRg98b2_iXiiL3dv4iCbDeRDEqFI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b1b462a9d8e985cb75b4155bdba7a43754a040ed', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/hJpqWePhNifSmxkSRg98b2_iXiiL3dv4iCbDeRDEqFI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e4fe5a8c89df2f84ed72ddf9638f37ad803a49a', 'width': 320, 'height': 240}], 'variants': {}, 'id': '8eJKps36vEmdkfa8yiB1Lr0gS4vSCrsGaSMrnTAjBnc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ljj6vs,True,,claytantor,,1,True,all_ads,False,[],False,,/r/pytorch/comments/ljj6vs/b44e8280_i_am_worlds_him_and_the_one_pytorch/,all_ads,False,https://youtube.com/watch?v=b21Ftj908rw&amp;feature=share,7135,1613284374.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'b44e8280 I am worlds him and the one', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/b21Ftj908rw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'DeepOrb Machine Learning Art and Music', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/b21Ftj908rw/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCxf5jeaNJRfBS90tYZMdpcQ'}}",False,https://youtube.com/watch?v=b21Ftj908rw&amp;feature=share,,,,,,,
65,,pytorch,"Hi all, first time posting here.

I'd like to get your opinion on some of your best practices when using distributed training (e.g., nn.DistributedDataParallel). I find it a little hard to debug on this distributed setting since it spawns multiple processes. This makes it quite hard to run python debugger (on an IDE or pdb), and even printing outputs is really messy. Although I guess the latter can be resolved by adding a \`\`\`if is\_main\_process\`\`\` block every time you print. 

Is it best to just keep it to single process + single GPU until debugging is done, then switch to DDP? Or are there other alternatives that are better?

Thanks in advance",t2_6cqfwjq,False,,0,False,What's good practice for debugging distributed training?,[],r/pytorch,False,6,,0,,,False,t3_lj3cck,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1613261441.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, first time posting here.&lt;/p&gt;

&lt;p&gt;I&amp;#39;d like to get your opinion on some of your best practices when using distributed training (e.g., nn.DistributedDataParallel). I find it a little hard to debug on this distributed setting since it spawns multiple processes. This makes it quite hard to run python debugger (on an IDE or pdb), and even printing outputs is really messy. Although I guess the latter can be resolved by adding a ```if is_main_process``` block every time you print. &lt;/p&gt;

&lt;p&gt;Is it best to just keep it to single process + single GPU until debugging is done, then switch to DDP? Or are there other alternatives that are better?&lt;/p&gt;

&lt;p&gt;Thanks in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lj3cck,True,,ButthurtFeminists,,2,True,all_ads,False,[],False,,/r/pytorch/comments/lj3cck/whats_good_practice_for_debugging_distributed/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lj3cck/whats_good_practice_for_debugging_distributed/,7135,1613232641.0,0,,False,,,,,,,,
66,,pytorch,"https://github.com/pytorch/pytorch/issues/51886

Hopefully they include cuDNN 8.1.0 with it. Let's get those RTX 3000 speedups.",t2_798te,False,,0,False,PyTorch 1.8.0 coming out soon,[],r/pytorch,False,6,,0,,,False,t3_li583n,False,dark,1.0,,public,27,0,{},,,False,[],,False,False,,{},,False,27,,False,self,False,,[],{},self,,True,,1613139239.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://github.com/pytorch/pytorch/issues/51886""&gt;https://github.com/pytorch/pytorch/issues/51886&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hopefully they include cuDNN 8.1.0 with it. Let&amp;#39;s get those RTX 3000 speedups.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?auto=webp&amp;s=08c43430ff8793a25b767aa172468acb35be611a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4adad794ed3c0e2f76e2da992ae9eef8c074cae4', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5b3cb400fea60e0dfe87ebc5314f7db8016f8ab', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f149723b636ef3087022ddc1efa6ab53e3d4bc69', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_YPUxJ8AyYwgpO9r-tD7JUoQ-Nx8IlATLoEfmjW48iQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,li583n,True,,serg06,,19,True,all_ads,False,[],False,,/r/pytorch/comments/li583n/pytorch_180_coming_out_soon/,all_ads,False,https://www.reddit.com/r/pytorch/comments/li583n/pytorch_180_coming_out_soon/,7135,1613110439.0,0,,False,,,,,,,,
67,,pytorch,Anybody used CGCNN for time series data? I have few questions.,t2_a0p1r46j,False,,0,False,CGCNN pytorchGeo,[],r/pytorch,False,6,,0,,,False,t3_lhrkg3,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1613098182.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Anybody used CGCNN for time series data? I have few questions.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lhrkg3,True,,NoEnv98,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lhrkg3/cgcnn_pytorchgeo/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lhrkg3/cgcnn_pytorchgeo/,7135,1613069382.0,0,,False,,,,,,,,
68,,pytorch,,t2_1ffz9tjt,False,,0,False,Minimal implementation of SSD: Single Shot MultiBox Detector,[],r/pytorch,False,6,,0,78.0,,False,t3_lh6wsz,False,dark,0.99,,public,13,0,{},140.0,,False,[],,False,False,,{},,False,13,,False,https://a.thumbs.redditmedia.com/mTqHCfUCBSGEeOG11-nE1KPwO3JBfeGZhaM0p-sivT4.jpg,False,,[],{},,,False,,1613028041.0,text,6,,,text,reddit.com,False,,,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lh6wsz,True,,1991viet,,1,True,all_ads,False,[],False,,/r/pytorch/comments/lh6wsz/minimal_implementation_of_ssd_single_shot/,all_ads,False,https://www.reddit.com/gallery/lh6wsz,7135,1612999241.0,0,,False,https://www.reddit.com/gallery/lh6wsz,,,"{'1aiogebbhqg61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 204, 'x': 108, 'u': 'https://preview.redd.it/1aiogebbhqg61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e84581df724a60639f9e279105242da882da91c3'}, {'y': 409, 'x': 216, 'u': 'https://preview.redd.it/1aiogebbhqg61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0d0cdf0fc914383a65cf95cf824fbc42ea67451'}, {'y': 606, 'x': 320, 'u': 'https://preview.redd.it/1aiogebbhqg61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e82209140f98b433027efc2026fe0c78c206386'}], 's': {'y': 699, 'x': 369, 'u': 'https://preview.redd.it/1aiogebbhqg61.png?width=369&amp;format=png&amp;auto=webp&amp;s=b86e373a9a67f3e4442b1db05c98331c5ada859d'}, 'id': '1aiogebbhqg61'}, 'ijnlggbbhqg61': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 71, 'x': 108, 'u': 'https://preview.redd.it/ijnlggbbhqg61.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=232c68963a64522c885855fd38fa1c0b2a36d086'}, {'y': 143, 'x': 216, 'u': 'https://preview.redd.it/ijnlggbbhqg61.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d1c1613bce99be3a0ebcb1e439f45a9e7c7ed083'}, {'y': 213, 'x': 320, 'u': 'https://preview.redd.it/ijnlggbbhqg61.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2878ac417a95e49f8ffd541aa8ab694c180d3131'}], 's': {'y': 333, 'x': 500, 'u': 'https://preview.redd.it/ijnlggbbhqg61.jpg?width=500&amp;format=pjpg&amp;auto=webp&amp;s=0405717b6dd4f4254fb5ed707cb9d2eafeda0b6f'}, 'id': 'ijnlggbbhqg61'}, '8pj4pfbbhqg61': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 108, 'x': 108, 'u': 'https://preview.redd.it/8pj4pfbbhqg61.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f7d3d384c8be6f0eb3bb3b9e1b276b3ad94edb27'}, {'y': 216, 'x': 216, 'u': 'https://preview.redd.it/8pj4pfbbhqg61.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=37b70466c321f2e10bf1a2cdabdddbfd479c183b'}, {'y': 320, 'x': 320, 'u': 'https://preview.redd.it/8pj4pfbbhqg61.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=57a71c9b06a90fbfdabee5d4b2b29114f567f54e'}], 's': {'y': 612, 'x': 612, 'u': 'https://preview.redd.it/8pj4pfbbhqg61.jpg?width=612&amp;format=pjpg&amp;auto=webp&amp;s=ce83a49089280ccefb7c0b613c649b989ea0f21d'}, 'id': '8pj4pfbbhqg61'}, '917gz2dbhqg61': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/917gz2dbhqg61.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=6aa2d379ba41b4b3f1a6a67ba5532f019c4676fa'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/917gz2dbhqg61.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=321eb9bee226bc1f7aca728230aa8c38cb1bdd2f'}, {'y': 180, 'x': 320, 'u': 'https://preview.redd.it/917gz2dbhqg61.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=f89748638c81ced3cc244fa755537ca6c8649757'}, {'y': 360, 'x': 640, 'u': 'https://preview.redd.it/917gz2dbhqg61.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=8c16e1a1fdfc8607dac853353c29e588094fa19a'}, {'y': 540, 'x': 960, 'u': 'https://preview.redd.it/917gz2dbhqg61.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=a90b284595cb15cb1633b53c874cd04f5c9b873e'}, {'y': 607, 'x': 1080, 'u': 'https://preview.redd.it/917gz2dbhqg61.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=669e0522841b48a341dee310504bd1cfbfdda6b5'}], 's': {'y': 720, 'gif': 'https://i.redd.it/917gz2dbhqg61.gif', 'mp4': 'https://preview.redd.it/917gz2dbhqg61.gif?format=mp4&amp;s=49cabffecea2bc7471b8ffe097ce2746e978e97c', 'x': 1280}, 'id': '917gz2dbhqg61'}, 't21yxdbbhqg61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 32, 'x': 108, 'u': 'https://preview.redd.it/t21yxdbbhqg61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc7d68a8ce47794e04412781d0221fc04c204e47'}, {'y': 64, 'x': 216, 'u': 'https://preview.redd.it/t21yxdbbhqg61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b941b36d61e5b3c0c324e290c4c61f1cc5167e59'}, {'y': 95, 'x': 320, 'u': 'https://preview.redd.it/t21yxdbbhqg61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=15b11f36bbe496f9df491cb91bae5f86e0046fec'}, {'y': 190, 'x': 640, 'u': 'https://preview.redd.it/t21yxdbbhqg61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e35b65f4b0e9964bf87867db35938fdfa107ba33'}], 's': {'y': 216, 'x': 726, 'u': 'https://preview.redd.it/t21yxdbbhqg61.png?width=726&amp;format=png&amp;auto=webp&amp;s=92c754187121cb67772329746ec0975148f0b641'}, 'id': 't21yxdbbhqg61'}}",True,"{'items': [{'media_id': '917gz2dbhqg61', 'id': 27255599}, {'media_id': 'ijnlggbbhqg61', 'id': 27255600}, {'media_id': '8pj4pfbbhqg61', 'id': 27255601}, {'media_id': '1aiogebbhqg61', 'id': 27255602}, {'media_id': 't21yxdbbhqg61', 'id': 27255603}]}",,
69,,pytorch," So I trained by pytorch DC-GAN (deep convolutional GAN) for 30 epochs on grayscale faces, and my GAN pretty much failed. I added batch normalization and leaky relu's to my generator and discriminator (I heard those are ways to make the GAN converge), and the Adam optimizer. My GAN still only putting out random grayscale pixels (nothing even remotely related to faces.) I have no problem with the discriminator, my discriminator works very well. I then implemented weight decay of 0.01 on my discriminator to make my GAN train better (since my discriminator was doing better than my generator) but to no avail. My GAN still generates just random pixels, sometimes outputting completely black.

Please view my code here: [https://www.kaggle.com/rohjoshi828/emotiongan](https://www.kaggle.com/rohjoshi828/emotiongan)

so that you can give me feedback on how to improve my GAN, because nothing I am trying is working (I once even tried training for 60 epochs but that failed too). Anyway, more more info, the GAN training method I used worked for the MNIST dataset (but I used a way simpler GAN architecture for that.)",t2_77x7ml6a,False,,0,False,My DC-GAN on grayscale face images is not training well.,[],r/pytorch,False,6,,0,,,False,t3_lhar51,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,True,self,False,,[],{},self,,True,,1613039487.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I trained by pytorch DC-GAN (deep convolutional GAN) for 30 epochs on grayscale faces, and my GAN pretty much failed. I added batch normalization and leaky relu&amp;#39;s to my generator and discriminator (I heard those are ways to make the GAN converge), and the Adam optimizer. My GAN still only putting out random grayscale pixels (nothing even remotely related to faces.) I have no problem with the discriminator, my discriminator works very well. I then implemented weight decay of 0.01 on my discriminator to make my GAN train better (since my discriminator was doing better than my generator) but to no avail. My GAN still generates just random pixels, sometimes outputting completely black.&lt;/p&gt;

&lt;p&gt;Please view my code here: &lt;a href=""https://www.kaggle.com/rohjoshi828/emotiongan""&gt;https://www.kaggle.com/rohjoshi828/emotiongan&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;so that you can give me feedback on how to improve my GAN, because nothing I am trying is working (I once even tried training for 60 epochs but that failed too). Anyway, more more info, the GAN training method I used worked for the MNIST dataset (but I used a way simpler GAN architecture for that.)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/hIZn74UCUzMss02AeqYwTjjr9Q4P0f4_c6SjRwG_mWM.jpg?auto=webp&amp;s=317a9be4dd095d5a4b95cfdd96ada08acea08513', 'width': 160, 'height': 160}, 'resolutions': [{'url': 'https://external-preview.redd.it/hIZn74UCUzMss02AeqYwTjjr9Q4P0f4_c6SjRwG_mWM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0ef6b067fd0b46d01dc9f262edb560782bc9f2c', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'p15coSqe7L8wApjnVlwASEYE50BcnmvRuPbSVpGUPaM'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lhar51,True,,TheAnonymous123456,,2,True,all_ads,False,[],False,,/r/pytorch/comments/lhar51/my_dcgan_on_grayscale_face_images_is_not_training/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lhar51/my_dcgan_on_grayscale_face_images_is_not_training/,7135,1613010687.0,0,,False,,,,,,,,
70,,pytorch,,t2_sswdj,False,,0,False,"Retrieval Augmented Generation with Huggingface Transformers, PyTorch, and Ray",[],r/pytorch,False,6,,0,76.0,,False,t3_lh1rg3,False,dark,1.0,,public,4,0,{},140.0,,False,[],,False,False,,{},,False,4,,False,https://b.thumbs.redditmedia.com/oJqYEByZIciHTUaSEtqGXYQRcRHIYIBIcSUaoOQPRtg.jpg,False,,[],{},link,,False,,1613014661.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/u8EjVLullfjY9w0A4UWrPWVBDZzIlk-OfSrIUrEPwSA.jpg?auto=webp&amp;s=81e20a006eb23bca2bde89a9a9e92b63ac7a7eb9', 'width': 1200, 'height': 654}, 'resolutions': [{'url': 'https://external-preview.redd.it/u8EjVLullfjY9w0A4UWrPWVBDZzIlk-OfSrIUrEPwSA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a90a3b273fd21839868ae544287568b0fa0f0b58', 'width': 108, 'height': 58}, {'url': 'https://external-preview.redd.it/u8EjVLullfjY9w0A4UWrPWVBDZzIlk-OfSrIUrEPwSA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cf8f8149d5630b212200569e375dd56b9fc65d45', 'width': 216, 'height': 117}, {'url': 'https://external-preview.redd.it/u8EjVLullfjY9w0A4UWrPWVBDZzIlk-OfSrIUrEPwSA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6e2c455cf9bef06bb3bd5cefed28b42fcf8ace7', 'width': 320, 'height': 174}, {'url': 'https://external-preview.redd.it/u8EjVLullfjY9w0A4UWrPWVBDZzIlk-OfSrIUrEPwSA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3419810ba6295b99bf1d50f9c8c6245c9cc16a12', 'width': 640, 'height': 348}, {'url': 'https://external-preview.redd.it/u8EjVLullfjY9w0A4UWrPWVBDZzIlk-OfSrIUrEPwSA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=36f3e4f6fb7c0860a0baca9b05d96b17cfdf96fc', 'width': 960, 'height': 523}, {'url': 'https://external-preview.redd.it/u8EjVLullfjY9w0A4UWrPWVBDZzIlk-OfSrIUrEPwSA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ce528f78b40628bac0c1eff233a13f507573c4a3', 'width': 1080, 'height': 588}], 'variants': {}, 'id': 'qaLKn6OoBu23_u-mp9nYhKzBjYgORiI0UyyFYDr_y4w'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lh1rg3,True,,mgalarny,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lh1rg3/retrieval_augmented_generation_with_huggingface/,all_ads,False,https://medium.com/distributed-computing-with-ray/retrieval-augmented-generation-with-huggingface-transformers-and-ray-b09b56161b1e,7135,1612985861.0,0,,False,https://medium.com/distributed-computing-with-ray/retrieval-augmented-generation-with-huggingface-transformers-and-ray-b09b56161b1e,,,,,,,
71,,pytorch,"I've been trying to find a deep and clear explanation on how to manipulate data inside the latent space of a compressive autoencoder to further reduce the bpp of an image, but can't find any. I don't know if this is the correct subreddit but i need help badly. 

If there are other techniques apart from quantization I'm all ears.
Thanks in advance.",t2_9c0pqtaw,False,,0,False,Need help with autoencoders latente space quantization,[],r/pytorch,False,6,,0,,,False,t3_lgx9v7,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1613003111.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been trying to find a deep and clear explanation on how to manipulate data inside the latent space of a compressive autoencoder to further reduce the bpp of an image, but can&amp;#39;t find any. I don&amp;#39;t know if this is the correct subreddit but i need help badly. &lt;/p&gt;

&lt;p&gt;If there are other techniques apart from quantization I&amp;#39;m all ears.
Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lgx9v7,True,,ChunkyFunkyFloater,,2,True,all_ads,False,[],False,,/r/pytorch/comments/lgx9v7/need_help_with_autoencoders_latente_space/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lgx9v7/need_help_with_autoencoders_latente_space/,7135,1612974311.0,0,,False,,,,,,,,
72,,pytorch,"I'm using optuna for the first time, and after the study was completed, I picked the best parameters as per minimum loss and tried to train the same model again on the same data independently (without optuna).

However the lowest val loss on this 2nd training is much higher than that given in the trial.

Any idea on what could be the issue?",t2_4xhrybaz,False,,0,False,Optuna best trial not reproducible,[],r/pytorch,False,6,,0,,,False,t3_lg41ac,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1612908453.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m using optuna for the first time, and after the study was completed, I picked the best parameters as per minimum loss and tried to train the same model again on the same data independently (without optuna).&lt;/p&gt;

&lt;p&gt;However the lowest val loss on this 2nd training is much higher than that given in the trial.&lt;/p&gt;

&lt;p&gt;Any idea on what could be the issue?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lg41ac,True,,banenvy,,5,True,all_ads,False,[],False,,/r/pytorch/comments/lg41ac/optuna_best_trial_not_reproducible/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lg41ac/optuna_best_trial_not_reproducible/,7135,1612879653.0,0,,False,,,,,,,,
73,,pytorch,,t2_44mbtmjy,False,,0,False,State of the art in image manipulation (stylegan)!,[],r/pytorch,False,6,,0,51.0,,False,t3_lfkowo,False,dark,0.8,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/j0OveLlsWgR7hJJ61i111m-YTKJUVtfA3tZcijdt8vU.jpg,False,,[],{},link,,False,,1612844145.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QTZsLnR1yTzShu63Znx4m9abpRF_-1P9JQQWoQYS6MA.jpg?auto=webp&amp;s=d73a16528c23140aa6fa3b35112e35919d719894', 'width': 638, 'height': 236}, 'resolutions': [{'url': 'https://external-preview.redd.it/QTZsLnR1yTzShu63Znx4m9abpRF_-1P9JQQWoQYS6MA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b65601b42d413442aea42b1876cf886dcf7be8b6', 'width': 108, 'height': 39}, {'url': 'https://external-preview.redd.it/QTZsLnR1yTzShu63Znx4m9abpRF_-1P9JQQWoQYS6MA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7850da61a76aa40a077745492f7821d88a0321ca', 'width': 216, 'height': 79}, {'url': 'https://external-preview.redd.it/QTZsLnR1yTzShu63Znx4m9abpRF_-1P9JQQWoQYS6MA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2feafd93edc3a83db0f19cdbfce4328d9c3f2eae', 'width': 320, 'height': 118}], 'variants': {}, 'id': 'Qkz_4Hq1CNLL1JvlMbimeQYC4nlkk0jQFDlASqi6_qs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lfkowo,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lfkowo/state_of_the_art_in_image_manipulation_stylegan/,all_ads,False,/r/LatestInML/comments/lfk3ij/state_of_the_art_in_image_manipulation_stylegan/,7135,1612815345.0,0,,False,/r/LatestInML/comments/lfk3ij/state_of_the_art_in_image_manipulation_stylegan/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""[link to paper](https://www.catalyzex.com/paper/arxiv:2102.02766)\n\nhttps://preview.redd.it/jx1dmm6y5bg61.png?width=848&amp;format=png&amp;auto=webp&amp;s=46862c1cbc40fcd944f4e56a31fc7384dfa219c7\n\n👇 Free extension to get code for ML papers (❤️' by Andrew Ng) Chrome: https://chrome.google.com/webstore/detail/find-code-for-research-pa/aikkeehnlfpamidigaffhfmgbkdeheil\n\nFirefox: https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex"", 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'State of the art in image manipulation (stylegan)!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 51, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'jx1dmm6y5bg61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 46, 'x': 108, 'u': 'https://preview.redd.it/jx1dmm6y5bg61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=928e01b735e413fb60ca70251c9755984cecaad1'}, {'y': 92, 'x': 216, 'u': 'https://preview.redd.it/jx1dmm6y5bg61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=08807fc1cfa09089d465af4c003a7fa4b79cf0b1'}, {'y': 136, 'x': 320, 'u': 'https://preview.redd.it/jx1dmm6y5bg61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=75b3e344e5dafc093bb00cf61855aa5a45a31f70'}, {'y': 273, 'x': 640, 'u': 'https://preview.redd.it/jx1dmm6y5bg61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=81806a95fd3166127105d809706f2ae36e8cba40'}], 's': {'y': 362, 'x': 848, 'u': 'https://preview.redd.it/jx1dmm6y5bg61.png?width=848&amp;format=png&amp;auto=webp&amp;s=46862c1cbc40fcd944f4e56a31fc7384dfa219c7'}, 'id': 'jx1dmm6y5bg61'}}, 'name': 't3_lfk3ij', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 16, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 16, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/j0OveLlsWgR7hJJ61i111m-YTKJUVtfA3tZcijdt8vU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1612842613.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.catalyzex.com/paper/arxiv:2102.02766""&gt;link to paper&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/jx1dmm6y5bg61.png?width=848&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=46862c1cbc40fcd944f4e56a31fc7384dfa219c7""&gt;https://preview.redd.it/jx1dmm6y5bg61.png?width=848&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=46862c1cbc40fcd944f4e56a31fc7384dfa219c7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;👇 Free extension to get code for ML papers (❤️&amp;#39; by Andrew Ng) Chrome: &lt;a href=""https://chrome.google.com/webstore/detail/find-code-for-research-pa/aikkeehnlfpamidigaffhfmgbkdeheil""&gt;https://chrome.google.com/webstore/detail/find-code-for-research-pa/aikkeehnlfpamidigaffhfmgbkdeheil&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Firefox: &lt;a href=""https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex""&gt;https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/QTZsLnR1yTzShu63Znx4m9abpRF_-1P9JQQWoQYS6MA.jpg?auto=webp&amp;s=d73a16528c23140aa6fa3b35112e35919d719894', 'width': 638, 'height': 236}, 'resolutions': [{'url': 'https://external-preview.redd.it/QTZsLnR1yTzShu63Znx4m9abpRF_-1P9JQQWoQYS6MA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b65601b42d413442aea42b1876cf886dcf7be8b6', 'width': 108, 'height': 39}, {'url': 'https://external-preview.redd.it/QTZsLnR1yTzShu63Znx4m9abpRF_-1P9JQQWoQYS6MA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7850da61a76aa40a077745492f7821d88a0321ca', 'width': 216, 'height': 79}, {'url': 'https://external-preview.redd.it/QTZsLnR1yTzShu63Znx4m9abpRF_-1P9JQQWoQYS6MA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2feafd93edc3a83db0f19cdbfce4328d9c3f2eae', 'width': 320, 'height': 118}], 'variants': {}, 'id': 'Qkz_4Hq1CNLL1JvlMbimeQYC4nlkk0jQFDlASqi6_qs'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'lfk3ij', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/lfk3ij/state_of_the_art_in_image_manipulation_stylegan/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/lfk3ij/state_of_the_art_in_image_manipulation_stylegan/', 'subreddit_subscribers': 6676, 'created_utc': 1612813813.0, 'num_crossposts': 10, 'media': None, 'is_video': False}]",t3_lfk3ij,,,,,
74,,pytorch,"I need to train a model using the imagenet dataset. I have a version of imagenet stored in a s3 bucket. I use kaggle or google colab notebooks. I have to access the dataset, which is stored in s3, from the notebooks. I can't find a way of doing it using pytorch.",t2_2e1j2cvf,False,,0,False,I can't find a way to use pytorch for machine learning,[],r/pytorch,False,6,,0,,,False,t3_lfoyn6,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1612855719.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need to train a model using the imagenet dataset. I have a version of imagenet stored in a s3 bucket. I use kaggle or google colab notebooks. I have to access the dataset, which is stored in s3, from the notebooks. I can&amp;#39;t find a way of doing it using pytorch.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lfoyn6,True,,gamechanger4r,,12,True,all_ads,False,[],False,,/r/pytorch/comments/lfoyn6/i_cant_find_a_way_to_use_pytorch_for_machine/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lfoyn6/i_cant_find_a_way_to_use_pytorch_for_machine/,7135,1612826919.0,0,,False,,,,,,,,
75,,pytorch,,t2_a7i59xms,False,,0,False,Implementing a custom optimizer (Video Tutorial),[],r/pytorch,False,6,,0,105.0,,False,t3_lenk6o,False,dark,0.94,,public,13,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/zvp8K4iX2Cs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Custom optimizer in PyTorch', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/zvp8K4iX2Cs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'mildlyoverfitted', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/zvp8K4iX2Cs/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYBSjwkGTK06NnDnFsOcR7g'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/zvp8K4iX2Cs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lenk6o', 'height': 200}",,False,13,,False,https://b.thumbs.redditmedia.com/erxdwbxk_2CGYJ9-8vMggMmdUk3qxFV1D9Scn6DxUuM.jpg,False,,[],{},rich:video,,False,,1612737259.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/2a2fLg81_7qvnh3i4AcxjItdmvX73T_HZmFNDqozcs8.jpg?auto=webp&amp;s=c13bb5605bdc42b32733217b6def61483b1826c7', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/2a2fLg81_7qvnh3i4AcxjItdmvX73T_HZmFNDqozcs8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a36c4d7cc713dbbb63079c4eaafdc2456979eb5c', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/2a2fLg81_7qvnh3i4AcxjItdmvX73T_HZmFNDqozcs8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=df6534c1d617e5b65aa27dc7497b48942219c31f', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/2a2fLg81_7qvnh3i4AcxjItdmvX73T_HZmFNDqozcs8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=525affb12339b62faacdcdcfa1760164d2e737dd', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'EU0FbqGBCIW0HDKalFTN4eXO10IEnkgIbnaeQ20ON-Y'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lenk6o,True,,mildlyoverfitted,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lenk6o/implementing_a_custom_optimizer_video_tutorial/,all_ads,False,https://youtu.be/zvp8K4iX2Cs,7135,1612708459.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Custom optimizer in PyTorch', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/zvp8K4iX2Cs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'mildlyoverfitted', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/zvp8K4iX2Cs/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYBSjwkGTK06NnDnFsOcR7g'}}",False,https://youtu.be/zvp8K4iX2Cs,,,,,,,
76,,pytorch,"I have a UNet architecture for a GAN which requires to save the downsample tensor results then concatenate them with those of the same size on the upsample.  The only problem is that this requires me to store 8 tensors in memory, which totally kills my batch size to 16 even on a v100.  I don't really want to do double GPUs because it'd get pretty expensive, so is there a better way to organize this or do I just have to deal with it?",t2_5jo16juy,False,,0,False,UNet encoder/decoder concatenate memory issues,[],r/pytorch,False,6,,0,,,False,t3_lesbia,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1612751629.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a UNet architecture for a GAN which requires to save the downsample tensor results then concatenate them with those of the same size on the upsample.  The only problem is that this requires me to store 8 tensors in memory, which totally kills my batch size to 16 even on a v100.  I don&amp;#39;t really want to do double GPUs because it&amp;#39;d get pretty expensive, so is there a better way to organize this or do I just have to deal with it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lesbia,True,,CauchySchwartzDaddy,,4,True,all_ads,False,[],False,,/r/pytorch/comments/lesbia/unet_encoderdecoder_concatenate_memory_issues/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lesbia/unet_encoderdecoder_concatenate_memory_issues/,7135,1612722829.0,0,,False,,,,,,,,
77,,pytorch,"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#combine-ddp-with-model-parallelism

Above link explains how to combine distributed data parallel with model parallelism on single machine with multiple GPUs.

Is it possible to do such with multi machine multi GPU (multiple GPUs per machine) system? If so how?",t2_3q4w2o4,False,,0,False,DDP with model parallelism with multi host multi GPU system,[],r/pytorch,False,6,,0,,,False,t3_lelxnm,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1612731307.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#combine-ddp-with-model-parallelism""&gt;https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#combine-ddp-with-model-parallelism&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Above link explains how to combine distributed data parallel with model parallelism on single machine with multiple GPUs.&lt;/p&gt;

&lt;p&gt;Is it possible to do such with multi machine multi GPU (multiple GPUs per machine) system? If so how?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lelxnm,True,,hp2304,,3,True,all_ads,False,[],False,,/r/pytorch/comments/lelxnm/ddp_with_model_parallelism_with_multi_host_multi/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lelxnm/ddp_with_model_parallelism_with_multi_host_multi/,7135,1612702507.0,0,,False,,,,,,,,
78,,pytorch,,t2_a7i59xms,False,,0,False,Visualizing activations with forward hooks (Video Tutorial),[],r/pytorch,False,6,,0,105.0,,False,t3_le5kap,False,dark,0.89,,public,14,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/1ZbLA7ofasY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Visualizing activations with forward hooks (PyTorch)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/1ZbLA7ofasY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'mildlyoverfitted', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/1ZbLA7ofasY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYBSjwkGTK06NnDnFsOcR7g'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/1ZbLA7ofasY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/le5kap', 'height': 200}",,False,14,,False,https://b.thumbs.redditmedia.com/AaeXTs4rtLGA3Cu0fTNd4aTRzpka94nRYPLahiRazeI.jpg,False,,[],{},rich:video,,False,,1612670702.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/8laGp4Mr8eQlqFnCfbkgl_kIgK-1Dhi6SBTo_putRFA.jpg?auto=webp&amp;s=ca1a1c6c913cad2f66fa584d9597a51dd1b1b742', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/8laGp4Mr8eQlqFnCfbkgl_kIgK-1Dhi6SBTo_putRFA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=472085a001069500827ed1148fb78fae4d343c41', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/8laGp4Mr8eQlqFnCfbkgl_kIgK-1Dhi6SBTo_putRFA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ebb7177936306dc5ef464a7277517e87c5c550a', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/8laGp4Mr8eQlqFnCfbkgl_kIgK-1Dhi6SBTo_putRFA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=52882cef8143b6cecce202ee84c4a5cfc151be94', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'UEL_aUd-HQiv6co4kESn_E_yj4rJt3i5BkxdXzOUgVM'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,le5kap,True,,mildlyoverfitted,,0,True,all_ads,False,[],False,,/r/pytorch/comments/le5kap/visualizing_activations_with_forward_hooks_video/,all_ads,False,https://youtu.be/1ZbLA7ofasY,7135,1612641902.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Visualizing activations with forward hooks (PyTorch)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/1ZbLA7ofasY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'mildlyoverfitted', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/1ZbLA7ofasY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYBSjwkGTK06NnDnFsOcR7g'}}",False,https://youtu.be/1ZbLA7ofasY,,,,,,,
79,,pytorch,,t2_5owk7j7,False,,0,False,Is there a wrapper package of pytorch that help to visualize the intermediate layer for the purpose of explainable AI,[],r/pytorch,False,6,,0,,,False,t3_le2k0z,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1612662291.0,text,6,,,text,self.pytorch,False,,,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,le2k0z,True,,mrtac96,,6,True,all_ads,False,[],False,,/r/pytorch/comments/le2k0z/is_there_a_wrapper_package_of_pytorch_that_help/,all_ads,False,https://www.reddit.com/r/pytorch/comments/le2k0z/is_there_a_wrapper_package_of_pytorch_that_help/,7135,1612633491.0,0,,False,,,,,,,,
80,,pytorch,"Hi guys,

here is part of a code from hugging faces that is support to share the weights of two embedding layers, can someone explain why simply setting .weight from one module to the other shares the parameter? 

I'm confused by the way tying the weights work in PyTorch, and there are so many posts that are really confusing.

&amp;#x200B;

thanks",t2_szc6x,False,,0,False,PyTorch: weight sharing,[],r/pytorch,False,6,,0,,,False,t3_ldiopx,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1612591829.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys,&lt;/p&gt;

&lt;p&gt;here is part of a code from hugging faces that is support to share the weights of two embedding layers, can someone explain why simply setting .weight from one module to the other shares the parameter? &lt;/p&gt;

&lt;p&gt;I&amp;#39;m confused by the way tying the weights work in PyTorch, and there are so many posts that are really confusing.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ldiopx,True,,hassanzadeh,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ldiopx/pytorch_weight_sharing/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ldiopx/pytorch_weight_sharing/,7135,1612563029.0,0,,False,,,,,,,,
81,,pytorch,"Sorry if this is the wrong place to post this, but I could not find a forum for libtorch.

Is learning libtorch worth it? 
Does anyone use libtorch for training models or for anything for that matter?

Are there any cloud services that provide the environment for this, free providers would be nice!!

Thanks in advance for the answers!!

P.S - formatting maybe bad because I am posting from mobile.",t2_48v57lad,False,,0,False,Libtorch - worth it?,[],r/pytorch,False,6,,0,,,False,t3_ld4ln7,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1612550438.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sorry if this is the wrong place to post this, but I could not find a forum for libtorch.&lt;/p&gt;

&lt;p&gt;Is learning libtorch worth it? 
Does anyone use libtorch for training models or for anything for that matter?&lt;/p&gt;

&lt;p&gt;Are there any cloud services that provide the environment for this, free providers would be nice!!&lt;/p&gt;

&lt;p&gt;Thanks in advance for the answers!!&lt;/p&gt;

&lt;p&gt;P.S - formatting maybe bad because I am posting from mobile.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ld4ln7,True,,raghhuveer,,14,True,all_ads,False,[],False,,/r/pytorch/comments/ld4ln7/libtorch_worth_it/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ld4ln7/libtorch_worth_it/,7135,1612521638.0,0,,False,,,,,,,,
82,,pytorch,,t2_766u1eio,False,,0,False,What is the standard way to batch and do a forward pass through tree structured data (ASTs) in pytorch so to leverage the power of GPUs?,[],r/pytorch,False,6,,0,140.0,,False,t3_lcn8xz,False,dark,1.0,,public,4,0,{},140.0,,False,[],,False,False,,{},,False,4,,False,https://b.thumbs.redditmedia.com/6nTvRqlCaSprkxez_-ks9ZZ47j2I61-rM3wAo_P7UcE.jpg,False,,[],{},link,,False,,1612494378.0,text,6,,,text,discuss.pytorch.org,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lcn8xz,True,,No_Ad3397,,1,True,all_ads,False,[],False,,/r/pytorch/comments/lcn8xz/what_is_the_standard_way_to_batch_and_do_a/,all_ads,False,https://discuss.pytorch.org/t/what-is-the-standard-way-to-batch-and-do-a-forward-pass-through-tree-structured-data-asts-in-pytorch-so-to-leverage-the-power-of-gpus/110939,7135,1612465578.0,0,,False,https://discuss.pytorch.org/t/what-is-the-standard-way-to-batch-and-do-a-forward-pass-through-tree-structured-data-asts-in-pytorch-so-to-leverage-the-power-of-gpus/110939,,,,,,,
83,,pytorch,,t2_44mbtmjy,False,,0,False,Latest from KDnuggets: Find code implementation for any AI/ML paper using this new chrome extension,[],r/pytorch,False,6,,0,,,False,t3_lcagui,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,default,False,,[],{},link,,False,,1612452903.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/eTftZFzXUkqi1qEEPO-xDCDMkT2126BLpo6nD5d8oOA.jpg?auto=webp&amp;s=02659bad52081873a10146ca8daad01706405bee', 'width': 1202, 'height': 234}, 'resolutions': [{'url': 'https://external-preview.redd.it/eTftZFzXUkqi1qEEPO-xDCDMkT2126BLpo6nD5d8oOA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c3e5dbc27f65729bfccb1f019f5c4fdf4705dc15', 'width': 108, 'height': 21}, {'url': 'https://external-preview.redd.it/eTftZFzXUkqi1qEEPO-xDCDMkT2126BLpo6nD5d8oOA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=438e1c7ce9e109e9bfc7476344700ec350e37edb', 'width': 216, 'height': 42}, {'url': 'https://external-preview.redd.it/eTftZFzXUkqi1qEEPO-xDCDMkT2126BLpo6nD5d8oOA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2085485fec5b5e1b2c6490d7efb502dcc22eccbf', 'width': 320, 'height': 62}, {'url': 'https://external-preview.redd.it/eTftZFzXUkqi1qEEPO-xDCDMkT2126BLpo6nD5d8oOA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f721d898297cce574f35eb473bc6efa3c5767f1', 'width': 640, 'height': 124}, {'url': 'https://external-preview.redd.it/eTftZFzXUkqi1qEEPO-xDCDMkT2126BLpo6nD5d8oOA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=57823910b22cd3dc8c1cb72538b0587a78805ce3', 'width': 960, 'height': 186}, {'url': 'https://external-preview.redd.it/eTftZFzXUkqi1qEEPO-xDCDMkT2126BLpo6nD5d8oOA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a6fd62fb158b9fd6f72ad0536f2e8c7102775c6f', 'width': 1080, 'height': 210}], 'variants': {}, 'id': '1IgdrGJHua-eumc0w7TSLq57TXilPA59tdWiEScPk-8'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lcagui,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lcagui/latest_from_kdnuggets_find_code_implementation/,all_ads,False,/r/LatestInML/comments/lcag4n/latest_from_kdnuggets_find_code_implementation/,7135,1612424103.0,0,,False,/r/LatestInML/comments/lcag4n/latest_from_kdnuggets_find_code_implementation/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': '[https://www.kdnuggets.com/2021/01/catalyzex-browser-extension-machine-learning.html](https://www.kdnuggets.com/2021/01/catalyzex-browser-extension-machine-learning.html)', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Latest from KDnuggets: Find code implementation for any AI/ML paper using this new chrome extension', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_lcag4n', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 5, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1612452819.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.kdnuggets.com/2021/01/catalyzex-browser-extension-machine-learning.html""&gt;https://www.kdnuggets.com/2021/01/catalyzex-browser-extension-machine-learning.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/eTftZFzXUkqi1qEEPO-xDCDMkT2126BLpo6nD5d8oOA.jpg?auto=webp&amp;s=02659bad52081873a10146ca8daad01706405bee', 'width': 1202, 'height': 234}, 'resolutions': [{'url': 'https://external-preview.redd.it/eTftZFzXUkqi1qEEPO-xDCDMkT2126BLpo6nD5d8oOA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c3e5dbc27f65729bfccb1f019f5c4fdf4705dc15', 'width': 108, 'height': 21}, {'url': 'https://external-preview.redd.it/eTftZFzXUkqi1qEEPO-xDCDMkT2126BLpo6nD5d8oOA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=438e1c7ce9e109e9bfc7476344700ec350e37edb', 'width': 216, 'height': 42}, {'url': 'https://external-preview.redd.it/eTftZFzXUkqi1qEEPO-xDCDMkT2126BLpo6nD5d8oOA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2085485fec5b5e1b2c6490d7efb502dcc22eccbf', 'width': 320, 'height': 62}, {'url': 'https://external-preview.redd.it/eTftZFzXUkqi1qEEPO-xDCDMkT2126BLpo6nD5d8oOA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f721d898297cce574f35eb473bc6efa3c5767f1', 'width': 640, 'height': 124}, {'url': 'https://external-preview.redd.it/eTftZFzXUkqi1qEEPO-xDCDMkT2126BLpo6nD5d8oOA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=57823910b22cd3dc8c1cb72538b0587a78805ce3', 'width': 960, 'height': 186}, {'url': 'https://external-preview.redd.it/eTftZFzXUkqi1qEEPO-xDCDMkT2126BLpo6nD5d8oOA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a6fd62fb158b9fd6f72ad0536f2e8c7102775c6f', 'width': 1080, 'height': 210}], 'variants': {}, 'id': '1IgdrGJHua-eumc0w7TSLq57TXilPA59tdWiEScPk-8'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'lcag4n', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/lcag4n/latest_from_kdnuggets_find_code_implementation/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/lcag4n/latest_from_kdnuggets_find_code_implementation/', 'subreddit_subscribers': 6676, 'created_utc': 1612424019.0, 'num_crossposts': 9, 'media': None, 'is_video': False}]",t3_lcag4n,,,,,
84,,pytorch,,t2_44mbtmjy,False,,0,False,Latest from google researchers: state of the art in video stabilization!,[],r/pytorch,False,6,,0,71.0,,False,t3_lc6dh8,False,dark,1.0,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/sl2tvu1dqmRZXkGhgwnaw6-XQwZvqOzxpPj1SseBzTg.jpg,False,,[],{},link,,False,,1612438063.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BCtig_3O7m4KzmmlzHKo3G-36_DczlX2F8QxESWLPJw.jpg?auto=webp&amp;s=4e738a002de3f9450789d69fc6115e5a7fd3f041', 'width': 1240, 'height': 632}, 'resolutions': [{'url': 'https://external-preview.redd.it/BCtig_3O7m4KzmmlzHKo3G-36_DczlX2F8QxESWLPJw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e95fa3897baf4356be692bc81ee171b81487339a', 'width': 108, 'height': 55}, {'url': 'https://external-preview.redd.it/BCtig_3O7m4KzmmlzHKo3G-36_DczlX2F8QxESWLPJw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d1a5f0ba46416713344bc45640347b69a1d14ed', 'width': 216, 'height': 110}, {'url': 'https://external-preview.redd.it/BCtig_3O7m4KzmmlzHKo3G-36_DczlX2F8QxESWLPJw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a0a403c0e7cf2b6351b10f601a1155de610873be', 'width': 320, 'height': 163}, {'url': 'https://external-preview.redd.it/BCtig_3O7m4KzmmlzHKo3G-36_DczlX2F8QxESWLPJw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8552e95eaceb767319081c80956741a94412eeb3', 'width': 640, 'height': 326}, {'url': 'https://external-preview.redd.it/BCtig_3O7m4KzmmlzHKo3G-36_DczlX2F8QxESWLPJw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9ba49f60c13c0e444d3e0f54aec37fe35bf0642a', 'width': 960, 'height': 489}, {'url': 'https://external-preview.redd.it/BCtig_3O7m4KzmmlzHKo3G-36_DczlX2F8QxESWLPJw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=22331740e5f91099692c467312e9479f556dc5b6', 'width': 1080, 'height': 550}], 'variants': {}, 'id': 'VZmZjUxqj5ORIGN95ja4d0G8KXzabDQtbpllGn1KoX0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lc6dh8,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lc6dh8/latest_from_google_researchers_state_of_the_art/,all_ads,False,/r/LatestInML/comments/lc64ei/latest_from_google_researchers_state_of_the_art/,7135,1612409263.0,0,,False,/r/LatestInML/comments/lc64ei/latest_from_google_researchers_state_of_the_art/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""[link to paper](https://www.catalyzex.com/paper/arxiv:2102.01279)\n\nhttps://reddit.com/link/lc64ei/video/65g4j80modf61/player\n\n👇 Free extension to get code for ML papers (❤️' by Andrew Ng) Chrome: https://chrome.google.com/webstore/detail/find-code-for-research-pa/aikkeehnlfpamidigaffhfmgbkdeheil\n\nFirefox: https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex"", 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Latest from google researchers: state of the art in video stabilization!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 71, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'65g4j80modf61': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/lc64ei/asset/65g4j80modf61/DASHPlaylist.mpd?a=1618044129%2CZTEwYjlkMGEwMmU2ODM3MzYyMmU4ZDc2OWU3NmFkMTBiMjQwM2UyYjAyNzBjYjEwZWY5MTVmMzRhYTg1ZTVhZg%3D%3D&amp;v=1&amp;f=sd', 'x': 640, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/lc64ei/asset/65g4j80modf61/HLSPlaylist.m3u8?a=1618044129%2CNGRiOGRkMTU5MjcyOWRkMDg4NDRlZTJlODQ0MTMyYmZhMDgwYjI5NDlkN2UwMzZhYmUyNTNhMzcxNThjYWIyMw%3D%3D&amp;v=1&amp;f=sd', 'id': '65g4j80modf61', 'isGif': False}}, 'name': 't3_lc64ei', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 41, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 41, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/sl2tvu1dqmRZXkGhgwnaw6-XQwZvqOzxpPj1SseBzTg.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1612437264.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.catalyzex.com/paper/arxiv:2102.01279""&gt;link to paper&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/lc64ei/video/65g4j80modf61/player""&gt;https://reddit.com/link/lc64ei/video/65g4j80modf61/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;👇 Free extension to get code for ML papers (❤️&amp;#39; by Andrew Ng) Chrome: &lt;a href=""https://chrome.google.com/webstore/detail/find-code-for-research-pa/aikkeehnlfpamidigaffhfmgbkdeheil""&gt;https://chrome.google.com/webstore/detail/find-code-for-research-pa/aikkeehnlfpamidigaffhfmgbkdeheil&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Firefox: &lt;a href=""https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex""&gt;https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/BCtig_3O7m4KzmmlzHKo3G-36_DczlX2F8QxESWLPJw.jpg?auto=webp&amp;s=4e738a002de3f9450789d69fc6115e5a7fd3f041', 'width': 1240, 'height': 632}, 'resolutions': [{'url': 'https://external-preview.redd.it/BCtig_3O7m4KzmmlzHKo3G-36_DczlX2F8QxESWLPJw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e95fa3897baf4356be692bc81ee171b81487339a', 'width': 108, 'height': 55}, {'url': 'https://external-preview.redd.it/BCtig_3O7m4KzmmlzHKo3G-36_DczlX2F8QxESWLPJw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d1a5f0ba46416713344bc45640347b69a1d14ed', 'width': 216, 'height': 110}, {'url': 'https://external-preview.redd.it/BCtig_3O7m4KzmmlzHKo3G-36_DczlX2F8QxESWLPJw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a0a403c0e7cf2b6351b10f601a1155de610873be', 'width': 320, 'height': 163}, {'url': 'https://external-preview.redd.it/BCtig_3O7m4KzmmlzHKo3G-36_DczlX2F8QxESWLPJw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8552e95eaceb767319081c80956741a94412eeb3', 'width': 640, 'height': 326}, {'url': 'https://external-preview.redd.it/BCtig_3O7m4KzmmlzHKo3G-36_DczlX2F8QxESWLPJw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9ba49f60c13c0e444d3e0f54aec37fe35bf0642a', 'width': 960, 'height': 489}, {'url': 'https://external-preview.redd.it/BCtig_3O7m4KzmmlzHKo3G-36_DczlX2F8QxESWLPJw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=22331740e5f91099692c467312e9479f556dc5b6', 'width': 1080, 'height': 550}], 'variants': {}, 'id': 'VZmZjUxqj5ORIGN95ja4d0G8KXzabDQtbpllGn1KoX0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'lc64ei', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/lc64ei/latest_from_google_researchers_state_of_the_art/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/lc64ei/latest_from_google_researchers_state_of_the_art/', 'subreddit_subscribers': 6676, 'created_utc': 1612408464.0, 'num_crossposts': 11, 'media': None, 'is_video': False}]",t3_lc64ei,,,,,
85,,pytorch,"Hello, I’ve been using tensorflow for a while but I wanted to try out pytorch. However I can across another framework for pytorch known as pytorch lightning. I was going to start out by learning pure pytorch, but then I realized pytorch lightning is faster to start working with. Do you recommend I start with learning plain vanilla pytorch or (pure pytorch) before jumping into the various frameworks?",t2_5w4i5kd1,False,,0,False,Pure pytorch before lightning?,[],r/pytorch,False,6,,0,,,False,t3_lbqpdd,False,dark,1.0,,public,8,0,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},,,True,,1612396772.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I’ve been using tensorflow for a while but I wanted to try out pytorch. However I can across another framework for pytorch known as pytorch lightning. I was going to start out by learning pure pytorch, but then I realized pytorch lightning is faster to start working with. Do you recommend I start with learning plain vanilla pytorch or (pure pytorch) before jumping into the various frameworks?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lbqpdd,True,,veeeerain,,10,True,all_ads,False,[],False,,/r/pytorch/comments/lbqpdd/pure_pytorch_before_lightning/,all_ads,False,https://www.reddit.com/r/pytorch/comments/lbqpdd/pure_pytorch_before_lightning/,7135,1612367972.0,0,,False,,,,,,,,
86,,pytorch,,t2_83t22c3q,False,,0,False,Image dataset normalization is one of the most common practises to avoid neural network overfitting but do you know how to calculate the mean and standard deviation of your own custom image dataset?,[],r/pytorch,False,6,,0,105.0,,False,t3_lc1y56,False,dark,0.67,,public,1,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/z3kB3ISIPAg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': '[3] How to calculate the mean and standard deviation of your image dataset (PyTorch)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/z3kB3ISIPAg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'if else statement', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/z3kB3ISIPAg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj4PWxdz8-f0whMeZ3b5dEQ'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/z3kB3ISIPAg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lc1y56', 'height': 200}",,False,1,,False,https://b.thumbs.redditmedia.com/AG1LSVEYDkzsYlfjT0EVHNuIrZZ17XCRm303L9AJAXY.jpg,False,,[],{},rich:video,,False,,1612424918.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/zNictvEt0oomuoDqvp3lDt6g-7BORr5W5IQN_o7uJIo.jpg?auto=webp&amp;s=877b9de447e33a6b9583e9beeaa7712d1c1514c3', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/zNictvEt0oomuoDqvp3lDt6g-7BORr5W5IQN_o7uJIo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c3665476ca394ecd5c1fa989c2448fd3183cb1d4', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/zNictvEt0oomuoDqvp3lDt6g-7BORr5W5IQN_o7uJIo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fa7722bdb3a5c7f102d89fb67bf68325db78849', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/zNictvEt0oomuoDqvp3lDt6g-7BORr5W5IQN_o7uJIo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=20365dbd87e54ba1ae57749fb9c65cc94213d981', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'SJejc2UQ7maSx8ZgO4tGhlwgmQciYBmrcB6K3oDw2Nk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lc1y56,True,,ifelsestatement007,,1,True,all_ads,False,[],False,,/r/pytorch/comments/lc1y56/image_dataset_normalization_is_one_of_the_most/,all_ads,False,https://www.youtube.com/watch?v=z3kB3ISIPAg,7135,1612396118.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': '[3] How to calculate the mean and standard deviation of your image dataset (PyTorch)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/z3kB3ISIPAg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'if else statement', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/z3kB3ISIPAg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj4PWxdz8-f0whMeZ3b5dEQ'}}",False,https://www.youtube.com/watch?v=z3kB3ISIPAg,,,,,,,
87,,pytorch,"Hi all,

I've been doing research with PyTorch for a while now, and I just packaged up some code that I wrote to handle module hook registration and published it to PyPI. If there are any of you who use module hooks in your work and haven't yet developed an infrastructure for handling them of your own, I'm hoping you'll find it useful.

Please check out the [github](https://github.com/IsaacRe/tacklebox) for usage documentation. I've also made some wakthrough videos that you can access through the [website](https://isaacrehg.com/tacklebox/). I haven't made a readthedocs for it yet, but was hoping to get some feedback before sinking more time into it.

If you run into any issues in using it, open up an issue on the github and I'll respond back as quickly as possible.

Thanks!",t2_5dm2meu4,False,,0,False,TackleBox - A simple hook management framework for PyTorch,[],r/pytorch,False,6,,0,,,False,t3_layx25,False,dark,0.96,,public,18,0,{},,,False,[],,False,False,,{},,False,18,,False,self,False,,[],{},self,,True,,1612310329.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been doing research with PyTorch for a while now, and I just packaged up some code that I wrote to handle module hook registration and published it to PyPI. If there are any of you who use module hooks in your work and haven&amp;#39;t yet developed an infrastructure for handling them of your own, I&amp;#39;m hoping you&amp;#39;ll find it useful.&lt;/p&gt;

&lt;p&gt;Please check out the &lt;a href=""https://github.com/IsaacRe/tacklebox""&gt;github&lt;/a&gt; for usage documentation. I&amp;#39;ve also made some wakthrough videos that you can access through the &lt;a href=""https://isaacrehg.com/tacklebox/""&gt;website&lt;/a&gt;. I haven&amp;#39;t made a readthedocs for it yet, but was hoping to get some feedback before sinking more time into it.&lt;/p&gt;

&lt;p&gt;If you run into any issues in using it, open up an issue on the github and I&amp;#39;ll respond back as quickly as possible.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/SMUB3Xvo1H8Gms9uSxCi2E__OVBSR0S5j5dwJYYXgXY.jpg?auto=webp&amp;s=cddb6b2016882f1f11386a91ba5ce10a949b231b', 'width': 300, 'height': 300}, 'resolutions': [{'url': 'https://external-preview.redd.it/SMUB3Xvo1H8Gms9uSxCi2E__OVBSR0S5j5dwJYYXgXY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d032ab5ef0a4aeca73f8fafa60ec01d1d7790c8', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/SMUB3Xvo1H8Gms9uSxCi2E__OVBSR0S5j5dwJYYXgXY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=573c7240ab567f7e624458eb8f4367447c292c8b', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'iTxhTlyk2RF_4Fuy0AmXgTw7QrJk4bBoSktGb_33e2E'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,layx25,True,,isaacrehg,,3,True,all_ads,False,[],False,,/r/pytorch/comments/layx25/tacklebox_a_simple_hook_management_framework_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/layx25/tacklebox_a_simple_hook_management_framework_for/,7135,1612281529.0,0,,False,,,,,,,,
88,,pytorch,,t2_766u1eio,False,,0,False,How does one execute an individual patched module using the higher pytorch library?,[],r/pytorch,False,6,,0,140.0,,False,t3_lb7py6,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/UWKfil-yXWgakIuRgBjYsyAkgBQTwbEA8h5-OC1IvXc.jpg,False,,[],{},link,,False,,1612331734.0,text,6,,,text,stackoverflow.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,lb7py6,True,,No_Ad3397,,0,True,all_ads,False,[],False,,/r/pytorch/comments/lb7py6/how_does_one_execute_an_individual_patched_module/,all_ads,False,https://stackoverflow.com/questions/66018179/how-does-one-execute-an-individual-patched-module-using-the-higher-pytorch-libra,7135,1612302934.0,0,,False,https://stackoverflow.com/questions/66018179/how-does-one-execute-an-individual-patched-module-using-the-higher-pytorch-libra,,,,,,,
89,,pytorch,"I'm somewhat new to deep learning and I was really surprised to see that training a simple CNN was actually slightly faster on my CPU (97 seconds) vs my GPU (99 seconds).  
Is this normal? 

I play games very rarely and I bought this GPU for some ""light"" deep learning projects and I feel kinda stupid right now.",t2_10m7gq,False,,0,False,5950x than RTX 3070 for Deep Learning with PyTorch (CPU vs GPU),[],r/pytorch,False,6,,0,,,False,t3_la70no,False,dark,1.0,,public,8,1,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},,,True,,1612225347.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m somewhat new to deep learning and I was really surprised to see that training a simple CNN was actually slightly faster on my CPU (97 seconds) vs my GPU (99 seconds).&lt;br/&gt;
Is this normal? &lt;/p&gt;

&lt;p&gt;I play games very rarely and I bought this GPU for some &amp;quot;light&amp;quot; deep learning projects and I feel kinda stupid right now.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,la70no,True,,LocSta29,,16,True,all_ads,False,[],False,,/r/pytorch/comments/la70no/5950x_than_rtx_3070_for_deep_learning_with/,all_ads,False,https://www.reddit.com/r/pytorch/comments/la70no/5950x_than_rtx_3070_for_deep_learning_with/,7135,1612196547.0,0,,False,,,,,,,,
90,,pytorch,"I have this roughly 200 gb dataset of medical images that I get from a zip and unzip them into my vm.  The images are 16 bit so I have to convert them to 8 bit to ""colorize"" the image from its raw state, which I do by just looping over the directories with a separate script and converting the PIL images to 8 bit numpy arrays then resaving them in that form.

Only problem is that this takes a stupid long amount of time.  I was running this script for maybe 4 hours yesterday on a 50 GB 170k image subset for testing, and it still didn't get close to finishing.  It's really slow because of all the I/O.  I thought of parallelizing it a bit but I don't know if there's some smarter way to do things.

As far as I know I can't exactly do this process in the transforms since there's some non trivial transformations that I have to do on the images, and I wasn't getting it to work with a custom data transform either.  Do I just have to bit the bullet here and let the script run for a long time until it's done?",t2_5jo16juy,False,,0,False,Is there a smarter way to preprocess my big dataset in the cloud?,[],r/pytorch,False,6,,0,,,False,t3_l9uhpq,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1612181643.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have this roughly 200 gb dataset of medical images that I get from a zip and unzip them into my vm.  The images are 16 bit so I have to convert them to 8 bit to &amp;quot;colorize&amp;quot; the image from its raw state, which I do by just looping over the directories with a separate script and converting the PIL images to 8 bit numpy arrays then resaving them in that form.&lt;/p&gt;

&lt;p&gt;Only problem is that this takes a stupid long amount of time.  I was running this script for maybe 4 hours yesterday on a 50 GB 170k image subset for testing, and it still didn&amp;#39;t get close to finishing.  It&amp;#39;s really slow because of all the I/O.  I thought of parallelizing it a bit but I don&amp;#39;t know if there&amp;#39;s some smarter way to do things.&lt;/p&gt;

&lt;p&gt;As far as I know I can&amp;#39;t exactly do this process in the transforms since there&amp;#39;s some non trivial transformations that I have to do on the images, and I wasn&amp;#39;t getting it to work with a custom data transform either.  Do I just have to bit the bullet here and let the script run for a long time until it&amp;#39;s done?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l9uhpq,True,,CauchySchwartzDaddy,,4,True,all_ads,False,[],False,,/r/pytorch/comments/l9uhpq/is_there_a_smarter_way_to_preprocess_my_big/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l9uhpq/is_there_a_smarter_way_to_preprocess_my_big/,7135,1612152843.0,0,,False,,,,,,,,
91,,pytorch,"Anybody on this subreddit having the same issue?

[https://github.com/pytorch/pytorch/issues/51426](https://github.com/pytorch/pytorch/issues/51426)

## 🐛 Bug
When running karpathy minigpt:
[play_math.ipynb](https://github.com/karpathy/minGPT/blob/master/play_math.ipynb)
On my local computer with two GPUs I get different results on my RTX 2060s vs my GTX 1060 with the exact same code, all I do is choose which card is available for CUDA.
I ran these tests multiple times, always same outcome for both. Also same issue whether I use Ubuntu 18.04 or Windows 10 (dual boot, Ubuntu is not on a VM running inside windows)

GTX 1060 result:
```
epoch 50 iter 17: train loss 0.05737. lr 6.000000e-05: 100%|███████████████████████████████████████████████████████████████| 18/18 [00:00&lt;00:00, 24.47it/s] 
01/31/2021 13:17:58 - INFO - mingpt.trainer -   test loss: 0.004358
final score: 9000/9000 = 100.00% correct
```

RTX 2060 super result:
```
epoch 50 iter 17: train loss 0.04646. lr 6.000000e-05: 100%|███████████████████████████████████████████████████████████████| 18/18 [00:00&lt;00:00, 23.80it/s]
01/31/2021 13:24:21 - INFO - mingpt.trainer -   test loss: 0.004733
final score: 9000/9000 = 100.00% correct
GPT claims that 055 + 045 = 090 (gt is 100; NOPE)
final score: 999/1000 = 99.90% correct
```

## Expected behavior
I would expect to be the same

## Environment

PyTorch version: 1.7.1+cu110        
Is debug build: False
CUDA used to build PyTorch: 11.0    
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 10 Pro
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.7 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: GeForce GTX 1060 5GB
GPU 1: GeForce RTX 2060 SUPER

Nvidia driver version: 461.40
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] torch==1.7.1+cu110
[pip3] torchaudio==0.7.2
[pip3] torchvision==0.8.2+cu110
[conda] Could not collect",t2_35wgqre4,False,,0,False,Different results on RTX 2060s vs GTX 1060 with gpt model,[],r/pytorch,False,6,,0,,,False,t3_l9sp4s,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1612175845.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Anybody on this subreddit having the same issue?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/pytorch/pytorch/issues/51426""&gt;https://github.com/pytorch/pytorch/issues/51426&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;🐛 Bug&lt;/h2&gt;

&lt;p&gt;When running karpathy minigpt:
&lt;a href=""https://github.com/karpathy/minGPT/blob/master/play_math.ipynb""&gt;play_math.ipynb&lt;/a&gt;
On my local computer with two GPUs I get different results on my RTX 2060s vs my GTX 1060 with the exact same code, all I do is choose which card is available for CUDA.
I ran these tests multiple times, always same outcome for both. Also same issue whether I use Ubuntu 18.04 or Windows 10 (dual boot, Ubuntu is not on a VM running inside windows)&lt;/p&gt;

&lt;p&gt;GTX 1060 result:
&lt;code&gt;
epoch 50 iter 17: train loss 0.05737. lr 6.000000e-05: 100%|███████████████████████████████████████████████████████████████| 18/18 [00:00&amp;lt;00:00, 24.47it/s] 
01/31/2021 13:17:58 - INFO - mingpt.trainer -   test loss: 0.004358
final score: 9000/9000 = 100.00% correct
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;RTX 2060 super result:
&lt;code&gt;
epoch 50 iter 17: train loss 0.04646. lr 6.000000e-05: 100%|███████████████████████████████████████████████████████████████| 18/18 [00:00&amp;lt;00:00, 23.80it/s]
01/31/2021 13:24:21 - INFO - mingpt.trainer -   test loss: 0.004733
final score: 9000/9000 = 100.00% correct
GPT claims that 055 + 045 = 090 (gt is 100; NOPE)
final score: 999/1000 = 99.90% correct
&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;Expected behavior&lt;/h2&gt;

&lt;p&gt;I would expect to be the same&lt;/p&gt;

&lt;h2&gt;Environment&lt;/h2&gt;

&lt;p&gt;PyTorch version: 1.7.1+cu110&lt;br/&gt;
Is debug build: False
CUDA used to build PyTorch: 11.0&lt;br/&gt;
ROCM used to build PyTorch: N/A&lt;/p&gt;

&lt;p&gt;OS: Microsoft Windows 10 Pro
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect&lt;/p&gt;

&lt;p&gt;Python version: 3.7 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: GeForce GTX 1060 5GB
GPU 1: GeForce RTX 2060 SUPER&lt;/p&gt;

&lt;p&gt;Nvidia driver version: 461.40
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A&lt;/p&gt;

&lt;p&gt;Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] torch==1.7.1+cu110
[pip3] torchaudio==0.7.2
[pip3] torchvision==0.8.2+cu110
[conda] Could not collect&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?auto=webp&amp;s=08c43430ff8793a25b767aa172468acb35be611a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4adad794ed3c0e2f76e2da992ae9eef8c074cae4', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5b3cb400fea60e0dfe87ebc5314f7db8016f8ab', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f149723b636ef3087022ddc1efa6ab53e3d4bc69', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_YPUxJ8AyYwgpO9r-tD7JUoQ-Nx8IlATLoEfmjW48iQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l9sp4s,True,,matpoliquin,,6,True,all_ads,False,[],False,,/r/pytorch/comments/l9sp4s/different_results_on_rtx_2060s_vs_gtx_1060_with/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l9sp4s/different_results_on_rtx_2060s_vs_gtx_1060_with/,7135,1612147045.0,0,,False,,,,,,,,
92,,pytorch,"Hello community, I'm working on Transformer and did some texting, and was wondering how to add a mask to the transformers to only look into the past, by adding a src mask?

&amp;#x200B;

Thank you !",t2_7l9ti89m,False,,0,False,Transformers : add src mask to the forward function,[],r/pytorch,False,6,,0,,,False,t3_l9igdg,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1612146555.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello community, I&amp;#39;m working on Transformer and did some texting, and was wondering how to add a mask to the transformers to only look into the past, by adding a src mask?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l9igdg,True,,rayanaay,,1,True,all_ads,False,[],False,,/r/pytorch/comments/l9igdg/transformers_add_src_mask_to_the_forward_function/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l9igdg/transformers_add_src_mask_to_the_forward_function/,7135,1612117755.0,0,,False,,,,,,,,
93,,pytorch,"So there is only a 64 bit pytorch wheel, and everytime I try to install pytorch on my 32 bit system it gives an error. When I look up if it is possible, some people on the internet say it is not possible to install pytorch on a 32 bit system. Does anybody have any suggestions for installing pytorch on a 32 bit system: I really need it for local hosting.",t2_77x7ml6a,False,,0,False,How do I install pytorch on a 32 bit system?,[],r/pytorch,False,6,,0,,,False,t3_l94t6b,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,True,self,False,,[],{},,,True,,1612097197.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So there is only a 64 bit pytorch wheel, and everytime I try to install pytorch on my 32 bit system it gives an error. When I look up if it is possible, some people on the internet say it is not possible to install pytorch on a 32 bit system. Does anybody have any suggestions for installing pytorch on a 32 bit system: I really need it for local hosting.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l94t6b,True,,TheAnonymous123456,,3,True,all_ads,False,[],False,,/r/pytorch/comments/l94t6b/how_do_i_install_pytorch_on_a_32_bit_system/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l94t6b/how_do_i_install_pytorch_on_a_32_bit_system/,7135,1612068397.0,0,,False,,,,,,,,
94,,pytorch,"Hello
Graphics device installed in my laptop is Intel(R) HD
Can i use GPU to train a image model ? (By using cuda)
Or how can i check if my graphics devices is compatible for train or not.

I will appreaciate any command 
Sisterly",t2_6bnk8xm6,False,,0,False,GPU,[],r/pytorch,False,6,,0,,,False,t3_l8ywjs,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1612078557.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello
Graphics device installed in my laptop is Intel(R) HD
Can i use GPU to train a image model ? (By using cuda)
Or how can i check if my graphics devices is compatible for train or not.&lt;/p&gt;

&lt;p&gt;I will appreaciate any command 
Sisterly&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l8ywjs,True,,aurora365,,2,True,all_ads,False,[],False,,/r/pytorch/comments/l8ywjs/gpu/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l8ywjs/gpu/,7135,1612049757.0,0,,False,,,,,,,,
95,,pytorch,,t2_1ffz9tjt,False,,0,False,Colored ASCII generator (image2image and video2video) (Code: https://github.com/uvipen/ASCII-generator),[],r/pytorch,False,6,,0,65.0,,False,t3_l7v1m1,False,dark,0.88,,public,18,0,{},140.0,,False,[],,False,False,,{},,False,18,,False,https://a.thumbs.redditmedia.com/XM7dWm9zgaF0mukJPAhzEbhH6VuyD3FOk8iuwl9lRr8.jpg,False,,[],{},,,False,,1611962251.0,text,6,,,text,reddit.com,False,,,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l7v1m1,True,,1991viet,,1,True,all_ads,False,[],False,,/r/pytorch/comments/l7v1m1/colored_ascii_generator_image2image_and/,all_ads,False,https://www.reddit.com/gallery/l7v1m1,7135,1611933451.0,0,,False,https://www.reddit.com/gallery/l7v1m1,,,"{'lo916vz5gae61': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 50, 'x': 108, 'u': 'https://preview.redd.it/lo916vz5gae61.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=70a444eb23153b0617ecbca3af5aceda13cf9be1'}, {'y': 101, 'x': 216, 'u': 'https://preview.redd.it/lo916vz5gae61.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=c1113c5ee53f3c0ef20e65990da9c1726257a82b'}, {'y': 150, 'x': 320, 'u': 'https://preview.redd.it/lo916vz5gae61.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=7b19477fa2f22149882e2b7a697a4c7ebef020a7'}, {'y': 300, 'x': 640, 'u': 'https://preview.redd.it/lo916vz5gae61.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=311f5243be0c407b938e9c954a087db3443c87a9'}, {'y': 450, 'x': 960, 'u': 'https://preview.redd.it/lo916vz5gae61.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=7b9b4c647a2bd19a7368cae39212f69452aa1de9'}], 's': {'y': 450, 'gif': 'https://i.redd.it/lo916vz5gae61.gif', 'mp4': 'https://preview.redd.it/lo916vz5gae61.gif?format=mp4&amp;s=dcc602de5fcd218e46609707bec2d6011d8551ab', 'x': 960}, 'id': 'lo916vz5gae61'}, '54tcr3z5gae61': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 47, 'x': 108, 'u': 'https://preview.redd.it/54tcr3z5gae61.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b2f000386203cfdb06629ecd31d7bd00557cac4f'}, {'y': 94, 'x': 216, 'u': 'https://preview.redd.it/54tcr3z5gae61.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=13cc1c1a3f9ad1cc2f9a218abefe2f25d33f8165'}, {'y': 139, 'x': 320, 'u': 'https://preview.redd.it/54tcr3z5gae61.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8c355ac45bdc4e9239187c69e608479c5af5e0e0'}, {'y': 278, 'x': 640, 'u': 'https://preview.redd.it/54tcr3z5gae61.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=41e455f8a5bb311c9dcffdc1a389875df0653423'}, {'y': 418, 'x': 960, 'u': 'https://preview.redd.it/54tcr3z5gae61.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=11f6f5bbd79fb3d28cb7e6655524984ba5f9fe04'}, {'y': 470, 'x': 1080, 'u': 'https://preview.redd.it/54tcr3z5gae61.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ed9efa1b42782a52fb3387d61efdbef6aff03822'}], 's': {'y': 1046, 'x': 2400, 'u': 'https://preview.redd.it/54tcr3z5gae61.jpg?width=2400&amp;format=pjpg&amp;auto=webp&amp;s=f6e06e25d295af40e2776effe47a0b3c988759ce'}, 'id': '54tcr3z5gae61'}}",True,"{'items': [{'media_id': 'lo916vz5gae61', 'id': 25010909}, {'media_id': '54tcr3z5gae61', 'id': 25010910}]}",,
96,,pytorch,"Added Switch Transformer implementation to our collection of  deep learning algorithms.

Switch Transformer routes (switches) tokens among a set  of position-wise feed forward networks based on the token embedding. This allows it to have a many more parameters but use the same amount of compute.

Code with side-by-side notes: [https://nn.labml.ai/transformers/switch/index.html](https://nn.labml.ai/transformers/switch/index.html)

Github: [https://github.com/lab-ml/nn/blob/master/labml\_nn/transformers/switch/\_\_init\_\_.py](https://github.com/lab-ml/nn/blob/master/labml_nn/transformers/switch/__init__.py)

Paper: [https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961)",t2_1jyhaoq,False,,0,False,Switch Transformer Single GPU PyTorch implementation/tutorial,[],r/pytorch,False,6,,0,,,False,t3_l7wxf1,False,dark,1.0,,public,9,0,{},,,False,[],,False,False,,{},,False,9,,False,self,False,,[],{},self,,True,,1611966666.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Added Switch Transformer implementation to our collection of  deep learning algorithms.&lt;/p&gt;

&lt;p&gt;Switch Transformer routes (switches) tokens among a set  of position-wise feed forward networks based on the token embedding. This allows it to have a many more parameters but use the same amount of compute.&lt;/p&gt;

&lt;p&gt;Code with side-by-side notes: &lt;a href=""https://nn.labml.ai/transformers/switch/index.html""&gt;https://nn.labml.ai/transformers/switch/index.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/lab-ml/nn/blob/master/labml_nn/transformers/switch/__init__.py""&gt;https://github.com/lab-ml/nn/blob/master/labml_nn/transformers/switch/__init__.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/abs/2101.03961""&gt;https://arxiv.org/abs/2101.03961&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?auto=webp&amp;s=54844664dea21c579bf85bb9f77db2f366d995b5', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4935835b34570436c07bbf78c25cdcda892a4e7f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dca994f869c1d4df2b64aa8e606b9df61a08437', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7825efebb9ce056a4cec7a106daf8f326e9c2856', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'oze48py95sVXMx6dFac2s_Kf1E0eS-CySSSJBqwJb-o'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l7wxf1,True,,mlvpj,,0,True,all_ads,False,[],False,,/r/pytorch/comments/l7wxf1/switch_transformer_single_gpu_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l7wxf1/switch_transformer_single_gpu_pytorch/,7135,1611937866.0,0,,False,,,,,,,,
97,,pytorch,,t2_83t22c3q,False,,0,False,PyTorch Dataloaders and Transorms,[],r/pytorch,False,6,,0,105.0,,False,t3_l88xm5,False,dark,0.4,,public,0,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ovZ_54IUSU4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': '[4] Image dataset preparation in PyTorch (Dataloaders and Transforms)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ovZ_54IUSU4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'if else statement', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ovZ_54IUSU4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj4PWxdz8-f0whMeZ3b5dEQ'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ovZ_54IUSU4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/l88xm5', 'height': 200}",,False,0,,False,https://a.thumbs.redditmedia.com/XSD6RUp6UvFW1i_H6B_7tmDszsFBKgUgdVhii94Wwn8.jpg,False,,[],{},rich:video,,False,,1611994856.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/9A1HJvpNaR2CTiSxNlh3vJ3aWiunN4dhTyitS4Q_nlI.jpg?auto=webp&amp;s=138cb4f45f80f88f4b00a0c49c31d810f466da8a', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/9A1HJvpNaR2CTiSxNlh3vJ3aWiunN4dhTyitS4Q_nlI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5fe6a5250e02982fb976e7c9be81f52a44f8fe6f', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/9A1HJvpNaR2CTiSxNlh3vJ3aWiunN4dhTyitS4Q_nlI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=624acfd8f9d180c81a05fefbaf9b2ca5a3d24d79', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/9A1HJvpNaR2CTiSxNlh3vJ3aWiunN4dhTyitS4Q_nlI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=794112a1630db7cfbcbcc5f103d00a9f2151d7c5', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'SFyBF61W8Ntja6WkYWY04a9gb6hZrDy_RkRZ56BIYj0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l88xm5,True,,ifelsestatement007,,0,True,all_ads,False,[],False,,/r/pytorch/comments/l88xm5/pytorch_dataloaders_and_transorms/,all_ads,False,https://www.youtube.com/watch?v=ovZ_54IUSU4,7135,1611966056.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': '[4] Image dataset preparation in PyTorch (Dataloaders and Transforms)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ovZ_54IUSU4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'if else statement', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ovZ_54IUSU4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj4PWxdz8-f0whMeZ3b5dEQ'}}",False,https://www.youtube.com/watch?v=ovZ_54IUSU4,,,,,,,
98,,pytorch,"Medium Post: [https://alvarobartt.medium.com/serving-pytorch-models-with-torchserve-6b8e8cbdb632](https://alvarobartt.medium.com/serving-pytorch-models-with-torchserve-6b8e8cbdb632)

Source Code: [https://github.com/alvarobartt/serving-pytorch-models](https://github.com/alvarobartt/serving-pytorch-models)",t2_xk7en7t,False,,0,False,Serving PyTorch models with TorchServe 🔥,[],r/pytorch,False,6,,0,,,False,t3_l77c7d,False,dark,1.0,,public,9,0,{},,,False,[],,False,False,,{},,False,9,,False,self,False,,[],{},self,,True,,1611893491.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Medium Post: &lt;a href=""https://alvarobartt.medium.com/serving-pytorch-models-with-torchserve-6b8e8cbdb632""&gt;https://alvarobartt.medium.com/serving-pytorch-models-with-torchserve-6b8e8cbdb632&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Source Code: &lt;a href=""https://github.com/alvarobartt/serving-pytorch-models""&gt;https://github.com/alvarobartt/serving-pytorch-models&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/_C-zrXmeF4rSyGJw6QwdmUEgZLBgUIM9E4KAhas7IKM.jpg?auto=webp&amp;s=c30c630d60ffc6baacaf3dde15b866a17ed2a407', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/_C-zrXmeF4rSyGJw6QwdmUEgZLBgUIM9E4KAhas7IKM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=31abd766b803c2593dcb96e82b14f1391bbc7713', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/_C-zrXmeF4rSyGJw6QwdmUEgZLBgUIM9E4KAhas7IKM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a4433f67ff9557dac968d21a6f26857c21e4de9', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/_C-zrXmeF4rSyGJw6QwdmUEgZLBgUIM9E4KAhas7IKM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db61a61c46c159526e19c89c01f19a5f7d5674a8', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/_C-zrXmeF4rSyGJw6QwdmUEgZLBgUIM9E4KAhas7IKM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=475c973470c115c4b694b0efbd421bc20a7f5b4f', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/_C-zrXmeF4rSyGJw6QwdmUEgZLBgUIM9E4KAhas7IKM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=44b36e25294c12d2317551715cec8cc311c4484b', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/_C-zrXmeF4rSyGJw6QwdmUEgZLBgUIM9E4KAhas7IKM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=010942629978ef98c143410e467b20403cbd5a23', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'FIm3zp3y8t_y_CBTfzduFvsIwBOQwnhPQyKCcdB565o'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l77c7d,True,,alvarobartt,,0,True,all_ads,False,[],False,,/r/pytorch/comments/l77c7d/serving_pytorch_models_with_torchserve/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l77c7d/serving_pytorch_models_with_torchserve/,7135,1611864691.0,0,,False,,,,,,,,
99,,pytorch,,t2_13kkch,False,,0,False,Generating music with PyTorch and HuggingFace,[],r/pytorch,False,6,,0,68.0,,False,t3_l6xfxv,False,dark,0.9,,public,8,0,{},140.0,,False,[],,False,False,,{},,False,8,,False,https://a.thumbs.redditmedia.com/Lhrox1Ny40C-B6uP9nXNslR-tRBOA9VjkDkIGhZ8cX0.jpg,False,,[],{},link,,False,,1611871606.0,text,6,,,text,alxmamaev.medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/i8_vT61uB2l6ZIBmznQ6ANXShu32wyHTj0WKCyHclos.jpg?auto=webp&amp;s=4714d707dec68a700974197a855714c0f4d6c923', 'width': 1200, 'height': 586}, 'resolutions': [{'url': 'https://external-preview.redd.it/i8_vT61uB2l6ZIBmznQ6ANXShu32wyHTj0WKCyHclos.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f266746a65608c089ac36de9a125f03188e9d337', 'width': 108, 'height': 52}, {'url': 'https://external-preview.redd.it/i8_vT61uB2l6ZIBmznQ6ANXShu32wyHTj0WKCyHclos.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f56309eef92c1f77eb8d4a1fc67447fad1374a3', 'width': 216, 'height': 105}, {'url': 'https://external-preview.redd.it/i8_vT61uB2l6ZIBmznQ6ANXShu32wyHTj0WKCyHclos.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b8c89a22e8421955dd133b5818a38f8c842b9a1', 'width': 320, 'height': 156}, {'url': 'https://external-preview.redd.it/i8_vT61uB2l6ZIBmznQ6ANXShu32wyHTj0WKCyHclos.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e10715915ac73e1aff075b448a9fffbc29258ba', 'width': 640, 'height': 312}, {'url': 'https://external-preview.redd.it/i8_vT61uB2l6ZIBmznQ6ANXShu32wyHTj0WKCyHclos.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cc90ea767be2f84421f9be6f1d6edfdbff62b9a0', 'width': 960, 'height': 468}, {'url': 'https://external-preview.redd.it/i8_vT61uB2l6ZIBmznQ6ANXShu32wyHTj0WKCyHclos.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2cf6a071624eeceb48d90f1de3f10dd15ced5237', 'width': 1080, 'height': 527}], 'variants': {}, 'id': 'jAsnuj9NiXe8HiccpDaaEriMrAXyxT_b7WjCAnpI08c'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l6xfxv,True,,alxmamaev,,0,True,all_ads,False,[],False,,/r/pytorch/comments/l6xfxv/generating_music_with_pytorch_and_huggingface/,all_ads,False,https://alxmamaev.medium.com/generating-music-with-ai-or-transformers-go-brrrr-3a3ac5a04126,7135,1611842806.0,0,,False,https://alxmamaev.medium.com/generating-music-with-ai-or-transformers-go-brrrr-3a3ac5a04126,,,,,,,
100,,pytorch,,t2_4xhrybaz,False,,0,False,Really need help here.,[],r/pytorch,False,6,,0,,,False,t3_l6om6s,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,default,False,,[],{},,,False,,1611839124.0,text,6,,,text,self.deeplearning,False,,,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l6om6s,True,,banenvy,,1,False,all_ads,False,[],False,,/r/pytorch/comments/l6om6s/really_need_help_here/,all_ads,False,/r/deeplearning/comments/l61h34/how_to_extract_attention_weights_from/,7135,1611810324.0,0,,False,/r/deeplearning/comments/l61h34/how_to_extract_attention_weights_from/,"[{'approved_at_utc': None, 'subreddit': 'deeplearning', 'selftext': 'I saw that it is possible to do that using GATLayer. But I already have a code which uses GATConv and I cannot find a way to do it.\n\nI need these weights to make attention maps. Please help.', 'author_fullname': 't2_4xhrybaz', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to extract attention weights from dgl.nn.pytorch.conv.GATConv ?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/deeplearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_l61h34', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.33, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 0, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1611771116.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.deeplearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I saw that it is possible to do that using GATLayer. But I already have a code which uses GATConv and I cannot find a way to do it.&lt;/p&gt;\n\n&lt;p&gt;I need these weights to make attention maps. Please help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2t5eh', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'l61h34', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'banenvy', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/deeplearning/comments/l61h34/how_to_extract_attention_weights_from/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/deeplearning/comments/l61h34/how_to_extract_attention_weights_from/', 'subreddit_subscribers': 62501, 'created_utc': 1611742316.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_l61h34,,,,,
101,,pytorch," Hi, everyone.

I am attempting to constrain some outputs of my regression network, say x, y, z = model(data), where x, y, z are scalars. The constrain that I want to impose is that when predicting all three dependent variables, the condition “x + y &lt;=1.0” must be honored. Given this description, can I implement this in a forward function?

Thank you!",t2_4ecq42y4,False,,0,False,Constrain outputs in a regression problem,[],r/pytorch,False,6,,0,,,False,t3_l6borr,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1611802376.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, everyone.&lt;/p&gt;

&lt;p&gt;I am attempting to constrain some outputs of my regression network, say x, y, z = model(data), where x, y, z are scalars. The constrain that I want to impose is that when predicting all three dependent variables, the condition “x + y &amp;lt;=1.0” must be honored. Given this description, can I implement this in a forward function?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l6borr,True,,ncuxomun,,8,True,all_ads,False,[],False,,/r/pytorch/comments/l6borr/constrain_outputs_in_a_regression_problem/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l6borr/constrain_outputs_in_a_regression_problem/,7135,1611773576.0,0,,False,,,,,,,,
102,,pytorch,"Hi /r/pytorch readers!

We have created a [labelling tool](https://humanlambdas.com/solutions/data-labelling) that can be customized to display all sorts of data models and tasks. Here are a couple of examples for [NLP](https://humanlambdas.com/templates/nlp-news-article-annotation) and [CV](https://humanlambdas.com/templates/computer-vision-annotation). 

I hope some of you will find this useful, and if you have any thoughts I would love to hear your feedback!",t2_bk9jh,False,,0,False,Tool for Complex Data Labelling Tasks,[],r/pytorch,False,6,,0,,,False,t3_l67u11,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1611792786.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi &lt;a href=""/r/pytorch""&gt;/r/pytorch&lt;/a&gt; readers!&lt;/p&gt;

&lt;p&gt;We have created a &lt;a href=""https://humanlambdas.com/solutions/data-labelling""&gt;labelling tool&lt;/a&gt; that can be customized to display all sorts of data models and tasks. Here are a couple of examples for &lt;a href=""https://humanlambdas.com/templates/nlp-news-article-annotation""&gt;NLP&lt;/a&gt; and &lt;a href=""https://humanlambdas.com/templates/computer-vision-annotation""&gt;CV&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;I hope some of you will find this useful, and if you have any thoughts I would love to hear your feedback!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/FCsoQqCJBLR0U4NXFhqSlTtl3D0eGea4kgPP_frE6x0.jpg?auto=webp&amp;s=235e2355b6130fbfcb78b89d705af907d386db92', 'width': 2880, 'height': 1800}, 'resolutions': [{'url': 'https://external-preview.redd.it/FCsoQqCJBLR0U4NXFhqSlTtl3D0eGea4kgPP_frE6x0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=72b84a4dca953ac834a8e3f8e4ed6eddb45022b7', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/FCsoQqCJBLR0U4NXFhqSlTtl3D0eGea4kgPP_frE6x0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=391529d2d5afd4789494398f33e095575e6458f1', 'width': 216, 'height': 135}, {'url': 'https://external-preview.redd.it/FCsoQqCJBLR0U4NXFhqSlTtl3D0eGea4kgPP_frE6x0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0fd8360f94d842f3c3df69621a16b9fbc111da10', 'width': 320, 'height': 200}, {'url': 'https://external-preview.redd.it/FCsoQqCJBLR0U4NXFhqSlTtl3D0eGea4kgPP_frE6x0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=36f30f8e4d3e78f928b1f404a13bed8fcd6b0ad5', 'width': 640, 'height': 400}, {'url': 'https://external-preview.redd.it/FCsoQqCJBLR0U4NXFhqSlTtl3D0eGea4kgPP_frE6x0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=250e4fa92b2e3f0487cf1aba5e958362494347fa', 'width': 960, 'height': 600}, {'url': 'https://external-preview.redd.it/FCsoQqCJBLR0U4NXFhqSlTtl3D0eGea4kgPP_frE6x0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d8929d596e259409c5a45d3e24d315b9ce58f967', 'width': 1080, 'height': 675}], 'variants': {}, 'id': '6taeEAVxAwEj0xQWmYL9qf3ZXM5UBBQknCvmdtvgw4U'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l67u11,True,,bernatfp,,4,True,all_ads,False,[],False,,/r/pytorch/comments/l67u11/tool_for_complex_data_labelling_tasks/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l67u11/tool_for_complex_data_labelling_tasks/,7135,1611763986.0,0,,False,,,,,,,,
103,,pytorch,"My model is training and while the training loss is decreasing, my test loss is increasing.

I understand that this is because my model is overfitting. There is already a dropout layer in my architecture, so how can I decrease my test loss more?",t2_ebu4m,False,,0,False,How can I decrease my test loss?,[],r/pytorch,False,6,,0,,,False,t3_l4qpws,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1611619217.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My model is training and while the training loss is decreasing, my test loss is increasing.&lt;/p&gt;

&lt;p&gt;I understand that this is because my model is overfitting. There is already a dropout layer in my architecture, so how can I decrease my test loss more?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l4qpws,True,,Pepipasta,,4,True,all_ads,False,[],False,,/r/pytorch/comments/l4qpws/how_can_i_decrease_my_test_loss/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l4qpws/how_can_i_decrease_my_test_loss/,7135,1611590417.0,0,,False,,,,,,,,
104,,pytorch,"Hello community, I’m reading the famous "" attention is all you need "" paper, and was wondering:
Does   A multi head attention  with only one head , is equivalent to an attention layer ( the classical/basic one ) ?",t2_7l9ti89m,False,,0,False,Transformer and attention mechanism,[],r/pytorch,False,6,,0,,,False,t3_l455fn,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1611542399.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello community, I’m reading the famous &amp;quot; attention is all you need &amp;quot; paper, and was wondering:
Does   A multi head attention  with only one head , is equivalent to an attention layer ( the classical/basic one ) ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l455fn,True,,rayanaay,,1,True,all_ads,False,[],False,,/r/pytorch/comments/l455fn/transformer_and_attention_mechanism/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l455fn/transformer_and_attention_mechanism/,7135,1611513599.0,0,,False,,,,,,,,
105,,pytorch,"[https://medium.com/towards-artificial-intelligence/deploy-deep-learning-models-using-streamlit-and-heroku-22f6efae9141](https://medium.com/towards-artificial-intelligence/deploy-deep-learning-models-using-streamlit-and-heroku-22f6efae9141)

Deploying Deep Learning models with an interactive UI isn't easy. In this hands-on tutorial blog, a NLP model with a very minimal frontend is deployed using Streamlit. The last part of the blog includes steps to deploy the frontend and the backend to the internet using Heroku. A live version is available @ [https://classifyquestions.herokuapp.com.](https://classifyquestions.herokuapp.com/?fbclid=IwAR3ly66YyJ0mZNW_omXLcODQSGcr8P_ARhWfydKaYZl0u4Xk1M3aT0IthwU)

Have a nice read. If you have any feedback or question comment down below.

&amp;#x200B;

https://reddit.com/link/l3t9f4/video/f9fq25llt7d61/player",t2_namkp97,False,,0,False,How to easily deploy any PyTorch model to the web?!,[],r/pytorch,False,6,,0,93.0,,False,t3_l3t9f4,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/luX91K4K4zKFkAI3VbXIKPiivTLjhH6CTDd6wemJAdg.jpg,False,,[],{},self,,True,,1611494594.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://medium.com/towards-artificial-intelligence/deploy-deep-learning-models-using-streamlit-and-heroku-22f6efae9141""&gt;https://medium.com/towards-artificial-intelligence/deploy-deep-learning-models-using-streamlit-and-heroku-22f6efae9141&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Deploying Deep Learning models with an interactive UI isn&amp;#39;t easy. In this hands-on tutorial blog, a NLP model with a very minimal frontend is deployed using Streamlit. The last part of the blog includes steps to deploy the frontend and the backend to the internet using Heroku. A live version is available @ &lt;a href=""https://classifyquestions.herokuapp.com/?fbclid=IwAR3ly66YyJ0mZNW_omXLcODQSGcr8P_ARhWfydKaYZl0u4Xk1M3aT0IthwU""&gt;https://classifyquestions.herokuapp.com.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Have a nice read. If you have any feedback or question comment down below.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://reddit.com/link/l3t9f4/video/f9fq25llt7d61/player""&gt;https://reddit.com/link/l3t9f4/video/f9fq25llt7d61/player&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Ztddh2JRH9ZJ0bkjTL2ME0orqSL9adILqGnIkoKRVlo.jpg?auto=webp&amp;s=02d6b1f86c765b68c449ceb9f095436e238649fd', 'width': 1200, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/Ztddh2JRH9ZJ0bkjTL2ME0orqSL9adILqGnIkoKRVlo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ea6c8fb4ca83f3ffb59050b5e4a539a4756a762', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/Ztddh2JRH9ZJ0bkjTL2ME0orqSL9adILqGnIkoKRVlo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0011c3903411e2cac87759d18fd5a23f73bd00c0', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/Ztddh2JRH9ZJ0bkjTL2ME0orqSL9adILqGnIkoKRVlo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=75b0ae4d8f7efe989bf597ad321a8bb9d6c9425f', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/Ztddh2JRH9ZJ0bkjTL2ME0orqSL9adILqGnIkoKRVlo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=10327a00d3da0ae5da51102a202bb6c7112d9d1f', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/Ztddh2JRH9ZJ0bkjTL2ME0orqSL9adILqGnIkoKRVlo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d158a5d3d2964b6f50a922c7f755a54dbfecc469', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/Ztddh2JRH9ZJ0bkjTL2ME0orqSL9adILqGnIkoKRVlo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=646f3f7ad0757acc272ee97d148fd944f0698dcb', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'PNyvmXrivV3KRD-EJ-ueEB8AgGOfhx3GIreUvl1UsiI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l3t9f4,True,,thevatsalsaglani,,0,True,all_ads,False,[],False,,/r/pytorch/comments/l3t9f4/how_to_easily_deploy_any_pytorch_model_to_the_web/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l3t9f4/how_to_easily_deploy_any_pytorch_model_to_the_web/,7135,1611465794.0,0,,False,,,,"{'f9fq25llt7d61': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/l3t9f4/asset/f9fq25llt7d61/DASHPlaylist.mpd?a=1618044137%2CNmU3ZTU2YzA2MWU3MzNlMmU1Y2M4NmU5Y2I0ZmQ1NGMzZTMwNGRmNzlkZDk2OGFiZjhkMjhkMTUzOGI1YTEyYQ%3D%3D&amp;v=1&amp;f=sd', 'x': 1280, 'y': 699, 'hlsUrl': 'https://v.redd.it/link/l3t9f4/asset/f9fq25llt7d61/HLSPlaylist.m3u8?a=1618044137%2CYjczMTdiYmFhN2I3MmU3ODEzMThkNTRiYTk0ZjMwYTU4MmE3YThhMzFkZTNjYTQ5MDg5ZjY3YTYzZGY1NWVmZQ%3D%3D&amp;v=1&amp;f=sd', 'id': 'f9fq25llt7d61', 'isGif': True}}",,,,
106,,pytorch,"(Apologies if this is repeatitive) So I have an RL pipeline, and the GPU RAM is exceeded after a certain epochs. I have already tried using Python's del() function as well as torch.cuda.empty\_cache()",t2_2whhhjzo,False,,0,False,How to keep GPU from getting full?,[],r/pytorch,False,6,,0,,,False,t3_l20955,False,dark,1.0,,public,8,0,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},,,True,,1611270633.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;(Apologies if this is repeatitive) So I have an RL pipeline, and the GPU RAM is exceeded after a certain epochs. I have already tried using Python&amp;#39;s del() function as well as torch.cuda.empty_cache()&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l20955,True,,pandudon,,15,True,all_ads,False,[],False,,/r/pytorch/comments/l20955/how_to_keep_gpu_from_getting_full/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l20955/how_to_keep_gpu_from_getting_full/,7135,1611241833.0,0,,False,,,,,,,,
107,,pytorch,"I am planning to make an age detector with 10 classes, each of them has range from 2-6 years old, 7-12 years old and so on.I use pretrained model from resnet18. During training, I did not freeze the layers, instead, I just let it update all the parameters. The loss function I am using is cross-entropy, with Adam optimizer and lr-scheduler. The datasets contain 10000 images for training and about 3000 images for validation.

The problem that keeping me stuck is that although the training loss seems able to continuous decreased, but the validation loss is not. No matter how I change the hyper-parameters, It will drop until minimum 33 and bounce back to 60+, which is the initial value when I started training. The accuracy for validation is only at most 46%

This is my code. Please have a look at what is causing this problem. Or is there any problem with datasets? Such as not enough datasets for training etc?

`if train_mode:`  
`train_loader, test_loader = data_load()`  
 `for iteration in range(iteration_start, epoch):`  
`time_start = time.time()`  
 `for current_mode in ['train', 'valid']:`  
`total_loss = 0`  
 `total_accuracy = 0`  
 `if current_mode == 'train':`  
`loader = train_loader`  
`model.train()`  
 `else:`  
`loader = test_loader`  
`model.eval()`  


`for batch in loader:`  
`images, labels = batch`  
`images = images.to(device)`  
`labels = labels.to(device)`  


`with torch.set_grad_enabled(current_mode == 'train'):`  
`output = model(images)`  
`loss = loss_function(output, labels)`  
`total_loss += loss.item()`  
 `if current_mode == 'train':`  
`optimizer.zero_grad()`  
`loss.backward()`  
`optimizer.step()`  
 `else:`  
`accuracy = calculate_accuracy(output, labels)`  
`total_accuracy += accuracy`  


`record_loss(total_loss, current_mode, iteration)`  
 `if current_mode == 'valid':`  
`scheduler.step(total_loss)  # total_loss or total_accuracy, based on which you want to enhance`  
 `record_accuracy(total_accuracy / len(loader), iteration)`

this is my model

`def custom_model():`  
`my_model = models.resnet18(pretrained=True)`  
 `# for paras in my_model.parameters():`  
`#     paras.requires_grad = False`  
 `num_fc_layer = my_model.fc.in_features`  
`custom_fc_layers = nn.Sequential(`  
`nn.BatchNorm1d(num_fc_layer),`  
 `nn.Dropout(0.5),`  
 `nn.Linear(num_fc_layer, num_class)`  
`)`  
`my_model.fc = custom_fc_layers`  


`return my_model`

And this is my optimizer and loss function, and scheduler

`if __name__ == '__main__':`  
`model = custom_model()`  
`model.to(device)`  
`optimizer = optim.Adam(model.parameters(), lr=learning_rate)`  
`scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=1, verbose=True)`  
`loss_function = nn.CrossEntropyLoss()`  
`main()`  
",t2_2gldcpub,False,,0,False,"Wanting to make age detector, but valid loss is high with low accuracy, not more than 46%",[],r/pytorch,False,6,,0,,,False,t3_l20uv2,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1611243986.0,,[],{},,,True,,1611272537.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am planning to make an age detector with 10 classes, each of them has range from 2-6 years old, 7-12 years old and so on.I use pretrained model from resnet18. During training, I did not freeze the layers, instead, I just let it update all the parameters. The loss function I am using is cross-entropy, with Adam optimizer and lr-scheduler. The datasets contain 10000 images for training and about 3000 images for validation.&lt;/p&gt;

&lt;p&gt;The problem that keeping me stuck is that although the training loss seems able to continuous decreased, but the validation loss is not. No matter how I change the hyper-parameters, It will drop until minimum 33 and bounce back to 60+, which is the initial value when I started training. The accuracy for validation is only at most 46%&lt;/p&gt;

&lt;p&gt;This is my code. Please have a look at what is causing this problem. Or is there any problem with datasets? Such as not enough datasets for training etc?&lt;/p&gt;

&lt;p&gt;&lt;code&gt;if train_mode:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;train_loader, test_loader = data_load()&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;for iteration in range(iteration_start, epoch):&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;time_start = time.time()&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;for current_mode in [&amp;#39;train&amp;#39;, &amp;#39;valid&amp;#39;]:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;total_loss = 0&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;total_accuracy = 0&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;if current_mode == &amp;#39;train&amp;#39;:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;loader = train_loader&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;model.train()&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;else:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;loader = test_loader&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;model.eval()&lt;/code&gt;  &lt;/p&gt;

&lt;p&gt;&lt;code&gt;for batch in loader:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;images, labels = batch&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;images = images.to(device)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;labels = labels.to(device)&lt;/code&gt;  &lt;/p&gt;

&lt;p&gt;&lt;code&gt;with torch.set_grad_enabled(current_mode == &amp;#39;train&amp;#39;):&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;output = model(images)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;loss = loss_function(output, labels)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;total_loss += loss.item()&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;if current_mode == &amp;#39;train&amp;#39;:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;optimizer.zero_grad()&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;loss.backward()&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;optimizer.step()&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;else:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;accuracy = calculate_accuracy(output, labels)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;total_accuracy += accuracy&lt;/code&gt;  &lt;/p&gt;

&lt;p&gt;&lt;code&gt;record_loss(total_loss, current_mode, iteration)&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;if current_mode == &amp;#39;valid&amp;#39;:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;scheduler.step(total_loss)  # total_loss or total_accuracy, based on which you want to enhance&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;record_accuracy(total_accuracy / len(loader), iteration)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;this is my model&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def custom_model():&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;my_model = models.resnet18(pretrained=True)&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;# for paras in my_model.parameters():&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;#     paras.requires_grad = False&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;num_fc_layer = my_model.fc.in_features&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;custom_fc_layers = nn.Sequential(&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;nn.BatchNorm1d(num_fc_layer),&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;nn.Dropout(0.5),&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;nn.Linear(num_fc_layer, num_class)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;my_model.fc = custom_fc_layers&lt;/code&gt;  &lt;/p&gt;

&lt;p&gt;&lt;code&gt;return my_model&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And this is my optimizer and loss function, and scheduler&lt;/p&gt;

&lt;p&gt;&lt;code&gt;if __name__ == &amp;#39;__main__&amp;#39;:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;model = custom_model()&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;model.to(device)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;optimizer = optim.Adam(model.parameters(), lr=learning_rate)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=1, verbose=True)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;loss_function = nn.CrossEntropyLoss()&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;main()&lt;/code&gt;  &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l20uv2,True,,kmlkmlpg,,6,True,all_ads,False,[],False,,/r/pytorch/comments/l20uv2/wanting_to_make_age_detector_but_valid_loss_is/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l20uv2/wanting_to_make_age_detector_but_valid_loss_is/,7135,1611243737.0,0,,False,,,,,,,,
108,,pytorch,,t2_83t22c3q,False,,0,False,Want to learn how to train the neural network to classify the images?,[],r/pytorch,False,6,,0,105.0,,False,t3_l29aqn,False,dark,0.38,,public,0,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/r3cVTxa0u3E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': '[5] How to train the neural network for image classification in PyTorch', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/r3cVTxa0u3E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': 'if else statement', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/r3cVTxa0u3E/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj4PWxdz8-f0whMeZ3b5dEQ'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/r3cVTxa0u3E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/l29aqn', 'height': 200}",,False,0,,False,https://b.thumbs.redditmedia.com/RNUHzu-Xm2yT0O1LEe1nfuszOzihC-xm_nnA9bpRJLU.jpg,False,,[],{},rich:video,,False,,1611296400.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/oz30xns4oU7BDud0UAk0Pa1z-EI92T_JpTjXzgY9vnk.jpg?auto=webp&amp;s=ee84540f035aca5db5b359193c771da59c646beb', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/oz30xns4oU7BDud0UAk0Pa1z-EI92T_JpTjXzgY9vnk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=368f33f89eddefd6caa27c243da78f74f4df25bf', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/oz30xns4oU7BDud0UAk0Pa1z-EI92T_JpTjXzgY9vnk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=01b961cb45b0be4e77f9e2fc3346912133b7bf0d', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/oz30xns4oU7BDud0UAk0Pa1z-EI92T_JpTjXzgY9vnk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=936a455a088ee7abe6378961ffecd45d9aaa797a', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'C1NoCz6O4DxzGNuITQsyL1VN0ICcb5OlxQYBGtUXWbk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l29aqn,True,,ifelsestatement007,,0,True,all_ads,False,[],False,,/r/pytorch/comments/l29aqn/want_to_learn_how_to_train_the_neural_network_to/,all_ads,False,https://www.youtube.com/watch?v=r3cVTxa0u3E,7135,1611267600.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': '[5] How to train the neural network for image classification in PyTorch', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/r3cVTxa0u3E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': 'if else statement', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/r3cVTxa0u3E/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj4PWxdz8-f0whMeZ3b5dEQ'}, 'type': 'youtube.com'}",False,https://www.youtube.com/watch?v=r3cVTxa0u3E,,,,,,,
109,,pytorch,"[Original paper](https://arxiv.org/abs/1508.06576). The [PyTorch docs](https://pytorch.org/docs/stable/torchvision/models.html) state that all models were trained using images that were in the range of `[0, 1]`. However, there seem to be better results when using images in the range `[0, 255]`:

&amp;#x200B;

Consider this output, which uses the `style loss` described in the original paper. Both set of results use an identical process, but the results on the bottom transform the tensor into the range of `[0, 255]` before applying backpropagation.

https://preview.redd.it/5dhpfkpo4ic61.png?width=1776&amp;format=png&amp;auto=webp&amp;s=690d509eb074e27d7a2dbfc0e86cd89ba62823c8

The results are more visually appealing for `[0, 255]`, and the behavior of the loss is better as well - images in the range of `[0, 1]` reach a nonzero convergence limit, whereas images in the range of `[0, 255]` do not reach this limit for 1000+ epochs.

&amp;#x200B;

&amp;#x200B;

Why does the range of `[0, 255]` work at all? If these models were trained in the range of `[0, 1]`, wouldn't it interpret any pixel above `1` as being purely white?",t2_12pb1i,False,,0,False,"Why does Neural Style Transfer work on images with range [0,255] if pytorch models are trained on images with range [0,1]?",[],r/pytorch,False,6,,0,42.0,,False,t3_l1av1b,False,dark,0.86,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/HjWgomZPTKqXkTeKLlwuZLjKiY-ndjS-kBHo6QkgDfY.jpg,1611155331.0,,[],{},,,True,,1611183755.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://arxiv.org/abs/1508.06576""&gt;Original paper&lt;/a&gt;. The &lt;a href=""https://pytorch.org/docs/stable/torchvision/models.html""&gt;PyTorch docs&lt;/a&gt; state that all models were trained using images that were in the range of &lt;code&gt;[0, 1]&lt;/code&gt;. However, there seem to be better results when using images in the range &lt;code&gt;[0, 255]&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Consider this output, which uses the &lt;code&gt;style loss&lt;/code&gt; described in the original paper. Both set of results use an identical process, but the results on the bottom transform the tensor into the range of &lt;code&gt;[0, 255]&lt;/code&gt; before applying backpropagation.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/5dhpfkpo4ic61.png?width=1776&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=690d509eb074e27d7a2dbfc0e86cd89ba62823c8""&gt;https://preview.redd.it/5dhpfkpo4ic61.png?width=1776&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=690d509eb074e27d7a2dbfc0e86cd89ba62823c8&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The results are more visually appealing for &lt;code&gt;[0, 255]&lt;/code&gt;, and the behavior of the loss is better as well - images in the range of &lt;code&gt;[0, 1]&lt;/code&gt; reach a nonzero convergence limit, whereas images in the range of &lt;code&gt;[0, 255]&lt;/code&gt; do not reach this limit for 1000+ epochs.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Why does the range of &lt;code&gt;[0, 255]&lt;/code&gt; work at all? If these models were trained in the range of &lt;code&gt;[0, 1]&lt;/code&gt;, wouldn&amp;#39;t it interpret any pixel above &lt;code&gt;1&lt;/code&gt; as being purely white?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l1av1b,True,,Nick_Pyth,,2,True,all_ads,False,[],False,,/r/pytorch/comments/l1av1b/why_does_neural_style_transfer_work_on_images/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l1av1b/why_does_neural_style_transfer_work_on_images/,7135,1611154955.0,0,,False,,,,"{'5dhpfkpo4ic61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 33, 'x': 108, 'u': 'https://preview.redd.it/5dhpfkpo4ic61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=56c3bffc63b5829a24c755de6729eacd08851cb5'}, {'y': 66, 'x': 216, 'u': 'https://preview.redd.it/5dhpfkpo4ic61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=254da10c05b7f8af0af46833eb14be48df5c09a1'}, {'y': 98, 'x': 320, 'u': 'https://preview.redd.it/5dhpfkpo4ic61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e7d4d13b5b7b1591c23448d625a999cc31033a0f'}, {'y': 196, 'x': 640, 'u': 'https://preview.redd.it/5dhpfkpo4ic61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c7ce65ebdaddf89bb7a25d4b1bf921a5a473318'}, {'y': 294, 'x': 960, 'u': 'https://preview.redd.it/5dhpfkpo4ic61.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=881c55e6b6eba40dd1847a8ce143a0738e984a25'}, {'y': 330, 'x': 1080, 'u': 'https://preview.redd.it/5dhpfkpo4ic61.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b2d6fb511e149d13a56d06bba8f6bf318d284e5'}], 's': {'y': 544, 'x': 1776, 'u': 'https://preview.redd.it/5dhpfkpo4ic61.png?width=1776&amp;format=png&amp;auto=webp&amp;s=690d509eb074e27d7a2dbfc0e86cd89ba62823c8'}, 'id': '5dhpfkpo4ic61'}}",,,,
110,,pytorch,"I'm currently doing the pytorch's tutorial itself and I want something that I can use as a supplement but there so much many books, and I want to narrow it down.",t2_egciz,False,,0,False,Any additional books to level up my skill in pytorch?,[],r/pytorch,False,6,,0,,,False,t3_l12yg1,False,dark,1.0,,public,10,0,{},,,False,[],,False,False,,{},,False,10,,False,self,False,,[],{},,,True,,1611150565.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently doing the pytorch&amp;#39;s tutorial itself and I want something that I can use as a supplement but there so much many books, and I want to narrow it down.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l12yg1,True,,noodlepotato,,5,True,all_ads,False,[],False,,/r/pytorch/comments/l12yg1/any_additional_books_to_level_up_my_skill_in/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l12yg1/any_additional_books_to_level_up_my_skill_in/,7135,1611121765.0,0,,False,,,,,,,,
111,,pytorch,,t2_83t22c3q,False,,0,False,Ways to save your neural network,[],r/pytorch,False,6,,0,105.0,,False,t3_l0x61e,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/j9u1qm4T4Y0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': '[6] Ways to save the neural network in PyTorch', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/j9u1qm4T4Y0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': 'if else statement', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/j9u1qm4T4Y0/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj4PWxdz8-f0whMeZ3b5dEQ'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/j9u1qm4T4Y0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/l0x61e', 'height': 200}",,False,0,,False,https://b.thumbs.redditmedia.com/MsfqRWNLU5VJMQqo50tISAJj7ULetLDslcuGTlBPXCY.jpg,False,,[],{},rich:video,,False,,1611130683.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ChyRgGaRBT0TJ6Spb3axZ4xm-8Qt1yh3ARaA4VlpwpU.jpg?auto=webp&amp;s=0d4cf28808d4f0cff08c784e281faf96412d53de', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/ChyRgGaRBT0TJ6Spb3axZ4xm-8Qt1yh3ARaA4VlpwpU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4c98b68d4b9b0751e29d2495de0aecebf2870f52', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/ChyRgGaRBT0TJ6Spb3axZ4xm-8Qt1yh3ARaA4VlpwpU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5cc57a9c7891462424ee8d7b16d271ba85d9dec6', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/ChyRgGaRBT0TJ6Spb3axZ4xm-8Qt1yh3ARaA4VlpwpU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cad16b655b0e7320d8f42a652a37f4f82c432ec5', 'width': 320, 'height': 240}], 'variants': {}, 'id': '0gw4ofYg0dkE6er--litEQXbIKO2QcKE2okdpF2XEVE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l0x61e,True,,ifelsestatement007,,0,True,all_ads,False,[],False,,/r/pytorch/comments/l0x61e/ways_to_save_your_neural_network/,all_ads,False,https://www.youtube.com/watch?v=j9u1qm4T4Y0,7135,1611101883.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': '[6] Ways to save the neural network in PyTorch', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/j9u1qm4T4Y0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': 'if else statement', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/j9u1qm4T4Y0/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj4PWxdz8-f0whMeZ3b5dEQ'}, 'type': 'youtube.com'}",False,https://www.youtube.com/watch?v=j9u1qm4T4Y0,,,,,,,
112,,pytorch,,t2_8fgbbr8x,False,,0,False,"RecBole: A unified, comprehensive and efficient recommendation library.",[],r/pytorch,False,6,,0,54.0,,False,t3_l0m1fj,False,dark,0.81,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/RF4CP8_q6lOUk-Wk6QDMSmPFgyBylHjC2DHVOSc5uxE.jpg,False,,[],{},,,False,,1611098894.0,text,6,,,text,self.recommendersystems,False,,,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l0m1fj,True,,EliverQ,,0,True,all_ads,False,[],False,,/r/pytorch/comments/l0m1fj/recbole_a_unified_comprehensive_and_efficient/,all_ads,False,/r/recommendersystems/comments/l0lyri/recbole_a_unified_comprehensive_and_efficient/,7135,1611070094.0,0,,False,/r/recommendersystems/comments/l0lyri/recbole_a_unified_comprehensive_and_efficient/,"[{'approved_at_utc': None, 'subreddit': 'recommendersystems', 'selftext': ""RecBole is developed based on Python and PyTorch for reproducing and developing recommendation algorithms in a unified, comprehensive and efficient framework for research purposes.\n\nIn the RecBole framework, users only need to make a simple configuration to test the performance of different models on different datasets. And it's convenient for users to make secondary developments and add new models. The main features of RecBole are as follows:\n\n## General and extensible data structure.\n\nRecBole designs general and extensible data structures to unify the formatting and usage of various recommendation datasets.\n\nIn order to realize the unified management and usage of each dataset, a new data storage format has been developed in RecBole, which can support all common datasets and realize efficient storage and loading. It contains 4 feature types and 6 optional file types. Datasets that are private to the user can be automatically managed under this framework just by processing this file format.\n\nEach atomic file can be viewed as a m×n table (except header), where n is the number of features and m is the number of data records. The first row corresponds to feature names, in which each entry has the form of feat\\_name:feat\\_type，indicating the feature name and feature type. We support four feature types, which can be processed by tensors in batch.\n\n[4 feature types](https://preview.redd.it/ypbinz51a9c61.png?width=854&amp;format=png&amp;auto=webp&amp;s=07b075aa75fc961003bf057b4a98b876ebd3db45)\n\nSo far, our library introduces six atomic file types, we identify different files by their suffixes.\n\n&amp;#x200B;\n\n[6 optional file types](https://preview.redd.it/157ung9u2bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=12790c0c108072952702a70f214c640ecfbf6c47)\n\n# Comprehensive benchmark models and datasets.\n\nRecBole implements 64 commonly used recommendation algorithms, and provide the formatted copies of 27 recommendation datasets.\n\n&amp;#x200B;\n\n[General recommendation models](https://preview.redd.it/wk1x0chy2bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=bc38c53d94e7271ee65cdfd4af1307cde3c432ac)\n\n&amp;#x200B;\n\n[Context recommendation models](https://preview.redd.it/afgid3r23bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=e6538e253053fd1426405030b86283f4524737e0)\n\n&amp;#x200B;\n\n[Sequential recommendation models](https://preview.redd.it/425bk1653bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=3fcc38dcd6d62b18836ef23ccf4247dbccdde304)\n\n&amp;#x200B;\n\n[Knowledge recommendation models](https://preview.redd.it/1zefui273bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=b8ec8b5d264648ee8d425e0c3cf7013be18ad126)\n\nAnd collected datasets in our library RecBole are as follows (Users need to download copies of the original data, and then use the pre-processing script provided by RecBole to process it, or download the processed datasets directly from the address provided).\n\n&amp;#x200B;\n\n[All datasets](https://preview.redd.it/v67j7fr93bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=63a70d8cefb912f8eea05b5cb6fcb2c3d87d20ff)\n\n# Efficient GPU-accelerated execution.\n\nRecBole optimizes the efficiency with a number of improved techniques oriented to the GPU environment.\n\nWe constructed preliminary experiments to test the time and memory cost on three different-sized datasets (small, medium and large). Here is the result of General recommendation models on ml-1m dataset (If you want to know more result, please go to our GitHub Homepage at the end of this article) :\n\n&amp;#x200B;\n\nhttps://preview.redd.it/bihm4cid3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=7f8b2b7f42500a60e2afbcc0cb7f13e095918b82\n\n# Extensive and standard evaluation protocols.\n\nRecBole supports a series of widely adopted evaluation protocols or settings for testing and comparing recommendation algorithms.\n\nFor advanced users and secondary developers, RecBole also provides a very flexible evaluation interface. Users can use simple codes and parameters to realize different combinations of sampling and data segmentation, and package the commonly used combinations to achieve quick configuration. As far as we know, this is the most comprehensive open-source framework that currently supports metrics, which supports different dataset segmentation, sampling, etc.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/lt1twf2g3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=63f8744e249373761604a02ccd812f25c7d8d575\n\n**Active GitHub Community.**\n\nSo far, we have received 65 issues and replied to each one carefully.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/0j9t9ksh3bc61.png?width=616&amp;format=png&amp;auto=webp&amp;s=2699844573c666ab1499bf723c57e3dbaf23d656\n\nMeanwhile, we also opened the discussion board. All enthusiastic users are welcome to put forward questions or suggestions on RecBole.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/glzdhg9k3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=7838bd5f7ee44b0cbb215df5923df53e0f811fe3\n\n# Quick start from source.\n\nWith the source of RecBole, you can run the provided script for initial usage of our library:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/vkq8i5tn3bc61.png?width=679&amp;format=png&amp;auto=webp&amp;s=32a8d41c791215e9cd7d9058b3f383df996943c0\n\nThis script will run the BPR model on the ml-100k dataset. Typically, this example takes less than one minute. We will obtain some output like:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/o5s0q0wn3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=d51a87617509909dedf01f96a089588b5f4dab7c\n\n&amp;#x200B;\n\nhttps://preview.redd.it/tmz88xjp3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=d9e619840492822ab89f3830605dc2376f0d71a8\n\nBegin Training:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/361u85mr3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=9c9db65b91faaa090f4183f87508fb164ed3f78e\n\n&amp;#x200B;\n\nhttps://preview.redd.it/74eksyit3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=c913d7094921b2df0941dc96e131bbc205d43920\n\nIf you want to change the parameters, such as learning\\_rate, embedding\\_size, just set the additional command parameters as you need:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/hrcgr72v3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=e0673ed14df84eaeaaca42b5e201564728aacc66\n\nIf you want to change the models, just run the script by setting additional command parameters:\n\nhttps://preview.redd.it/4vko4ykw3bc61.png?width=852&amp;format=png&amp;auto=webp&amp;s=d5de3fde8800e03aa4d90bb6b5f0c1be5dfa52f7\n\nFor more usage information , please visit our HomePage and GitHub.\n\nWe will continue to open up for development team members from contributing single code to developing core modules. Welcome to join us by contacting emails.\n\n**HomePage:** [**https://recbole.io**](https://recbole.io/)\n\n**GitHub:** [**https://github.com/RUCAIBox/RecBole**](https://github.com/RUCAIBox/RecBole)\n\n**Paper:** [**https://arxiv.org/abs/2011.01731**](https://arxiv.org/abs/2011.01731)\n\n**Emails:** [**recbole@outlook.com**](mailto:recbole@outlook.com)"", 'author_fullname': 't2_8fgbbr8x', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'RecBole: A unified, comprehensive and efficient recommendation library.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/recommendersystems', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 54, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'0j9t9ksh3bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 27, 'x': 108, 'u': 'https://preview.redd.it/0j9t9ksh3bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1a8f592937c3281a4adea8ca39c7ba5d8c14f43'}, {'y': 55, 'x': 216, 'u': 'https://preview.redd.it/0j9t9ksh3bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bffb5e2b7252679b2ba3252978e658f6aeec05a7'}, {'y': 82, 'x': 320, 'u': 'https://preview.redd.it/0j9t9ksh3bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fafb63f1b2ec388ca40045e819e18516f4dbba4b'}], 's': {'y': 158, 'x': 616, 'u': 'https://preview.redd.it/0j9t9ksh3bc61.png?width=616&amp;format=png&amp;auto=webp&amp;s=2699844573c666ab1499bf723c57e3dbaf23d656'}, 'id': '0j9t9ksh3bc61'}, 'afgid3r23bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 38, 'x': 108, 'u': 'https://preview.redd.it/afgid3r23bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=adaff84916f7f791dc6975893d39972e77a86fd1'}, {'y': 76, 'x': 216, 'u': 'https://preview.redd.it/afgid3r23bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6cc3931ab6bb503074e03622bfaaf45a377fb0e2'}, {'y': 113, 'x': 320, 'u': 'https://preview.redd.it/afgid3r23bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cb6c14f7370ea7070d26a859d76b52964af91225'}, {'y': 226, 'x': 640, 'u': 'https://preview.redd.it/afgid3r23bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b8551de04fc426a615e9cee0d44ff38b411b69e4'}], 's': {'y': 302, 'x': 854, 'u': 'https://preview.redd.it/afgid3r23bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=e6538e253053fd1426405030b86283f4524737e0'}, 'id': 'afgid3r23bc61'}, 'wk1x0chy2bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 45, 'x': 108, 'u': 'https://preview.redd.it/wk1x0chy2bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=97e69257e509adfc573a0cc67b9d8c6373c91c41'}, {'y': 91, 'x': 216, 'u': 'https://preview.redd.it/wk1x0chy2bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d16489200f5b0cdbf309f7e4582dc9975642f492'}, {'y': 135, 'x': 320, 'u': 'https://preview.redd.it/wk1x0chy2bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=df10c646809d79d7dc8a6100bd46a8daf49111db'}, {'y': 271, 'x': 640, 'u': 'https://preview.redd.it/wk1x0chy2bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=abf4f975c715cc4236efee0d5286740105f46458'}], 's': {'y': 362, 'x': 854, 'u': 'https://preview.redd.it/wk1x0chy2bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=bc38c53d94e7271ee65cdfd4af1307cde3c432ac'}, 'id': 'wk1x0chy2bc61'}, '1zefui273bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 18, 'x': 108, 'u': 'https://preview.redd.it/1zefui273bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e838964d3575a62c93ec3e3368ac3b1df3a3bfba'}, {'y': 37, 'x': 216, 'u': 'https://preview.redd.it/1zefui273bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63080d6408e363de101d3432d250aa7a288fb438'}, {'y': 56, 'x': 320, 'u': 'https://preview.redd.it/1zefui273bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cd48162bbc90aff56a9c3c68e109892dcd073065'}, {'y': 112, 'x': 640, 'u': 'https://preview.redd.it/1zefui273bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=15658c106f413a325f4c640ae031a5cbd0eed5e1'}], 's': {'y': 150, 'x': 854, 'u': 'https://preview.redd.it/1zefui273bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=b8ec8b5d264648ee8d425e0c3cf7013be18ad126'}, 'id': '1zefui273bc61'}, 'lt1twf2g3bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 26, 'x': 108, 'u': 'https://preview.redd.it/lt1twf2g3bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=36aa892e09fa4ba09f0d4bbc9e35d065e9b55b2b'}, {'y': 53, 'x': 216, 'u': 'https://preview.redd.it/lt1twf2g3bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=907db4e3577fbc1c9a17aeccfb2e3fc30da6e62f'}, {'y': 79, 'x': 320, 'u': 'https://preview.redd.it/lt1twf2g3bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=df4ed97b39392848b572fa2d7f004c12c295d920'}, {'y': 158, 'x': 640, 'u': 'https://preview.redd.it/lt1twf2g3bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea657a7f608945b771541c1156508b59c09a2a2f'}], 's': {'y': 212, 'x': 854, 'u': 'https://preview.redd.it/lt1twf2g3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=63f8744e249373761604a02ccd812f25c7d8d575'}, 'id': 'lt1twf2g3bc61'}, 'bihm4cid3bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 79, 'x': 108, 'u': 'https://preview.redd.it/bihm4cid3bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a79e7629d88d55e375835fbe980670cdf40dcf75'}, {'y': 158, 'x': 216, 'u': 'https://preview.redd.it/bihm4cid3bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c6fab6ccc2a8273fb95485d6194e7b35a2da4e67'}, {'y': 234, 'x': 320, 'u': 'https://preview.redd.it/bihm4cid3bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=393e7f57ebba6be82ad22723fe135d43a7be4546'}, {'y': 468, 'x': 640, 'u': 'https://preview.redd.it/bihm4cid3bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=717ede15f447521945a5097ee01e032f58c1f318'}], 's': {'y': 625, 'x': 854, 'u': 'https://preview.redd.it/bihm4cid3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=7f8b2b7f42500a60e2afbcc0cb7f13e095918b82'}, 'id': 'bihm4cid3bc61'}, '74eksyit3bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 27, 'x': 108, 'u': 'https://preview.redd.it/74eksyit3bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=54f5092a3e1ac934f572d7c634c7b3d928b35dca'}, {'y': 54, 'x': 216, 'u': 'https://preview.redd.it/74eksyit3bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9681be33c915f2571efa99788db875900a0bc157'}, {'y': 80, 'x': 320, 'u': 'https://preview.redd.it/74eksyit3bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2619b6e260669d0e4e189d41acc227526a6e0358'}, {'y': 160, 'x': 640, 'u': 'https://preview.redd.it/74eksyit3bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=31286ccc7f60023eb05b4b3fdf6a4b92eb1074f9'}], 's': {'y': 214, 'x': 854, 'u': 'https://preview.redd.it/74eksyit3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=c913d7094921b2df0941dc96e131bbc205d43920'}, 'id': '74eksyit3bc61'}, 'o5s0q0wn3bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 79, 'x': 108, 'u': 'https://preview.redd.it/o5s0q0wn3bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=faaf245a0b0a1c4a35658082f87fccadb6bd2be0'}, {'y': 159, 'x': 216, 'u': 'https://preview.redd.it/o5s0q0wn3bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=efe0ccf7811255c7566419613c58d32f5f73ea3a'}, {'y': 236, 'x': 320, 'u': 'https://preview.redd.it/o5s0q0wn3bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=16fc5fc1664465893a6f445ecdd67e340dcd2405'}, {'y': 472, 'x': 640, 'u': 'https://preview.redd.it/o5s0q0wn3bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f68521a86af4e26f2ed9ab25ee59bbc95550973'}], 's': {'y': 631, 'x': 854, 'u': 'https://preview.redd.it/o5s0q0wn3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=d51a87617509909dedf01f96a089588b5f4dab7c'}, 'id': 'o5s0q0wn3bc61'}, '361u85mr3bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 51, 'x': 108, 'u': 'https://preview.redd.it/361u85mr3bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e3b1b003710fdfd03f12181a327c8d626547ddc1'}, {'y': 102, 'x': 216, 'u': 'https://preview.redd.it/361u85mr3bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f9a39c82755b0da5b41fb2b11fc9c7dc3c75401'}, {'y': 151, 'x': 320, 'u': 'https://preview.redd.it/361u85mr3bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2b6df9486d392c059d0f62081d190579e4e466e'}, {'y': 302, 'x': 640, 'u': 'https://preview.redd.it/361u85mr3bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4dcb3db546ddd03866f169218d4ca791a595fa93'}], 's': {'y': 404, 'x': 854, 'u': 'https://preview.redd.it/361u85mr3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=9c9db65b91faaa090f4183f87508fb164ed3f78e'}, 'id': '361u85mr3bc61'}, 'hrcgr72v3bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 4, 'x': 108, 'u': 'https://preview.redd.it/hrcgr72v3bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9da81144be2f8fddf119dd0ad5c6158dafe7ab38'}, {'y': 8, 'x': 216, 'u': 'https://preview.redd.it/hrcgr72v3bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2d4c41be7a6c38c278e716c8b83dcf5512962bee'}, {'y': 12, 'x': 320, 'u': 'https://preview.redd.it/hrcgr72v3bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a30d750772444a8f28a95c37eafed7ac2508c0c2'}, {'y': 24, 'x': 640, 'u': 'https://preview.redd.it/hrcgr72v3bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa2da7aa7f51bee3bc68962684d2cb281c7fca64'}], 's': {'y': 33, 'x': 854, 'u': 'https://preview.redd.it/hrcgr72v3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=e0673ed14df84eaeaaca42b5e201564728aacc66'}, 'id': 'hrcgr72v3bc61'}, '425bk1653bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 55, 'x': 108, 'u': 'https://preview.redd.it/425bk1653bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=05d6d85a0713a484bc3c7a82d2792f33f19bc526'}, {'y': 110, 'x': 216, 'u': 'https://preview.redd.it/425bk1653bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d221088101beceeb63951686d6e34eaa0f42c5b'}, {'y': 163, 'x': 320, 'u': 'https://preview.redd.it/425bk1653bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=76df86043a7ac927e8d3377fe37ec8a21b08c6eb'}, {'y': 327, 'x': 640, 'u': 'https://preview.redd.it/425bk1653bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7b88a326289da712a89421370f4963f4dcd95bab'}], 's': {'y': 437, 'x': 854, 'u': 'https://preview.redd.it/425bk1653bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=3fcc38dcd6d62b18836ef23ccf4247dbccdde304'}, 'id': '425bk1653bc61'}, '157ung9u2bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 59, 'x': 108, 'u': 'https://preview.redd.it/157ung9u2bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=57fca1cdcad0250725eab4ab74448b0d2cf77fe5'}, {'y': 118, 'x': 216, 'u': 'https://preview.redd.it/157ung9u2bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1db092f9b5b77205f0def1f06682bcbbaf137d99'}, {'y': 175, 'x': 320, 'u': 'https://preview.redd.it/157ung9u2bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9422ffc24491d585834f4b9716297afe1c29bb8f'}, {'y': 350, 'x': 640, 'u': 'https://preview.redd.it/157ung9u2bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=547af971309bfd7869a5b93cec400efc6c00be56'}], 's': {'y': 468, 'x': 854, 'u': 'https://preview.redd.it/157ung9u2bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=12790c0c108072952702a70f214c640ecfbf6c47'}, 'id': '157ung9u2bc61'}, 'tmz88xjp3bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 34, 'x': 108, 'u': 'https://preview.redd.it/tmz88xjp3bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=34c3ca5b7c12ba23846e25e4065b9f38d0d4cb39'}, {'y': 69, 'x': 216, 'u': 'https://preview.redd.it/tmz88xjp3bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0668aaa8eb2a6562eca9f275c2a94fc5a020f539'}, {'y': 103, 'x': 320, 'u': 'https://preview.redd.it/tmz88xjp3bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b101d756395f2d8e0bd2f524e5cd98000e70d35d'}, {'y': 206, 'x': 640, 'u': 'https://preview.redd.it/tmz88xjp3bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=75b86b38457ff0cf5872be02b05370263ebde8ef'}], 's': {'y': 275, 'x': 854, 'u': 'https://preview.redd.it/tmz88xjp3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=d9e619840492822ab89f3830605dc2376f0d71a8'}, 'id': 'tmz88xjp3bc61'}, 'ypbinz51a9c61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 41, 'x': 108, 'u': 'https://preview.redd.it/ypbinz51a9c61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=affba0756f58f76d7eef6f7279f8748f69d9c421'}, {'y': 83, 'x': 216, 'u': 'https://preview.redd.it/ypbinz51a9c61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=90c1504d6c5aa0b0510b6157a967fbd1f8ef536f'}, {'y': 124, 'x': 320, 'u': 'https://preview.redd.it/ypbinz51a9c61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e2e5610c07051e0a8fe6ed2fd966750c22ab63a'}, {'y': 248, 'x': 640, 'u': 'https://preview.redd.it/ypbinz51a9c61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4acc285368814d666d1d944814ea993850cb8758'}], 's': {'y': 331, 'x': 854, 'u': 'https://preview.redd.it/ypbinz51a9c61.png?width=854&amp;format=png&amp;auto=webp&amp;s=07b075aa75fc961003bf057b4a98b876ebd3db45'}, 'id': 'ypbinz51a9c61'}, 'glzdhg9k3bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 55, 'x': 108, 'u': 'https://preview.redd.it/glzdhg9k3bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a67bda324f238c9316d98ffaaee957316abc6be2'}, {'y': 111, 'x': 216, 'u': 'https://preview.redd.it/glzdhg9k3bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63a3036c801122885b7fb4ddb7809c6ee7888803'}, {'y': 164, 'x': 320, 'u': 'https://preview.redd.it/glzdhg9k3bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fd179a827a7441e6dd0c9d46f28130aa7dceb2e1'}, {'y': 328, 'x': 640, 'u': 'https://preview.redd.it/glzdhg9k3bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dfff4d96cc4e8845b455fe10261cd6d075d2ccd7'}], 's': {'y': 439, 'x': 854, 'u': 'https://preview.redd.it/glzdhg9k3bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=7838bd5f7ee44b0cbb215df5923df53e0f811fe3'}, 'id': 'glzdhg9k3bc61'}, '4vko4ykw3bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 3, 'x': 108, 'u': 'https://preview.redd.it/4vko4ykw3bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=634d7087adfc51b90c5ff4007b005999e2c10520'}, {'y': 7, 'x': 216, 'u': 'https://preview.redd.it/4vko4ykw3bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=370dfe3a2f4ba23386270435c2eaf168ded72cb0'}, {'y': 11, 'x': 320, 'u': 'https://preview.redd.it/4vko4ykw3bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=43d54cd77c3da47295141b73ad3f6619e6f75f70'}, {'y': 23, 'x': 640, 'u': 'https://preview.redd.it/4vko4ykw3bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d718e9386d72825361d26bac58d8e59a26957a56'}], 's': {'y': 31, 'x': 852, 'u': 'https://preview.redd.it/4vko4ykw3bc61.png?width=852&amp;format=png&amp;auto=webp&amp;s=d5de3fde8800e03aa4d90bb6b5f0c1be5dfa52f7'}, 'id': '4vko4ykw3bc61'}, 'vkq8i5tn3bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 16, 'x': 108, 'u': 'https://preview.redd.it/vkq8i5tn3bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d1da5c8938891fad2fd3f2b9a3df95b407f0bd5'}, {'y': 33, 'x': 216, 'u': 'https://preview.redd.it/vkq8i5tn3bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=03b7d290ce20f38874e2bc24102bdef515a4270c'}, {'y': 49, 'x': 320, 'u': 'https://preview.redd.it/vkq8i5tn3bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cb27fa6859b166ca7c244dd99b8e87783ae7b371'}, {'y': 98, 'x': 640, 'u': 'https://preview.redd.it/vkq8i5tn3bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3cb06a81f15fbb5480e86f22f51097266dc309aa'}], 's': {'y': 104, 'x': 679, 'u': 'https://preview.redd.it/vkq8i5tn3bc61.png?width=679&amp;format=png&amp;auto=webp&amp;s=32a8d41c791215e9cd7d9058b3f383df996943c0'}, 'id': 'vkq8i5tn3bc61'}, 'v67j7fr93bc61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 86, 'x': 108, 'u': 'https://preview.redd.it/v67j7fr93bc61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=863a80f09cc5603dd3bb5428ea55002afeed39da'}, {'y': 173, 'x': 216, 'u': 'https://preview.redd.it/v67j7fr93bc61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5252782735620a9b783f559457dd917567115ab6'}, {'y': 257, 'x': 320, 'u': 'https://preview.redd.it/v67j7fr93bc61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7816dc570539e4092c41d01282193740ff1cd787'}, {'y': 514, 'x': 640, 'u': 'https://preview.redd.it/v67j7fr93bc61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee4ac7f518959ddd35a459c7948f858f56a01b92'}], 's': {'y': 687, 'x': 854, 'u': 'https://preview.redd.it/v67j7fr93bc61.png?width=854&amp;format=png&amp;auto=webp&amp;s=63a70d8cefb912f8eea05b5cb6fcb2c3d87d20ff'}, 'id': 'v67j7fr93bc61'}}, 'name': 't3_l0lyri', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 7, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1611098667.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.recommendersystems', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;RecBole is developed based on Python and PyTorch for reproducing and developing recommendation algorithms in a unified, comprehensive and efficient framework for research purposes.&lt;/p&gt;\n\n&lt;p&gt;In the RecBole framework, users only need to make a simple configuration to test the performance of different models on different datasets. And it&amp;#39;s convenient for users to make secondary developments and add new models. The main features of RecBole are as follows:&lt;/p&gt;\n\n&lt;h2&gt;General and extensible data structure.&lt;/h2&gt;\n\n&lt;p&gt;RecBole designs general and extensible data structures to unify the formatting and usage of various recommendation datasets.&lt;/p&gt;\n\n&lt;p&gt;In order to realize the unified management and usage of each dataset, a new data storage format has been developed in RecBole, which can support all common datasets and realize efficient storage and loading. It contains 4 feature types and 6 optional file types. Datasets that are private to the user can be automatically managed under this framework just by processing this file format.&lt;/p&gt;\n\n&lt;p&gt;Each atomic file can be viewed as a m×n table (except header), where n is the number of features and m is the number of data records. The first row corresponds to feature names, in which each entry has the form of feat_name:feat_type，indicating the feature name and feature type. We support four feature types, which can be processed by tensors in batch.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/ypbinz51a9c61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07b075aa75fc961003bf057b4a98b876ebd3db45""&gt;4 feature types&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So far, our library introduces six atomic file types, we identify different files by their suffixes.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/157ung9u2bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12790c0c108072952702a70f214c640ecfbf6c47""&gt;6 optional file types&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Comprehensive benchmark models and datasets.&lt;/h1&gt;\n\n&lt;p&gt;RecBole implements 64 commonly used recommendation algorithms, and provide the formatted copies of 27 recommendation datasets.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/wk1x0chy2bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc38c53d94e7271ee65cdfd4af1307cde3c432ac""&gt;General recommendation models&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/afgid3r23bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e6538e253053fd1426405030b86283f4524737e0""&gt;Context recommendation models&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/425bk1653bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3fcc38dcd6d62b18836ef23ccf4247dbccdde304""&gt;Sequential recommendation models&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/1zefui273bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b8ec8b5d264648ee8d425e0c3cf7013be18ad126""&gt;Knowledge recommendation models&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And collected datasets in our library RecBole are as follows (Users need to download copies of the original data, and then use the pre-processing script provided by RecBole to process it, or download the processed datasets directly from the address provided).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/v67j7fr93bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63a70d8cefb912f8eea05b5cb6fcb2c3d87d20ff""&gt;All datasets&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Efficient GPU-accelerated execution.&lt;/h1&gt;\n\n&lt;p&gt;RecBole optimizes the efficiency with a number of improved techniques oriented to the GPU environment.&lt;/p&gt;\n\n&lt;p&gt;We constructed preliminary experiments to test the time and memory cost on three different-sized datasets (small, medium and large). Here is the result of General recommendation models on ml-1m dataset (If you want to know more result, please go to our GitHub Homepage at the end of this article) :&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/bihm4cid3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f8b2b7f42500a60e2afbcc0cb7f13e095918b82""&gt;https://preview.redd.it/bihm4cid3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f8b2b7f42500a60e2afbcc0cb7f13e095918b82&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Extensive and standard evaluation protocols.&lt;/h1&gt;\n\n&lt;p&gt;RecBole supports a series of widely adopted evaluation protocols or settings for testing and comparing recommendation algorithms.&lt;/p&gt;\n\n&lt;p&gt;For advanced users and secondary developers, RecBole also provides a very flexible evaluation interface. Users can use simple codes and parameters to realize different combinations of sampling and data segmentation, and package the commonly used combinations to achieve quick configuration. As far as we know, this is the most comprehensive open-source framework that currently supports metrics, which supports different dataset segmentation, sampling, etc.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/lt1twf2g3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63f8744e249373761604a02ccd812f25c7d8d575""&gt;https://preview.redd.it/lt1twf2g3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63f8744e249373761604a02ccd812f25c7d8d575&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Active GitHub Community.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So far, we have received 65 issues and replied to each one carefully.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/0j9t9ksh3bc61.png?width=616&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2699844573c666ab1499bf723c57e3dbaf23d656""&gt;https://preview.redd.it/0j9t9ksh3bc61.png?width=616&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2699844573c666ab1499bf723c57e3dbaf23d656&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Meanwhile, we also opened the discussion board. All enthusiastic users are welcome to put forward questions or suggestions on RecBole.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/glzdhg9k3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7838bd5f7ee44b0cbb215df5923df53e0f811fe3""&gt;https://preview.redd.it/glzdhg9k3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7838bd5f7ee44b0cbb215df5923df53e0f811fe3&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Quick start from source.&lt;/h1&gt;\n\n&lt;p&gt;With the source of RecBole, you can run the provided script for initial usage of our library:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/vkq8i5tn3bc61.png?width=679&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32a8d41c791215e9cd7d9058b3f383df996943c0""&gt;https://preview.redd.it/vkq8i5tn3bc61.png?width=679&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32a8d41c791215e9cd7d9058b3f383df996943c0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This script will run the BPR model on the ml-100k dataset. Typically, this example takes less than one minute. We will obtain some output like:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/o5s0q0wn3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d51a87617509909dedf01f96a089588b5f4dab7c""&gt;https://preview.redd.it/o5s0q0wn3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d51a87617509909dedf01f96a089588b5f4dab7c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/tmz88xjp3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d9e619840492822ab89f3830605dc2376f0d71a8""&gt;https://preview.redd.it/tmz88xjp3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d9e619840492822ab89f3830605dc2376f0d71a8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Begin Training:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/361u85mr3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c9db65b91faaa090f4183f87508fb164ed3f78e""&gt;https://preview.redd.it/361u85mr3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c9db65b91faaa090f4183f87508fb164ed3f78e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/74eksyit3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c913d7094921b2df0941dc96e131bbc205d43920""&gt;https://preview.redd.it/74eksyit3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c913d7094921b2df0941dc96e131bbc205d43920&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you want to change the parameters, such as learning_rate, embedding_size, just set the additional command parameters as you need:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/hrcgr72v3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0673ed14df84eaeaaca42b5e201564728aacc66""&gt;https://preview.redd.it/hrcgr72v3bc61.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0673ed14df84eaeaaca42b5e201564728aacc66&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you want to change the models, just run the script by setting additional command parameters:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/4vko4ykw3bc61.png?width=852&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d5de3fde8800e03aa4d90bb6b5f0c1be5dfa52f7""&gt;https://preview.redd.it/4vko4ykw3bc61.png?width=852&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d5de3fde8800e03aa4d90bb6b5f0c1be5dfa52f7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For more usage information , please visit our HomePage and GitHub.&lt;/p&gt;\n\n&lt;p&gt;We will continue to open up for development team members from contributing single code to developing core modules. Welcome to join us by contacting emails.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;HomePage:&lt;/strong&gt; &lt;a href=""https://recbole.io/""&gt;&lt;strong&gt;https://recbole.io&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href=""https://github.com/RUCAIBox/RecBole""&gt;&lt;strong&gt;https://github.com/RUCAIBox/RecBole&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href=""https://arxiv.org/abs/2011.01731""&gt;&lt;strong&gt;https://arxiv.org/abs/2011.01731&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Emails:&lt;/strong&gt; [&lt;strong&gt;&lt;a href=""mailto:recbole@outlook.com""&gt;recbole@outlook.com&lt;/a&gt;&lt;/strong&gt;](mailto:&lt;a href=""mailto:recbole@outlook.com""&gt;recbole@outlook.com&lt;/a&gt;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_39m8g', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'l0lyri', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'EliverQ', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/recommendersystems/comments/l0lyri/recbole_a_unified_comprehensive_and_efficient/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/recommendersystems/comments/l0lyri/recbole_a_unified_comprehensive_and_efficient/', 'subreddit_subscribers': 180, 'created_utc': 1611069867.0, 'num_crossposts': 4, 'media': None, 'is_video': False}]",t3_l0lyri,,,,,
113,,pytorch,"&amp;#x200B;

https://preview.redd.it/m9kajptoz8c61.png?width=432&amp;format=png&amp;auto=webp&amp;s=9625b414178899503c944f66b40f391ab72b2a24",t2_8fgbbr8x,False,,0,False,"RecBole: A unified, comprehensive and efficient recommendation library.",[],r/pytorch,False,6,,0,140.0,,False,t3_l0fiw6,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/EnJ1pt6brt8OoC4eF2ucr9MymdAwEodi9A6WCPf5bHU.jpg,False,,[],{},,,True,,1611072925.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/m9kajptoz8c61.png?width=432&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9625b414178899503c944f66b40f391ab72b2a24""&gt;https://preview.redd.it/m9kajptoz8c61.png?width=432&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9625b414178899503c944f66b40f391ab72b2a24&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l0fiw6,True,,EliverQ,,1,True,all_ads,False,[],False,,/r/pytorch/comments/l0fiw6/recbole_a_unified_comprehensive_and_efficient/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l0fiw6/recbole_a_unified_comprehensive_and_efficient/,7135,1611044125.0,0,,False,,,,"{'m9kajptoz8c61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 154, 'x': 108, 'u': 'https://preview.redd.it/m9kajptoz8c61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1fda937f389e7a687da474a8f925778feccf1e89'}, {'y': 309, 'x': 216, 'u': 'https://preview.redd.it/m9kajptoz8c61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f2d806d6d01a04fddeb19f7577ccbac6df303188'}, {'y': 457, 'x': 320, 'u': 'https://preview.redd.it/m9kajptoz8c61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6a1f20b33b50f4a47f681835093c221fb2d5e8f0'}], 's': {'y': 618, 'x': 432, 'u': 'https://preview.redd.it/m9kajptoz8c61.png?width=432&amp;format=png&amp;auto=webp&amp;s=9625b414178899503c944f66b40f391ab72b2a24'}, 'id': 'm9kajptoz8c61'}}",,,,
114,,pytorch,"I felt that the packages available online to do this were getting ahead of themselves, so I whipped this up. It's a simple wrapper that goes around whatever module you want, keeps track of masks, lets you apply whatever function you want to get to change masks over time, can load the parameters from another model with the same architecture, can be saved and loaded, trained, etc. No fancy work, just use it to wrap an existing model, and away you go.

    def mask_fn(param, old_mask, percentile):
        if len(param.shape) &gt; 1:
            absparam = abs(param)
            new_mask = (absparam &gt; torch.quantile(absparam, percentile)).to(param.dtype)
            return(new_mask * old_mask)
        else:
            return(old_mask)
    
    class binary_mask_model(torch.nn.Module):
        # Becomes a copy of a 'parent model,' with the extra attribute that it holds onto
        # binary element-wise masks of each parameter, and can apply or modify them at will.
        # This allows you to easily prune models according to methodologies like the one
        # presented in ""The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks""
        # (Frankle, J. and Carbin, M., 2019)
        
        def __init__(self, parent_model):
            super(binary_mask_model, self).__init__()
            
            # Copies the existing model, and adds it to itself
            self.model = copy.deepcopy(parent_model)
            self.mask_set = []
            
            # Adds, to every parameter in the model, another 1-valued mask tensor of equal shape
            for param in self.model.parameters():
                self.mask_set.append(torch.ones(param.shape, dtype = param.dtype, device = param.device))
    
        def apply_mask(self):
            # Multiplies all parameters by their mask
            for mask, param in zip(self.mask_set, self.model.parameters()):
                param.data = mask * param.data
    
        def forward(self, *args, **kwargs):
            # Re-masks the tensors in case training changed some 0-values entries, then
            # executes the exact same forward pass as the parent model
            if self.training:
                self.apply_mask()
            return(self.model(*args, **kwargs))
            
        def mod_mask(self, mask_fn = mask_fn, *args, **kwargs):
            # Applies a function of one's own design to update the masks, and then
            # masks the parameters again
            for i, param in zip(range(len(self.mask_set)), self.model.parameters()):
                self.mask_set[i] = mask_fn(param, self.mask_set[i], *args, **kwargs)
            self.apply_mask()
            
        def load_parameters(self, source_model):
            # Pulls in the parameters from another model with the same architecture, in case you
            # want to reload the parent model, or pull from a pretrained model
            for param, source_param in zip(self.model.parameters(), source_model.parameters()):
                param.data = source_param.data
            self.apply_mask()
            
    # Example
    import torchvision.models as models
    
    parent_model = models.resnet18().to('cuda')
    masked_model = binary_mask_model(parent_model)
    masked_model.mod_mask(percentile = 0.9)
    torch.save(masked_model, 'mask.torch')
    loaded_model = torch.load('mask.torch')
    opt = torch.optim.Adam(loaded_model.parameters(), lr = 0.0005)
    loss = loaded_model(torch.randn(5, 3, 128, 128, device = 'cuda')).mean()
    loss.backward()
    opt.step()",t2_qaixi,False,,0,False,A bit of code to implement binary masking for arbitrary models a la Lottery Ticket Hypothesis,[],r/pytorch,False,6,,0,,,False,t3_l03udy,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1611004610.0,,[],{},,,True,,1611032815.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I felt that the packages available online to do this were getting ahead of themselves, so I whipped this up. It&amp;#39;s a simple wrapper that goes around whatever module you want, keeps track of masks, lets you apply whatever function you want to get to change masks over time, can load the parameters from another model with the same architecture, can be saved and loaded, trained, etc. No fancy work, just use it to wrap an existing model, and away you go.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def mask_fn(param, old_mask, percentile):
    if len(param.shape) &amp;gt; 1:
        absparam = abs(param)
        new_mask = (absparam &amp;gt; torch.quantile(absparam, percentile)).to(param.dtype)
        return(new_mask * old_mask)
    else:
        return(old_mask)

class binary_mask_model(torch.nn.Module):
    # Becomes a copy of a &amp;#39;parent model,&amp;#39; with the extra attribute that it holds onto
    # binary element-wise masks of each parameter, and can apply or modify them at will.
    # This allows you to easily prune models according to methodologies like the one
    # presented in &amp;quot;The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks&amp;quot;
    # (Frankle, J. and Carbin, M., 2019)

    def __init__(self, parent_model):
        super(binary_mask_model, self).__init__()

        # Copies the existing model, and adds it to itself
        self.model = copy.deepcopy(parent_model)
        self.mask_set = []

        # Adds, to every parameter in the model, another 1-valued mask tensor of equal shape
        for param in self.model.parameters():
            self.mask_set.append(torch.ones(param.shape, dtype = param.dtype, device = param.device))

    def apply_mask(self):
        # Multiplies all parameters by their mask
        for mask, param in zip(self.mask_set, self.model.parameters()):
            param.data = mask * param.data

    def forward(self, *args, **kwargs):
        # Re-masks the tensors in case training changed some 0-values entries, then
        # executes the exact same forward pass as the parent model
        if self.training:
            self.apply_mask()
        return(self.model(*args, **kwargs))

    def mod_mask(self, mask_fn = mask_fn, *args, **kwargs):
        # Applies a function of one&amp;#39;s own design to update the masks, and then
        # masks the parameters again
        for i, param in zip(range(len(self.mask_set)), self.model.parameters()):
            self.mask_set[i] = mask_fn(param, self.mask_set[i], *args, **kwargs)
        self.apply_mask()

    def load_parameters(self, source_model):
        # Pulls in the parameters from another model with the same architecture, in case you
        # want to reload the parent model, or pull from a pretrained model
        for param, source_param in zip(self.model.parameters(), source_model.parameters()):
            param.data = source_param.data
        self.apply_mask()

# Example
import torchvision.models as models

parent_model = models.resnet18().to(&amp;#39;cuda&amp;#39;)
masked_model = binary_mask_model(parent_model)
masked_model.mod_mask(percentile = 0.9)
torch.save(masked_model, &amp;#39;mask.torch&amp;#39;)
loaded_model = torch.load(&amp;#39;mask.torch&amp;#39;)
opt = torch.optim.Adam(loaded_model.parameters(), lr = 0.0005)
loss = loaded_model(torch.randn(5, 3, 128, 128, device = &amp;#39;cuda&amp;#39;)).mean()
loss.backward()
opt.step()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l03udy,True,,MrAcurite,,0,True,all_ads,False,[],False,,/r/pytorch/comments/l03udy/a_bit_of_code_to_implement_binary_masking_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l03udy/a_bit_of_code_to_implement_binary_masking_for/,7135,1611004015.0,0,,False,,,,,,,,
115,,pytorch,"i find some libraly. it called gan or tecogan.

i find some tutorial about gan but tecogan tutorial not many.

 What should i do at first?",t2_9dq3e,False,,0,False,i want remove mosaic of picture by python.what should need for first step of beginner?,[],r/pytorch,False,6,,0,,,False,t3_l046bs,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1611033793.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;i find some libraly. it called gan or tecogan.&lt;/p&gt;

&lt;p&gt;i find some tutorial about gan but tecogan tutorial not many.&lt;/p&gt;

&lt;p&gt;What should i do at first?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,l046bs,True,,shibainuisno1,,1,True,all_ads,False,[],False,,/r/pytorch/comments/l046bs/i_want_remove_mosaic_of_picture_by_pythonwhat/,all_ads,False,https://www.reddit.com/r/pytorch/comments/l046bs/i_want_remove_mosaic_of_picture_by_pythonwhat/,7135,1611004993.0,0,,False,,,,,,,,
116,,pytorch,"I noticed that a few codes make their custom losses just as a method while some make an inherited class of nn.Module and define its forward and backward methods. 

What's the right way to do it?",t2_4xhrybaz,False,,0,False,The right way to make custom losses,[],r/pytorch,False,6,,0,,,False,t3_kxw9y0,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1610751888.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I noticed that a few codes make their custom losses just as a method while some make an inherited class of nn.Module and define its forward and backward methods. &lt;/p&gt;

&lt;p&gt;What&amp;#39;s the right way to do it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kxw9y0,True,,banenvy,,9,True,all_ads,False,[],False,,/r/pytorch/comments/kxw9y0/the_right_way_to_make_custom_losses/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kxw9y0/the_right_way_to_make_custom_losses/,7135,1610723088.0,0,,False,,,,,,,,
117,,pytorch,"I have 8 GPUs on machine 1 and 4 GPUs on machine 2, and I'd like to perform multi-gpu training on all 12 GPUs.  

Is it ok to train on 8 GPUs and 4 GPUs in two machines, despite using the different amount of GPUs on each one?",t2_32juucp7,False,,0,False,Is the amount of GPU on each machine expected to be identical when multi-machine multi-gpu training using distributed.launch ?,[],r/pytorch,False,6,,0,,,False,t3_kxr2cr,False,dark,1.0,,public,8,0,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},,,True,,1610731248.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have 8 GPUs on machine 1 and 4 GPUs on machine 2, and I&amp;#39;d like to perform multi-gpu training on all 12 GPUs.  &lt;/p&gt;

&lt;p&gt;Is it ok to train on 8 GPUs and 4 GPUs in two machines, despite using the different amount of GPUs on each one?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kxr2cr,True,,AlphaGoMK,,1,True,all_ads,False,[],False,,/r/pytorch/comments/kxr2cr/is_the_amount_of_gpu_on_each_machine_expected_to/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kxr2cr/is_the_amount_of_gpu_on_each_machine_expected_to/,7135,1610702448.0,0,,False,,,,,,,,
118,,pytorch,"When I try to run OpenAI’ gym CartPole I got this

Warning: Expected min height of view: (&lt;NSPopoverTouchBarItemButton: 0x7fcd151c7b10&gt;) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.

Anyone got idea what is this error?",t2_4newbtnu,False,,0,False,ErrorQuestion about NSPopoverBarltemButton,[],r/pytorch,False,6,,0,,,False,t3_kvfudr,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1610439149.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When I try to run OpenAI’ gym CartPole I got this&lt;/p&gt;

&lt;p&gt;Warning: Expected min height of view: (&amp;lt;NSPopoverTouchBarItemButton: 0x7fcd151c7b10&amp;gt;) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.&lt;/p&gt;

&lt;p&gt;Anyone got idea what is this error?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kvfudr,True,,reyScrooge,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kvfudr/errorquestion_about_nspopoverbarltembutton/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kvfudr/errorquestion_about_nspopoverbarltembutton/,7135,1610410349.0,0,,False,,,,,,,,
119,,pytorch,"Added Feedback Transformer implementation/guide to our collection of neural network architectures/algorithms. Feedback Transformer uses recurrent attention to previous steps, and there for can give fast predictions.

Github Repo: [https://github.com/lab-ml/nn](https://github.com/lab-ml/nn)

Source code with side-by-side notes: [https://lab-ml.com/labml\_nn/transformers/feedback/](https://lab-ml.com/labml_nn/transformers/feedback/)",t2_1jyhaoq,False,,0,False,Feedback Transformer PyTorch implementation,[],r/pytorch,False,6,,0,,,False,t3_kufh9e,False,dark,0.94,,public,13,0,{},,,False,[],,False,False,,{},,False,13,,False,self,False,,[],{},self,,True,,1610318117.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Added Feedback Transformer implementation/guide to our collection of neural network architectures/algorithms. Feedback Transformer uses recurrent attention to previous steps, and there for can give fast predictions.&lt;/p&gt;

&lt;p&gt;Github Repo: &lt;a href=""https://github.com/lab-ml/nn""&gt;https://github.com/lab-ml/nn&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Source code with side-by-side notes: &lt;a href=""https://lab-ml.com/labml_nn/transformers/feedback/""&gt;https://lab-ml.com/labml_nn/transformers/feedback/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QBErmjc0OkORf4rXA_DMNHMPFAc6PFELLMpOG516FEs.jpg?auto=webp&amp;s=fc63e193431f72d1e34ac5d9360ad2d83d12f68a', 'width': 2256, 'height': 1844}, 'resolutions': [{'url': 'https://external-preview.redd.it/QBErmjc0OkORf4rXA_DMNHMPFAc6PFELLMpOG516FEs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0fbfe649d1ee4bc726c73fcb345d61a3c41a65c5', 'width': 108, 'height': 88}, {'url': 'https://external-preview.redd.it/QBErmjc0OkORf4rXA_DMNHMPFAc6PFELLMpOG516FEs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ff2e50b7ce3107b5fc8dcc0f51e109603ddb8a9', 'width': 216, 'height': 176}, {'url': 'https://external-preview.redd.it/QBErmjc0OkORf4rXA_DMNHMPFAc6PFELLMpOG516FEs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8d94ecf78529caf28dd38df35bd91c5cfa39786a', 'width': 320, 'height': 261}, {'url': 'https://external-preview.redd.it/QBErmjc0OkORf4rXA_DMNHMPFAc6PFELLMpOG516FEs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a29eb11739307d5faae19abeedf28d586bdcf130', 'width': 640, 'height': 523}, {'url': 'https://external-preview.redd.it/QBErmjc0OkORf4rXA_DMNHMPFAc6PFELLMpOG516FEs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5741a40be6a5be241bd444e6c8e1ddf7e58dc675', 'width': 960, 'height': 784}, {'url': 'https://external-preview.redd.it/QBErmjc0OkORf4rXA_DMNHMPFAc6PFELLMpOG516FEs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=16739b43fbf21c1310110d45bf0d5965b582f931', 'width': 1080, 'height': 882}], 'variants': {}, 'id': 'swosTSsBWmAu9aHBijwsahMx3INAuq4kkzPFfO7Hy_I'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kufh9e,True,,mlvpj,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kufh9e/feedback_transformer_pytorch_implementation/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kufh9e/feedback_transformer_pytorch_implementation/,7135,1610289317.0,0,,False,,,,,,,,
120,,pytorch,"With regards to the thread topic, could anyone help to advise what is wrong with the [loss\_function() computation logic](https://gist.github.com/promach/52c5199e98647c694163abd0b3af3dae#file-net-py-L149-L173) (especially `policy_output_discrete` and `value_output_discrete`) in my NN ?

&amp;#x200B;

    # Forward Pass
    policy_output, value_output = net(_board_features_and_turn)
    
    # Since both policy_output and value_output are of continuous probability nature,
    # we need to change them to discrete number for loss_function() computation
    policy_output_discrete = torch.zeros(len(_score), NUM_OF_POSSIBLE_MOVES, requires_grad=True)
    if USE_CUDA:
        policy_output_discrete = policy_output_discrete.cuda()
    
    for topk_index in range(len(_score)):  # functionally equivalent to softmax()
        policy_output_discrete[topk_index][policy_output.topk(1).indices[topk_index]] = 1
    
    # substract 1 because score is one of these [-1, 0, 1] values
    value_output_discrete = torch.topk(value_output, 1).indices - 1
    
    # Loss at each iteration by comparing to target(moves)
    loss1 = loss_function(policy_output_discrete, move)
    # Loss at each iteration by comparing to target(score)
    loss2 = loss_function(value_output_discrete, _score)
    
    loss = loss1 + loss2
    
    # Backpropagating gradient of loss
    optimizer.zero_grad()
    loss.backward()

&amp;#x200B;",t2_bpftl,False,,0,False,Plateau-ing training loss and LOW train_accuracy/test_accuracy,[],r/pytorch,False,6,,0,,,False,t3_kusuln,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1610366384.0,,[],{},self,,True,,1610359911.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;With regards to the thread topic, could anyone help to advise what is wrong with the &lt;a href=""https://gist.github.com/promach/52c5199e98647c694163abd0b3af3dae#file-net-py-L149-L173""&gt;loss_function() computation logic&lt;/a&gt; (especially &lt;code&gt;policy_output_discrete&lt;/code&gt; and &lt;code&gt;value_output_discrete&lt;/code&gt;) in my NN ?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Forward Pass
policy_output, value_output = net(_board_features_and_turn)

# Since both policy_output and value_output are of continuous probability nature,
# we need to change them to discrete number for loss_function() computation
policy_output_discrete = torch.zeros(len(_score), NUM_OF_POSSIBLE_MOVES, requires_grad=True)
if USE_CUDA:
    policy_output_discrete = policy_output_discrete.cuda()

for topk_index in range(len(_score)):  # functionally equivalent to softmax()
    policy_output_discrete[topk_index][policy_output.topk(1).indices[topk_index]] = 1

# substract 1 because score is one of these [-1, 0, 1] values
value_output_discrete = torch.topk(value_output, 1).indices - 1

# Loss at each iteration by comparing to target(moves)
loss1 = loss_function(policy_output_discrete, move)
# Loss at each iteration by comparing to target(score)
loss2 = loss_function(value_output_discrete, _score)

loss = loss1 + loss2

# Backpropagating gradient of loss
optimizer.zero_grad()
loss.backward()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?auto=webp&amp;s=079a7260ec149880c73263d64811698adb22760a', 'width': 1280, 'height': 640}, 'resolutions': [{'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5811c5bda5fece1040636a6af8702ba790f0fd4', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eee576fd4da7535eb53ceb88dd8b52f073048441', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=72872d880460efa723918c000adca0ed259cf775', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3545b9335d763c9da9c16bf7bf9a3f907dbd6f6', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d241ace0f1c07088fac3f8469dbad3b05d2d419', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9055f11bdc00beb0b3589e1cae5817d6070d83bc', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kusuln,True,,promach,,1,True,all_ads,False,[],False,,/r/pytorch/comments/kusuln/plateauing_training_loss_and_low_train/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kusuln/plateauing_training_loss_and_low_train/,7135,1610331111.0,1,,False,,,,,,,,
121,,pytorch,"I reproduced YOLOv3 by a single short script.

It loads the pre-training parameters provided by the darknet official website directly without conversion. This means that a model trained with Darknet can be converted to a Pytorch model using this script.

The required weight file and test picture are automatically downloaded from the official website, and no other files are dependent.

Except for the basic library of Python, it only depends on OpenCV and Pytorch 1.7 (including TorchVision).

The Forward does not use the advanced features of Pytorch, and can be directly Scripted or Traced for further deployment.

[https://gist.github.com/devymex/1f76224b2428d0ddbf92b93def6c587c](https://gist.github.com/devymex/1f76224b2428d0ddbf92b93def6c587c)",t2_c8t97,False,,0,False,Reproduced YOLOv3 based on Pytorch (darknet),[],r/pytorch,False,6,,0,,,False,t3_kts9zp,False,dark,1.0,,public,18,0,{},,,False,[],,False,False,,{},,False,18,,False,self,1610204360.0,,[],{},self,,True,,1610232948.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I reproduced YOLOv3 by a single short script.&lt;/p&gt;

&lt;p&gt;It loads the pre-training parameters provided by the darknet official website directly without conversion. This means that a model trained with Darknet can be converted to a Pytorch model using this script.&lt;/p&gt;

&lt;p&gt;The required weight file and test picture are automatically downloaded from the official website, and no other files are dependent.&lt;/p&gt;

&lt;p&gt;Except for the basic library of Python, it only depends on OpenCV and Pytorch 1.7 (including TorchVision).&lt;/p&gt;

&lt;p&gt;The Forward does not use the advanced features of Pytorch, and can be directly Scripted or Traced for further deployment.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://gist.github.com/devymex/1f76224b2428d0ddbf92b93def6c587c""&gt;https://gist.github.com/devymex/1f76224b2428d0ddbf92b93def6c587c&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?auto=webp&amp;s=079a7260ec149880c73263d64811698adb22760a', 'width': 1280, 'height': 640}, 'resolutions': [{'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5811c5bda5fece1040636a6af8702ba790f0fd4', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eee576fd4da7535eb53ceb88dd8b52f073048441', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=72872d880460efa723918c000adca0ed259cf775', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3545b9335d763c9da9c16bf7bf9a3f907dbd6f6', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d241ace0f1c07088fac3f8469dbad3b05d2d419', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9055f11bdc00beb0b3589e1cae5817d6070d83bc', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kts9zp,True,,devymex,,1,True,all_ads,False,[],False,,/r/pytorch/comments/kts9zp/reproduced_yolov3_based_on_pytorch_darknet/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kts9zp/reproduced_yolov3_based_on_pytorch_darknet/,7135,1610204148.0,0,,False,,,,,,,,
122,,pytorch,"Hey Guys,

I have a question which is more about Python than Pytorch. When we assign a module to a member field in the construction (e.g. self.linear = nn.Linear(5,10)) it gets registered implicitly, how does that work?",t2_712ebe22,False,,0,False,How implicit registration of modules work,[],r/pytorch,False,6,,0,,,False,t3_ku4ngk,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1610271968.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey Guys,&lt;/p&gt;

&lt;p&gt;I have a question which is more about Python than Pytorch. When we assign a module to a member field in the construction (e.g. self.linear = nn.Linear(5,10)) it gets registered implicitly, how does that work?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ku4ngk,True,,hhh312,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ku4ngk/how_implicit_registration_of_modules_work/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ku4ngk/how_implicit_registration_of_modules_work/,7135,1610243168.0,0,,False,,,,,,,,
123,,pytorch,Is there a way to convert this layer to TensorRT? ONNX opset doesn’t seem to support it,t2_9qg8639v,False,,0,False,PyTorch grid_sample to TensorRT with or without ONNX,[],r/pytorch,False,6,,0,,,False,t3_ktmj24,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1610206476.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there a way to convert this layer to TensorRT? ONNX opset doesn’t seem to support it&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ktmj24,True,,orgoro,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ktmj24/pytorch_grid_sample_to_tensorrt_with_or_without/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ktmj24/pytorch_grid_sample_to_tensorrt_with_or_without/,7135,1610177676.0,0,,False,,,,,,,,
124,,pytorch,"Hello Pytorch enthusiasts, has anyone tried doing custom regularization using pytorch and do you have any recommendations, links to share on how to implement this?",t2_15p51r,False,,0,False,Custom Regularization in PyTorch,[],r/pytorch,False,6,,0,,,False,t3_kss820,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1610098941.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello Pytorch enthusiasts, has anyone tried doing custom regularization using pytorch and do you have any recommendations, links to share on how to implement this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kss820,True,,pascaltuna,,1,True,all_ads,False,[],False,,/r/pytorch/comments/kss820/custom_regularization_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kss820/custom_regularization_in_pytorch/,7135,1610070141.0,0,,False,,,,,,,,
125,,pytorch,"I am using PyTorch 1.7 and Python 3.8 with CIFAR-10 dataset. I am  trying to create a block with: conv -&gt; conv -&gt; pool -&gt; fc.  Fully connected layer (fc) has 256 neurons. The code for this is as  follows:

    # Testing- conv1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, stride = 1, padding = 1, bias = True) 
    conv2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1,     padding = 1, bias = True)
    pool = nn.MaxPool2d(kernel_size = 2, stride = 2)
    fc1 = nn.Linear(in_features = 64 * 16 * 16, out_features = 256, bias = True)  
    
    images.shape
    # torch.Size([32, 3, 32, 32])
    x = conv1(images)
    x.shape
    # torch.Size([32, 64, 32, 32])
    x = conv2(x)
    x.shape
    # torch.Size([32, 64, 32, 32])
    x = pool(x)
    x.shape
    # torch.Size([32, 64, 16, 16])
    
    # This line of code gives error-
    x = fc1(x)

&gt;RuntimeError: mat1 and mat2 shapes cannot be multiplied (32768x16 and 16384x256)  
 

&amp;#x200B;

What's going wrong?",t2_2mmql89p,False,,0,False,PyTorch convolutional block - CIFAR10 - RuntimeError,[],r/pytorch,False,6,,0,,,False,t3_ksxp6o,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1610119809.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am using PyTorch 1.7 and Python 3.8 with CIFAR-10 dataset. I am  trying to create a block with: conv -&amp;gt; conv -&amp;gt; pool -&amp;gt; fc.  Fully connected layer (fc) has 256 neurons. The code for this is as  follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Testing- conv1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, stride = 1, padding = 1, bias = True) 
conv2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1,     padding = 1, bias = True)
pool = nn.MaxPool2d(kernel_size = 2, stride = 2)
fc1 = nn.Linear(in_features = 64 * 16 * 16, out_features = 256, bias = True)  

images.shape
# torch.Size([32, 3, 32, 32])
x = conv1(images)
x.shape
# torch.Size([32, 64, 32, 32])
x = conv2(x)
x.shape
# torch.Size([32, 64, 32, 32])
x = pool(x)
x.shape
# torch.Size([32, 64, 16, 16])

# This line of code gives error-
x = fc1(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;RuntimeError: mat1 and mat2 shapes cannot be multiplied (32768x16 and 16384x256)  &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What&amp;#39;s going wrong?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ksxp6o,True,,grid_world,,4,True,all_ads,False,[],False,,/r/pytorch/comments/ksxp6o/pytorch_convolutional_block_cifar10_runtimeerror/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ksxp6o/pytorch_convolutional_block_cifar10_runtimeerror/,7135,1610091009.0,0,,False,,,,,,,,
126,,pytorch,"Hey guys, I have implemented a [Conv-6 CNN CIFAR-10](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/PyTorch_Conv6_CIFAR10.ipynb) classification in PyTorch. I will  be happy to hear your feedback.",t2_2mmql89p,False,,0,False,PyTorch Conv-6 CIFAR-10,[],r/pytorch,False,6,,0,,,False,t3_kt0nsa,False,dark,0.2,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1610134050.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, I have implemented a &lt;a href=""https://github.com/arjun-majumdar/CNN_Classifications/blob/master/PyTorch_Conv6_CIFAR10.ipynb""&gt;Conv-6 CNN CIFAR-10&lt;/a&gt; classification in PyTorch. I will  be happy to hear your feedback.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?auto=webp&amp;s=f2a8d2644c171302d258e04bb4f56341e0932f5c', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a712b927d57fefc453338d3cc567c9a6a5682180', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8fdbd8b838cc4c3070a600980ec621b534c468c2', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/WofnX18KeQbjmQqI8GXJtkg1lx4T8zdZDJuq43gZGsw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2541ecf1cf94955a97e415c8325f05533b49e8f9', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'SxmolEBMl5rw9UEn7Ftv1WN1nmytQ0dLmTjHbIRwo8E'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kt0nsa,True,,grid_world,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kt0nsa/pytorch_conv6_cifar10/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kt0nsa/pytorch_conv6_cifar10/,7135,1610105250.0,0,,False,,,,,,,,
127,,pytorch,,t2_766u1eio,False,,0,False,"What is a good cca, cka library for pytorch that works (ideally with GPU)?",[],r/pytorch,False,6,,0,140.0,,False,t3_ksocf8,False,dark,0.6,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/cyRBfRCFJ1a81rktVqUIwljNrNXlYShtquFSyq6rBkE.jpg,False,,[],{},link,,False,,1610086815.0,text,6,,,text,discuss.pytorch.org,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ksocf8,True,,No_Ad3397,,1,True,all_ads,False,[],False,,/r/pytorch/comments/ksocf8/what_is_a_good_cca_cka_library_for_pytorch_that/,all_ads,False,https://discuss.pytorch.org/t/what-is-a-good-cca-cka-library-for-pytorch-that-works-ideally-with-gpu/104889,7135,1610058015.0,0,,False,https://discuss.pytorch.org/t/what-is-a-good-cca-cka-library-for-pytorch-that-works-ideally-with-gpu/104889,,,,,,,
128,,pytorch,"In huggingface [https://huggingface.co/transformers/model\_doc/bert.html](https://huggingface.co/transformers/model_doc/bert.html), there are two models: BertModelLMHeadModel and BertForMaskedLM. What is the difference between these two models? Thanks.",t2_55wji4cd,False,,0,False,what is the difference between BertModelLMHeadModel and BertForMaskedLM,[],r/pytorch,False,6,,0,,,False,t3_ks7tcs,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1610031309.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In huggingface &lt;a href=""https://huggingface.co/transformers/model_doc/bert.html""&gt;https://huggingface.co/transformers/model_doc/bert.html&lt;/a&gt;, there are two models: BertModelLMHeadModel and BertForMaskedLM. What is the difference between these two models? Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ks7tcs,True,,AndyLeeeeee,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ks7tcs/what_is_the_difference_between/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ks7tcs/what_is_the_difference_between/,7135,1610002509.0,0,,False,,,,,,,,
129,,pytorch,"Pytorch is a deep learning framework and a scientific computing package. This is how the PyTorch team defines it. Originally torch was built on Lua programming language and for the ease of use, it is converted in Python by the Facebook AI research teams and many others.

[learn PyTorch chapter_1](https://www.dataspoof.info/post/pytorch-for-beginners-basics
)",t2_7jry6ise,False,,0,False,Pytorch for beginners,[],r/pytorch,False,6,,0,,,False,t3_ksfsjx,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1610063019.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Pytorch is a deep learning framework and a scientific computing package. This is how the PyTorch team defines it. Originally torch was built on Lua programming language and for the ease of use, it is converted in Python by the Facebook AI research teams and many others.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.dataspoof.info/post/pytorch-for-beginners-basics""&gt;learn PyTorch chapter_1&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AFYrMZG8ptj4IbbNxKWoPxxFBlWNLfouM1QRyTTyz_Q.jpg?auto=webp&amp;s=50e7f4a051770d4036257606cd8d7a4cc5b4460c', 'width': 940, 'height': 622}, 'resolutions': [{'url': 'https://external-preview.redd.it/AFYrMZG8ptj4IbbNxKWoPxxFBlWNLfouM1QRyTTyz_Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0aabc7bfe839784b8943746d5f46601e8cbe440c', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/AFYrMZG8ptj4IbbNxKWoPxxFBlWNLfouM1QRyTTyz_Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8a05f29e014d39fed7927adf8cbc13b409ed7545', 'width': 216, 'height': 142}, {'url': 'https://external-preview.redd.it/AFYrMZG8ptj4IbbNxKWoPxxFBlWNLfouM1QRyTTyz_Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1c842fc9732779ebc399b46d7e575bf01b76120a', 'width': 320, 'height': 211}, {'url': 'https://external-preview.redd.it/AFYrMZG8ptj4IbbNxKWoPxxFBlWNLfouM1QRyTTyz_Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db20e019a351a60fc822ee5cfda2a216159b9541', 'width': 640, 'height': 423}], 'variants': {}, 'id': 'HVsgYjYKvUOTp_LS7lCiCBl4B9BbNURqqvNJywhqkLw'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ksfsjx,True,,TrainRealistic6118,,4,True,all_ads,False,[],False,,/r/pytorch/comments/ksfsjx/pytorch_for_beginners/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ksfsjx/pytorch_for_beginners/,7135,1610034219.0,0,,False,,,,,,,,
130,,pytorch,,t2_40d0zt4s,False,,0,False,Hands-On Guide To Imaginaire: Nvidia Recently Launched GAN Library,[],r/pytorch,False,6,,0,69.0,,False,t3_krl351,False,dark,1.0,,public,8,0,{},140.0,,False,[],,False,False,,{},,False,8,,False,https://a.thumbs.redditmedia.com/LXRDhLRUR-DQ9IghDTUoERQ0TQBY4iR6cXeMNl4mtd0.jpg,False,,[],{},link,,False,,1609956588.0,text,6,,,text,analyticsindiamag.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xSij5jvAI8fLO0E7P-LInfbCWT1qQ37P3CI7HkL6ajc.jpg?auto=webp&amp;s=b8a86be9bf1d92540ff7383d4b9c2bbfba6658b9', 'width': 2337, 'height': 1153}, 'resolutions': [{'url': 'https://external-preview.redd.it/xSij5jvAI8fLO0E7P-LInfbCWT1qQ37P3CI7HkL6ajc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2cac60229a769d1bf5adb2704e61c130661dfa79', 'width': 108, 'height': 53}, {'url': 'https://external-preview.redd.it/xSij5jvAI8fLO0E7P-LInfbCWT1qQ37P3CI7HkL6ajc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ae7644a0645608b4788ed400334a4b741d1f4b49', 'width': 216, 'height': 106}, {'url': 'https://external-preview.redd.it/xSij5jvAI8fLO0E7P-LInfbCWT1qQ37P3CI7HkL6ajc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fd063708da58ea60dd7ec466f4796bb48e2f868c', 'width': 320, 'height': 157}, {'url': 'https://external-preview.redd.it/xSij5jvAI8fLO0E7P-LInfbCWT1qQ37P3CI7HkL6ajc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4321001e6af5f9540067f16f86105e1e11f717ae', 'width': 640, 'height': 315}, {'url': 'https://external-preview.redd.it/xSij5jvAI8fLO0E7P-LInfbCWT1qQ37P3CI7HkL6ajc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7ed6c2afc4db43783d75b9614711f7d9da5f5f45', 'width': 960, 'height': 473}, {'url': 'https://external-preview.redd.it/xSij5jvAI8fLO0E7P-LInfbCWT1qQ37P3CI7HkL6ajc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f8f798d2b03d45739f240d0300d3e0dfb8add24d', 'width': 1080, 'height': 532}], 'variants': {}, 'id': '65zoVj8aWFuIqTuvMEK0gdk5OdMW9s-63MvRvK_W0gk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,krl351,True,,analyticsindiam,,0,True,all_ads,False,[],False,,/r/pytorch/comments/krl351/handson_guide_to_imaginaire_nvidia_recently/,all_ads,False,https://analyticsindiamag.com/guide-to-nvidia-imaginaire-gan-library-in-python/,7135,1609927788.0,0,,False,https://analyticsindiamag.com/guide-to-nvidia-imaginaire-gan-library-in-python/,,,,,,,
131,,pytorch,"I have a simple LSTM layer and a fully connected later (n_hidden, n_outputs), however I was t to build a Seq2Seq model, where the model takes in a sequence and outputs a sequence. 


The model architecture is like:

Self.lstm = nn.LSTM(n_inp, n_hidden)
Self.fc = nn.Linear(n_hidden, n_output) 

With a relu in between. 

But I understand this gives me a 1xn_output vector, but I want a 1 x sequence_length x n_output.

How would I set up the linear layers",t2_ebu4m,False,,0,False,How do I set up the fully connected layers for a Seq2Seq LSTM?,[],r/pytorch,False,6,,0,,,False,t3_krii24,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1609944709.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a simple LSTM layer and a fully connected later (n_hidden, n_outputs), however I was t to build a Seq2Seq model, where the model takes in a sequence and outputs a sequence. &lt;/p&gt;

&lt;p&gt;The model architecture is like:&lt;/p&gt;

&lt;p&gt;Self.lstm = nn.LSTM(n_inp, n_hidden)
Self.fc = nn.Linear(n_hidden, n_output) &lt;/p&gt;

&lt;p&gt;With a relu in between. &lt;/p&gt;

&lt;p&gt;But I understand this gives me a 1xn_output vector, but I want a 1 x sequence_length x n_output.&lt;/p&gt;

&lt;p&gt;How would I set up the linear layers&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,krii24,True,,Pepipasta,,4,True,all_ads,False,[],False,,/r/pytorch/comments/krii24/how_do_i_set_up_the_fully_connected_layers_for_a/,all_ads,False,https://www.reddit.com/r/pytorch/comments/krii24/how_do_i_set_up_the_fully_connected_layers_for_a/,7135,1609915909.0,0,,False,,,,,,,,
132,,pytorch,"I have a 3D model, where the authors have used scipy to rescale the image as follows,

    from scipy import ndimage
    
    #input_image shape = [D x H x W] 
    out = ndimage.interpolation.zoom(input_image, scale, order=0)

I don't have much knowledge about spline interpolation. I've searched online to understand the topic, and some places it has been hinted that zeroth order is equivalent to 'Nearest Neighbour' interpolation, but nothing concrete has been written. 

I tried using nn.functional.interpolate with mode 'nearest' and 'trilinear' but the output image is not the same as when obtained by using scipy.

Is there any way I can perform the above operation in Pytorch?",t2_3964l43f,False,,0,False,How to perform spline interpolation of zeroth order?,[],r/pytorch,False,6,,0,,,False,t3_krgjxy,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1609937096.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a 3D model, where the authors have used scipy to rescale the image as follows,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from scipy import ndimage

#input_image shape = [D x H x W] 
out = ndimage.interpolation.zoom(input_image, scale, order=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I don&amp;#39;t have much knowledge about spline interpolation. I&amp;#39;ve searched online to understand the topic, and some places it has been hinted that zeroth order is equivalent to &amp;#39;Nearest Neighbour&amp;#39; interpolation, but nothing concrete has been written. &lt;/p&gt;

&lt;p&gt;I tried using nn.functional.interpolate with mode &amp;#39;nearest&amp;#39; and &amp;#39;trilinear&amp;#39; but the output image is not the same as when obtained by using scipy.&lt;/p&gt;

&lt;p&gt;Is there any way I can perform the above operation in Pytorch?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,krgjxy,True,,flabby_abs,,0,True,all_ads,False,[],False,,/r/pytorch/comments/krgjxy/how_to_perform_spline_interpolation_of_zeroth/,all_ads,False,https://www.reddit.com/r/pytorch/comments/krgjxy/how_to_perform_spline_interpolation_of_zeroth/,7135,1609908296.0,0,,False,,,,,,,,
133,,pytorch,"Hi Guys,

Recently, I have posted a series of blogs on medium regarding Self Attention networks and how can one code those using PyTorch and build and train a Classification model. In the series, I have shown various approaches to train a classification model for the dataset available [here](https://cogcomp.seas.upenn.edu/Data/QA/QC/).

Part - 1: [https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-1-33e990636e76](https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-1-33e990636e76)

Part - 1.1: [https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-1-1-3b4224cd4757](https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-1-1-3b4224cd4757)

Part - 2: [https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-2-910b89c7116a](https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-2-910b89c7116a)

Part - 3: [https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-3-74efbda22451](https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-3-74efbda22451)

Have a nice read. Share if you like the content. Comment for any discussions.

Thanks",t2_namkp97,False,,0,False,Coding Attention is All You Need in PyTorch for Question Classification,[],r/pytorch,False,6,,0,,,False,t3_kqxten,False,dark,1.0,,public,10,0,{},,,False,[],,False,False,,{},,False,10,,False,self,False,,[],{},,,True,,1609879529.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Guys,&lt;/p&gt;

&lt;p&gt;Recently, I have posted a series of blogs on medium regarding Self Attention networks and how can one code those using PyTorch and build and train a Classification model. In the series, I have shown various approaches to train a classification model for the dataset available &lt;a href=""https://cogcomp.seas.upenn.edu/Data/QA/QC/""&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Part - 1: &lt;a href=""https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-1-33e990636e76""&gt;https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-1-33e990636e76&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Part - 1.1: &lt;a href=""https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-1-1-3b4224cd4757""&gt;https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-1-1-3b4224cd4757&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Part - 2: &lt;a href=""https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-2-910b89c7116a""&gt;https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-2-910b89c7116a&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Part - 3: &lt;a href=""https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-3-74efbda22451""&gt;https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-3-74efbda22451&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Have a nice read. Share if you like the content. Comment for any discussions.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kqxten,True,,thevatsalsaglani,,4,True,all_ads,False,[],False,,/r/pytorch/comments/kqxten/coding_attention_is_all_you_need_in_pytorch_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kqxten/coding_attention_is_all_you_need_in_pytorch_for/,7135,1609850729.0,0,,False,,,,,,,,
134,,pytorch,,t2_44mbtmjy,False,,0,False,3-D Reconstruction of a moving person from a video!,[],r/pytorch,False,6,,0,,,False,t3_kqpbez,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,default,False,,[],{},link,,False,,1609845610.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/uLw3DhixR4C1UjvOrgLO3SsWsvd-rb0bUokzav3LoV4.jpg?auto=webp&amp;s=9a5ac1e1b753bce83b7b4f3605fab9edeb7b386f', 'width': 664, 'height': 374}, 'resolutions': [{'url': 'https://external-preview.redd.it/uLw3DhixR4C1UjvOrgLO3SsWsvd-rb0bUokzav3LoV4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5684cdda6836172feb9f4b98daa976467516b267', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/uLw3DhixR4C1UjvOrgLO3SsWsvd-rb0bUokzav3LoV4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7867f6fde1dd62d62ac3942958ab520529f2690', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/uLw3DhixR4C1UjvOrgLO3SsWsvd-rb0bUokzav3LoV4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b07950a5cace4fa7a4e6fabf3a48a0895bfe671', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/uLw3DhixR4C1UjvOrgLO3SsWsvd-rb0bUokzav3LoV4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b24e46e5d8714d7aa755edfdad070d08bc9cd6b8', 'width': 640, 'height': 360}], 'variants': {}, 'id': 'EAPXfcV-6QaQ0rinQrdhzTMiE4-BZi_N4OHO4P65JZ0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kqpbez,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kqpbez/3d_reconstruction_of_a_moving_person_from_a_video/,all_ads,False,/r/LatestInML/comments/kqpaou/3d_reconstruction_of_a_moving_person_from_a_video/,7135,1609816810.0,0,,False,/r/LatestInML/comments/kqpaou/3d_reconstruction_of_a_moving_person_from_a_video/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""3-D Reconstruction of a moving person from a video! [https://www.catalyzex.com/paper/arxiv:2012.15838](https://www.catalyzex.com/paper/arxiv:2012.15838)  \n\n\n👇 Free extension to get code for ML papers (❤️' by Andrew Ng) Chrome:[https://chrome.google.com/webstore/detail/find-code-for-research-pa/aikkeehnlfpamidigaffhfmgbkdeheil](https://chrome.google.com/webstore/detail/find-code-for-research-pa/aikkeehnlfpamidigaffhfmgbkdeheil)\n\nFirefox: [https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/](https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/)\n\n&amp;#x200B;\n\nhttps://reddit.com/link/kqpaou/video/b2cklqc4mf961/player\n\n**#ml** **#machinelearning** **#ai** **#artificialintelligence** **#deeplearning** **#datascience** **#deeplearning**"", 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '3-D Reconstruction of a moving person from a video!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 78, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'b2cklqc4mf961': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/kqpaou/asset/b2cklqc4mf961/DASHPlaylist.mpd?a=1618044142%2CMDZiMWJjN2VjNmZlYmEwNWU3NmFjNmE4NmNmMDAzMzhkM2U1ZGE0MTc5YmY4Mzg5MmI0ZTE1M2IyOTI2MWUxZA%3D%3D&amp;v=1&amp;f=sd', 'x': 426, 'y': 159, 'hlsUrl': 'https://v.redd.it/link/kqpaou/asset/b2cklqc4mf961/HLSPlaylist.m3u8?a=1618044142%2CODhmMjJlOGFjMmEwM2FjZGNjZWI2MjllZjdkYzUwNDcwMjY3YWYxNmNkZWE0ZDAwZTM2MmU3N2VkZTBmNjI5NQ%3D%3D&amp;v=1&amp;f=sd', 'id': 'b2cklqc4mf961', 'isGif': False}}, 'name': 't3_kqpaou', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 17, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 17, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/BWe_RDTgupf_iFG4GhNY0YsBVMSfY7rS7BMnWMvbCQ4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1609845544.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;3-D Reconstruction of a moving person from a video! &lt;a href=""https://www.catalyzex.com/paper/arxiv:2012.15838""&gt;https://www.catalyzex.com/paper/arxiv:2012.15838&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;👇 Free extension to get code for ML papers (❤️&amp;#39; by Andrew Ng) Chrome:&lt;a href=""https://chrome.google.com/webstore/detail/find-code-for-research-pa/aikkeehnlfpamidigaffhfmgbkdeheil""&gt;https://chrome.google.com/webstore/detail/find-code-for-research-pa/aikkeehnlfpamidigaffhfmgbkdeheil&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Firefox: &lt;a href=""https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/""&gt;https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/kqpaou/video/b2cklqc4mf961/player""&gt;https://reddit.com/link/kqpaou/video/b2cklqc4mf961/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;#ml&lt;/strong&gt; &lt;strong&gt;#machinelearning&lt;/strong&gt; &lt;strong&gt;#ai&lt;/strong&gt; &lt;strong&gt;#artificialintelligence&lt;/strong&gt; &lt;strong&gt;#deeplearning&lt;/strong&gt; &lt;strong&gt;#datascience&lt;/strong&gt; &lt;strong&gt;#deeplearning&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/uLw3DhixR4C1UjvOrgLO3SsWsvd-rb0bUokzav3LoV4.jpg?auto=webp&amp;s=9a5ac1e1b753bce83b7b4f3605fab9edeb7b386f', 'width': 664, 'height': 374}, 'resolutions': [{'url': 'https://external-preview.redd.it/uLw3DhixR4C1UjvOrgLO3SsWsvd-rb0bUokzav3LoV4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5684cdda6836172feb9f4b98daa976467516b267', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/uLw3DhixR4C1UjvOrgLO3SsWsvd-rb0bUokzav3LoV4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7867f6fde1dd62d62ac3942958ab520529f2690', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/uLw3DhixR4C1UjvOrgLO3SsWsvd-rb0bUokzav3LoV4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b07950a5cace4fa7a4e6fabf3a48a0895bfe671', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/uLw3DhixR4C1UjvOrgLO3SsWsvd-rb0bUokzav3LoV4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b24e46e5d8714d7aa755edfdad070d08bc9cd6b8', 'width': 640, 'height': 360}], 'variants': {}, 'id': 'EAPXfcV-6QaQ0rinQrdhzTMiE4-BZi_N4OHO4P65JZ0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'kqpaou', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/kqpaou/3d_reconstruction_of_a_moving_person_from_a_video/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/kqpaou/3d_reconstruction_of_a_moving_person_from_a_video/', 'subreddit_subscribers': 6676, 'created_utc': 1609816744.0, 'num_crossposts': 3, 'media': None, 'is_video': False}]",t3_kqpaou,,,,,
135,,pytorch,"Hi everyone,

I want to learn how to program in pytorch while learning the theory of artificial intelligence.


Does anyone know any courses or tutorials which explains the fundamentals of artificial intelligence and also explains the applications of pytorch.


For example, the definition of neural network and everything that comes with it. Afterwards, how to code in pytorch a neural network.",t2_3d7ywpmt,False,,0,False,Theory to pytorch,[],r/pytorch,False,6,,0,,,False,t3_kqq208,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1609820477.0,,[],{},,,True,,1609848103.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I want to learn how to program in pytorch while learning the theory of artificial intelligence.&lt;/p&gt;

&lt;p&gt;Does anyone know any courses or tutorials which explains the fundamentals of artificial intelligence and also explains the applications of pytorch.&lt;/p&gt;

&lt;p&gt;For example, the definition of neural network and everything that comes with it. Afterwards, how to code in pytorch a neural network.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kqq208,True,,Shai_Brin,,7,True,all_ads,False,[],False,,/r/pytorch/comments/kqq208/theory_to_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kqq208/theory_to_pytorch/,7135,1609819303.0,0,,False,,,,,,,,
136,,pytorch,"This is a small project we created to train a character level autoregressive transformer (or LSTM) model to predict Python source code. We trained it on GitHub repositories found on awesome pytorch list.

Github repo: [https://github.com/lab-ml/python\_autocomplete](https://github.com/lab-ml/python_autocomplete)

You can try training on Google Colab: [https://colab.research.google.com/github/lab-ml/python\_autocomplete/blob/master/notebooks/train.ipynb](https://colab.research.google.com/github/lab-ml/python_autocomplete/blob/master/notebooks/train.ipynb)

Here are some sample evaluations/visualizations of the trained model: [https://colab.research.google.com/github/lab-ml/python\_autocomplete/blob/master/notebooks/evaluate.ipynb](https://colab.research.google.com/github/lab-ml/python_autocomplete/blob/master/notebooks/evaluate.ipynb)

Working on a simple VSCode extension to test this out. Will open source it soon on the same repository.",t2_1jyhaoq,False,,0,False,Autocomplete Python code with transformers,[],r/pytorch,False,6,,0,,,False,t3_kq84hs,False,dark,1.0,,public,11,0,{},,,False,[],,False,False,,{},,False,11,,False,self,False,,[],{},self,,True,,1609793347.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is a small project we created to train a character level autoregressive transformer (or LSTM) model to predict Python source code. We trained it on GitHub repositories found on awesome pytorch list.&lt;/p&gt;

&lt;p&gt;Github repo: &lt;a href=""https://github.com/lab-ml/python_autocomplete""&gt;https://github.com/lab-ml/python_autocomplete&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can try training on Google Colab: &lt;a href=""https://colab.research.google.com/github/lab-ml/python_autocomplete/blob/master/notebooks/train.ipynb""&gt;https://colab.research.google.com/github/lab-ml/python_autocomplete/blob/master/notebooks/train.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here are some sample evaluations/visualizations of the trained model: &lt;a href=""https://colab.research.google.com/github/lab-ml/python_autocomplete/blob/master/notebooks/evaluate.ipynb""&gt;https://colab.research.google.com/github/lab-ml/python_autocomplete/blob/master/notebooks/evaluate.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Working on a simple VSCode extension to test this out. Will open source it soon on the same repository.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?auto=webp&amp;s=54844664dea21c579bf85bb9f77db2f366d995b5', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4935835b34570436c07bbf78c25cdcda892a4e7f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dca994f869c1d4df2b64aa8e606b9df61a08437', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7825efebb9ce056a4cec7a106daf8f326e9c2856', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'oze48py95sVXMx6dFac2s_Kf1E0eS-CySSSJBqwJb-o'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kq84hs,True,,mlvpj,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kq84hs/autocomplete_python_code_with_transformers/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kq84hs/autocomplete_python_code_with_transformers/,7135,1609764547.0,0,,False,,,,,,,,
137,,pytorch,"Hello All. New here! Ive been messing around with this error and I cant seem to get ignite to give me the MSError after each batch.

Here's a few blocks of my code that I'm trying to get to work. Most of it is copied from this link

[https://colab.research.google.com/github/pytorch/ignite/blob/master/examples/notebooks/FashionMNIST.ipynb](https://colab.research.google.com/github/pytorch/ignite/blob/master/examples/notebooks/FashionMNIST.ipynb)

    trainer = create_supervised_trainer(model, optimizer, criterion, device=device)
    
    metrics = {
        'accuracy':Accuracy(),
        'mse':MeanSquaredError(),
        'cm':ConfusionMatrix(num_classes=10)
    }
    train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)
    val_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)
    training_history = {'accuracy':[],'loss':[]}
    validation_history = {'accuracy':[],'loss':[]}
    last_epoch = []

&amp;#x200B;

    # creating model, and defining optimizer and loss
    model = CNN()
    # moving model to gpu if available
    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
    model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate) 
    criterion = nn.MSELoss()
    #criterion = nn.NLLLoss()

&amp;#x200B;

    @trainer.on(Events.EPOCH_COMPLETED)
    def log_training_results(trainer):
        train_evaluator.run(train_loader)
        metrics = train_evaluator.state.metrics
        accuracy = metrics['accuracy']*100
        loss = metrics['mse']
        last_epoch.append(0)
        training_history['accuracy'].append(accuracy)
        training_history['loss'].append(loss)
        print(""Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg MSE loss: {:.2f} ""
              .format(trainer.state.epoch, accuracy, loss))
    
    def log_validation_results(trainer):
        val_evaluator.run(val_loader)
        metrics = val_evaluator.state.metrics
        accuracy = metrics['accuracy']*100
        loss = metrics['mse']
        validation_history['accuracy'].append(accuracy)
        validation_history['loss'].append(loss)
        print(""Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg MSE loss: {:.2f} ""
              .format(trainer.state.epoch, accuracy, loss))
        
    trainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results) 

And my model looks like this

# ----------------------------------------------------------------Layer (type)               Output Shape         Param

Conv2d-1           \[64, 64, 28, 28\]             576

ReLU-2           \[64, 64, 28, 28\]               0

MaxPool2d-3           \[64, 64, 14, 14\]               0

Linear-4                 \[64, 6000\]      75,270,000

Linear-5                 \[64, 1200\]       7,201,200

Linear-6                  \[64, 120\]         144,120

Linear-7                   \[64, 10\]           1,210

&amp;#x200B;

But i get this error

&amp;#x200B;

    RuntimeError: The size of tensor a (10) must match the size of tensor b (64) at non-singleton dimension 1

And I have no idea how to fix it. My main goal is to find the MSError after each batch, and this works if my criterion is NLLLoss but not with MSE. I'm not sure if its a pytorch or an ignite issue. Any help would be greatly appreciated!",t2_7dv7g4em,False,,0,False,MeanSquaredError() troubles,[],r/pytorch,False,6,,0,,,False,t3_kq34be,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1609770876.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello All. New here! Ive been messing around with this error and I cant seem to get ignite to give me the MSError after each batch.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s a few blocks of my code that I&amp;#39;m trying to get to work. Most of it is copied from this link&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://colab.research.google.com/github/pytorch/ignite/blob/master/examples/notebooks/FashionMNIST.ipynb""&gt;https://colab.research.google.com/github/pytorch/ignite/blob/master/examples/notebooks/FashionMNIST.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;trainer = create_supervised_trainer(model, optimizer, criterion, device=device)

metrics = {
    &amp;#39;accuracy&amp;#39;:Accuracy(),
    &amp;#39;mse&amp;#39;:MeanSquaredError(),
    &amp;#39;cm&amp;#39;:ConfusionMatrix(num_classes=10)
}
train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)
val_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)
training_history = {&amp;#39;accuracy&amp;#39;:[],&amp;#39;loss&amp;#39;:[]}
validation_history = {&amp;#39;accuracy&amp;#39;:[],&amp;#39;loss&amp;#39;:[]}
last_epoch = []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# creating model, and defining optimizer and loss
model = CNN()
# moving model to gpu if available
device = torch.device(&amp;quot;cuda:0&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;)
model.to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate) 
criterion = nn.MSELoss()
#criterion = nn.NLLLoss()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@trainer.on(Events.EPOCH_COMPLETED)
def log_training_results(trainer):
    train_evaluator.run(train_loader)
    metrics = train_evaluator.state.metrics
    accuracy = metrics[&amp;#39;accuracy&amp;#39;]*100
    loss = metrics[&amp;#39;mse&amp;#39;]
    last_epoch.append(0)
    training_history[&amp;#39;accuracy&amp;#39;].append(accuracy)
    training_history[&amp;#39;loss&amp;#39;].append(loss)
    print(&amp;quot;Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg MSE loss: {:.2f} &amp;quot;
          .format(trainer.state.epoch, accuracy, loss))

def log_validation_results(trainer):
    val_evaluator.run(val_loader)
    metrics = val_evaluator.state.metrics
    accuracy = metrics[&amp;#39;accuracy&amp;#39;]*100
    loss = metrics[&amp;#39;mse&amp;#39;]
    validation_history[&amp;#39;accuracy&amp;#39;].append(accuracy)
    validation_history[&amp;#39;loss&amp;#39;].append(loss)
    print(&amp;quot;Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg MSE loss: {:.2f} &amp;quot;
          .format(trainer.state.epoch, accuracy, loss))

trainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And my model looks like this&lt;/p&gt;

&lt;h1&gt;----------------------------------------------------------------Layer (type)               Output Shape         Param&lt;/h1&gt;

&lt;p&gt;Conv2d-1           [64, 64, 28, 28]             576&lt;/p&gt;

&lt;p&gt;ReLU-2           [64, 64, 28, 28]               0&lt;/p&gt;

&lt;p&gt;MaxPool2d-3           [64, 64, 14, 14]               0&lt;/p&gt;

&lt;p&gt;Linear-4                 [64, 6000]      75,270,000&lt;/p&gt;

&lt;p&gt;Linear-5                 [64, 1200]       7,201,200&lt;/p&gt;

&lt;p&gt;Linear-6                  [64, 120]         144,120&lt;/p&gt;

&lt;p&gt;Linear-7                   [64, 10]           1,210&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;But i get this error&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RuntimeError: The size of tensor a (10) must match the size of tensor b (64) at non-singleton dimension 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And I have no idea how to fix it. My main goal is to find the MSError after each batch, and this works if my criterion is NLLLoss but not with MSE. I&amp;#39;m not sure if its a pytorch or an ignite issue. Any help would be greatly appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/MrcDZx2izDY9ERwgWmMS-Hm2M3GEKZgeYLDszSh-KrQ.jpg?auto=webp&amp;s=73eb91ea5a5347f216c0f0c4d6796396826aae49', 'width': 260, 'height': 260}, 'resolutions': [{'url': 'https://external-preview.redd.it/MrcDZx2izDY9ERwgWmMS-Hm2M3GEKZgeYLDszSh-KrQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b647239f77bf713f4a6209cfa4867351c055fd9', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/MrcDZx2izDY9ERwgWmMS-Hm2M3GEKZgeYLDszSh-KrQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f4234ff3f4f4ebd7f77236dedb03a2faee3e04a', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kq34be,True,,LilBitcoinStreams,,4,True,all_ads,False,[],False,,/r/pytorch/comments/kq34be/meansquarederror_troubles/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kq34be/meansquarederror_troubles/,7135,1609742076.0,0,,False,,,,,,,,
138,,pytorch,"Added HyperLSTM (introduced in paper HyperNetworks by Ha et al.) implementation with explanations to our collection of implementations of neural network architectures/algorithms. HyperLSTM uses a smaller LSTM network (hyper network) to alter (row-wise scale) parameters of the actual LSTM. That is, the parameters of the LSTM change at each step.

Source code with side-by-side notes: [https://lab-ml.com/labml\_nn/hypernetworks/hyper\_lstm.html](https://lab-ml.com/labml_nn/hypernetworks/hyper_lstm.html)

Github Repo: [https://github.com/lab-ml/nn](https://github.com/lab-ml/nn)",t2_1jyhaoq,False,,0,False,HyperLSTM PyTorch implementation,[],r/pytorch,False,6,,0,,,False,t3_kpjvzc,False,dark,0.88,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},self,,True,,1609706042.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Added HyperLSTM (introduced in paper HyperNetworks by Ha et al.) implementation with explanations to our collection of implementations of neural network architectures/algorithms. HyperLSTM uses a smaller LSTM network (hyper network) to alter (row-wise scale) parameters of the actual LSTM. That is, the parameters of the LSTM change at each step.&lt;/p&gt;

&lt;p&gt;Source code with side-by-side notes: &lt;a href=""https://lab-ml.com/labml_nn/hypernetworks/hyper_lstm.html""&gt;https://lab-ml.com/labml_nn/hypernetworks/hyper_lstm.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github Repo: &lt;a href=""https://github.com/lab-ml/nn""&gt;https://github.com/lab-ml/nn&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?auto=webp&amp;s=54844664dea21c579bf85bb9f77db2f366d995b5', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4935835b34570436c07bbf78c25cdcda892a4e7f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dca994f869c1d4df2b64aa8e606b9df61a08437', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7825efebb9ce056a4cec7a106daf8f326e9c2856', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'oze48py95sVXMx6dFac2s_Kf1E0eS-CySSSJBqwJb-o'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kpjvzc,True,,mlvpj,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kpjvzc/hyperlstm_pytorch_implementation/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kpjvzc/hyperlstm_pytorch_implementation/,7135,1609677242.0,0,,False,,,,,,,,
139,,pytorch,"I have a model which is train on 14 labels.
I have to fine tune the model on a new set of data with 19 labels. The weights of the initial training are there, but I'm unable to load these weights, as the output size is different.

Do I just save the weights of the previous model without the output layer, load that into the new model and then explicitly attach a new output layer for training?

I'm still new to this, a little confused.",t2_4xhrybaz,False,,0,False,How do I perform transfer learning on a model with different number of outputs?,[],r/pytorch,False,6,,0,,,False,t3_kpot4i,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1609724287.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a model which is train on 14 labels.
I have to fine tune the model on a new set of data with 19 labels. The weights of the initial training are there, but I&amp;#39;m unable to load these weights, as the output size is different.&lt;/p&gt;

&lt;p&gt;Do I just save the weights of the previous model without the output layer, load that into the new model and then explicitly attach a new output layer for training?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m still new to this, a little confused.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kpot4i,True,,banenvy,,5,True,all_ads,False,[],False,,/r/pytorch/comments/kpot4i/how_do_i_perform_transfer_learning_on_a_model/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kpot4i/how_do_i_perform_transfer_learning_on_a_model/,7135,1609695487.0,0,,False,,,,,,,,
140,,pytorch,"I feed an image as an input to a pre-trained cnn model after applying the following transformation operations to the image.

    self.transform = transforms.Compose([
        transforms.Resize(352, 352),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])])

I want to calculate the gradients over these operations. One method that has been suggested is to use the [kornia](https://kornia.readthedocs.io/en/latest/introduction.html) library. But I is there a way I can do this by adding another layer at the beginning of the pre-trained unet model that perform the same transformations.

Till now I have tried the following methods to create a custom layer and then add it using nn.Sequential() method

1) By modifying the source code of [kornia.enhance.normalize](https://kornia.readthedocs.io/en/latest/_modules/kornia/enhance/normalize.html#normalize) method.

    class MyModel(nn.Module):
    
        def __init__(self):
            super().__init__()
    
        def forward(self, input):
            image = input.view(3,-1)
            mean = image.mean(1)
            std = image.std(1)
    
            if mean.shape:
                mean = mean[..., :, None, None].to(input.device)
    
            if std.shape:
                std = std[..., :, None, None].to(input.device)
    
            out = (input - mean) / std
    
            return out.unsqueeze(0)
    
    myModel = MyModel()
    new_model = nn.Sequential(myModel, unet)

2) By simply adding a BatchNorm Layer and modifying the mean and standard deviation parameters of that layer

    new_model = nn.Sequential(nn.BatchNorm2d(3), unet)

3) 

    class MyModel(nn.Module):
      def __init__(self, mean, std):
        super().__init__()
        self.mean = mean
        self.std = std
    
      def forward(self, image):
        img = transforms.Normalize(self.mean, self.std)(image)
        img = img.unsqueeze(0)
        return img

In all the methods, the image wasn't even transformed in the first place (the unnormalized input was passed onto the second layer).

How can I  
1) Transform the image inside the model  
2) Back-propogate the gradients during inference.",t2_3964l43f,False,,0,False,Gradient backpropagation over transformation operations,[],r/pytorch,False,6,,0,,,False,t3_kpjinv,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1609704195.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I feed an image as an input to a pre-trained cnn model after applying the following transformation operations to the image.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;self.transform = transforms.Compose([
    transforms.Resize(352, 352),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I want to calculate the gradients over these operations. One method that has been suggested is to use the &lt;a href=""https://kornia.readthedocs.io/en/latest/introduction.html""&gt;kornia&lt;/a&gt; library. But I is there a way I can do this by adding another layer at the beginning of the pre-trained unet model that perform the same transformations.&lt;/p&gt;

&lt;p&gt;Till now I have tried the following methods to create a custom layer and then add it using nn.Sequential() method&lt;/p&gt;

&lt;p&gt;1) By modifying the source code of &lt;a href=""https://kornia.readthedocs.io/en/latest/_modules/kornia/enhance/normalize.html#normalize""&gt;kornia.enhance.normalize&lt;/a&gt; method.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class MyModel(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, input):
        image = input.view(3,-1)
        mean = image.mean(1)
        std = image.std(1)

        if mean.shape:
            mean = mean[..., :, None, None].to(input.device)

        if std.shape:
            std = std[..., :, None, None].to(input.device)

        out = (input - mean) / std

        return out.unsqueeze(0)

myModel = MyModel()
new_model = nn.Sequential(myModel, unet)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2) By simply adding a BatchNorm Layer and modifying the mean and standard deviation parameters of that layer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;new_model = nn.Sequential(nn.BatchNorm2d(3), unet)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3) &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class MyModel(nn.Module):
  def __init__(self, mean, std):
    super().__init__()
    self.mean = mean
    self.std = std

  def forward(self, image):
    img = transforms.Normalize(self.mean, self.std)(image)
    img = img.unsqueeze(0)
    return img
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In all the methods, the image wasn&amp;#39;t even transformed in the first place (the unnormalized input was passed onto the second layer).&lt;/p&gt;

&lt;p&gt;How can I&lt;br/&gt;
1) Transform the image inside the model&lt;br/&gt;
2) Back-propogate the gradients during inference.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kpjinv,True,,flabby_abs,,2,True,all_ads,False,[],False,,/r/pytorch/comments/kpjinv/gradient_backpropagation_over_transformation/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kpjinv/gradient_backpropagation_over_transformation/,7135,1609675395.0,0,,False,,,,,,,,
141,,pytorch,"I have two separate folders , one contain images and the other one contain text files of labels.

Importing the images with the data loader without any problems.But when importing the labels manually with for loops then transforming them to Pytorch Tensor it won't work.

Is there any way to load the images and the labels compatible with the Pytorch tensor format ?",t2_25pt4vpu,False,,0,False,"newbie question, how to load data from disk?",[],r/pytorch,False,6,,0,,,False,t3_koalff,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1609533157.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have two separate folders , one contain images and the other one contain text files of labels.&lt;/p&gt;

&lt;p&gt;Importing the images with the data loader without any problems.But when importing the labels manually with for loops then transforming them to Pytorch Tensor it won&amp;#39;t work.&lt;/p&gt;

&lt;p&gt;Is there any way to load the images and the labels compatible with the Pytorch tensor format ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,koalff,True,,KAkrm,,15,True,all_ads,False,[],False,,/r/pytorch/comments/koalff/newbie_question_how_to_load_data_from_disk/,all_ads,False,https://www.reddit.com/r/pytorch/comments/koalff/newbie_question_how_to_load_data_from_disk/,7135,1609504357.0,0,,False,,,,,,,,
142,,pytorch,"The [inference coding](https://github.com/promach/mcts/blob/main/play.py) gave out repeated wrong results, it seems like the model output trained by the [training code](https://github.com/promach/mcts/blob/main/Net.py) is wrong.  Any idea why ?

&amp;#x200B;

line 64 of the inference coding : 

    next_move = np.binary_repr(next_move_probabilities.argmax())

always give the same result this means the trained model is definitely wrong  


Could anyone advise ?",t2_bpftl,False,,0,False,Neural Network for tic tac toe,[],r/pytorch,False,6,,0,,,False,t3_ko9u7p,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1609529303.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The &lt;a href=""https://github.com/promach/mcts/blob/main/play.py""&gt;inference coding&lt;/a&gt; gave out repeated wrong results, it seems like the model output trained by the &lt;a href=""https://github.com/promach/mcts/blob/main/Net.py""&gt;training code&lt;/a&gt; is wrong.  Any idea why ?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;line 64 of the inference coding : &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;next_move = np.binary_repr(next_move_probabilities.argmax())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;always give the same result this means the trained model is definitely wrong  &lt;/p&gt;

&lt;p&gt;Could anyone advise ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/9d05FBviry4xZbtj5hrL39dEeESpYm-eW_O8NwWaMJc.jpg?auto=webp&amp;s=a71ee0a57683973328fb511934f60e94e7bf4a88', 'width': 420, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/9d05FBviry4xZbtj5hrL39dEeESpYm-eW_O8NwWaMJc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=56fe2c0f3efd18dc06f39795737c2aa1d6c57fbc', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/9d05FBviry4xZbtj5hrL39dEeESpYm-eW_O8NwWaMJc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=31badc65b6c5ce3ceccdf9a2ff55cb48d248bd11', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/9d05FBviry4xZbtj5hrL39dEeESpYm-eW_O8NwWaMJc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a2da7c378cffe706a1da92423226256192c7715f', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'Rjx6RIFbZs51PhPAFkKZU4Pl7b6dp5BwJZUq2CuhXCU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ko9u7p,True,,promach,,11,True,all_ads,False,[],False,,/r/pytorch/comments/ko9u7p/neural_network_for_tic_tac_toe/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ko9u7p/neural_network_for_tic_tac_toe/,7135,1609500503.0,0,,False,,,,,,,,
143,,pytorch," It seems the newer versions of Pytorch are giving errors with certain Deep RL implementations, especially those involving a common network stem bracnhed to give two different outputs( like in Actor Critic methods). Appreciate any help, and please ask for further details if required.

This is the error -

RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation:

Downgrading to Pytorch &lt;1.4.0 doesn't give the same error, hence the issue since all the implementation seem to be based on earlier versions.",t2_2whhhjzo,False,,0,False,Issue of recent updates with RL algorithms.,[],r/pytorch,False,6,,0,,,False,t3_knoaf3,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1609442250.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;It seems the newer versions of Pytorch are giving errors with certain Deep RL implementations, especially those involving a common network stem bracnhed to give two different outputs( like in Actor Critic methods). Appreciate any help, and please ask for further details if required.&lt;/p&gt;

&lt;p&gt;This is the error -&lt;/p&gt;

&lt;p&gt;RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation:&lt;/p&gt;

&lt;p&gt;Downgrading to Pytorch &amp;lt;1.4.0 doesn&amp;#39;t give the same error, hence the issue since all the implementation seem to be based on earlier versions.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,knoaf3,True,,pandudon,,3,True,all_ads,False,[],False,,/r/pytorch/comments/knoaf3/issue_of_recent_updates_with_rl_algorithms/,all_ads,False,https://www.reddit.com/r/pytorch/comments/knoaf3/issue_of_recent_updates_with_rl_algorithms/,7135,1609413450.0,0,,False,,,,,,,,
144,,pytorch," 

In the pytorch implementation of BLSTM-CRF tutorial ([https://pytorch.org/tutorials/beginner/nlp/advanced\_tutorial.html#advanced-making-dynamic-decisions-and-the-bi-lstm-crf](https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#advanced-making-dynamic-decisions-and-the-bi-lstm-crf)), log-sum-exp operation is used to calcualted forward features, which is lised below

        def _forward_alg(self, feats):
            # Do the forward algorithm to compute the partition function
            init_alphas = torch.full((1, self.tagset_size), -10000.)
            # START_TAG has all of the score.
            init_alphas[0][self.tag_to_ix[START_TAG]] = 0.
    
            # Wrap in a variable so that we will get automatic backprop
            forward_var = init_alphas
    
            # Iterate through the sentence
            for feat in feats:
                alphas_t = []  # The forward tensors at this timestep
                for next_tag in range(self.tagset_size):
                    # broadcast the emission score: it is the same regardless of
                    # the previous tag
                    emit_score = feat[next_tag].view(
                        1, -1).expand(1, self.tagset_size)
                    # the ith entry of trans_score is the score of transitioning to
                    # next_tag from i
                    trans_score = self.transitions[next_tag].view(1, -1)
                    # The ith entry of next_tag_var is the value for the
                    # edge (i -&gt; next_tag) before we do log-sum-exp
                    next_tag_var = forward_var + trans_score + emit_score
                    # The forward variable for this tag is log-sum-exp of all the
                    # scores.
                    alphas_t.append(log_sum_exp(next_tag_var).view(1))
                forward_var = torch.cat(alphas_t).view(1, -1)
            terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]
            alpha = log_sum_exp(terminal_var)
            return alpha
    

The log\_sum\_exp operation on the calculated features is in the line

                   alphas_t.append(log_sum_exp(next_tag_var).view(1)) 

My question is that why log\_sum\_exp operation is needed? Thanks.",t2_55wji4cd,False,,0,False,why using log_sum_exp in calculating forward features in BLSTM-CRF,[],r/pytorch,False,6,,0,,,False,t3_knnpj0,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1609439263.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In the pytorch implementation of BLSTM-CRF tutorial (&lt;a href=""https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#advanced-making-dynamic-decisions-and-the-bi-lstm-crf""&gt;https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#advanced-making-dynamic-decisions-and-the-bi-lstm-crf&lt;/a&gt;), log-sum-exp operation is used to calcualted forward features, which is lised below&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    def _forward_alg(self, feats):
        # Do the forward algorithm to compute the partition function
        init_alphas = torch.full((1, self.tagset_size), -10000.)
        # START_TAG has all of the score.
        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.

        # Wrap in a variable so that we will get automatic backprop
        forward_var = init_alphas

        # Iterate through the sentence
        for feat in feats:
            alphas_t = []  # The forward tensors at this timestep
            for next_tag in range(self.tagset_size):
                # broadcast the emission score: it is the same regardless of
                # the previous tag
                emit_score = feat[next_tag].view(
                    1, -1).expand(1, self.tagset_size)
                # the ith entry of trans_score is the score of transitioning to
                # next_tag from i
                trans_score = self.transitions[next_tag].view(1, -1)
                # The ith entry of next_tag_var is the value for the
                # edge (i -&amp;gt; next_tag) before we do log-sum-exp
                next_tag_var = forward_var + trans_score + emit_score
                # The forward variable for this tag is log-sum-exp of all the
                # scores.
                alphas_t.append(log_sum_exp(next_tag_var).view(1))
            forward_var = torch.cat(alphas_t).view(1, -1)
        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]
        alpha = log_sum_exp(terminal_var)
        return alpha
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The log_sum_exp operation on the calculated features is in the line&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;               alphas_t.append(log_sum_exp(next_tag_var).view(1)) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My question is that why log_sum_exp operation is needed? Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,knnpj0,True,,AndyLeeeeee,,2,True,all_ads,False,[],False,,/r/pytorch/comments/knnpj0/why_using_log_sum_exp_in_calculating_forward/,all_ads,False,https://www.reddit.com/r/pytorch/comments/knnpj0/why_using_log_sum_exp_in_calculating_forward/,7135,1609410463.0,0,,False,,,,,,,,
145,,pytorch,,t2_40d0zt4s,False,,0,False,Guide To GluonTS and PytorchTS For Time-Series Forecasting (With Python Implementation),[],r/pytorch,False,6,,0,77.0,,False,t3_kn74yw,False,dark,0.82,,public,7,0,{},140.0,,False,[],,False,False,,{},,False,7,,False,https://b.thumbs.redditmedia.com/dGsl1Ve3_Fi3spfSIlUSIgZkmTOxROWPVTAmuPnaU1Q.jpg,False,,[],{},link,,False,,1609380017.0,text,6,,,text,analyticsindiamag.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/twNfEfc-BHBpbHgaADDTwFPNnoB0gVYvJz7_dQJZBls.jpg?auto=webp&amp;s=938f8c1fd65bf946d1d519761a2e93d6c55ee11d', 'width': 1816, 'height': 1000}, 'resolutions': [{'url': 'https://external-preview.redd.it/twNfEfc-BHBpbHgaADDTwFPNnoB0gVYvJz7_dQJZBls.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=008723bc75d8fed5d87381adf84a6fdc20f28514', 'width': 108, 'height': 59}, {'url': 'https://external-preview.redd.it/twNfEfc-BHBpbHgaADDTwFPNnoB0gVYvJz7_dQJZBls.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=081e8ffb2e51396de332774b553fa7458caf8714', 'width': 216, 'height': 118}, {'url': 'https://external-preview.redd.it/twNfEfc-BHBpbHgaADDTwFPNnoB0gVYvJz7_dQJZBls.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=90e6c9e12b381149d8d7d569bbc9d061038f9869', 'width': 320, 'height': 176}, {'url': 'https://external-preview.redd.it/twNfEfc-BHBpbHgaADDTwFPNnoB0gVYvJz7_dQJZBls.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0be7f95c1e3e1fba5a58cfa6571f60ac4b1f4901', 'width': 640, 'height': 352}, {'url': 'https://external-preview.redd.it/twNfEfc-BHBpbHgaADDTwFPNnoB0gVYvJz7_dQJZBls.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f037537395aa65624bc76f009e5450ee6605759', 'width': 960, 'height': 528}, {'url': 'https://external-preview.redd.it/twNfEfc-BHBpbHgaADDTwFPNnoB0gVYvJz7_dQJZBls.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8eb84b9a3b77cdb6b727880d55dbdad775b57ece', 'width': 1080, 'height': 594}], 'variants': {}, 'id': 'L_566vCEFet2u1kUDz-BJ0WfztdfgHuc-qK7j5J_I7U'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kn74yw,True,,analyticsindiam,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kn74yw/guide_to_gluonts_and_pytorchts_for_timeseries/,all_ads,False,https://analyticsindiamag.com/gluonts-pytorchts-for-time-series-forecasting/?fbclid=IwAR15n6h2buDoz5923Q_hRYzsToFgmI4epCVF7tA_MElybtWCov2JqElMgSU,7135,1609351217.0,0,,False,https://analyticsindiamag.com/gluonts-pytorchts-for-time-series-forecasting/?fbclid=IwAR15n6h2buDoz5923Q_hRYzsToFgmI4epCVF7tA_MElybtWCov2JqElMgSU,,,,,,,
146,,pytorch,"Hi everyone, I am trying to build pytorch from the rocm github. Using the script to transpile CUDA to ROCm is working, but when compiling it fails linkink libtorch_hip.so and c++ tells me that -E or -x is required when the input is feom the standard input. Is there anyone that can help me ?",t2_3p9011e2,False,,0,False,Building pytorch on rocm,[],r/pytorch,False,6,,0,,,False,t3_kn6xqa,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1609379398.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, I am trying to build pytorch from the rocm github. Using the script to transpile CUDA to ROCm is working, but when compiling it fails linkink libtorch_hip.so and c++ tells me that -E or -x is required when the input is feom the standard input. Is there anyone that can help me ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kn6xqa,True,,baalroga,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kn6xqa/building_pytorch_on_rocm/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kn6xqa/building_pytorch_on_rocm/,7135,1609350598.0,1,,False,,,,,,,,
147,,pytorch,"I'm putting something together where, in one process, I am training a model, and in another process, I am loading samples from disk, performing pre-processing, and assembling my input and label tensors. I currently have code that will do this, so that the moment my model finishes with one batch, bam, the next batch is ready to go.

The thing is, I'm wondering whether or not this pre-fetching is something that is already implemented in the dataloaders? I saw some things about ""pre-fetch factors"" in the source code, but I'm not super certain how that works when it comes to actually enumerating the dataloader, if it does all the pre-fetching right when you enumerate it, if each individual batch is being pre-fetched while the model runs, and is delivered when needed, etc.

I went through some of the source code, but I couldn't really make heads or tails of it. So if somebody could explain to me what's actually going on under the hood with dataloaders, I would appreciate it greatly.",t2_qaixi,False,,0,False,"How, specifically, does the pre-fetching in DataLoaders work?",[],r/pytorch,False,6,,0,,,False,t3_kmo5k4,False,dark,0.43,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1609308435.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m putting something together where, in one process, I am training a model, and in another process, I am loading samples from disk, performing pre-processing, and assembling my input and label tensors. I currently have code that will do this, so that the moment my model finishes with one batch, bam, the next batch is ready to go.&lt;/p&gt;

&lt;p&gt;The thing is, I&amp;#39;m wondering whether or not this pre-fetching is something that is already implemented in the dataloaders? I saw some things about &amp;quot;pre-fetch factors&amp;quot; in the source code, but I&amp;#39;m not super certain how that works when it comes to actually enumerating the dataloader, if it does all the pre-fetching right when you enumerate it, if each individual batch is being pre-fetched while the model runs, and is delivered when needed, etc.&lt;/p&gt;

&lt;p&gt;I went through some of the source code, but I couldn&amp;#39;t really make heads or tails of it. So if somebody could explain to me what&amp;#39;s actually going on under the hood with dataloaders, I would appreciate it greatly.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kmo5k4,True,,MrAcurite,,3,True,all_ads,False,[],False,,/r/pytorch/comments/kmo5k4/how_specifically_does_the_prefetching_in/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kmo5k4/how_specifically_does_the_prefetching_in/,7135,1609279635.0,0,,False,,,,,,,,
148,,pytorch,"Hey, 

I am dabbling with AI to create visuals for bands and found this repo:  
[https://github.com/JCBrouwer/maua-stylegan2](https://github.com/JCBrouwer/maua-stylegan2)  
I would love to get it to run, but finally my lack of pytorch knowledge stands in my way.

When I try to run generate\_audiovisual.py (on Windows, with a 2080TI GPU), ninja starts building. And fails. I have cl.exe and gcc on my machine, but first of all, I think it should not even compile anything at all as I instlled pytorch

    pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html

I have spent a lot of time with this already, maybe I am missing something that is obvious to you.

Thanks in advance!",t2_cuq69,False,,0,False,Very basic pytorch installation question,[],r/pytorch,False,6,,0,,,False,t3_kmggsf,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1609284571.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, &lt;/p&gt;

&lt;p&gt;I am dabbling with AI to create visuals for bands and found this repo:&lt;br/&gt;
&lt;a href=""https://github.com/JCBrouwer/maua-stylegan2""&gt;https://github.com/JCBrouwer/maua-stylegan2&lt;/a&gt;&lt;br/&gt;
I would love to get it to run, but finally my lack of pytorch knowledge stands in my way.&lt;/p&gt;

&lt;p&gt;When I try to run generate_audiovisual.py (on Windows, with a 2080TI GPU), ninja starts building. And fails. I have cl.exe and gcc on my machine, but first of all, I think it should not even compile anything at all as I instlled pytorch&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have spent a lot of time with this already, maybe I am missing something that is obvious to you.&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/dK04Fc6GJlodkyA_6x4xh7ysTFk2qpk-XacdlxrBVVE.jpg?auto=webp&amp;s=8aa2f10550f2e56f12b0421470a4fd297cc66623', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/dK04Fc6GJlodkyA_6x4xh7ysTFk2qpk-XacdlxrBVVE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=94977da681c5b4be0160cfc71d8e4d7db5d159a0', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/dK04Fc6GJlodkyA_6x4xh7ysTFk2qpk-XacdlxrBVVE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9d4cef597f6636020f5ac043f639b8f70cad3d92', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/dK04Fc6GJlodkyA_6x4xh7ysTFk2qpk-XacdlxrBVVE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=42299d913fddb1e60f46f9a8098d62a23c79cd0f', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'fioXuEwNcL14N0B04lcjeyPiLWla4RmbG_gdawERzk0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kmggsf,True,,lazerozen,,4,True,all_ads,False,[],False,,/r/pytorch/comments/kmggsf/very_basic_pytorch_installation_question/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kmggsf/very_basic_pytorch_installation_question/,7135,1609255771.0,0,,False,,,,,,,,
149,,pytorch,"Hi, I have two lists containing 3d tensors, any idea how to merge them in PyTorch. [torch.cat](https://torch.cat) is causing problem.",t2_9e3lxp9j,False,,0,False,List concatenation in Pytorch,[],r/pytorch,False,6,,0,,,False,t3_kman4y,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1609260272.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I have two lists containing 3d tensors, any idea how to merge them in PyTorch. &lt;a href=""https://torch.cat""&gt;torch.cat&lt;/a&gt; is causing problem.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kman4y,True,,Traditional_Use_91,,2,True,all_ads,False,[],False,,/r/pytorch/comments/kman4y/list_concatenation_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kman4y/list_concatenation_in_pytorch/,7135,1609231472.0,0,,False,,,,,,,,
150,,pytorch,"I have a pretrained UNet model with the following architecture

    UNet(
      (encoder1): Sequential(
        (enc1conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (enc1relu1): ReLU(inplace=True)
        (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (enc1relu2): ReLU(inplace=True)
      )
      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (encoder2): Sequential(
        (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (enc2relu1): ReLU(inplace=True)
        (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (enc2relu2): ReLU(inplace=True)
      )
      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (encoder3): Sequential(
        (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (enc3relu1): ReLU(inplace=True)
        (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (enc3relu2): ReLU(inplace=True)
      )
      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (encoder4): Sequential(
        (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (enc4relu1): ReLU(inplace=True)
        (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (enc4relu2): ReLU(inplace=True)
      )
      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (bottleneck): Sequential(
        (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (bottleneckrelu1): ReLU(inplace=True)
        (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (bottleneckrelu2): ReLU(inplace=True)
      )
      (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
      (decoder4): Sequential(
        (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dec4relu1): ReLU(inplace=True)
        (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dec4relu2): ReLU(inplace=True)
      )
      (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
      (decoder3): Sequential(
        (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dec3relu1): ReLU(inplace=True)
        (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dec3relu2): ReLU(inplace=True)
      )
      (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
      (decoder2): Sequential(
        (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dec2relu1): ReLU(inplace=True)
        (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dec2relu2): ReLU(inplace=True)
      )
      (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (decoder1): Sequential(
        (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dec1relu1): ReLU(inplace=True)
        (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dec1relu2): ReLU(inplace=True)
      )
      (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))
    )

The model takes an input image which has been normalized using min-max normalization. I want add a batch/layer norm layer before the first layer which does the same work for me (i.e. normalize the input image) so that I can feed the original image as it is.

Edit: Made changes clarifying the aim of this question.",t2_3964l43f,False,,0,False,Add normalization layer in the beginning of a pretrained model,[],r/pytorch,False,6,,0,,,False,t3_kmalfc,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1609233412.0,,[],{},,,True,,1609260053.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a pretrained UNet model with the following architecture&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;UNet(
  (encoder1): Sequential(
    (enc1conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc1relu1): ReLU(inplace=True)
    (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc1relu2): ReLU(inplace=True)
  )
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (encoder2): Sequential(
    (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc2relu1): ReLU(inplace=True)
    (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc2relu2): ReLU(inplace=True)
  )
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (encoder3): Sequential(
    (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc3relu1): ReLU(inplace=True)
    (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc3relu2): ReLU(inplace=True)
  )
  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (encoder4): Sequential(
    (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc4relu1): ReLU(inplace=True)
    (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc4relu2): ReLU(inplace=True)
  )
  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (bottleneck): Sequential(
    (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (bottleneckrelu1): ReLU(inplace=True)
    (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (bottleneckrelu2): ReLU(inplace=True)
  )
  (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
  (decoder4): Sequential(
    (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec4relu1): ReLU(inplace=True)
    (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec4relu2): ReLU(inplace=True)
  )
  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
  (decoder3): Sequential(
    (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec3relu1): ReLU(inplace=True)
    (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec3relu2): ReLU(inplace=True)
  )
  (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
  (decoder2): Sequential(
    (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec2relu1): ReLU(inplace=True)
    (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec2relu2): ReLU(inplace=True)
  )
  (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
  (decoder1): Sequential(
    (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec1relu1): ReLU(inplace=True)
    (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec1relu2): ReLU(inplace=True)
  )
  (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The model takes an input image which has been normalized using min-max normalization. I want add a batch/layer norm layer before the first layer which does the same work for me (i.e. normalize the input image) so that I can feed the original image as it is.&lt;/p&gt;

&lt;p&gt;Edit: Made changes clarifying the aim of this question.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kmalfc,True,,flabby_abs,,4,True,all_ads,False,[],False,,/r/pytorch/comments/kmalfc/add_normalization_layer_in_the_beginning_of_a/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kmalfc/add_normalization_layer_in_the_beginning_of_a/,7135,1609231253.0,0,,False,,,,,,,,
151,,pytorch,"My model normalizes my dataset of images from \[0,255 \] to \[0,1\]. The ToTensor function is not ideal for this as it converts it to 0,1 when I want the dataset to be \[0,255\] so I can train my model that way. Is there anything that I could do to this code to do this?

 

        def build_model(self):
            """""" DataLoader """"""
            train_transform = transforms.Compose([
                transforms.RandomHorizontalFlip(),
                transforms.Resize((self.img_size + 30, self.img_size+30)),
                transforms.RandomCrop(self.img_size),
                transforms.ToTensor(),
                transforms.Normalize(mean=(0,)*3, std=(255,)*3)
            ])
            test_transform = transforms.Compose([
                transforms.Resize((self.img_size, self.img_size)),
                transforms.ToTensor(),
                transforms.Normalize(mean=(0,)*3, std=(255,)*3)
            ])",t2_1319n8,False,,0,False,"Normalize images to train model from [0,255] instead of [0,1]",[],r/pytorch,False,6,,0,,,False,t3_kldyrt,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1609140134.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My model normalizes my dataset of images from [0,255 ] to [0,1]. The ToTensor function is not ideal for this as it converts it to 0,1 when I want the dataset to be [0,255] so I can train my model that way. Is there anything that I could do to this code to do this?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    def build_model(self):
        &amp;quot;&amp;quot;&amp;quot; DataLoader &amp;quot;&amp;quot;&amp;quot;
        train_transform = transforms.Compose([
            transforms.RandomHorizontalFlip(),
            transforms.Resize((self.img_size + 30, self.img_size+30)),
            transforms.RandomCrop(self.img_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=(0,)*3, std=(255,)*3)
        ])
        test_transform = transforms.Compose([
            transforms.Resize((self.img_size, self.img_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=(0,)*3, std=(255,)*3)
        ])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kldyrt,True,,rtolps,,11,True,all_ads,False,[],False,,/r/pytorch/comments/kldyrt/normalize_images_to_train_model_from_0255_instead/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kldyrt/normalize_images_to_train_model_from_0255_instead/,7135,1609111334.0,0,,False,,,,,,,,
152,,pytorch,"Say I am building a neural network using torch.nn. 

How do I know which parameters I need to specify in the argument call for each object layer?",t2_2hvu0062,False,,0,False,How to know what is the required parameters to specify?,[],r/pytorch,False,6,,0,,,False,t3_kld488,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1609137314.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Say I am building a neural network using torch.nn. &lt;/p&gt;

&lt;p&gt;How do I know which parameters I need to specify in the argument call for each object layer?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kld488,True,,golden543,,2,True,all_ads,False,[],False,,/r/pytorch/comments/kld488/how_to_know_what_is_the_required_parameters_to/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kld488/how_to_know_what_is_the_required_parameters_to/,7135,1609108514.0,0,,False,,,,,,,,
153,,pytorch,"I've created a network with a single Conv2d layer. Printing out the length of the parameters shows that it has 2 parameters. Printing out the parameter object shows that one of them is the matrix I would expect, a large tensor with size corresponding to the layer's number of input channels, output channels, and convolution kernel; then I also see an unexpected 2nd tensor of size 1x(num\_output\_channels). 

What is the purpose of this 2nd tensor? Isn't a convolution layer completely defined by the first tensor?",t2_4vrmp3a1,False,,0,False,Why do Conv2d layers have 2 parameters?,[],r/pytorch,False,6,,0,,,False,t3_kkuwyk,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1609063277.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve created a network with a single Conv2d layer. Printing out the length of the parameters shows that it has 2 parameters. Printing out the parameter object shows that one of them is the matrix I would expect, a large tensor with size corresponding to the layer&amp;#39;s number of input channels, output channels, and convolution kernel; then I also see an unexpected 2nd tensor of size 1x(num_output_channels). &lt;/p&gt;

&lt;p&gt;What is the purpose of this 2nd tensor? Isn&amp;#39;t a convolution layer completely defined by the first tensor?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kkuwyk,True,,asfarley--,,2,True,all_ads,False,[],False,,/r/pytorch/comments/kkuwyk/why_do_conv2d_layers_have_2_parameters/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kkuwyk/why_do_conv2d_layers_have_2_parameters/,7135,1609034477.0,0,,False,,,,,,,,
154,,pytorch,"Hi, I'm *very* new to PyTorch and neural networks as a whole so excuse this post.

My goal is to implement a machine learning algorithm that can predict a specific single numerical value. The inputs from which it learns from are NumPy arrays; each data entry is as follows:

array 1 | array 2 | numerical value

Can I easily make a neural network in PyTorch that will learn from arrays (filled with numerical values) and have it predict what the numerical value will be if I made a test set of other arrays?

Anything helps,

Thanks.",t2_23wdtz1v,False,,0,False,PyTorch NN for Numerical Inputs: Questions,[],r/pytorch,False,6,,0,,,False,t3_kkw674,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1609068192.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m &lt;em&gt;very&lt;/em&gt; new to PyTorch and neural networks as a whole so excuse this post.&lt;/p&gt;

&lt;p&gt;My goal is to implement a machine learning algorithm that can predict a specific single numerical value. The inputs from which it learns from are NumPy arrays; each data entry is as follows:&lt;/p&gt;

&lt;p&gt;array 1 | array 2 | numerical value&lt;/p&gt;

&lt;p&gt;Can I easily make a neural network in PyTorch that will learn from arrays (filled with numerical values) and have it predict what the numerical value will be if I made a test set of other arrays?&lt;/p&gt;

&lt;p&gt;Anything helps,&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kkw674,True,,smartpandaman,,3,True,all_ads,False,[],False,,/r/pytorch/comments/kkw674/pytorch_nn_for_numerical_inputs_questions/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kkw674/pytorch_nn_for_numerical_inputs_questions/,7135,1609039392.0,0,,False,,,,,,,,
155,,pytorch,"`keras4torch` is a high-level API like pytorch-lightning. It is designed for beginners who are new to pytorch but familar with Keras, then reduce the cost of migration.

[https://github.com/blueloveTH/keras4torch](https://github.com/blueloveTH/keras4torch)

Here is [an example of MNIST](https://nbviewer.jupyter.org/github/blueloveTH/keras4torch/blob/main/tutorials/MNIST_example.ipynb).

This project was developed when I'm migrating models from tf.keras to pytorch. `keras4torch` provides NumPy workflow conforming to Keras interfaces as much as possible. There is also a DataLoader workflow for more flexible usage.

The stable version of `keras4torch` can be installed via pip now. You can fork it for further development or modify it as a template with no limitations.

Welcome to take a look at our [repo page](https://github.com/blueloveTH/keras4torch) and documentations.

Thanks for reading!",t2_9i39qe4l,False,,0,False,Keras4Torch: A Ready-to-Use Wrapper for Training PyTorch Models✨,[],r/pytorch,False,6,,0,,,False,t3_kkh51u,False,dark,0.78,,public,10,0,{},,,False,[],,False,False,,{},,False,10,,False,self,1608982628.0,,[],{},self,,True,,1609010662.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;code&gt;keras4torch&lt;/code&gt; is a high-level API like pytorch-lightning. It is designed for beginners who are new to pytorch but familar with Keras, then reduce the cost of migration.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/blueloveTH/keras4torch""&gt;https://github.com/blueloveTH/keras4torch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here is &lt;a href=""https://nbviewer.jupyter.org/github/blueloveTH/keras4torch/blob/main/tutorials/MNIST_example.ipynb""&gt;an example of MNIST&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This project was developed when I&amp;#39;m migrating models from tf.keras to pytorch. &lt;code&gt;keras4torch&lt;/code&gt; provides NumPy workflow conforming to Keras interfaces as much as possible. There is also a DataLoader workflow for more flexible usage.&lt;/p&gt;

&lt;p&gt;The stable version of &lt;code&gt;keras4torch&lt;/code&gt; can be installed via pip now. You can fork it for further development or modify it as a template with no limitations.&lt;/p&gt;

&lt;p&gt;Welcome to take a look at our &lt;a href=""https://github.com/blueloveTH/keras4torch""&gt;repo page&lt;/a&gt; and documentations.&lt;/p&gt;

&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/auf1d4GbsURL0jE2s7yHlF91kvH6ENoH9bg2Ycf-Ydk.jpg?auto=webp&amp;s=f9a947b735c13d3a408e7a20d93dd627dfa062e9', 'width': 1280, 'height': 640}, 'resolutions': [{'url': 'https://external-preview.redd.it/auf1d4GbsURL0jE2s7yHlF91kvH6ENoH9bg2Ycf-Ydk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e5cf5237f636885bfc28fdc0805c62660cf6732b', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/auf1d4GbsURL0jE2s7yHlF91kvH6ENoH9bg2Ycf-Ydk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6bcec30ff98fada128efeb5c763fa21ae7e0785a', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/auf1d4GbsURL0jE2s7yHlF91kvH6ENoH9bg2Ycf-Ydk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=25ede71c5eba2f79bfdf9beec5d4ae0aaf533d90', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/auf1d4GbsURL0jE2s7yHlF91kvH6ENoH9bg2Ycf-Ydk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f730165deadc1c8d5da5ec880d28cf5887623cd', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/auf1d4GbsURL0jE2s7yHlF91kvH6ENoH9bg2Ycf-Ydk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2574832fe3b87737777d96bddf92e8e40af32512', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/auf1d4GbsURL0jE2s7yHlF91kvH6ENoH9bg2Ycf-Ydk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=88d7c63a1b21e8cef35cee89e258b9598bd0ee16', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'xoCV6zM1Q-srjG-RkoBjlKzTlcLec0vRtJCJ_kZdnEQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kkh51u,True,,blueloveTH,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kkh51u/keras4torch_a_readytouse_wrapper_for_training/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kkh51u/keras4torch_a_readytouse_wrapper_for_training/,7135,1608981862.0,0,,False,,,,,,,,
156,,pytorch,"For those who are not familiar, LabML ([https://github.com/lab-ml/labml](https://github.com/lab-ml/labml)) is a little library and an app that let you monitor model training on a mobile web app. We just implemented a callback for PyTorch lightening, and you can integrate it with a single line of code.

Here are lightening MNIST samples modified with labml callback: [https://github.com/lab-ml/samples/tree/master/labml\_samples/lightening](https://github.com/lab-ml/samples/tree/master/labml_samples/lightening)",t2_1jyhaoq,False,,0,False,LabML: Monitor PyTorch Lightening model training from a smartphone,[],r/pytorch,False,6,,0,,,False,t3_kit02y,False,dark,0.92,,public,17,1,{},,,False,[],,False,False,,{},,False,17,,False,self,False,,[],{},self,,True,,1608758592.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For those who are not familiar, LabML (&lt;a href=""https://github.com/lab-ml/labml""&gt;https://github.com/lab-ml/labml&lt;/a&gt;) is a little library and an app that let you monitor model training on a mobile web app. We just implemented a callback for PyTorch lightening, and you can integrate it with a single line of code.&lt;/p&gt;

&lt;p&gt;Here are lightening MNIST samples modified with labml callback: &lt;a href=""https://github.com/lab-ml/samples/tree/master/labml_samples/lightening""&gt;https://github.com/lab-ml/samples/tree/master/labml_samples/lightening&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Wyk2ZaFFadSlp1tc7k5arE_nmZKMnftTCT36vL5y218.jpg?auto=webp&amp;s=a636d4b7d6a8fb759cdd14a621f273122e4a0ec2', 'width': 4000, 'height': 2000}, 'resolutions': [{'url': 'https://external-preview.redd.it/Wyk2ZaFFadSlp1tc7k5arE_nmZKMnftTCT36vL5y218.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a8153802b90f936592b35cab133c1972e17d87c1', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/Wyk2ZaFFadSlp1tc7k5arE_nmZKMnftTCT36vL5y218.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e49a726bb52004c0074f8bdf0dac715731247356', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/Wyk2ZaFFadSlp1tc7k5arE_nmZKMnftTCT36vL5y218.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a0047664c67ee46169907530015f048b3bb6b45f', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/Wyk2ZaFFadSlp1tc7k5arE_nmZKMnftTCT36vL5y218.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3382ebb8680dd06eca9f9ec88bde355f7464e0e', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/Wyk2ZaFFadSlp1tc7k5arE_nmZKMnftTCT36vL5y218.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f918109570b8673ad41b5d53ac77a78e34acd68d', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/Wyk2ZaFFadSlp1tc7k5arE_nmZKMnftTCT36vL5y218.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28dae149d5cd0544c764c196db02f27df1656437', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'Yf_TQTLAaWX4-QGo4UivNwAS_DvsJz9xuNZ8OX3OfyM'}], 'enabled': False}","[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kit02y,True,,mlvpj,,1,True,all_ads,False,[],False,,/r/pytorch/comments/kit02y/labml_monitor_pytorch_lightening_model_training/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kit02y/labml_monitor_pytorch_lightening_model_training/,7135,1608729792.0,0,,False,,,,,,,,
157,,pytorch,"I defined an attention layer for ResNet, called ResNetAttn, and I added a new trainable variable for the attention computation called emb. In order to so this, I made the following changes,
 
Under ResNet __ init __, I added this
self.emb = nn.Parameter(torch.randn(10))

In the BasicBlock, I had the ResNetAttn layer in both __ init __ and forward, 
for init:
self.att = ResNetAttn()
for forward:
out = self.att(out)

Inside my ResNetAttn, under forward, I had
torch.cat((avg_pool,self.emb),1)

And it complains ResNetAttn object has not attribute 'emb'

I figured emb is not 'passed' to this layer properly, how should I fix this?

Thank you!",t2_1220en,False,,0,False,Trying to add a new trainable variable to a layer I defined for ResNet,[],r/pytorch,False,6,,0,,,False,t3_kj63v6,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1608802804.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I defined an attention layer for ResNet, called ResNetAttn, and I added a new trainable variable for the attention computation called emb. In order to so this, I made the following changes,&lt;/p&gt;

&lt;p&gt;Under ResNet __ init __, I added this
self.emb = nn.Parameter(torch.randn(10))&lt;/p&gt;

&lt;p&gt;In the BasicBlock, I had the ResNetAttn layer in both __ init __ and forward, 
for init:
self.att = ResNetAttn()
for forward:
out = self.att(out)&lt;/p&gt;

&lt;p&gt;Inside my ResNetAttn, under forward, I had
torch.cat((avg_pool,self.emb),1)&lt;/p&gt;

&lt;p&gt;And it complains ResNetAttn object has not attribute &amp;#39;emb&amp;#39;&lt;/p&gt;

&lt;p&gt;I figured emb is not &amp;#39;passed&amp;#39; to this layer properly, how should I fix this?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kj63v6,True,,yumworm,,2,True,all_ads,False,[],False,,/r/pytorch/comments/kj63v6/trying_to_add_a_new_trainable_variable_to_a_layer/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kj63v6/trying_to_add_a_new_trainable_variable_to_a_layer/,7135,1608774004.0,0,,False,,,,,,,,
158,,pytorch,"I'm working on a problem that requires me to have a neural network produce accurate results were the accuracy of the derivatives of network output with respect to the network input are also equally as important in terms of accuracy.

It there a particular network architecture, acitavation function, or normalization scheme that can aid in this problem?",t2_y5i84,False,,0,False,Are there ways to construct a network to improve accuracy of derivatives?,[],r/pytorch,False,6,,0,,,False,t3_kj380c,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1608792676.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a problem that requires me to have a neural network produce accurate results were the accuracy of the derivatives of network output with respect to the network input are also equally as important in terms of accuracy.&lt;/p&gt;

&lt;p&gt;It there a particular network architecture, acitavation function, or normalization scheme that can aid in this problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kj380c,True,,fractalmagic,,3,True,all_ads,False,[],False,,/r/pytorch/comments/kj380c/are_there_ways_to_construct_a_network_to_improve/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kj380c/are_there_ways_to_construct_a_network_to_improve/,7135,1608763876.0,0,,False,,,,,,,,
159,,pytorch,"I am running a pretty straightforward DRN training loop using openai gym, and my training loop is hanging and there really aren't any clues coming from PyTorch as to why it hangs. I am sure it is GPU related because when I set the device to cuda:0 it hangs and I have to kill the process. Does anyone have any hits for figuring out why this may be happening?  


    PyTorch version: 1.7.1
    Is debug build: False
    CUDA used to build PyTorch: 10.2
    ROCM used to build PyTorch: N/A
    
    OS: Ubuntu 20.04.1 LTS (x86_64)
    GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
    Clang version: Could not collect
    CMake version: version 3.16.3
    
    Python version: 3.7 (64-bit runtime)
    Is CUDA available: True
    CUDA runtime version: 10.1.243
    GPU models and configuration: GPU 0: GeForce RTX 2080 Ti
    Nvidia driver version: 460.27.04
    cuDNN version: Probably one of the following:
    /usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.5
    /usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.5
    /usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.5
    /usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.5
    /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.5
    /usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.5
    /usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.5",t2_75peu,False,,0,False,Breadcrumbs on PyTorch Hanging,[],r/pytorch,False,6,,0,,,False,t3_kj0aza,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1608783068.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am running a pretty straightforward DRN training loop using openai gym, and my training loop is hanging and there really aren&amp;#39;t any clues coming from PyTorch as to why it hangs. I am sure it is GPU related because when I set the device to cuda:0 it hangs and I have to kill the process. Does anyone have any hits for figuring out why this may be happening?  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PyTorch version: 1.7.1
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.16.3

Python version: 3.7 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.1.243
GPU models and configuration: GPU 0: GeForce RTX 2080 Ti
Nvidia driver version: 460.27.04
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.5
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kj0aza,True,,claytantor,,1,True,all_ads,False,[],False,,/r/pytorch/comments/kj0aza/breadcrumbs_on_pytorch_hanging/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kj0aza/breadcrumbs_on_pytorch_hanging/,7135,1608754268.0,0,,False,,,,,,,,
160,,pytorch,"Say, I have an input image `img` and I apply transformations to this image 

    transform = transforms.Compose([transforms.ToTensor(),
                                    tansforms.Normalize(mean, std)])

Is there a way I can backpropagate the gradients over these transformations?",t2_3964l43f,False,,0,False,Gradient backpropagation through transformation operations.,[],r/pytorch,False,6,,0,,,False,t3_kiux96,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1608765644.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Say, I have an input image &lt;code&gt;img&lt;/code&gt; and I apply transformations to this image &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;transform = transforms.Compose([transforms.ToTensor(),
                                tansforms.Normalize(mean, std)])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Is there a way I can backpropagate the gradients over these transformations?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kiux96,True,,flabby_abs,,1,True,all_ads,False,[],False,,/r/pytorch/comments/kiux96/gradient_backpropagation_through_transformation/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kiux96/gradient_backpropagation_through_transformation/,7135,1608736844.0,0,,False,,,,,,,,
161,,pytorch,"I want to train a character-level language model and use it to do a rescoring. For that purpose, I took a github code that contains all the necessary files, so that I can just train it on my data.

After I train the model and try to do text generation, the output seems alright, but if I load the trained model and do text generation, the output seems random.

Here is an example output: ""jgJty&amp;JWJ[C1HW1KJWJ&amp;?&amp;lKRx"".

I am saving the model the following way: `torch.save(model.state_dict(), 'lm.pt')`

and then loading it as:
`model = CharRNN(chars, n_hidden, n_layers).cuda()`

 `model.load_state_dict(torch.load('lm.pt'))`

 `model.eval()`

I have successfully saved and loaded models this way many times, but for some reason it doesn't work with the language model.

I have also tried printing the `model.state_dict()` after training and after loading the model and they look the same.

Saving the model with `torch.save(model, 'lm.pt')` and then doing:
`model = torch.load('lm.pt')` seems to work but I don't want to use that.

My suspicion is that the `state_dict` does not have all the weights, so when I load the model some of them are randomly initialized, but I am not sure.

Does anyone know what might be the issue and how to solve it?",t2_b7l557y,False,,0,False,Pytorch does not load the model properly,[],r/pytorch,False,6,,0,,,False,t3_kismui,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1608729244.0,,[],{},,,True,,1608757168.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to train a character-level language model and use it to do a rescoring. For that purpose, I took a github code that contains all the necessary files, so that I can just train it on my data.&lt;/p&gt;

&lt;p&gt;After I train the model and try to do text generation, the output seems alright, but if I load the trained model and do text generation, the output seems random.&lt;/p&gt;

&lt;p&gt;Here is an example output: &amp;quot;jgJty&amp;amp;JWJ[C1HW1KJWJ&amp;amp;?&amp;amp;lKRx&amp;quot;.&lt;/p&gt;

&lt;p&gt;I am saving the model the following way: &lt;code&gt;torch.save(model.state_dict(), &amp;#39;lm.pt&amp;#39;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and then loading it as:
&lt;code&gt;model = CharRNN(chars, n_hidden, n_layers).cuda()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model.load_state_dict(torch.load(&amp;#39;lm.pt&amp;#39;))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model.eval()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I have successfully saved and loaded models this way many times, but for some reason it doesn&amp;#39;t work with the language model.&lt;/p&gt;

&lt;p&gt;I have also tried printing the &lt;code&gt;model.state_dict()&lt;/code&gt; after training and after loading the model and they look the same.&lt;/p&gt;

&lt;p&gt;Saving the model with &lt;code&gt;torch.save(model, &amp;#39;lm.pt&amp;#39;)&lt;/code&gt; and then doing:
&lt;code&gt;model = torch.load(&amp;#39;lm.pt&amp;#39;)&lt;/code&gt; seems to work but I don&amp;#39;t want to use that.&lt;/p&gt;

&lt;p&gt;My suspicion is that the &lt;code&gt;state_dict&lt;/code&gt; does not have all the weights, so when I load the model some of them are randomly initialized, but I am not sure.&lt;/p&gt;

&lt;p&gt;Does anyone know what might be the issue and how to solve it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kismui,True,,tetrix994,,2,True,all_ads,False,[],False,,/r/pytorch/comments/kismui/pytorch_does_not_load_the_model_properly/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kismui/pytorch_does_not_load_the_model_properly/,7135,1608728368.0,0,,False,,,,,,,,
162,,pytorch,"Hiya

&amp;#x200B;

I'm trying to find a full lstm example where it demonstrates how to predict tomorrow's (or even a week's) future result  of whatever based on the past data used in training.

I seem to find many examples of people getting training data and splitting it, training and then using the last N% to ""predict"" - which seems incorrect as you already have the data that you normally wouldn't have. I can build  an lstm but just need that little bit to show me how to use it to forecast the future

&amp;#x200B;

any suggestions would be most welcome",t2_1n30fhdd,False,,0,False,lstm example?,[],r/pytorch,False,6,,0,,,False,t3_kinba8,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1608702849.0,,[],{},,,True,,1608731259.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hiya&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to find a full lstm example where it demonstrates how to predict tomorrow&amp;#39;s (or even a week&amp;#39;s) future result  of whatever based on the past data used in training.&lt;/p&gt;

&lt;p&gt;I seem to find many examples of people getting training data and splitting it, training and then using the last N% to &amp;quot;predict&amp;quot; - which seems incorrect as you already have the data that you normally wouldn&amp;#39;t have. I can build  an lstm but just need that little bit to show me how to use it to forecast the future&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;any suggestions would be most welcome&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kinba8,True,,skateparksaturday,,1,True,all_ads,False,[],False,,/r/pytorch/comments/kinba8/lstm_example/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kinba8/lstm_example/,7135,1608702459.0,0,,False,,,,,,,,
163,,pytorch,,t2_40d0zt4s,False,,0,False,Guide to Pytorch Time-Series Forecasting,[],r/pytorch,False,6,,0,73.0,,False,t3_ki3c74,False,dark,1.0,,public,23,0,{},140.0,,False,[],,False,False,,{},,False,23,,False,https://b.thumbs.redditmedia.com/tOG88L4Zkr6F6in29TOvDjx198SjYq2sdlF8wuPw2zo.jpg,False,,[],{},link,,False,,1608664213.0,text,6,,,text,analyticsindiamag.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/u42EKVmwWOo_WitwXBZIpOM-DaCW_Z6eEMzSPmtG6Ow.jpg?auto=webp&amp;s=6a7cc87b10aafd678562ccca3dbd82c855a425cd', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/u42EKVmwWOo_WitwXBZIpOM-DaCW_Z6eEMzSPmtG6Ow.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=23af23dad111dc7e50e02fe2f199a0b14de48c77', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/u42EKVmwWOo_WitwXBZIpOM-DaCW_Z6eEMzSPmtG6Ow.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a61f8924dabafce63e5a80071c6bdd6dda994b13', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/u42EKVmwWOo_WitwXBZIpOM-DaCW_Z6eEMzSPmtG6Ow.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=85e3b668129340540138d06587d69fd28f770468', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/u42EKVmwWOo_WitwXBZIpOM-DaCW_Z6eEMzSPmtG6Ow.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bcda8b543fffe9fd8bd99a656533ca1f1898cef', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/u42EKVmwWOo_WitwXBZIpOM-DaCW_Z6eEMzSPmtG6Ow.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=899793b8a934b7053f10d101d85e6804171c5570', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/u42EKVmwWOo_WitwXBZIpOM-DaCW_Z6eEMzSPmtG6Ow.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=de0a6f6d035290126601300d1168bc08b8635ef8', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'uHQRpzna5UAFWLq7d2K8BHKm9ksTmyrrbW67VMhvHZQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ki3c74,True,,analyticsindiam,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ki3c74/guide_to_pytorch_timeseries_forecasting/,all_ads,False,https://analyticsindiamag.com/guide-to-pytorch-time-series-forecasting/,7135,1608635413.0,0,,False,https://analyticsindiamag.com/guide-to-pytorch-time-series-forecasting/,,,,,,,
164,,pytorch,"I need to create a DataLoader where the collator function would require to have non trivial computation, actually a double layer loop which is significantly slowing down the training process. For example, consider this toy code where I try to use numba to JIT the collate function:

    import torch
    import torch.utils.data
    
    import numba as nb
    
    
    class Dataset(torch.utils.data.Dataset):
        def __init__(self):
            self.A = np.zeros((100000, 300))
            self.B = np.ones((100000, 300))
        
        def __getitem__(self, index):
            return self.A[index], self.B[index]
        
        def __len__(self):
            return self.A.shape[0]
    
    @nb.njit(cache=True)
    def _collate_fn(batch):
        batch_data = np.zeros((len(batch), 300))
        for i in range(len(batch)):
            batch_data[i] = batch[i][0] + batch[i][1]
    
        return batch_data

and then I create the DataLoader as follows:

    train_dataset = Dataset()
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=256,
        num_workers=6,
        collate_fn=_collate_fn,
        shuffle=True)

However, this just gets stuck but works fine if I remove the JITing of the \_collate\_fn. I am not able to understand what is happening here. I don't have to stick to numba and can use anything which will help me overcome the loop inefficiencies in Python. TIA and Happy 12,021",t2_2qknhgbu,False,,0,False,JIT the collate function in Pytorch,[],r/pytorch,False,6,,0,,,False,t3_ki8bpk,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1608682553.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need to create a DataLoader where the collator function would require to have non trivial computation, actually a double layer loop which is significantly slowing down the training process. For example, consider this toy code where I try to use numba to JIT the collate function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import torch
import torch.utils.data

import numba as nb


class Dataset(torch.utils.data.Dataset):
    def __init__(self):
        self.A = np.zeros((100000, 300))
        self.B = np.ones((100000, 300))

    def __getitem__(self, index):
        return self.A[index], self.B[index]

    def __len__(self):
        return self.A.shape[0]

@nb.njit(cache=True)
def _collate_fn(batch):
    batch_data = np.zeros((len(batch), 300))
    for i in range(len(batch)):
        batch_data[i] = batch[i][0] + batch[i][1]

    return batch_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then I create the DataLoader as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_dataset = Dataset()

train_loader = torch.utils.data.DataLoader(
    train_dataset,
    batch_size=256,
    num_workers=6,
    collate_fn=_collate_fn,
    shuffle=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, this just gets stuck but works fine if I remove the JITing of the _collate_fn. I am not able to understand what is happening here. I don&amp;#39;t have to stick to numba and can use anything which will help me overcome the loop inefficiencies in Python. TIA and Happy 12,021&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ki8bpk,True,,alchemist119,,5,True,all_ads,False,[],False,,/r/pytorch/comments/ki8bpk/jit_the_collate_function_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ki8bpk/jit_the_collate_function_in_pytorch/,7135,1608653753.0,0,,False,,,,,,,,
165,,pytorch,"Trying a many to many LSTM for learning purposes. Code snippet:

\`\`class model(nn.Module):  
def \_\_init\_\_(self, BATCH\_SIZE, SEQ\_LEN, vocab\_size):  
super(model, self).\_\_init\_\_()  
self.batch\_size = BATCH\_SIZE  
self.seq\_len = SEQ\_LEN  
self.vocab\_size = vocab\_size  
self.emb = nn.Embedding(vocab\_size, 512)  
self.lstm = nn.LSTM(512, 256, 3, dropout = 0.2)  
self.lin = nn.Linear(256, 87)  
self.criterion = nn.CrossEntropyLoss()

def forward(self, X, Y):  
out = self.emb(X)  
h, c = self.lstm(out)  
out = self.lin(h)  
loss = self.criterion(out, Y)

return loss\`\`

&amp;#x200B;

BATCH\_SIZE = 16  
SEQ\_LEN = 64  
vocab\_size = 87

Size of X: \[16, 87\]  
Size of Y = \[16, 64, 87\]  
Size of out = \[16, 64, 87\]

Still I get the above error in the \`\`loss = self.criterion(out, Y)\`\` line. I can't understand why. Please help.

&amp;#x200B;

Ref: [https://github.com/ranasingh-gkp/Music\_generation\_char-RNN](https://github.com/ranasingh-gkp/Music_generation_char-RNN)",t2_4xhrybaz,False,,0,False,"ValueError: Expected target size (16, 87), got torch.Size([16, 64, 87]) in CrossEntropyLoss",[],r/pytorch,False,6,,0,,,False,t3_khg3tj,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1608552423.0,,[],{},self,,True,,1608581008.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Trying a many to many LSTM for learning purposes. Code snippet:&lt;/p&gt;

&lt;p&gt;``class model(nn.Module):&lt;br/&gt;
def __init__(self, BATCH_SIZE, SEQ_LEN, vocab_size):&lt;br/&gt;
super(model, self).__init__()&lt;br/&gt;
self.batch_size = BATCH_SIZE&lt;br/&gt;
self.seq_len = SEQ_LEN&lt;br/&gt;
self.vocab_size = vocab_size&lt;br/&gt;
self.emb = nn.Embedding(vocab_size, 512)&lt;br/&gt;
self.lstm = nn.LSTM(512, 256, 3, dropout = 0.2)&lt;br/&gt;
self.lin = nn.Linear(256, 87)&lt;br/&gt;
self.criterion = nn.CrossEntropyLoss()&lt;/p&gt;

&lt;p&gt;def forward(self, X, Y):&lt;br/&gt;
out = self.emb(X)&lt;br/&gt;
h, c = self.lstm(out)&lt;br/&gt;
out = self.lin(h)&lt;br/&gt;
loss = self.criterion(out, Y)&lt;/p&gt;

&lt;p&gt;return loss``&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;BATCH_SIZE = 16&lt;br/&gt;
SEQ_LEN = 64&lt;br/&gt;
vocab_size = 87&lt;/p&gt;

&lt;p&gt;Size of X: [16, 87]&lt;br/&gt;
Size of Y = [16, 64, 87]&lt;br/&gt;
Size of out = [16, 64, 87]&lt;/p&gt;

&lt;p&gt;Still I get the above error in the ``loss = self.criterion(out, Y)`` line. I can&amp;#39;t understand why. Please help.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Ref: &lt;a href=""https://github.com/ranasingh-gkp/Music_generation_char-RNN""&gt;https://github.com/ranasingh-gkp/Music_generation_char-RNN&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/YKJ2hzomXlEBUIZ-ZFQ6MY8ntIeXgeMfNtYlOrRDjsk.jpg?auto=webp&amp;s=b1bc0de933c4b9275db3b43c2097116da759444f', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/YKJ2hzomXlEBUIZ-ZFQ6MY8ntIeXgeMfNtYlOrRDjsk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=88db78cb4629ee269aae316c90c86623c61804cb', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/YKJ2hzomXlEBUIZ-ZFQ6MY8ntIeXgeMfNtYlOrRDjsk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=830f92ccfeb4d6b6e43098137a76c01b62dab529', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/YKJ2hzomXlEBUIZ-ZFQ6MY8ntIeXgeMfNtYlOrRDjsk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=758f222c22e4671a8e950466650d60dd9ef4a890', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'vl2lhOIlSJ-vu68uE_4L6wT6OWcNp_iQfL0qvtPNklA'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,khg3tj,True,,banenvy,,5,True,all_ads,False,[],False,,/r/pytorch/comments/khg3tj/valueerror_expected_target_size_16_87_got/,all_ads,False,https://www.reddit.com/r/pytorch/comments/khg3tj/valueerror_expected_target_size_16_87_got/,7135,1608552208.0,0,,False,,,,,,,,
166,,pytorch,"Hey everyone!

I wrote a small helper library to make multi-task learning with PyTorch easier: [torchMTL](https://github.com/chrisby/torchMTL). You just need to define a dictionary of layers and torchMTL builds a model that returns the losses of the different tasks that you can then combine in the standard training loop. 

I'd be happy to get some feedback on it!",t2_1s3ah3gc,False,,0,False,torchMTL: A simple multi-task learning module for PyTorch,[],r/pytorch,False,6,,0,,,False,t3_kgeb73,False,dark,1.0,,public,17,0,{},,,False,[],,False,False,,{},,False,17,,False,self,False,,[],{},self,,True,,1608434513.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;

&lt;p&gt;I wrote a small helper library to make multi-task learning with PyTorch easier: &lt;a href=""https://github.com/chrisby/torchMTL""&gt;torchMTL&lt;/a&gt;. You just need to define a dictionary of layers and torchMTL builds a model that returns the losses of the different tasks that you can then combine in the standard training loop. &lt;/p&gt;

&lt;p&gt;I&amp;#39;d be happy to get some feedback on it!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/EAe7YowAsBl1Vt1oHfpoEPTQiFWG7Ye2dLd9Cq3tibc.jpg?auto=webp&amp;s=283b81785dacd2dc80a0a3eabcfd73a043cb8c44', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/EAe7YowAsBl1Vt1oHfpoEPTQiFWG7Ye2dLd9Cq3tibc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=26bff3519ed863841aeac22644333aa4cd425d7e', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/EAe7YowAsBl1Vt1oHfpoEPTQiFWG7Ye2dLd9Cq3tibc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b4159ce2a4a40c1c1e5e65d2612e6662833cb6c', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/EAe7YowAsBl1Vt1oHfpoEPTQiFWG7Ye2dLd9Cq3tibc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c85ecff54b0e75ff1db3051b0638fae2b611245f', 'width': 320, 'height': 320}], 'variants': {}, 'id': '55WJLI_srVcGiQLsitr8awZwtO3tZuYuKKRkJmQGw-w'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kgeb73,True,,cbock90,,4,True,all_ads,False,[],False,,/r/pytorch/comments/kgeb73/torchmtl_a_simple_multitask_learning_module_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kgeb73/torchmtl_a_simple_multitask_learning_module_for/,7135,1608405713.0,0,,False,,,,,,,,
167,,pytorch,"Hey, again, 

I'm trying to connect to the TPU's accessible on google colab, and having some trouble.  My end goal is to install the TPU's so I can use Pytorch Lightning to run the training on the TPU. 

Currently, my code to import the XLA library (needed to connect to TPU from Pytorch)  is returning a really cryptic error. 

&amp;#x200B;

Code to install XLA: 

`# install XLA to allow connection between Pytorch and TPU`  
`VERSION = ""20200325"" #@param [""1.5"" , ""20200325"", ""nightly""]`  
`!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py`  
`!python` [`pytorch-xla-env-setup.py`](https://pytorch-xla-env-setup.py) `--version $VERSION`

&amp;#x200B;

Code to import XLA: 

`# imports pytorch`  
`import torch`  
`# imports the torch_xla package`  
`import torch_xla`  
`import torch_xla.core.xla_model as xm`

&amp;#x200B;

Error being returned: 

 `--------------------------------------------------------------------------- ImportError                               Traceback (most recent call last)` [`&lt;ipython-input-66-ebe519c076f6&gt;`](https://localhost:8080/#) `in &lt;module&gt;()3        4 # imports the torch_xla package ----&gt; 5 import torch_xla       6 import torch_xla.core.xla_model as xm`  [`/usr/local/lib/python3.6/dist-packages/torch_xla/__init__.py`](https://localhost:8080/#) `in &lt;module&gt;()      39 import torch      40 from .version import __version__ ---&gt; 41 import _XLAC      42       43 _XLAC._initialize_aten_bindings()  ImportError: /usr/local/lib/python3.6/dist-packages/_XLAC.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN6caffe28TypeMeta21_typeMetaDataInstanceISt7complexIfEEEPKNS_6detail12TypeMetaDataEv` 

&amp;#x200B;

I copied this import code straight from the [official guide](https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb#scrollTo=42avAvSg17by) which works for me, but just doesn't work on my own document. 

Here's my [Colab document](https://colab.research.google.com/drive/1EaCJpyYK8xLuzO79ESAsIlGZAcWjOh8u?usp=sharing), if anyone is wondering. The XLA stuff is at the very top.

Thank you! 

A",t2_3ke4xozp,False,,0,False,Trouble connecting to TPUs using XLA library,[],r/pytorch,False,6,,0,,,False,t3_kgkg08,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1608455225.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, again, &lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to connect to the TPU&amp;#39;s accessible on google colab, and having some trouble.  My end goal is to install the TPU&amp;#39;s so I can use Pytorch Lightning to run the training on the TPU. &lt;/p&gt;

&lt;p&gt;Currently, my code to import the XLA library (needed to connect to TPU from Pytorch)  is returning a really cryptic error. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Code to install XLA: &lt;/p&gt;

&lt;p&gt;&lt;code&gt;# install XLA to allow connection between Pytorch and TPU&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;VERSION = &amp;quot;20200325&amp;quot; #@param [&amp;quot;1.5&amp;quot; , &amp;quot;20200325&amp;quot;, &amp;quot;nightly&amp;quot;]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;!python&lt;/code&gt; &lt;a href=""https://pytorch-xla-env-setup.py""&gt;&lt;code&gt;pytorch-xla-env-setup.py&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--version $VERSION&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Code to import XLA: &lt;/p&gt;

&lt;p&gt;&lt;code&gt;# imports pytorch&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;import torch&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;# imports the torch_xla package&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;import torch_xla&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;import torch_xla.core.xla_model as xm&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Error being returned: &lt;/p&gt;

&lt;p&gt;&lt;code&gt;--------------------------------------------------------------------------- ImportError                               Traceback (most recent call last)&lt;/code&gt; &lt;a href=""https://localhost:8080/#""&gt;&lt;code&gt;&amp;lt;ipython-input-66-ebe519c076f6&amp;gt;&lt;/code&gt;&lt;/a&gt; &lt;code&gt;in &amp;lt;module&amp;gt;()3        4 # imports the torch_xla package ----&amp;gt; 5 import torch_xla       6 import torch_xla.core.xla_model as xm&lt;/code&gt;  &lt;a href=""https://localhost:8080/#""&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/torch_xla/__init__.py&lt;/code&gt;&lt;/a&gt; &lt;code&gt;in &amp;lt;module&amp;gt;()      39 import torch      40 from .version import __version__ ---&amp;gt; 41 import _XLAC      42       43 _XLAC._initialize_aten_bindings()  ImportError: /usr/local/lib/python3.6/dist-packages/_XLAC.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN6caffe28TypeMeta21_typeMetaDataInstanceISt7complexIfEEEPKNS_6detail12TypeMetaDataEv&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I copied this import code straight from the &lt;a href=""https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb#scrollTo=42avAvSg17by""&gt;official guide&lt;/a&gt; which works for me, but just doesn&amp;#39;t work on my own document. &lt;/p&gt;

&lt;p&gt;Here&amp;#39;s my &lt;a href=""https://colab.research.google.com/drive/1EaCJpyYK8xLuzO79ESAsIlGZAcWjOh8u?usp=sharing""&gt;Colab document&lt;/a&gt;, if anyone is wondering. The XLA stuff is at the very top.&lt;/p&gt;

&lt;p&gt;Thank you! &lt;/p&gt;

&lt;p&gt;A&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kgkg08,True,,kirbyburgers,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kgkg08/trouble_connecting_to_tpus_using_xla_library/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kgkg08/trouble_connecting_to_tpus_using_xla_library/,7135,1608426425.0,0,,False,,,,,,,,
168,,pytorch,"I've seen that in pytorch you can do the short time fourier transform through the torch module. Can you/are there plans for more direct transformations like audio to mel frequency cepstrum, chroma, etc.?",t2_8qews7z,False,,0,False,More audio feature transformations in pytorch?,[],r/pytorch,False,6,,0,,,False,t3_kfldc1,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1608327921.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve seen that in pytorch you can do the short time fourier transform through the torch module. Can you/are there plans for more direct transformations like audio to mel frequency cepstrum, chroma, etc.?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kfldc1,True,,chrisuz,,1,True,all_ads,False,[],False,,/r/pytorch/comments/kfldc1/more_audio_feature_transformations_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kfldc1/more_audio_feature_transformations_in_pytorch/,7135,1608299121.0,0,,False,,,,,,,,
169,,pytorch,"I have an image segmentation task but a very small dataset. I want to use data augmentation, but I can’t seem to find a way to apply the same transformations to images and masks. 

Any help would be appreciated!",t2_zzfsj,False,,0,False,Applying transforms to both image and mask,[],r/pytorch,False,6,,0,,,False,t3_keuyhs,False,dark,0.88,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1608229318.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have an image segmentation task but a very small dataset. I want to use data augmentation, but I can’t seem to find a way to apply the same transformations to images and masks. &lt;/p&gt;

&lt;p&gt;Any help would be appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,keuyhs,True,,ezgiofrivia,,8,True,all_ads,False,[],False,,/r/pytorch/comments/keuyhs/applying_transforms_to_both_image_and_mask/,all_ads,False,https://www.reddit.com/r/pytorch/comments/keuyhs/applying_transforms_to_both_image_and_mask/,7135,1608200518.0,0,,False,,,,,,,,
170,,pytorch,"Hi All, 

For one my first solo machine learning project I'm trying to create an algorithm that can distinguish between six remarkably similar species of bird (for those wanting to search it up, the Willow Flycatcher, Pacific-Slope Flycatcher, Least Flycatcher, Hammond's Flycatcher, Western-Wood Pewee and Olive-sided Flycatcher).  I'm using data from Flickr and making a CNN from ""scratch"" (in scratch I mean using pytorch tools but not transferring from a premade model)  

I have exactly 2000 images per my six classes. Since I did not have the ability to access a larger database (at least, yet), I was only able to get about 600-1000 unique images per class. I created a function that would automatically fill the gaps in to 2000 with augmented data. 

Alone, it takes about 55 minutes of runtime for all the data to be loaded AND for the augmented data to be added. 

Once that is done, and all the other stuff is done, training can begin. My images are quite large (256 by 256), which might slow me down.  I'm going to add dropout in a later phase of the experiment (looking over the training and validation losses shows that I'm not overfitting anyways). 

My convolutional model is as follows.

`model = nn.Sequential(`  
   
 `# before, (bs, 3, 256, 256)`  
    `nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),`  
    `nn.ReLU(),`   
    `nn.MaxPool2d(2, 2), # output will be (bs, 16, 128, 128)`  
    `nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),`  
    `nn.ReLU(),`  
    `nn.MaxPool2d(2, 2), #ouput will be (bs, 16, 64, 64)`  
    `nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),`  
    `nn.ReLU(),`   
    `nn.MaxPool2d(2, 2), # output is (bs, 16, 32, 32)`  
    `nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),`  
    `nn.ReLU(),`   
    `nn.MaxPool2d(2, 2), # output is (bs, 16, 16, 16)`  
    `nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),`  
    `nn.ReLU(),`   
    `nn.MaxPool2d(2, 2), # output is (bs, 16, 8, 8)`   
    `nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),`  
    `nn.ReLU(),`   
    `nn.MaxPool2d(2, 2), # output is (bs, 16, 4, 4)`   
    `nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),`  
    `nn.ReLU(),`   
    `nn.MaxPool2d(2, 2), # output is (bs, 16, 2, 2)`   
    `nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),`  
    `nn.ReLU(),`   
    `nn.MaxPool2d(2, 2), # output is (bs, 16, 1, 1)`   
 `# connected layer`  
    `nn.Flatten(), # output a bs, 16 size vector`  
    `nn.Linear(16, 6) # output a bs x 6`  
`)`

At  batch size = 81, I've been taking exactly 15 minutes for each epoch to train.  Since I'm on colab, which has usage limits for GPUs, this means getting about 13 or 14 epochs max before I reach my daily limit of GPU usage. 

Looking at the accuracy, it seems as though if I am able to run a few more epochs, I will be able to get a better score accuracy on the validation set. 

&amp;#x200B;

[Seems to be a steady increase in accuracy.](https://preview.redd.it/2u1o8bmnlp561.png?width=517&amp;format=png&amp;auto=webp&amp;s=51e5f368ab2fe522e9a6b5b0d276d0c935b41cf6)

If randomly picked, the accuracy of identifying the right bird would be around 16%. The highest accuracy I've gotten so far is 41%, on the 13th epoch. Colab kicked me after that one, so I couldn't find out if I could have gone higher.

Also, my losses if you'd like:

&amp;#x200B;

https://preview.redd.it/1cq3z890mp561.png?width=527&amp;format=png&amp;auto=webp&amp;s=4825f567cc03c8c558228cfd60b292aba5f05ab3

As well as that, the long training time makes it hard to make changes to the algorithm and immediately see their effect - I'm worried that this'll make improving upon the 41% impossible! 

I have a few ideas (mainly, reducing image size) but I'm worried that will reduce the accuracy, since the differentiating features between the birds are so minute.

Thanks for reading this the whole way through, and if you'd have any suggestions on cutting the time, that'd be much appreciated! 

**edit:** just found out I was adding augmented data before splitting which most likely raised the accuracies on the val set. The question still stands, though. 

Thanks, 

A",t2_3ke4xozp,False,,0,False,15 minute training time per epoch limiting ability to improve algorithm... any way to speed it up?,[],r/pytorch,False,6,,0,103.0,,False,t3_kett98,False,dark,0.84,,public,4,0,{},140.0,,False,[],,False,False,,{},,False,4,,False,https://b.thumbs.redditmedia.com/F107ngCcsxcwW4FVPonabbERXGQA2gRpDkEHwH2LZxA.jpg,False,,[],{},,,True,,1608223527.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All, &lt;/p&gt;

&lt;p&gt;For one my first solo machine learning project I&amp;#39;m trying to create an algorithm that can distinguish between six remarkably similar species of bird (for those wanting to search it up, the Willow Flycatcher, Pacific-Slope Flycatcher, Least Flycatcher, Hammond&amp;#39;s Flycatcher, Western-Wood Pewee and Olive-sided Flycatcher).  I&amp;#39;m using data from Flickr and making a CNN from &amp;quot;scratch&amp;quot; (in scratch I mean using pytorch tools but not transferring from a premade model)  &lt;/p&gt;

&lt;p&gt;I have exactly 2000 images per my six classes. Since I did not have the ability to access a larger database (at least, yet), I was only able to get about 600-1000 unique images per class. I created a function that would automatically fill the gaps in to 2000 with augmented data. &lt;/p&gt;

&lt;p&gt;Alone, it takes about 55 minutes of runtime for all the data to be loaded AND for the augmented data to be added. &lt;/p&gt;

&lt;p&gt;Once that is done, and all the other stuff is done, training can begin. My images are quite large (256 by 256), which might slow me down.  I&amp;#39;m going to add dropout in a later phase of the experiment (looking over the training and validation losses shows that I&amp;#39;m not overfitting anyways). &lt;/p&gt;

&lt;p&gt;My convolutional model is as follows.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model = nn.Sequential(&lt;/code&gt;  &lt;/p&gt;

&lt;p&gt;&lt;code&gt;# before, (bs, 3, 256, 256)&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.ReLU(),&lt;/code&gt; &lt;br/&gt;
    &lt;code&gt;nn.MaxPool2d(2, 2), # output will be (bs, 16, 128, 128)&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.ReLU(),&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.MaxPool2d(2, 2), #ouput will be (bs, 16, 64, 64)&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.ReLU(),&lt;/code&gt; &lt;br/&gt;
    &lt;code&gt;nn.MaxPool2d(2, 2), # output is (bs, 16, 32, 32)&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.ReLU(),&lt;/code&gt; &lt;br/&gt;
    &lt;code&gt;nn.MaxPool2d(2, 2), # output is (bs, 16, 16, 16)&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.ReLU(),&lt;/code&gt; &lt;br/&gt;
    &lt;code&gt;nn.MaxPool2d(2, 2), # output is (bs, 16, 8, 8)&lt;/code&gt; &lt;br/&gt;
    &lt;code&gt;nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.ReLU(),&lt;/code&gt; &lt;br/&gt;
    &lt;code&gt;nn.MaxPool2d(2, 2), # output is (bs, 16, 4, 4)&lt;/code&gt; &lt;br/&gt;
    &lt;code&gt;nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.ReLU(),&lt;/code&gt; &lt;br/&gt;
    &lt;code&gt;nn.MaxPool2d(2, 2), # output is (bs, 16, 2, 2)&lt;/code&gt; &lt;br/&gt;
    &lt;code&gt;nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.ReLU(),&lt;/code&gt; &lt;br/&gt;
    &lt;code&gt;nn.MaxPool2d(2, 2), # output is (bs, 16, 1, 1)&lt;/code&gt; &lt;br/&gt;
 &lt;code&gt;# connected layer&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.Flatten(), # output a bs, 16 size vector&lt;/code&gt;&lt;br/&gt;
    &lt;code&gt;nn.Linear(16, 6) # output a bs x 6&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;At  batch size = 81, I&amp;#39;ve been taking exactly 15 minutes for each epoch to train.  Since I&amp;#39;m on colab, which has usage limits for GPUs, this means getting about 13 or 14 epochs max before I reach my daily limit of GPU usage. &lt;/p&gt;

&lt;p&gt;Looking at the accuracy, it seems as though if I am able to run a few more epochs, I will be able to get a better score accuracy on the validation set. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/2u1o8bmnlp561.png?width=517&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51e5f368ab2fe522e9a6b5b0d276d0c935b41cf6""&gt;Seems to be a steady increase in accuracy.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If randomly picked, the accuracy of identifying the right bird would be around 16%. The highest accuracy I&amp;#39;ve gotten so far is 41%, on the 13th epoch. Colab kicked me after that one, so I couldn&amp;#39;t find out if I could have gone higher.&lt;/p&gt;

&lt;p&gt;Also, my losses if you&amp;#39;d like:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/1cq3z890mp561.png?width=527&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4825f567cc03c8c558228cfd60b292aba5f05ab3""&gt;https://preview.redd.it/1cq3z890mp561.png?width=527&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4825f567cc03c8c558228cfd60b292aba5f05ab3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As well as that, the long training time makes it hard to make changes to the algorithm and immediately see their effect - I&amp;#39;m worried that this&amp;#39;ll make improving upon the 41% impossible! &lt;/p&gt;

&lt;p&gt;I have a few ideas (mainly, reducing image size) but I&amp;#39;m worried that will reduce the accuracy, since the differentiating features between the birds are so minute.&lt;/p&gt;

&lt;p&gt;Thanks for reading this the whole way through, and if you&amp;#39;d have any suggestions on cutting the time, that&amp;#39;d be much appreciated! &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;edit:&lt;/strong&gt; just found out I was adding augmented data before splitting which most likely raised the accuracies on the val set. The question still stands, though. &lt;/p&gt;

&lt;p&gt;Thanks, &lt;/p&gt;

&lt;p&gt;A&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kett98,True,,kirbyburgers,,9,True,all_ads,False,[],False,,/r/pytorch/comments/kett98/15_minute_training_time_per_epoch_limiting/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kett98/15_minute_training_time_per_epoch_limiting/,7135,1608194727.0,0,,False,,,,"{'1cq3z890mp561': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 75, 'x': 108, 'u': 'https://preview.redd.it/1cq3z890mp561.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0dcaab093ca2a5bd2806d42d0099ee86e5853e8d'}, {'y': 151, 'x': 216, 'u': 'https://preview.redd.it/1cq3z890mp561.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e6f81d1fcb89b5c3cb000693afffc88290a5c22'}, {'y': 224, 'x': 320, 'u': 'https://preview.redd.it/1cq3z890mp561.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=82b16b80bec7f8240727a4dd15499a1897e94233'}], 's': {'y': 370, 'x': 527, 'u': 'https://preview.redd.it/1cq3z890mp561.png?width=527&amp;format=png&amp;auto=webp&amp;s=4825f567cc03c8c558228cfd60b292aba5f05ab3'}, 'id': '1cq3z890mp561'}, '2u1o8bmnlp561': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 80, 'x': 108, 'u': 'https://preview.redd.it/2u1o8bmnlp561.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b2cbf010ccfc117284c1417a74008d9e67fe4d0'}, {'y': 160, 'x': 216, 'u': 'https://preview.redd.it/2u1o8bmnlp561.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e9fe459920e4fc316602865cd08db5ad5f94a5c5'}, {'y': 237, 'x': 320, 'u': 'https://preview.redd.it/2u1o8bmnlp561.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d6459dad73f1028a0ba633ba033ace22ac31dc5'}], 's': {'y': 384, 'x': 517, 'u': 'https://preview.redd.it/2u1o8bmnlp561.png?width=517&amp;format=png&amp;auto=webp&amp;s=51e5f368ab2fe522e9a6b5b0d276d0c935b41cf6'}, 'id': '2u1o8bmnlp561'}}",,,,
171,,pytorch,"I have about 400 samples. I need to do a binary classification task. Each sample comprises 4 images (all images have a single channel). 

I am planning to use transfer learning with ResNet18 and just retrain the last layer. 

I want to concatenate these 4 images as my last layer. Can someone tell me how to do it?

Say, each of my images is - (1, 120, 90). So how do I concatenate 4 such images so that they can be used as the last layer of a Resnet?

Pardon me but I am not well versed with computer vision.",t2_nis5p,False,,0,False,How do I concat 4 images to be the last layer for a ResNet for transfer learning?,[],r/pytorch,False,6,,0,,,False,t3_ketdgw,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1608221292.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have about 400 samples. I need to do a binary classification task. Each sample comprises 4 images (all images have a single channel). &lt;/p&gt;

&lt;p&gt;I am planning to use transfer learning with ResNet18 and just retrain the last layer. &lt;/p&gt;

&lt;p&gt;I want to concatenate these 4 images as my last layer. Can someone tell me how to do it?&lt;/p&gt;

&lt;p&gt;Say, each of my images is - (1, 120, 90). So how do I concatenate 4 such images so that they can be used as the last layer of a Resnet?&lt;/p&gt;

&lt;p&gt;Pardon me but I am not well versed with computer vision.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ketdgw,True,,iCHAIT,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ketdgw/how_do_i_concat_4_images_to_be_the_last_layer_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ketdgw/how_do_i_concat_4_images_to_be_the_last_layer_for/,7135,1608192492.0,0,,False,,,,,,,,
172,,pytorch,"I have put together an nlp tutorial in pytorch, check it out: [https://github.com/will-thompson-k/deeplearning-nlp-models](https://github.com/will-thompson-k/deeplearning-nlp-models) . Would love some feedback!",t2_8o5bj9n7,False,,0,False,[P] NLP Tutorial PyTorch,[],r/pytorch,False,6,,0,,,False,t3_ke8n72,False,dark,1.0,,public,14,0,{},,,False,[],,False,False,,{},,False,14,,False,self,False,,[],{},self,,True,,1608151963.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have put together an nlp tutorial in pytorch, check it out: &lt;a href=""https://github.com/will-thompson-k/deeplearning-nlp-models""&gt;https://github.com/will-thompson-k/deeplearning-nlp-models&lt;/a&gt; . Would love some feedback!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/eFgp-2KTdiTmF8MVusHOdLl8O1Sh1MPSCCIH46xyVl4.jpg?auto=webp&amp;s=b3778a319fb0124adc8663236e12ed87318b03b3', 'width': 1280, 'height': 845}, 'resolutions': [{'url': 'https://external-preview.redd.it/eFgp-2KTdiTmF8MVusHOdLl8O1Sh1MPSCCIH46xyVl4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d407fa81fc6383a300ac9bf43b028e46ae6a4565', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/eFgp-2KTdiTmF8MVusHOdLl8O1Sh1MPSCCIH46xyVl4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f98b9165b0926b5a25b3fc7efc20b1a60d38ece3', 'width': 216, 'height': 142}, {'url': 'https://external-preview.redd.it/eFgp-2KTdiTmF8MVusHOdLl8O1Sh1MPSCCIH46xyVl4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=398a1a9c69cdfbfd1905b0ff02698c37e73c23d4', 'width': 320, 'height': 211}, {'url': 'https://external-preview.redd.it/eFgp-2KTdiTmF8MVusHOdLl8O1Sh1MPSCCIH46xyVl4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=98866afc428f0d7a2f047253657edc42b9e6ad1d', 'width': 640, 'height': 422}, {'url': 'https://external-preview.redd.it/eFgp-2KTdiTmF8MVusHOdLl8O1Sh1MPSCCIH46xyVl4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=94b848989f55f5dea49f62d4866a07bff016d9f1', 'width': 960, 'height': 633}, {'url': 'https://external-preview.redd.it/eFgp-2KTdiTmF8MVusHOdLl8O1Sh1MPSCCIH46xyVl4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4bdbe2138015186c53a0ea5d12ecfbe69d8326af', 'width': 1080, 'height': 712}], 'variants': {}, 'id': 'MSc-qlH_eObzrIY1-j9kaCYaUe15u_Yw4DSG_UdX8Q8'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ke8n72,True,,wilhelm____,,1,True,all_ads,False,[],False,,/r/pytorch/comments/ke8n72/p_nlp_tutorial_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ke8n72/p_nlp_tutorial_pytorch/,7135,1608123163.0,0,,False,,,,,,,,
173,,pytorch,,t2_44mbtmjy,False,,0,False,Paid ML gigs: Get compensated while further sharpening your skills on your own schedule.,[],r/pytorch,False,6,,0,,,False,t3_kenk99,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,default,False,,[],{},link,,False,,1608198855.0,text,6,,,text,self.LatestInML,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/S9crEQYpwA6ekxqYobdJTHS6jzTRVu1a7_Ndizlxh3k.jpg?auto=webp&amp;s=32672f4715d15292ce08ccff17f800fb7d1b3d79', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/S9crEQYpwA6ekxqYobdJTHS6jzTRVu1a7_Ndizlxh3k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8796a67b9cfabe02b628971a0adf9da5e36d0005', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/S9crEQYpwA6ekxqYobdJTHS6jzTRVu1a7_Ndizlxh3k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bce8aa4865efc4f013dc4dd410bc278cd406f330', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/S9crEQYpwA6ekxqYobdJTHS6jzTRVu1a7_Ndizlxh3k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1167efd7e761c44fcb6c62915c047d3b4e09281', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/S9crEQYpwA6ekxqYobdJTHS6jzTRVu1a7_Ndizlxh3k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f9d344ffeb2a258b78f23d5fd767e3fb6ed7a8b0', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/S9crEQYpwA6ekxqYobdJTHS6jzTRVu1a7_Ndizlxh3k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b095516fad657b7cd2d5fc08b5b5d83b934ca8b7', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/S9crEQYpwA6ekxqYobdJTHS6jzTRVu1a7_Ndizlxh3k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=043b04800775539618a7accc5373bdd9b38a2559', 'width': 1080, 'height': 567}], 'variants': {}, 'id': '-p_-e3JNb56wltrg0ZmyHZgAxcR8TN6LJ9nas3Ulox0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kenk99,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kenk99/paid_ml_gigs_get_compensated_while_further/,all_ads,False,/r/LatestInML/comments/kelv2u/paid_ml_gigs_get_compensated_while_further/,7135,1608170055.0,0,,False,/r/LatestInML/comments/kelv2u/paid_ml_gigs_get_compensated_while_further/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""Sharing this here as it may be of interest to many of us:\n\n\\&gt; If you have any technical skills in machine learning, computer vision, data science, natural language processing, deep learning, etc. and are interested in paid (remote) mini-projects and gigs on the side,\n\nthen this is a good opportunity to get compensated while further sharpening your skills on your own schedule.\n\nIMHO also useful if you're a grad student, have student loans, or just want to build up your portfolio.\n\nIf you're interested, please opt in here: [https://docs.google.com/forms/d/e/1FAIpQLScK-yztp2B70GkmvUmRDIeOkUxkutRlhzsGCDRhJksgWky4mg/viewform](https://docs.google.com/forms/d/e/1FAIpQLScK-yztp2B70GkmvUmRDIeOkUxkutRlhzsGCDRhJksgWky4mg/viewform)\n\nFeel free to email [gr2511@columbia.edu](mailto:gr2511@columbia.edu) for any questions."", 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Paid ML gigs: Get compensated while further sharpening your skills on your own schedule.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_kelv2u', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.59, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 6, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1608193325.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sharing this here as it may be of interest to many of us:&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; If you have any technical skills in machine learning, computer vision, data science, natural language processing, deep learning, etc. and are interested in paid (remote) mini-projects and gigs on the side,&lt;/p&gt;\n\n&lt;p&gt;then this is a good opportunity to get compensated while further sharpening your skills on your own schedule.&lt;/p&gt;\n\n&lt;p&gt;IMHO also useful if you&amp;#39;re a grad student, have student loans, or just want to build up your portfolio.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re interested, please opt in here: &lt;a href=""https://docs.google.com/forms/d/e/1FAIpQLScK-yztp2B70GkmvUmRDIeOkUxkutRlhzsGCDRhJksgWky4mg/viewform""&gt;https://docs.google.com/forms/d/e/1FAIpQLScK-yztp2B70GkmvUmRDIeOkUxkutRlhzsGCDRhJksgWky4mg/viewform&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Feel free to email [&lt;a href=""mailto:gr2511@columbia.edu""&gt;gr2511@columbia.edu&lt;/a&gt;](mailto:&lt;a href=""mailto:gr2511@columbia.edu""&gt;gr2511@columbia.edu&lt;/a&gt;) for any questions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/S9crEQYpwA6ekxqYobdJTHS6jzTRVu1a7_Ndizlxh3k.jpg?auto=webp&amp;s=32672f4715d15292ce08ccff17f800fb7d1b3d79', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/S9crEQYpwA6ekxqYobdJTHS6jzTRVu1a7_Ndizlxh3k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8796a67b9cfabe02b628971a0adf9da5e36d0005', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/S9crEQYpwA6ekxqYobdJTHS6jzTRVu1a7_Ndizlxh3k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bce8aa4865efc4f013dc4dd410bc278cd406f330', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/S9crEQYpwA6ekxqYobdJTHS6jzTRVu1a7_Ndizlxh3k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1167efd7e761c44fcb6c62915c047d3b4e09281', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/S9crEQYpwA6ekxqYobdJTHS6jzTRVu1a7_Ndizlxh3k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f9d344ffeb2a258b78f23d5fd767e3fb6ed7a8b0', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/S9crEQYpwA6ekxqYobdJTHS6jzTRVu1a7_Ndizlxh3k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b095516fad657b7cd2d5fc08b5b5d83b934ca8b7', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/S9crEQYpwA6ekxqYobdJTHS6jzTRVu1a7_Ndizlxh3k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=043b04800775539618a7accc5373bdd9b38a2559', 'width': 1080, 'height': 567}], 'variants': {}, 'id': '-p_-e3JNb56wltrg0ZmyHZgAxcR8TN6LJ9nas3Ulox0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'kelv2u', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/kelv2u/paid_ml_gigs_get_compensated_while_further/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/kelv2u/paid_ml_gigs_get_compensated_while_further/', 'subreddit_subscribers': 6676, 'created_utc': 1608164525.0, 'num_crossposts': 13, 'media': None, 'is_video': False}]",t3_kelv2u,,,,,
174,,pytorch,Have people been using deep learning to do regression? I noticed that fitting polynomials using least squares leads to much better accuracy! Is there any rule of thumb to get arbitrary accuracy with deep regression?,t2_617u1lh9,False,,0,False,deep regression,[],r/pytorch,False,6,,0,,,False,t3_keco2b,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1608165857.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Have people been using deep learning to do regression? I noticed that fitting polynomials using least squares leads to much better accuracy! Is there any rule of thumb to get arbitrary accuracy with deep regression?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,keco2b,True,,matibilkis,,3,True,all_ads,False,[],False,,/r/pytorch/comments/keco2b/deep_regression/,all_ads,False,https://www.reddit.com/r/pytorch/comments/keco2b/deep_regression/,7135,1608137057.0,0,,False,,,,,,,,
175,,pytorch,I have searched pytorch lightning docs but only found ways for finding metrics on train data for every n epochs.,t2_wjclo,False,,0,False,How to check test accuracy on every n train epochs using pytorch lightning?,[],r/pytorch,False,6,,0,,,False,t3_kebauw,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1608161577.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have searched pytorch lightning docs but only found ways for finding metrics on train data for every n epochs.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kebauw,True,,crazyb14,,2,True,all_ads,False,[],False,,/r/pytorch/comments/kebauw/how_to_check_test_accuracy_on_every_n_train/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kebauw/how_to_check_test_accuracy_on_every_n_train/,7135,1608132777.0,0,,False,,,,,,,,
176,,pytorch,"Hi!

I came across this [tensorflow](https://www.tensorflow.org/recommenders/) wrapper and it has some really nice guides and helpers to get going with building recommender systems - is there anything similar in the PyTorch ecosystem?

Thanks!",t2_2innaoj,False,,0,False,PyTorch recommenders,[],r/pytorch,False,6,,0,,,False,t3_ke9wfo,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1608156784.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;I came across this &lt;a href=""https://www.tensorflow.org/recommenders/""&gt;tensorflow&lt;/a&gt; wrapper and it has some really nice guides and helpers to get going with building recommender systems - is there anything similar in the PyTorch ecosystem?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/kODY_goZcW81ZQoi81jNGE4PPUdkaE6GisFQT5VfmhE.jpg?auto=webp&amp;s=614a4c65aba0820101975a1592695366c3b3dfc0', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/kODY_goZcW81ZQoi81jNGE4PPUdkaE6GisFQT5VfmhE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2dfb419178e33208de3bd2fa59c034758e1ed60c', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/kODY_goZcW81ZQoi81jNGE4PPUdkaE6GisFQT5VfmhE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=712145b8b83d8285dac95e81f28c7c32a850cc71', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/kODY_goZcW81ZQoi81jNGE4PPUdkaE6GisFQT5VfmhE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9738c29d01fb3c1c761f2663c08248405e75d84e', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/kODY_goZcW81ZQoi81jNGE4PPUdkaE6GisFQT5VfmhE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=35afc13be9dc8b2faf6ad93b0d6f742a7ab0c61a', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/kODY_goZcW81ZQoi81jNGE4PPUdkaE6GisFQT5VfmhE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d6f58d1724c40cf2256aa68fe1062fa76427d22f', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/kODY_goZcW81ZQoi81jNGE4PPUdkaE6GisFQT5VfmhE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=91bf04ba21cca549eac80965bfe0103201bb8ecd', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'uaUHiE-2tkETSSgWDTLsIcUo1I2mdfEi9rhTIzTyMZo'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ke9wfo,True,,ydennisy,,3,True,all_ads,False,[],False,,/r/pytorch/comments/ke9wfo/pytorch_recommenders/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ke9wfo/pytorch_recommenders/,7135,1608127984.0,0,,False,,,,,,,,
177,,pytorch,"The way you configure your loss functions can either make or break the performance of your algorithm.

By correctly configuring the loss function, you can make sure your model will work how you want it to.

A few key things to learn before you can properly choose the correct loss function are:

- What are loss functions and how to use them in PyTorch?
- Which loss functions are available?
- How to create a custom loss function?

Here’s our tutorial that will help you:

[PyTorch loss functions](https://neptune.ai/blog/pytorch-loss-functions?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-pytorch-loss-functions&amp;utm_content=pytorch)",t2_5hfacnnv,False,,0,False,[beginners tutorial] Guide to Pytorch Loss Functions + How to Build Custom Functions,[],r/pytorch,False,6,,0,,,False,t3_kdp5bd,False,dark,0.89,,public,14,0,{},,,False,[],,False,False,,{},,False,14,,False,self,False,,[],{},self,,True,,1608079345.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The way you configure your loss functions can either make or break the performance of your algorithm.&lt;/p&gt;

&lt;p&gt;By correctly configuring the loss function, you can make sure your model will work how you want it to.&lt;/p&gt;

&lt;p&gt;A few key things to learn before you can properly choose the correct loss function are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What are loss functions and how to use them in PyTorch?&lt;/li&gt;
&lt;li&gt;Which loss functions are available?&lt;/li&gt;
&lt;li&gt;How to create a custom loss function?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here’s our tutorial that will help you:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://neptune.ai/blog/pytorch-loss-functions?utm_source=reddit&amp;amp;utm_medium=post&amp;amp;utm_campaign=blog-pytorch-loss-functions&amp;amp;utm_content=pytorch""&gt;PyTorch loss functions&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/w9y15zvoOYd0oGu9yDpkpbvU7Sw19RTvp0SxujhuVck.jpg?auto=webp&amp;s=749a3a5d213be1d74d61d28d0b3bd3b7964dc22a', 'width': 1920, 'height': 1377}, 'resolutions': [{'url': 'https://external-preview.redd.it/w9y15zvoOYd0oGu9yDpkpbvU7Sw19RTvp0SxujhuVck.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=79e53e8ab07687b618cd2f06971c4cbc6a0938ea', 'width': 108, 'height': 77}, {'url': 'https://external-preview.redd.it/w9y15zvoOYd0oGu9yDpkpbvU7Sw19RTvp0SxujhuVck.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=041bfd227cba1c2b87e08641fa90afee453d5c8b', 'width': 216, 'height': 154}, {'url': 'https://external-preview.redd.it/w9y15zvoOYd0oGu9yDpkpbvU7Sw19RTvp0SxujhuVck.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=efd3eec07a0d55983c681b4949c0375620219174', 'width': 320, 'height': 229}, {'url': 'https://external-preview.redd.it/w9y15zvoOYd0oGu9yDpkpbvU7Sw19RTvp0SxujhuVck.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=144519d8694a305d4a8aea4dd6f7b7d7435d6148', 'width': 640, 'height': 459}, {'url': 'https://external-preview.redd.it/w9y15zvoOYd0oGu9yDpkpbvU7Sw19RTvp0SxujhuVck.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2152f4b964f7697da649a9580048fc2860a93e04', 'width': 960, 'height': 688}, {'url': 'https://external-preview.redd.it/w9y15zvoOYd0oGu9yDpkpbvU7Sw19RTvp0SxujhuVck.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=86005bf056acd66523e88e3f2320e4c575ca2b8c', 'width': 1080, 'height': 774}], 'variants': {}, 'id': '7yamULZZDjZSQ29QWb26m-oMCEykDAOps1Qn-PwXJ7s'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kdp5bd,True,,kk_ai,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kdp5bd/beginners_tutorial_guide_to_pytorch_loss/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kdp5bd/beginners_tutorial_guide_to_pytorch_loss/,7135,1608050545.0,0,,False,,,,,,,,
178,,pytorch,"Hi everybody!

I was wondering if there's a testing framework out there for PyTorch that you have used? For the past several months, I've been working with an existing codebase that has massive networks and huge datasets. I've been making changes to implement some new models. Running simple experiments just to ensure that everything *runs* without breaking due to size mismatches can take a while. The other day, I finished an epoch (took several hours) and the code bugged on some logging functionality that wasn't updated for the changes that were made to the model. 

I'm a Ph.D. student now but I came from industry in software engineering. We were spoiled for choice in terms of testing frameworks for anything we could ever write. I'm wondering if there are testing frameworks that you've used to make sure that your model *can* run before actually running it.",t2_1szu9v3l,False,,0,False,Testing Framework,[],r/pytorch,False,6,,0,,,False,t3_kd2dk4,False,dark,0.92,,public,9,0,{},,,False,[],,False,False,,{},,False,9,,False,self,False,,[],{},,,True,,1607996706.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everybody!&lt;/p&gt;

&lt;p&gt;I was wondering if there&amp;#39;s a testing framework out there for PyTorch that you have used? For the past several months, I&amp;#39;ve been working with an existing codebase that has massive networks and huge datasets. I&amp;#39;ve been making changes to implement some new models. Running simple experiments just to ensure that everything &lt;em&gt;runs&lt;/em&gt; without breaking due to size mismatches can take a while. The other day, I finished an epoch (took several hours) and the code bugged on some logging functionality that wasn&amp;#39;t updated for the changes that were made to the model. &lt;/p&gt;

&lt;p&gt;I&amp;#39;m a Ph.D. student now but I came from industry in software engineering. We were spoiled for choice in terms of testing frameworks for anything we could ever write. I&amp;#39;m wondering if there are testing frameworks that you&amp;#39;ve used to make sure that your model &lt;em&gt;can&lt;/em&gt; run before actually running it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kd2dk4,True,,swiftypat,,5,True,all_ads,False,[],False,,/r/pytorch/comments/kd2dk4/testing_framework/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kd2dk4/testing_framework/,7135,1607967906.0,0,,False,,,,,,,,
179,,pytorch,"I am working for a uni project on an implementation of the sliding window approach for object detection. For each frame I take the image, unfold it and run all the patches through a CNN classifier.

It was working fine until I changed the classifier model and the GPU memory started filling the 4GB (laptop) in a matter of seconds. I have been trying to debug the allocation/deallocation of tensors but I can't really understand what is taking so much space.

By proceeding one line at a time with the debugger I noticed that once I call

    classifier(patches) 

the memory usage jumps by +200MB on the first call, +1000MB on the second and by the third 3.5GB are used. This does not depend on the scope, once I exit the function the memory usage does not decrease. How can I check what is kept in memory? Is it storing some king of history? Can I disable it somehow since I am in eval mode? Many thanks",t2_16yknb,False,,0,False,Why is PyTorch filling the GPU memory?,[],r/pytorch,False,6,,0,,,False,t3_kctr3x,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1607934735.0,,[],{},,,True,,1607963035.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am working for a uni project on an implementation of the sliding window approach for object detection. For each frame I take the image, unfold it and run all the patches through a CNN classifier.&lt;/p&gt;

&lt;p&gt;It was working fine until I changed the classifier model and the GPU memory started filling the 4GB (laptop) in a matter of seconds. I have been trying to debug the allocation/deallocation of tensors but I can&amp;#39;t really understand what is taking so much space.&lt;/p&gt;

&lt;p&gt;By proceeding one line at a time with the debugger I noticed that once I call&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;classifier(patches) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the memory usage jumps by +200MB on the first call, +1000MB on the second and by the third 3.5GB are used. This does not depend on the scope, once I exit the function the memory usage does not decrease. How can I check what is kept in memory? Is it storing some king of history? Can I disable it somehow since I am in eval mode? Many thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kctr3x,True,,biscofil,,5,True,all_ads,False,[],False,,/r/pytorch/comments/kctr3x/why_is_pytorch_filling_the_gpu_memory/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kctr3x/why_is_pytorch_filling_the_gpu_memory/,7135,1607934235.0,0,,False,,,,,,,,
180,,pytorch,"\[I love pytorch\]

But bit more free software activist, I worry if Pytorch contains some survellience features\]",t2_6hhee0ag,False,,0,False,Does Pytorch source code contains facebook telemetry codes?,[],r/pytorch,False,6,,0,,,False,t3_kc6u6f,False,dark,0.64,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1607875082.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[I love pytorch]&lt;/p&gt;

&lt;p&gt;But bit more free software activist, I worry if Pytorch contains some survellience features]&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kc6u6f,True,,Psycho-logical-being,,3,True,all_ads,False,[],False,,/r/pytorch/comments/kc6u6f/does_pytorch_source_code_contains_facebook/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kc6u6f/does_pytorch_source_code_contains_facebook/,7135,1607846282.0,0,,False,,,,,,,,
181,,pytorch,,t2_44mbtmjy,False,,0,False,Free browser extension for ML community that thousands of machine learning engineers/data scientists use everyday! Drop a comment for any questions/feature requests you may have!,[],r/pytorch,False,6,,0,70.0,,False,t3_kc2z6u,False,dark,0.5,,public,0,0,{},70.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/tPTSJTzaygYGO_GVYZ_WjE2xV3A875hWMhGWKnpW8_c.jpg,False,,[],{},link,,False,,1607857573.0,text,6,,,text,self.LatestInML,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?auto=webp&amp;s=b2ec1cfa2b9e720731d4dc172cbef8096fb37c3a', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad5435b4b2d94d9df9309e4d5ceb4bc021b52783', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'TIQjNUpyCsCWmTM49ZEEAiVQDWg07G_PjT8zXCX4wSE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kc2z6u,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kc2z6u/free_browser_extension_for_ml_community_that/,all_ads,False,/r/LatestInML/comments/kc2vyx/free_browser_extension_for_ml_community_that/,7135,1607828773.0,0,,False,/r/LatestInML/comments/kc2vyx/free_browser_extension_for_ml_community_that/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'Liked by Andrew Ng as well!\n\nDrop a comment for any questions/feature requests you may have!-&gt;The extension finds code implementations for ML/AI papers anywhere on the internet!\n\nChrome [http://bit.ly/code\\_finder\\_chrome](https://bit.ly/code_finder_chrome?fbclid=IwAR23RdK9pFQwwPEXoJKIjYgXhQhmBFcGZnHVxIFFb-VZHHM3x4s4kBQnxqA)  \nFirefox [http://bit.ly/code\\_finder\\_firefox](https://bit.ly/code_finder_firefox?fbclid=IwAR2N_I1htqKSaHEZ-gYoa-J4AVnBpZKyZmLvK5Y8sjuS1hDDY_s-LEZKKbQ)\n\n&amp;#x200B;\n\nhttps://reddit.com/link/kc2vyx/video/prqwux4ldv461/player', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Free browser extension for ML community that thousands of machine learning engineers/data scientists use everyday! Drop a comment for any questions/feature requests you may have!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'prqwux4ldv461': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/kc2vyx/asset/prqwux4ldv461/DASHPlaylist.mpd?a=1618044151%2CZTNhY2NkMzQyOTdiOWNmYTlmNWZiOThmYzFiYTRiZDBiNDM0OWEzNjU2YzdmMGY2MWIyMDExZDY2OGI0ZTcwNg%3D%3D&amp;v=1&amp;f=sd', 'x': 1676, 'y': 1080, 'hlsUrl': 'https://v.redd.it/link/kc2vyx/asset/prqwux4ldv461/HLSPlaylist.m3u8?a=1618044151%2CNDRjNTU5NTJlNmUzODExMjg3MDBmM2M2MTVkNTU2YzM0MTkyYjczNzg5MTFkMTgyYzIxOTZjMzQ3YTc4Mzk0MQ%3D%3D&amp;v=1&amp;f=sd', 'id': 'prqwux4ldv461', 'isGif': False}}, 'name': 't3_kc2vyx', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 21, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 70, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 21, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/tPTSJTzaygYGO_GVYZ_WjE2xV3A875hWMhGWKnpW8_c.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1607857236.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Liked by Andrew Ng as well!&lt;/p&gt;\n\n&lt;p&gt;Drop a comment for any questions/feature requests you may have!-&amp;gt;The extension finds code implementations for ML/AI papers anywhere on the internet!&lt;/p&gt;\n\n&lt;p&gt;Chrome &lt;a href=""https://bit.ly/code_finder_chrome?fbclid=IwAR23RdK9pFQwwPEXoJKIjYgXhQhmBFcGZnHVxIFFb-VZHHM3x4s4kBQnxqA""&gt;http://bit.ly/code_finder_chrome&lt;/a&gt;&lt;br/&gt;\nFirefox &lt;a href=""https://bit.ly/code_finder_firefox?fbclid=IwAR2N_I1htqKSaHEZ-gYoa-J4AVnBpZKyZmLvK5Y8sjuS1hDDY_s-LEZKKbQ""&gt;http://bit.ly/code_finder_firefox&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/kc2vyx/video/prqwux4ldv461/player""&gt;https://reddit.com/link/kc2vyx/video/prqwux4ldv461/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?auto=webp&amp;s=b2ec1cfa2b9e720731d4dc172cbef8096fb37c3a', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad5435b4b2d94d9df9309e4d5ceb4bc021b52783', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'TIQjNUpyCsCWmTM49ZEEAiVQDWg07G_PjT8zXCX4wSE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'kc2vyx', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/kc2vyx/free_browser_extension_for_ml_community_that/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/kc2vyx/free_browser_extension_for_ml_community_that/', 'subreddit_subscribers': 6676, 'created_utc': 1607828436.0, 'num_crossposts': 16, 'media': None, 'is_video': False}]",t3_kc2vyx,,,,,
182,,pytorch,"I have a classifier module (nn.Linear) and I want the weights to be static, but treat the input vector as parameters to be updated. So I will still minimize the crossentropy(output, target), but the result discovers the values for an input that approximately minimizes the classifier loss. Is calculating the gradient w.r.t the input the right way to describe that?",t2_12zuf2,False,,0,False,Can I calculate gradient w.r.t. the input?,[],r/pytorch,False,6,,0,,,False,t3_kbl2hq,False,dark,1.0,,public,9,0,{},,,False,[],,False,False,,{},,False,9,,False,self,1607758652.0,,[],{},,,True,,1607787213.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a classifier module (nn.Linear) and I want the weights to be static, but treat the input vector as parameters to be updated. So I will still minimize the crossentropy(output, target), but the result discovers the values for an input that approximately minimizes the classifier loss. Is calculating the gradient w.r.t the input the right way to describe that?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kbl2hq,True,,throwaway775849,,10,True,all_ads,False,[],False,,/r/pytorch/comments/kbl2hq/can_i_calculate_gradient_wrt_the_input/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kbl2hq/can_i_calculate_gradient_wrt_the_input/,7135,1607758413.0,0,,False,,,,,,,,
183,,pytorch,"I have been running a model on a raspberry pi 3 A+ and got curious performance results.

For example an inference using resnet18 on a 256x256 image:

* 4 threads:  dt = 1.869842
* 1 thread: dt  = 1.510674

Using mobilenetv2 on a 256x256 image:

* 4 threads: dt = 2.509571
* 1 thread: dt = 1.208802

Using resnet18 on a 512x512 image:

* 4 threads: dt = 7.229154
* 1 thread: dt = 6.206228

Using mobilnetv2 on a 512x512 image:

* 4 threads: dt = 4.412468
* 1 thread: dt = 3.804896

&amp;#x200B;

The raspberry 3 chip has 4 cores, so I would expect a better performance when using all the cores. Has anyone experience something similar and know of any tricks to get a better performance using all cores?",t2_71zp4,False,,0,False,Inference using a single thread is faster than using 4 threads on a raspberry pi,[],r/pytorch,False,6,,0,,,False,t3_kb5ovd,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1607731951.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been running a model on a raspberry pi 3 A+ and got curious performance results.&lt;/p&gt;

&lt;p&gt;For example an inference using resnet18 on a 256x256 image:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;4 threads:  dt = 1.869842&lt;/li&gt;
&lt;li&gt;1 thread: dt  = 1.510674&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using mobilenetv2 on a 256x256 image:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;4 threads: dt = 2.509571&lt;/li&gt;
&lt;li&gt;1 thread: dt = 1.208802&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using resnet18 on a 512x512 image:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;4 threads: dt = 7.229154&lt;/li&gt;
&lt;li&gt;1 thread: dt = 6.206228&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using mobilnetv2 on a 512x512 image:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;4 threads: dt = 4.412468&lt;/li&gt;
&lt;li&gt;1 thread: dt = 3.804896&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The raspberry 3 chip has 4 cores, so I would expect a better performance when using all the cores. Has anyone experience something similar and know of any tricks to get a better performance using all cores?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kb5ovd,True,,lord_segfault,,6,True,all_ads,False,[],False,,/r/pytorch/comments/kb5ovd/inference_using_a_single_thread_is_faster_than/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kb5ovd/inference_using_a_single_thread_is_faster_than/,7135,1607703151.0,0,,False,,,,,,,,
184,,pytorch,"Hi! I’m working on a segmentation model, and I am using a custom dice loss. I’m working on medical scans and I realised that the output doesn’t quite perform well and demarcates the textured area of the image rather than the smooth area. And the smooth area is the ROI.
I was thinking maybe if I convert the output to int type, then only a few areas will be highlighted and a larger loss will be generated to penalise the model.

But as gradients are involved, I’m unable to convert the dtype of the output which is a GPU tensor to int. How do I do it?",t2_4xhrybaz,False,,0,False,Is it possible to convert the type of the output tensor from float to int in a custom loss function?,[],r/pytorch,False,6,,0,,,False,t3_kb2ko8,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1607721366.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi! I’m working on a segmentation model, and I am using a custom dice loss. I’m working on medical scans and I realised that the output doesn’t quite perform well and demarcates the textured area of the image rather than the smooth area. And the smooth area is the ROI.
I was thinking maybe if I convert the output to int type, then only a few areas will be highlighted and a larger loss will be generated to penalise the model.&lt;/p&gt;

&lt;p&gt;But as gradients are involved, I’m unable to convert the dtype of the output which is a GPU tensor to int. How do I do it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kb2ko8,True,,banenvy,,1,True,all_ads,False,[],False,,/r/pytorch/comments/kb2ko8/is_it_possible_to_convert_the_type_of_the_output/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kb2ko8/is_it_possible_to_convert_the_type_of_the_output/,7135,1607692566.0,0,,False,,,,,,,,
185,,pytorch,"I just purchased a Lenovo PC with a GeForce RTX 2070 GPU installed. The OS is Ubuntu Linux 20.10.

I installed CUDA Toolkit 11.0.221, then installed a CUDA-compatible version of PyTorch using the instructions at [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/):

pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch\_stable.html

Installation completed without any problems, but I'm unable to get &gt;&gt;&gt;torch.cuda.is\_available()  
 to return True.

Did I miss a configuration step?

(Apologies for the newb questions. This is my first time with CUDA programming.)",t2_pnz1b,False,,0,False,How to configure PyTorch to recognize an existing GPU?,[],r/pytorch,False,6,,0,,,False,t3_kb5fwn,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1607731193.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I just purchased a Lenovo PC with a GeForce RTX 2070 GPU installed. The OS is Ubuntu Linux 20.10.&lt;/p&gt;

&lt;p&gt;I installed CUDA Toolkit 11.0.221, then installed a CUDA-compatible version of PyTorch using the instructions at &lt;a href=""https://pytorch.org/get-started/locally/""&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio===0.7.2 -f &lt;a href=""https://download.pytorch.org/whl/torch%5C_stable.html""&gt;https://download.pytorch.org/whl/torch\_stable.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Installation completed without any problems, but I&amp;#39;m unable to get &amp;gt;&amp;gt;&amp;gt;torch.cuda.is_available()&lt;br/&gt;
 to return True.&lt;/p&gt;

&lt;p&gt;Did I miss a configuration step?&lt;/p&gt;

&lt;p&gt;(Apologies for the newb questions. This is my first time with CUDA programming.)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?auto=webp&amp;s=d0da5dbb7aff983a5173aba7d2bdd36fa28abe08', 'width': 2500, 'height': 2500}, 'resolutions': [{'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fef05ec37ddf39032979380cc7acc403e05834be', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e1de8e408f33ef3c93425e30e0bb5afffd5c631', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89562dde9cea0af5ebaaba50a9437e484d732513', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d6125bcd02c2fed8d84dac49cc8d73dbe205d81', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=40d7ca8d88566f27054acb632514012b5026acc9', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=757464a70e69532931dcbf0fbd397b9d433cf205', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'Mg0HAz_PmYkYWkaGTVyhCEzp15PpLW_TWWqmn9VNVFs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kb5fwn,True,,PullThisFinger,,10,True,all_ads,False,[],False,,/r/pytorch/comments/kb5fwn/how_to_configure_pytorch_to_recognize_an_existing/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kb5fwn/how_to_configure_pytorch_to_recognize_an_existing/,7135,1607702393.0,0,,False,,,,,,,,
186,,pytorch,,t2_7eslkpz,False,,0,False,PyTorch 1.7.1 - Bug fix release with updated binaries for Python 3.9 and cuDNN 8.0.5,[],r/pytorch,False,6,,0,140.0,,False,t3_kakwez,False,dark,1.0,,public,25,0,{},140.0,,False,[],,False,False,,{},,False,25,,False,https://a.thumbs.redditmedia.com/ue0-KOON8FMr_3QWCVXHd8bbAKzS8caOHmQxQ8p2sJ0.jpg,False,,[],{},link,,False,,1607653705.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?auto=webp&amp;s=08c43430ff8793a25b767aa172468acb35be611a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4adad794ed3c0e2f76e2da992ae9eef8c074cae4', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5b3cb400fea60e0dfe87ebc5314f7db8016f8ab', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f149723b636ef3087022ddc1efa6ab53e3d4bc69', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_YPUxJ8AyYwgpO9r-tD7JUoQ-Nx8IlATLoEfmjW48iQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kakwez,True,,Marha01,,5,False,all_ads,False,[],False,,/r/pytorch/comments/kakwez/pytorch_171_bug_fix_release_with_updated_binaries/,all_ads,False,https://github.com/pytorch/pytorch/releases/tag/v1.7.1,7135,1607624905.0,0,,False,https://github.com/pytorch/pytorch/releases/tag/v1.7.1,,,,,,,
187,,pytorch,"I've put together a small, annotated library of deeplearning models used in NLP here:

[https://github.com/will-thompson-k/deeplearning-nlp-models](https://github.com/will-thompson-k/deeplearning-nlp-models)

&amp;#x200B;

[BERT: Reading. Comprehending.](https://preview.redd.it/bly0gs3rvg461.jpg?width=320&amp;format=pjpg&amp;auto=webp&amp;s=84c9f0873c2ac9ef393f8f47943cd32f2550427b)

&amp;#x200B;

[Attention patterns ](https://preview.redd.it/0aeyi9vuvg461.png?width=1074&amp;format=png&amp;auto=webp&amp;s=29ddaabc11e1a8e2b0e1272a9bb45f43d70fed3f)

It's by no means comprehensive, but meant as a primer for those delving into model architectures. Let me know if you have any feedback!",t2_8o5bj9n7,False,,0,False,[P] Pytorch NLP Models (run w/ GPUs),[],r/pytorch,False,6,,0,92.0,,False,t3_katg8g,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/xZpYjt2UL2SkcFzt0XNqcfa_SDe4mlto3HykNL8oGTA.jpg,False,,[],{},,,True,,1607681726.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve put together a small, annotated library of deeplearning models used in NLP here:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/will-thompson-k/deeplearning-nlp-models""&gt;https://github.com/will-thompson-k/deeplearning-nlp-models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/bly0gs3rvg461.jpg?width=320&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=84c9f0873c2ac9ef393f8f47943cd32f2550427b""&gt;BERT: Reading. Comprehending.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/0aeyi9vuvg461.png?width=1074&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29ddaabc11e1a8e2b0e1272a9bb45f43d70fed3f""&gt;Attention patterns &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It&amp;#39;s by no means comprehensive, but meant as a primer for those delving into model architectures. Let me know if you have any feedback!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,katg8g,True,,wilhelm____,,0,True,all_ads,False,[],False,,/r/pytorch/comments/katg8g/p_pytorch_nlp_models_run_w_gpus/,all_ads,False,https://www.reddit.com/r/pytorch/comments/katg8g/p_pytorch_nlp_models_run_w_gpus/,7135,1607652926.0,0,,False,,,,"{'0aeyi9vuvg461': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 26, 'x': 108, 'u': 'https://preview.redd.it/0aeyi9vuvg461.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a38ae968519929f6f9f0a777886d5ef87e123feb'}, {'y': 53, 'x': 216, 'u': 'https://preview.redd.it/0aeyi9vuvg461.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e1e5714271cdf7fac39d2ff4a67d12ad452aa4bf'}, {'y': 79, 'x': 320, 'u': 'https://preview.redd.it/0aeyi9vuvg461.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7942a0f75e0af4008a5076884923e73eddbd1511'}, {'y': 159, 'x': 640, 'u': 'https://preview.redd.it/0aeyi9vuvg461.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=508adcb4268dd3d965008a1e180bedce829537a8'}, {'y': 239, 'x': 960, 'u': 'https://preview.redd.it/0aeyi9vuvg461.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4188a0d86c5d90bbf90a2fa794a7ed512234536c'}], 's': {'y': 268, 'x': 1074, 'u': 'https://preview.redd.it/0aeyi9vuvg461.png?width=1074&amp;format=png&amp;auto=webp&amp;s=29ddaabc11e1a8e2b0e1272a9bb45f43d70fed3f'}, 'id': '0aeyi9vuvg461'}, 'bly0gs3rvg461': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 71, 'x': 108, 'u': 'https://preview.redd.it/bly0gs3rvg461.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3fd8c1287a48e33de2123bcabd157a6a59e84664'}, {'y': 142, 'x': 216, 'u': 'https://preview.redd.it/bly0gs3rvg461.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a30e328b49e32c3128dd9ec5b57b64f1ff10e03'}, {'y': 211, 'x': 320, 'u': 'https://preview.redd.it/bly0gs3rvg461.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae7ebc868856ffa648d1e7bc7e0020a42f285b67'}], 's': {'y': 211, 'x': 320, 'u': 'https://preview.redd.it/bly0gs3rvg461.jpg?width=320&amp;format=pjpg&amp;auto=webp&amp;s=84c9f0873c2ac9ef393f8f47943cd32f2550427b'}, 'id': 'bly0gs3rvg461'}}",,,,
188,,pytorch,,t2_ljm67vi,False,,0,False,How to get the autograd backward graph with shape information in Pytorch？,[],r/pytorch,False,6,,0,,,False,t3_kau6vd,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1607684242.0,text,6,,,text,self.pytorch,False,,,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kau6vd,True,,xcoder-10,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kau6vd/how_to_get_the_autograd_backward_graph_with_shape/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kau6vd/how_to_get_the_autograd_backward_graph_with_shape/,7135,1607655442.0,0,,False,,,,,,,,
189,,pytorch,"Hello community,
Let’s say I have a data shape of =( 1300,60).

I applied windowing on it  with a sequence length of =20   to the data, and the shape become =(1297,20,60).
My goal is to compress this entity like this:
(1297,20,60) -&gt; (1297,1,3) 
Knowing that I apply a LSTM layer at the beginning.
In summary :
1. Reshape the data using windowing 
2. Apply an LSTM Layer
3. Apply an appropriate layer to compress lstm output into (1297,1,3)

Do you know any layers which can apply such a thing  to compress data ?",t2_7l9ti89m,False,,0,False,LSTM ENCODING,[],r/pytorch,False,6,,0,,,False,t3_kaitem,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1607647678.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello community,
Let’s say I have a data shape of =( 1300,60).&lt;/p&gt;

&lt;p&gt;I applied windowing on it  with a sequence length of =20   to the data, and the shape become =(1297,20,60).
My goal is to compress this entity like this:
(1297,20,60) -&amp;gt; (1297,1,3) 
Knowing that I apply a LSTM layer at the beginning.
In summary :
1. Reshape the data using windowing 
2. Apply an LSTM Layer
3. Apply an appropriate layer to compress lstm output into (1297,1,3)&lt;/p&gt;

&lt;p&gt;Do you know any layers which can apply such a thing  to compress data ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kaitem,True,,rayanaay,,1,True,all_ads,False,[],False,,/r/pytorch/comments/kaitem/lstm_encoding/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kaitem/lstm_encoding/,7135,1607618878.0,0,,False,,,,,,,,
190,,pytorch,"I've put together a small, annotated library of deeplearning models used in NLP here:

[https://github.com/will-thompson-k/deeplearning-nlp-models](https://github.com/will-thompson-k/deeplearning-nlp-models)

&amp;#x200B;

[BERT: Reading. Comprehending.](https://preview.redd.it/sfx5yhm8r9461.jpg?width=320&amp;format=pjpg&amp;auto=webp&amp;s=4856b694e87ddb9e5aea056c9802e1ac3a12a662)

It's by no means comprehensive, but meant as a primer for those delving into model architectures. Let me know if you have any feedback!",t2_8o5bj9n7,False,,0,False,[P] Deeplearning NLP Models Tutorial in PyTorch (w/ Colab GPU Notebooks),[],r/pytorch,False,6,,0,92.0,,False,t3_ka6c1b,False,dark,0.92,,public,9,0,{},140.0,,False,[],,False,False,,{},,False,9,,False,https://a.thumbs.redditmedia.com/p4nEX45PO74-bCkqPOaNybzP-SxJ7l_kFnl2BSalB74.jpg,False,,[],{},,,True,,1607595379.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve put together a small, annotated library of deeplearning models used in NLP here:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/will-thompson-k/deeplearning-nlp-models""&gt;https://github.com/will-thompson-k/deeplearning-nlp-models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/sfx5yhm8r9461.jpg?width=320&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4856b694e87ddb9e5aea056c9802e1ac3a12a662""&gt;BERT: Reading. Comprehending.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It&amp;#39;s by no means comprehensive, but meant as a primer for those delving into model architectures. Let me know if you have any feedback!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ka6c1b,True,,wilhelm____,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ka6c1b/p_deeplearning_nlp_models_tutorial_in_pytorch_w/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ka6c1b/p_deeplearning_nlp_models_tutorial_in_pytorch_w/,7135,1607566579.0,0,,False,,,,"{'sfx5yhm8r9461': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 71, 'x': 108, 'u': 'https://preview.redd.it/sfx5yhm8r9461.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fda2888442302790ee982f5f8b615f1991a68cf0'}, {'y': 142, 'x': 216, 'u': 'https://preview.redd.it/sfx5yhm8r9461.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83e24a83ada2ded0750abe8dfa45021728c944a3'}, {'y': 211, 'x': 320, 'u': 'https://preview.redd.it/sfx5yhm8r9461.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=69d3b849491ebfffc75ebd22c9c7dd29d41595d3'}], 's': {'y': 211, 'x': 320, 'u': 'https://preview.redd.it/sfx5yhm8r9461.jpg?width=320&amp;format=pjpg&amp;auto=webp&amp;s=4856b694e87ddb9e5aea056c9802e1ac3a12a662'}, 'id': 'sfx5yhm8r9461'}}",,,,
191,,pytorch,"Hello, I want to increase my dataset for a project where I do semantic segmentation of plants. Do you think that I can generate new images with a StyleGAN2 ada model? And that it would be high quality enough to help my semantic segmentation model with additional training data?

Have anyone done anything similar ?",t2_128ob4,False,,0,False,Generate new training data with StyleGAN2 ada ?,[],r/pytorch,False,6,,0,,,False,t3_kacefq,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1607621233.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I want to increase my dataset for a project where I do semantic segmentation of plants. Do you think that I can generate new images with a StyleGAN2 ada model? And that it would be high quality enough to help my semantic segmentation model with additional training data?&lt;/p&gt;

&lt;p&gt;Have anyone done anything similar ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,kacefq,True,,darvidas,,0,True,all_ads,False,[],False,,/r/pytorch/comments/kacefq/generate_new_training_data_with_stylegan2_ada/,all_ads,False,https://www.reddit.com/r/pytorch/comments/kacefq/generate_new_training_data_with_stylegan2_ada/,7135,1607592433.0,0,,False,,,,,,,,
192,,pytorch,https://zcu.io/FRBx,t2_40d0zt4s,False,,0,False,Hands-on Vision Transformers with PyTorch - Analytics India Magazine,[],r/pytorch,False,6,,0,,,False,t3_k9q8xc,False,dark,0.81,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},self,,True,,1607542112.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://zcu.io/FRBx""&gt;https://zcu.io/FRBx&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/WV4I7nJiHlnutxvHUnzjPwGkdX4rO0aW79NM4q0W_XA.jpg?auto=webp&amp;s=769561dd815c9fd75731c5abf522da8dfccf3975', 'width': 1350, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/WV4I7nJiHlnutxvHUnzjPwGkdX4rO0aW79NM4q0W_XA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=de2d4c2a01b52eb4ed6031be8727b72fdb04bcc5', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/WV4I7nJiHlnutxvHUnzjPwGkdX4rO0aW79NM4q0W_XA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9e17b7149bc7321549c82ee8af3aa3eb8f2c8d5d', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/WV4I7nJiHlnutxvHUnzjPwGkdX4rO0aW79NM4q0W_XA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dd8f18f067a3af53ee9f68814a371fc099762113', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/WV4I7nJiHlnutxvHUnzjPwGkdX4rO0aW79NM4q0W_XA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=61b0d83f44b78d3cded8c58d7a82366ff6cd8bd0', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/WV4I7nJiHlnutxvHUnzjPwGkdX4rO0aW79NM4q0W_XA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5eddbcfc43340c63d1f9e8daab1663b378c0ec8d', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/WV4I7nJiHlnutxvHUnzjPwGkdX4rO0aW79NM4q0W_XA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5276d095d6ffd7f2703c967a0c25e7ec75eb60a4', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'yahA1GRXu1Dv5Fbg_47Pm5fAm9FJqcIihfwRaIVkENo'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k9q8xc,True,,analyticsindiam,,0,True,all_ads,False,[],False,,/r/pytorch/comments/k9q8xc/handson_vision_transformers_with_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k9q8xc/handson_vision_transformers_with_pytorch/,7135,1607513312.0,0,,False,,,,,,,,
193,,pytorch,"Hi, guys! I hope you are staying safe and well!

I am currently looking for papers and blogs (with codes if possible) that describe how to use the generative models (e.g. GANs) for time-series data.

Can you please share recourses about the abovementioned topic if you know any?

Thanks a lot in advance!",t2_4ecq42y4,False,,0,False,Generative models for time-series data,[],r/pytorch,False,6,,0,,,False,t3_k9igvh,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1607508086.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, guys! I hope you are staying safe and well!&lt;/p&gt;

&lt;p&gt;I am currently looking for papers and blogs (with codes if possible) that describe how to use the generative models (e.g. GANs) for time-series data.&lt;/p&gt;

&lt;p&gt;Can you please share recourses about the abovementioned topic if you know any?&lt;/p&gt;

&lt;p&gt;Thanks a lot in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k9igvh,True,,ncuxomun,,0,True,all_ads,False,[],False,,/r/pytorch/comments/k9igvh/generative_models_for_timeseries_data/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k9igvh/generative_models_for_timeseries_data/,7135,1607479286.0,0,,False,,,,,,,,
194,,pytorch,"I have a problem where I need to get a new state of an object (e.g. the position) given the current state. However, I am not really sure if I can use GAN for this process. I already know that a basic GAN helps us to learn to generate samples from a given dataset. Nevertheless, I don't know if it is possible to predict a new state or states that change in time. If you knew some paper or information about that, it will very useful for me.",t2_4jztuhao,False,,0,False,Can someone help me? Can a Generative Adversarial Network (GAN) predict a new state?,[],r/pytorch,False,6,,0,,,False,t3_k9mf1w,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1607522829.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a problem where I need to get a new state of an object (e.g. the position) given the current state. However, I am not really sure if I can use GAN for this process. I already know that a basic GAN helps us to learn to generate samples from a given dataset. Nevertheless, I don&amp;#39;t know if it is possible to predict a new state or states that change in time. If you knew some paper or information about that, it will very useful for me.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k9mf1w,True,,ZosoV07,,1,True,all_ads,False,[],False,,/r/pytorch/comments/k9mf1w/can_someone_help_me_can_a_generative_adversarial/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k9mf1w/can_someone_help_me_can_a_generative_adversarial/,7135,1607494029.0,0,,False,,,,,,,,
195,,pytorch,"Archai is a platform for Neural Network Search (NAS) that allow you to generate efficient deep networks for your applications. 

[https://github.com/microsoft/archai](https://github.com/microsoft/archai)",t2_3ik2,False,,0,False,Archai for NAS,[],r/pytorch,False,6,,0,,,False,t3_k8jncc,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},self,,True,,1607386833.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Archai is a platform for Neural Network Search (NAS) that allow you to generate efficient deep networks for your applications. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/microsoft/archai""&gt;https://github.com/microsoft/archai&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/oUQzzQfA3-JGtzj_7Jxhrq-7ysdjKnXeVDKfTykHdaA.jpg?auto=webp&amp;s=cba779060580ed3f457410dab623d999244c625b', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/oUQzzQfA3-JGtzj_7Jxhrq-7ysdjKnXeVDKfTykHdaA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=614e6bf4d7570c9c4d0e9da5224fcb0f4538375a', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/oUQzzQfA3-JGtzj_7Jxhrq-7ysdjKnXeVDKfTykHdaA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=def3c0fba828bd11a25b8af3f4f19956f9a78586', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/oUQzzQfA3-JGtzj_7Jxhrq-7ysdjKnXeVDKfTykHdaA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eaf239dada5661bd9cc8cf071f171e89bae3b4ea', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'NJXrOyE3KfKBJ_zZ_HKaff4PvMVJeGk8i3forC7kCZE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k8jncc,True,,sytelus,,1,True,all_ads,False,[],False,,/r/pytorch/comments/k8jncc/archai_for_nas/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k8jncc/archai_for_nas/,7135,1607358033.0,0,,False,,,,,,,,
196,,pytorch,"I have an errors function that looks like:

```
def errors(x, y):
    err = (x - y).pow(2).sum(dim=1)
    return err
```

I pass to it an array of x values and a corresponding y labels array

However if I implement it with BCE:
```
def errors(x, y):
    err = F.binary_cross_entropy(x, y)
    return err
```

I get a single value. Is it possible to implement BCE on per row value?",t2_x6pudoo,False,,0,False,Manual MSE vs BinaryCrossEntropy,[],r/pytorch,False,6,,0,,,False,t3_k7the3,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1607288930.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have an errors function that looks like:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
def errors(x, y):
    err = (x - y).pow(2).sum(dim=1)
    return err
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I pass to it an array of x values and a corresponding y labels array&lt;/p&gt;

&lt;p&gt;However if I implement it with BCE:
&lt;code&gt;
def errors(x, y):
    err = F.binary_cross_entropy(x, y)
    return err
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I get a single value. Is it possible to implement BCE on per row value?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k7the3,True,,ralampay,,7,True,all_ads,False,[],False,,/r/pytorch/comments/k7the3/manual_mse_vs_binarycrossentropy/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k7the3/manual_mse_vs_binarycrossentropy/,7135,1607260130.0,0,,False,,,,,,,,
197,,pytorch,"Before I begin training, I initialize my loss function like this

loss_func = torch.nn.CrossEntropyLoss()

Then during the training I use

loss = loss_func(logits, train_y)

If I want to checkpoint my model during the training process by saving to file and resuming at the same point later, do I need to save loss_func too, or can I re-initialize with a clean slate and see the same behavior. Basically I'm curious if the loss_func object updates some attributes when it's used so it behaves differently on further uses, which I would want to save if that does happen.

On a similar note I am wondering if I can use the same loss function object with different models being trained at the same time without them interfering with each other, and I guess answering my original question will give me an answer to this too.",t2_95m8xo51,False,,0,False,Do I need to save the loss function if I'm checkpointing my model during training?,[],r/pytorch,False,6,,0,,,False,t3_k7hnvz,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1607237731.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Before I begin training, I initialize my loss function like this&lt;/p&gt;

&lt;p&gt;loss_func = torch.nn.CrossEntropyLoss()&lt;/p&gt;

&lt;p&gt;Then during the training I use&lt;/p&gt;

&lt;p&gt;loss = loss_func(logits, train_y)&lt;/p&gt;

&lt;p&gt;If I want to checkpoint my model during the training process by saving to file and resuming at the same point later, do I need to save loss_func too, or can I re-initialize with a clean slate and see the same behavior. Basically I&amp;#39;m curious if the loss_func object updates some attributes when it&amp;#39;s used so it behaves differently on further uses, which I would want to save if that does happen.&lt;/p&gt;

&lt;p&gt;On a similar note I am wondering if I can use the same loss function object with different models being trained at the same time without them interfering with each other, and I guess answering my original question will give me an answer to this too.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k7hnvz,True,,Known_Tumbleweed6116,,3,True,all_ads,False,[],False,,/r/pytorch/comments/k7hnvz/do_i_need_to_save_the_loss_function_if_im/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k7hnvz/do_i_need_to_save_the_loss_function_if_im/,7135,1607208931.0,0,,False,,,,,,,,
198,,pytorch,"I'm trying to perform SL or RL on a super simple model to predict the gain of a transfer function. I can use pandas and the Matlab python engine to correctly pass inputs to my model and extract workspace variables back out. However, then I try and wrap my NN around this training data, my gradients never update and my rained solution is essentially identical to my inputs, which is nonsensical. I'm wondering if anyone has experience performing SL or RL using pytorch that calls a Simulink model or if there is literature (examples) i could search for reference?",t2_2ww74vu,False,,0,False,Pytorch using Simulink model inputs,[],r/pytorch,False,6,,0,,,False,t3_k7h6o9,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1607236036.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to perform SL or RL on a super simple model to predict the gain of a transfer function. I can use pandas and the Matlab python engine to correctly pass inputs to my model and extract workspace variables back out. However, then I try and wrap my NN around this training data, my gradients never update and my rained solution is essentially identical to my inputs, which is nonsensical. I&amp;#39;m wondering if anyone has experience performing SL or RL using pytorch that calls a Simulink model or if there is literature (examples) i could search for reference?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k7h6o9,True,,arvarnargul,,3,True,all_ads,False,[],False,,/r/pytorch/comments/k7h6o9/pytorch_using_simulink_model_inputs/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k7h6o9/pytorch_using_simulink_model_inputs/,7135,1607207236.0,0,,False,,,,,,,,
199,,pytorch,"I think everything could be so much simpler with a more functional-programming style approach. I haven't coded in a mostly imperative style in ages - and suddenly it hits me just how confusing an imperative approach can be.  


The main pytorch tutorial is written like that, and the installation/dependencies is yet another pita to resolve.  


I'm really optimistic about pytorch/ML in general, but yeah this could be so much better and we would be able to reach a much bigger audience.",t2_c380w,False,,0,False,The amount of boilerplate and imperative magic-behind-the-scenes is infuriating in pytorch,[],r/pytorch,False,6,,0,,,False,t3_k79ulh,False,dark,0.4,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1607212585.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I think everything could be so much simpler with a more functional-programming style approach. I haven&amp;#39;t coded in a mostly imperative style in ages - and suddenly it hits me just how confusing an imperative approach can be.  &lt;/p&gt;

&lt;p&gt;The main pytorch tutorial is written like that, and the installation/dependencies is yet another pita to resolve.  &lt;/p&gt;

&lt;p&gt;I&amp;#39;m really optimistic about pytorch/ML in general, but yeah this could be so much better and we would be able to reach a much bigger audience.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k79ulh,True,,codecrunchie,,8,True,all_ads,False,[],False,,/r/pytorch/comments/k79ulh/the_amount_of_boilerplate_and_imperative/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k79ulh/the_amount_of_boilerplate_and_imperative/,7135,1607183785.0,0,,False,,,,,,,,
200,,pytorch,"Any help would be appreciated, I just got into coding and have scoured the internet for a solution plus tried all I can find. 

I'd rather not use anaconda because setting up environments confuses me, but if it's the best way I'll happily do so. 

Thank you!",t2_63og7owi,False,,0,False,"Hi, I'm struggling to install Pytorch through pycharm (pip)!",[],r/pytorch,False,6,,0,,,False,t3_k77xfv,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1607205548.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Any help would be appreciated, I just got into coding and have scoured the internet for a solution plus tried all I can find. &lt;/p&gt;

&lt;p&gt;I&amp;#39;d rather not use anaconda because setting up environments confuses me, but if it&amp;#39;s the best way I&amp;#39;ll happily do so. &lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k77xfv,True,,ninopeno,,8,True,all_ads,False,[],False,,/r/pytorch/comments/k77xfv/hi_im_struggling_to_install_pytorch_through/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k77xfv/hi_im_struggling_to_install_pytorch_through/,7135,1607176748.0,0,,False,,,,,,,,
201,,pytorch,"I'm currently using the following code:

```
dataloader = DataLoader(dataset=data, batch_size=batch_size, shuffle=True, num_workers=0)
```

If I enable CUDA with my model it works. But the moment I set `num_workers` to a value `&gt; 0`, it produces the following error:

```
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
```

Is this a known bug?

Performance is just as good as CPU with this setup. But I can confirm that the GPU is running with `nvidia-smi`

Stack trace:

```
Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/envs/research/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/home/ubuntu/anaconda3/envs/research/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/ubuntu/workspace/pyntrainer/pyntrainer/__main__.py"", line 107, in &lt;module&gt;
    net.train(training_data, epochs=epochs, lr=lr, batch_size=batch_size, loss=""aml"")
  File ""/home/ubuntu/workspace/pyntrainer/pyntrainer/./lib/autoencoder.py"", line 153, in train
    for i, (inputs, labels) in enumerate(dataloader):
  File ""/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 345, in __next__
    data = self._next_data()
  File ""/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 856, in _next_data
    return self._process_data(data)
  File ""/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 881, in _process_data
    data.reraise()
  File ""/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/_utils.py"", line 394, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py"", line 178, in _worker_loop
    data = fetcher.fetch(index)
  File ""/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in &lt;listcomp&gt;
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/ubuntu/workspace/pyntrainer/pyntrainer/./lib/./abstract_dataset.py"", line 10, in __getitem__
    return self.x[index], self.x[index]
RuntimeError: CUDA error: initialization error
```",t2_x6pudoo,False,,0,False,Can't use num_workers &gt; 0 with CUDA,[],r/pytorch,False,6,,0,,,False,t3_k6o3yh,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,1607101069.0,,[],{},,,True,,1607128035.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently using the following code:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
dataloader = DataLoader(dataset=data, batch_size=batch_size, shuffle=True, num_workers=0)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If I enable CUDA with my model it works. But the moment I set &lt;code&gt;num_workers&lt;/code&gt; to a value &lt;code&gt;&amp;gt; 0&lt;/code&gt;, it produces the following error:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Is this a known bug?&lt;/p&gt;

&lt;p&gt;Performance is just as good as CPU with this setup. But I can confirm that the GPU is running with &lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Stack trace:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
Traceback (most recent call last):
  File &amp;quot;/home/ubuntu/anaconda3/envs/research/lib/python3.8/runpy.py&amp;quot;, line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &amp;quot;/home/ubuntu/anaconda3/envs/research/lib/python3.8/runpy.py&amp;quot;, line 87, in _run_code
    exec(code, run_globals)
  File &amp;quot;/home/ubuntu/workspace/pyntrainer/pyntrainer/__main__.py&amp;quot;, line 107, in &amp;lt;module&amp;gt;
    net.train(training_data, epochs=epochs, lr=lr, batch_size=batch_size, loss=&amp;quot;aml&amp;quot;)
  File &amp;quot;/home/ubuntu/workspace/pyntrainer/pyntrainer/./lib/autoencoder.py&amp;quot;, line 153, in train
    for i, (inputs, labels) in enumerate(dataloader):
  File &amp;quot;/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/dataloader.py&amp;quot;, line 345, in __next__
    data = self._next_data()
  File &amp;quot;/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/dataloader.py&amp;quot;, line 856, in _next_data
    return self._process_data(data)
  File &amp;quot;/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/dataloader.py&amp;quot;, line 881, in _process_data
    data.reraise()
  File &amp;quot;/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/_utils.py&amp;quot;, line 394, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File &amp;quot;/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py&amp;quot;, line 178, in _worker_loop
    data = fetcher.fetch(index)
  File &amp;quot;/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py&amp;quot;, line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &amp;quot;/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py&amp;quot;, line 44, in &amp;lt;listcomp&amp;gt;
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &amp;quot;/home/ubuntu/workspace/pyntrainer/pyntrainer/./lib/./abstract_dataset.py&amp;quot;, line 10, in __getitem__
    return self.x[index], self.x[index]
RuntimeError: CUDA error: initialization error
&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k6o3yh,True,,ralampay,,12,True,all_ads,False,[],False,,/r/pytorch/comments/k6o3yh/cant_use_num_workers_0_with_cuda/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k6o3yh/cant_use_num_workers_0_with_cuda/,7135,1607099235.0,0,,False,,,,,,,,
202,,pytorch,"Hello community,  I want to reduce overfitting of my auto encoder because I don't have enough data to train on, and I found that making the decoder weight = transpose of the encoder weight will increase the efficiently of my model.

How can I do so in my model ?",t2_7l9ti89m,False,,0,False,I want to make my decoder weights equal the transpose of my encoder part,[],r/pytorch,False,6,,0,,,False,t3_k6pkza,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1607132440.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello community,  I want to reduce overfitting of my auto encoder because I don&amp;#39;t have enough data to train on, and I found that making the decoder weight = transpose of the encoder weight will increase the efficiently of my model.&lt;/p&gt;

&lt;p&gt;How can I do so in my model ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k6pkza,True,,rayanaay,,1,True,all_ads,False,[],False,,/r/pytorch/comments/k6pkza/i_want_to_make_my_decoder_weights_equal_the/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k6pkza/i_want_to_make_my_decoder_weights_equal_the/,7135,1607103640.0,0,,False,,,,,,,,
203,,pytorch,"Hello all, I am doing a project which involves me to get a comprehensive image set via augmentation, although the one which is the most hard for me to do (if possible) is augmenting based on distance. 

Basically, I want to be able to take a set of images which is considered to be like a seed for augmentation, and augment them based on distance to make it seem that the image is farther away from the point of the picture (along with other factors to augment them on).

I have been looking through torchvision.transforms for a viable transformation, but I can't seem to find one. I was wondering for those who may know more about different image processing/computer graphics algorithms to know if this is possible for me to do, or if I will have to go through the work of getting more data from different distances.",t2_311swib2,False,,0,False,Augmenting Image Dataset Based Upon Distance,[],r/pytorch,False,6,,0,,,False,t3_k5m8ps,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1606983957.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all, I am doing a project which involves me to get a comprehensive image set via augmentation, although the one which is the most hard for me to do (if possible) is augmenting based on distance. &lt;/p&gt;

&lt;p&gt;Basically, I want to be able to take a set of images which is considered to be like a seed for augmentation, and augment them based on distance to make it seem that the image is farther away from the point of the picture (along with other factors to augment them on).&lt;/p&gt;

&lt;p&gt;I have been looking through torchvision.transforms for a viable transformation, but I can&amp;#39;t seem to find one. I was wondering for those who may know more about different image processing/computer graphics algorithms to know if this is possible for me to do, or if I will have to go through the work of getting more data from different distances.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k5m8ps,True,,fenwicktreeguy,,2,True,all_ads,False,[],False,,/r/pytorch/comments/k5m8ps/augmenting_image_dataset_based_upon_distance/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k5m8ps/augmenting_image_dataset_based_upon_distance/,7135,1606955157.0,0,,False,,,,,,,,
204,,pytorch,"Newbie to PyTorch.

I'm following along with the third in this tutorial online [https://www.youtube.com/watch?v=GIsg-ZUy0MY&amp;ab\_channel=freeCodeCamp.org](https://www.youtube.com/watch?v=GIsg-ZUy0MY&amp;ab_channel=freeCodeCamp.org) for one-layer neural network with the MNIST dataset. I decided to try out the same thing for practice on the CIFAR-10 dataset, which can be accessed directly through the pytorch library.

I'm working in a google colab document and wrote a bunch of notes for myself. I worked my way through the tutorial pretty well.

I ran it a bunch of times,  and only got accuracies of around 35%, a little worse than even logistic regression on the same dataset.

Then, I tinkered around, I think I had forgot to switch to the GPU - that was the issue - and replayed, and was able to get my algorithm to a much better 67% accuracy, and after tinkering with the learning rate, I settled at 0.0005 being the best.

But now, when I run my algorithm, even on the GPU, I get stuff around the 35% (sometimes lower) mark. I looked in my diff, and even *restored the version that I was getting 67% on,* but when I ran it now, I got low accuracy.

[Heres my Colab Doc](https://colab.research.google.com/drive/14lhvXjjOHeHoVrQYjGxffsiNlB9dbncz?usp=sharing)

I'm pretty confused as to what this may be - any ideas? More importantly, any ideas on how to start getting high accuracies again?

Thanks a lot!

a",t2_3ke4xozp,False,,0,False,Neural network on CIFAR-10 and GPU showing wildly different accuracies on different sessions,[],r/pytorch,False,6,,0,,,False,t3_k4wsd2,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,1606867936.0,,[],{},self,,True,,1606893608.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Newbie to PyTorch.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m following along with the third in this tutorial online &lt;a href=""https://www.youtube.com/watch?v=GIsg-ZUy0MY&amp;amp;ab_channel=freeCodeCamp.org""&gt;https://www.youtube.com/watch?v=GIsg-ZUy0MY&amp;amp;ab_channel=freeCodeCamp.org&lt;/a&gt; for one-layer neural network with the MNIST dataset. I decided to try out the same thing for practice on the CIFAR-10 dataset, which can be accessed directly through the pytorch library.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working in a google colab document and wrote a bunch of notes for myself. I worked my way through the tutorial pretty well.&lt;/p&gt;

&lt;p&gt;I ran it a bunch of times,  and only got accuracies of around 35%, a little worse than even logistic regression on the same dataset.&lt;/p&gt;

&lt;p&gt;Then, I tinkered around, I think I had forgot to switch to the GPU - that was the issue - and replayed, and was able to get my algorithm to a much better 67% accuracy, and after tinkering with the learning rate, I settled at 0.0005 being the best.&lt;/p&gt;

&lt;p&gt;But now, when I run my algorithm, even on the GPU, I get stuff around the 35% (sometimes lower) mark. I looked in my diff, and even &lt;em&gt;restored the version that I was getting 67% on,&lt;/em&gt; but when I ran it now, I got low accuracy.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://colab.research.google.com/drive/14lhvXjjOHeHoVrQYjGxffsiNlB9dbncz?usp=sharing""&gt;Heres my Colab Doc&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m pretty confused as to what this may be - any ideas? More importantly, any ideas on how to start getting high accuracies again?&lt;/p&gt;

&lt;p&gt;Thanks a lot!&lt;/p&gt;

&lt;p&gt;a&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GwLm_dOukl0ncxHg3-AKQhvIrilFWfH3uwa_X0drfSQ.jpg?auto=webp&amp;s=1d1cd3f2b1f4a5c1867cd0b31d3da86a6609c94f', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/GwLm_dOukl0ncxHg3-AKQhvIrilFWfH3uwa_X0drfSQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=471858eefdffee9e3c241d8254e3ecaf6e0e13e4', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/GwLm_dOukl0ncxHg3-AKQhvIrilFWfH3uwa_X0drfSQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb379be16fce616b417a5527f421ccbeeb4f9671', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/GwLm_dOukl0ncxHg3-AKQhvIrilFWfH3uwa_X0drfSQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=21439f88abef3a9ceb93731dc8c6bdea77e4eda1', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'Tauh0aeoBTT0K_s4Y4fRV4BAix-hE5yoCH0D0TK70VI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k4wsd2,True,,kirbyburgers,,10,True,all_ads,False,[],False,,/r/pytorch/comments/k4wsd2/neural_network_on_cifar10_and_gpu_showing_wildly/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k4wsd2/neural_network_on_cifar10_and_gpu_showing_wildly/,7135,1606864808.0,0,,False,,,,,,,,
205,,pytorch,"We wrote this simple library [https://github.com/lab-ml/remote](https://github.com/lab-ml/remote) that can connect to remote computers and run PyTorch training jobs.

[Here's a guide on running distributed PyTorch 🔥 training with it](https://github.com/lab-ml/remote/blob/master/notes/pytorch-ddp.md)

Looking forward to any feedback that could help us improve the library. Thanks",t2_1jyhaoq,False,,0,False,A library to help run distributed PyTorch training on remote computers,[],r/pytorch,False,6,,0,,,False,t3_k4i7lc,False,dark,1.0,,public,10,0,{},,,False,[],,False,False,,{},,False,10,,False,self,False,,[],{},self,,True,,1606846166.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We wrote this simple library &lt;a href=""https://github.com/lab-ml/remote""&gt;https://github.com/lab-ml/remote&lt;/a&gt; that can connect to remote computers and run PyTorch training jobs.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/lab-ml/remote/blob/master/notes/pytorch-ddp.md""&gt;Here&amp;#39;s a guide on running distributed PyTorch 🔥 training with it&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Looking forward to any feedback that could help us improve the library. Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?auto=webp&amp;s=54844664dea21c579bf85bb9f77db2f366d995b5', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4935835b34570436c07bbf78c25cdcda892a4e7f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dca994f869c1d4df2b64aa8e606b9df61a08437', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/GfzuoFNtbpOCzXqUgz-IbK762rogPLY6rNEXH8TxQpE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7825efebb9ce056a4cec7a106daf8f326e9c2856', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'oze48py95sVXMx6dFac2s_Kf1E0eS-CySSSJBqwJb-o'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k4i7lc,True,,mlvpj,,0,True,all_ads,False,[],False,,/r/pytorch/comments/k4i7lc/a_library_to_help_run_distributed_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k4i7lc/a_library_to_help_run_distributed_pytorch/,7135,1606817366.0,0,,False,,,,,,,,
206,,pytorch,"Hello mates,

Im trying to do a cnn for chord recognition based on a paper that i found. Im a noob so im stuck.For now i have this:

    class CNN(nn.Module):
        def __init__(self):
            super(CNN, self).__init__()
            in_channels=1
            out_channels=1
            kernel_size=(1,16,6,25)
            self.conv1 = nn.Sequential(nn.Conv3d(in_channels,
                out_channels,kernel_size,stride=1,
                padding=2), nn.Conv3d(in_channels,
                out_channels,kernel_size,stride=1,
                padding=2))

I know maybe its wrong so any help will be usefull. Hope someone can let me a hand to continue with it because i dont know how to continue.

And this is the explanation on the paper to model the cnn:

https://preview.redd.it/iis0k4ivzk261.png?width=549&amp;format=png&amp;auto=webp&amp;s=e1bfe7a472c30442ac20dc522da8878be479c3c7",t2_ikir5ld,False,,0,False,need help with cnn pytorch,[],r/pytorch,False,6,,0,140.0,,False,t3_k4lc7w,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/R2V-GP7EuN_fdkWLj_zIZi3Zy5dEt1Zlh6ZYIKdW5NQ.jpg,False,,[],{},,,True,,1606859987.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello mates,&lt;/p&gt;

&lt;p&gt;Im trying to do a cnn for chord recognition based on a paper that i found. Im a noob so im stuck.For now i have this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        in_channels=1
        out_channels=1
        kernel_size=(1,16,6,25)
        self.conv1 = nn.Sequential(nn.Conv3d(in_channels,
            out_channels,kernel_size,stride=1,
            padding=2), nn.Conv3d(in_channels,
            out_channels,kernel_size,stride=1,
            padding=2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I know maybe its wrong so any help will be usefull. Hope someone can let me a hand to continue with it because i dont know how to continue.&lt;/p&gt;

&lt;p&gt;And this is the explanation on the paper to model the cnn:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/iis0k4ivzk261.png?width=549&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e1bfe7a472c30442ac20dc522da8878be479c3c7""&gt;https://preview.redd.it/iis0k4ivzk261.png?width=549&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e1bfe7a472c30442ac20dc522da8878be479c3c7&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k4lc7w,True,,Wh014m,,1,True,all_ads,False,[],False,,/r/pytorch/comments/k4lc7w/need_help_with_cnn_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k4lc7w/need_help_with_cnn_pytorch/,7135,1606831187.0,0,,False,,,,"{'iis0k4ivzk261': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 166, 'x': 108, 'u': 'https://preview.redd.it/iis0k4ivzk261.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=68fdede7b27d5671575dbb4787ff670d9330805f'}, {'y': 333, 'x': 216, 'u': 'https://preview.redd.it/iis0k4ivzk261.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b1ede225ac228927cea12898aa2382b851684c41'}, {'y': 493, 'x': 320, 'u': 'https://preview.redd.it/iis0k4ivzk261.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0defc11c04edf11bf9bd5111eb25bf1f177f8cfa'}], 's': {'y': 847, 'x': 549, 'u': 'https://preview.redd.it/iis0k4ivzk261.png?width=549&amp;format=png&amp;auto=webp&amp;s=e1bfe7a472c30442ac20dc522da8878be479c3c7'}, 'id': 'iis0k4ivzk261'}}",,,,
207,,pytorch," Hello, I want to do semantic segmentation with U-Net, with the data I have I'm able to remove the background automatically. Is it beneficial for the model feature extraction if remove the background and replace it with a white/geen/yellow ect background. Maybe use multiple colors mixed in the training set or something.",t2_128ob4,False,,0,False,Semantic segmentation - background removal preprocess?,[],r/pytorch,False,6,,0,,,False,t3_k493c8,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1606810531.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I want to do semantic segmentation with U-Net, with the data I have I&amp;#39;m able to remove the background automatically. Is it beneficial for the model feature extraction if remove the background and replace it with a white/geen/yellow ect background. Maybe use multiple colors mixed in the training set or something.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k493c8,True,,darvidas,,2,True,all_ads,False,[],False,,/r/pytorch/comments/k493c8/semantic_segmentation_background_removal/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k493c8/semantic_segmentation_background_removal/,7135,1606781731.0,0,,False,,,,,,,,
208,,pytorch,"Hello,

I am searching for a library that enables me to work with int8 based fixed point arithmetic in pytorch on the gpu, are there any libraries out there ?",t2_ys4cf,False,,0,False,are there any good fixed point arithmetic libraries for pytorch?,[],r/pytorch,False,6,,0,,,False,t3_k3vq9f,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,1606743241.0,,[],{},,,True,,1606771629.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am searching for a library that enables me to work with int8 based fixed point arithmetic in pytorch on the gpu, are there any libraries out there ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k3vq9f,True,,JohnAZoidberg77,,0,True,all_ads,False,[],False,,/r/pytorch/comments/k3vq9f/are_there_any_good_fixed_point_arithmetic/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k3vq9f/are_there_any_good_fixed_point_arithmetic/,7135,1606742829.0,0,,False,,,,,,,,
209,,pytorch,"Hi, After trying everything I can (and after hours of trying to debug the problems) I have been still unable to build Pytorch using Openblas and Cuda (on a AMD cpu + Nvidia gpu using Manjaro/Arch). I've tried deleting all mkl libraries from the system, using conda, building from source - but everything runs into error when I try to build it without MKL (conda has conflicts with nomkl installation and building from source runs into errors is using BLAS=OpenBLAS) and I can only build Pytorch with MKL (via pip or aur). This is harming my system's performance significantly.",t2_4jovit0z,False,,0,False,HOW to build PyTorch with OpenBLAS and CUDA on Linux??????,[],r/pytorch,False,6,,0,,,False,t3_k3zlxo,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1606783866.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, After trying everything I can (and after hours of trying to debug the problems) I have been still unable to build Pytorch using Openblas and Cuda (on a AMD cpu + Nvidia gpu using Manjaro/Arch). I&amp;#39;ve tried deleting all mkl libraries from the system, using conda, building from source - but everything runs into error when I try to build it without MKL (conda has conflicts with nomkl installation and building from source runs into errors is using BLAS=OpenBLAS) and I can only build Pytorch with MKL (via pip or aur). This is harming my system&amp;#39;s performance significantly.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k3zlxo,True,,vajra_,,2,True,all_ads,False,[],False,,/r/pytorch/comments/k3zlxo/how_to_build_pytorch_with_openblas_and_cuda_on/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k3zlxo/how_to_build_pytorch_with_openblas_and_cuda_on/,7135,1606755066.0,0,,False,,,,,,,,
210,,pytorch,,t2_1wi1cv4l,False,,0,False,Deep Learning with PyTorch: Free Course,[],r/pytorch,False,6,,0,105.0,,False,t3_k3m54s,False,dark,0.87,,public,11,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/5ioMqzMRFgM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'PyTorch Basics and Gradient Descent | Deep Learning with PyTorch: Zero to GANs | Part 1 of 6', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/5ioMqzMRFgM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'freeCodeCamp.org', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/5ioMqzMRFgM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC8butISFwT-Wl7EV0hUK0BQ'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/5ioMqzMRFgM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/k3m54s', 'height': 338}",,False,11,,False,https://b.thumbs.redditmedia.com/tLMT-3f29DbtDs8PBDLkvzDljL79lKvSRKOlaBSyegI.jpg,False,,[],{},rich:video,,False,,1606729232.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/B1Qh13UsXig7YqAOUxSX8RNopTYQw8Y1F4Xv0wjahPs.jpg?auto=webp&amp;s=168e67ebb50271dac7b04f6cf294329c53e1f3ec', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/B1Qh13UsXig7YqAOUxSX8RNopTYQw8Y1F4Xv0wjahPs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e66df60215ced15295be44178e5f41bdaecf699c', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/B1Qh13UsXig7YqAOUxSX8RNopTYQw8Y1F4Xv0wjahPs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9232bd1ba0492a74edefcc4dcfcb18b93f2650d4', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/B1Qh13UsXig7YqAOUxSX8RNopTYQw8Y1F4Xv0wjahPs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3211d0ee7654e5994149b32d2cce983fcd184bdf', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'n64CxOhYSbG4R3Vxidz0o24F-lugcnV_6lb-UNDLfZY'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k3m54s,True,,ConfidentMushroom,,0,True,all_ads,False,[],False,,/r/pytorch/comments/k3m54s/deep_learning_with_pytorch_free_course/,all_ads,False,https://www.youtube.com/watch?v=5ioMqzMRFgM&amp;feature=youtu.be,7135,1606700432.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'PyTorch Basics and Gradient Descent | Deep Learning with PyTorch: Zero to GANs | Part 1 of 6', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/5ioMqzMRFgM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'freeCodeCamp.org', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/5ioMqzMRFgM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC8butISFwT-Wl7EV0hUK0BQ'}}",False,https://www.youtube.com/watch?v=5ioMqzMRFgM&amp;feature=youtu.be,,,,,,,
211,,pytorch,"In this tutorial, we'll deal with a fundamental challenge in Machine Learning and Deep Learning that is easier said than done: loading and handling different types of data. Specifically, we'll cover:

* Looking at built-in datasets in-depth
* Using the Dataloader class
* Using GPUs vs. CPUs
* Transforming and rescaling images
* Loading and visualizing built-in datasets
* Building and loading custom datasets with PyTorch

Tutorial link: [https://blog.paperspace.com/dataloaders-abstractions-pytorch/](https://blog.paperspace.com/dataloaders-abstractions-pytorch/)

Run the code on a free GPU with Gradient Community Notebooks: [https://ml-showcase.paperspace.com/projects/working-with-data-in-pytorch](https://ml-showcase.paperspace.com/projects/working-with-data-in-pytorch)",t2_15en0l,False,,0,False,[Tutorial] A Comprehensive Guide to the DataLoader Class and Abstractions in PyTorch,[],r/pytorch,False,6,,0,,,False,t3_k07uzq,False,dark,0.9,,public,14,2,{},,,False,[],,False,False,,{},,False,14,,False,self,False,,[],{},self,,True,,1606263055.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In this tutorial, we&amp;#39;ll deal with a fundamental challenge in Machine Learning and Deep Learning that is easier said than done: loading and handling different types of data. Specifically, we&amp;#39;ll cover:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Looking at built-in datasets in-depth&lt;/li&gt;
&lt;li&gt;Using the Dataloader class&lt;/li&gt;
&lt;li&gt;Using GPUs vs. CPUs&lt;/li&gt;
&lt;li&gt;Transforming and rescaling images&lt;/li&gt;
&lt;li&gt;Loading and visualizing built-in datasets&lt;/li&gt;
&lt;li&gt;Building and loading custom datasets with PyTorch&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tutorial link: &lt;a href=""https://blog.paperspace.com/dataloaders-abstractions-pytorch/""&gt;https://blog.paperspace.com/dataloaders-abstractions-pytorch/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Run the code on a free GPU with Gradient Community Notebooks: &lt;a href=""https://ml-showcase.paperspace.com/projects/working-with-data-in-pytorch""&gt;https://ml-showcase.paperspace.com/projects/working-with-data-in-pytorch&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ZhjAzsyp9-nmewdJDnqLlvnL6gGEe27nrqEozSbbm6g.jpg?auto=webp&amp;s=950168d3da284a3de229cdb05b0ae9a08dcec6f7', 'width': 2000, 'height': 1340}, 'resolutions': [{'url': 'https://external-preview.redd.it/ZhjAzsyp9-nmewdJDnqLlvnL6gGEe27nrqEozSbbm6g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd8f57617fe8ed077a0ccaed4e9f96b7e8ffe059', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/ZhjAzsyp9-nmewdJDnqLlvnL6gGEe27nrqEozSbbm6g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9698c7256ce6a8d4c75b26f1521f71835258a186', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/ZhjAzsyp9-nmewdJDnqLlvnL6gGEe27nrqEozSbbm6g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3b4c7432beb88d369265556a664945b011a3d160', 'width': 320, 'height': 214}, {'url': 'https://external-preview.redd.it/ZhjAzsyp9-nmewdJDnqLlvnL6gGEe27nrqEozSbbm6g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d128883ecc815bb9fa2712e821dcf0aec71d5a65', 'width': 640, 'height': 428}, {'url': 'https://external-preview.redd.it/ZhjAzsyp9-nmewdJDnqLlvnL6gGEe27nrqEozSbbm6g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a2edf21c968dc44926cab50cb6e83254f29d6528', 'width': 960, 'height': 643}, {'url': 'https://external-preview.redd.it/ZhjAzsyp9-nmewdJDnqLlvnL6gGEe27nrqEozSbbm6g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5c5908dd8909c9ea3190650a3f2cb6ebc8f44194', 'width': 1080, 'height': 723}], 'variants': {}, 'id': '-wDEEqg0ApO-KebphyvRvijvHShTkRpTeKuQf6OoycM'}], 'enabled': False}","[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 300, 'id': 'award_28e8196b-d4e9-45bc-b612-cd4c7d3ed4b3', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/35d17tf5e5f61_oldrocketlike.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/35d17tf5e5f61_oldrocketlike.png?width=16&amp;height=16&amp;auto=webp&amp;s=9d714b25ca25d05e3310bc60bc1714ddf7951331', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/35d17tf5e5f61_oldrocketlike.png?width=32&amp;height=32&amp;auto=webp&amp;s=d584b15c8e17d61fa8ae319a91d351c8fe35b918', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/35d17tf5e5f61_oldrocketlike.png?width=48&amp;height=48&amp;auto=webp&amp;s=d9fb2c025611a15e6bb4437734f92db99b93fd12', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/35d17tf5e5f61_oldrocketlike.png?width=64&amp;height=64&amp;auto=webp&amp;s=744fb200d76bf21f6e023ba98d3b4189b34973e3', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/35d17tf5e5f61_oldrocketlike.png?width=128&amp;height=128&amp;auto=webp&amp;s=c180572afbc080622a8ac8441c3bc5214597d05a', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""When an upvote just isn't enough, smash the Rocket Like."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Rocket Like', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/9fmmyy3c68361_RocketLikeSanta.png?width=16&amp;height=16&amp;auto=webp&amp;s=24fc4d912e595c3fed2ce0deef1c13f70df56056', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/9fmmyy3c68361_RocketLikeSanta.png?width=32&amp;height=32&amp;auto=webp&amp;s=f9d869602e0d8b719186cc603864a42699e5c96e', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/9fmmyy3c68361_RocketLikeSanta.png?width=48&amp;height=48&amp;auto=webp&amp;s=b223ac8fdd206b683b840c2782307c3f01b04fb7', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/9fmmyy3c68361_RocketLikeSanta.png?width=64&amp;height=64&amp;auto=webp&amp;s=a0b840c6ecdee904012a6c53c40194733b72bca8', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/9fmmyy3c68361_RocketLikeSanta.png?width=128&amp;height=128&amp;auto=webp&amp;s=b2e2ca67e067f82ff9b4ea1fe1b39395ca622894', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/9fmmyy3c68361_RocketLikeSanta.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k07uzq,True,,hellopaperspace,,0,True,all_ads,False,[],False,,/r/pytorch/comments/k07uzq/tutorial_a_comprehensive_guide_to_the_dataloader/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k07uzq/tutorial_a_comprehensive_guide_to_the_dataloader/,7135,1606234255.0,0,,False,,,,,,,,
212,,pytorch,"I'm trying to implement DNF-Net ( [https://arxiv.org/abs/2006.06465](https://arxiv.org/abs/2006.06465) ) in pytorch.

In the paper, the authors used differentiable sign function, using the following trick:

Calculate the step function *exactly* in forward pass, use a differentiable proxy in backward pass. So:

*  sign(x) - forward pass
* tanh(x) - backward pass

How to create such function in pytorch?

I will be calling this function on a 1 dimensional tensor, if it helps.

Also, do tell your thoughts about this architecture if you happen to know it.",t2_2pqeaeed,False,,0,False,How to implement differentiable sign function?,[],r/pytorch,False,6,,0,,,False,t3_k0aen0,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1606270577.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to implement DNF-Net ( &lt;a href=""https://arxiv.org/abs/2006.06465""&gt;https://arxiv.org/abs/2006.06465&lt;/a&gt; ) in pytorch.&lt;/p&gt;

&lt;p&gt;In the paper, the authors used differentiable sign function, using the following trick:&lt;/p&gt;

&lt;p&gt;Calculate the step function &lt;em&gt;exactly&lt;/em&gt; in forward pass, use a differentiable proxy in backward pass. So:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; sign(x) - forward pass&lt;/li&gt;
&lt;li&gt;tanh(x) - backward pass&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;How to create such function in pytorch?&lt;/p&gt;

&lt;p&gt;I will be calling this function on a 1 dimensional tensor, if it helps.&lt;/p&gt;

&lt;p&gt;Also, do tell your thoughts about this architecture if you happen to know it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,k0aen0,True,,Ekesmar,,1,True,all_ads,False,[],False,,/r/pytorch/comments/k0aen0/how_to_implement_differentiable_sign_function/,all_ads,False,https://www.reddit.com/r/pytorch/comments/k0aen0/how_to_implement_differentiable_sign_function/,7135,1606241777.0,0,,False,,,,,,,,
213,,pytorch,"Hi all,

I trained a CNN in Pytorch to classify images as benign or malignant and calculated the accuracy for a training and a testing set. Now I wanna additionally calculate the recall. The recall is defined as TP/TP+FN. Therefore, I wanna extract the cases where my labels match 0 and then calculate the fraction of where my prediction also equals 0 at these places. Could you maybe give me a hint on how to do this in Pytorch? I have insered an image of my code here.

https://preview.redd.it/i7fzlrf7zz061.png?width=626&amp;format=png&amp;auto=webp&amp;s=eeb91b0faf48f0fc65746021fd898015fe25557a",t2_77bwjx2r,False,,0,False,[Q] How can I extract the positive labels to calculate the recall ?,[],r/pytorch,False,6,,0,117.0,,False,t3_jzi8m8,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/_d1XD8c-8RO6e8saAX3PXY88BgovIUjBQ--Q1rMvJKQ.jpg,False,,[],{},,,True,,1606169355.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I trained a CNN in Pytorch to classify images as benign or malignant and calculated the accuracy for a training and a testing set. Now I wanna additionally calculate the recall. The recall is defined as TP/TP+FN. Therefore, I wanna extract the cases where my labels match 0 and then calculate the fraction of where my prediction also equals 0 at these places. Could you maybe give me a hint on how to do this in Pytorch? I have insered an image of my code here.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/i7fzlrf7zz061.png?width=626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eeb91b0faf48f0fc65746021fd898015fe25557a""&gt;https://preview.redd.it/i7fzlrf7zz061.png?width=626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eeb91b0faf48f0fc65746021fd898015fe25557a&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jzi8m8,True,,AbroadPotential,,2,True,all_ads,False,[],False,,/r/pytorch/comments/jzi8m8/q_how_can_i_extract_the_positive_labels_to/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jzi8m8/q_how_can_i_extract_the_positive_labels_to/,7135,1606140555.0,0,,False,,,,"{'i7fzlrf7zz061': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 90, 'x': 108, 'u': 'https://preview.redd.it/i7fzlrf7zz061.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2b7de396db0c880573c20f56f3ef2594254b4ce'}, {'y': 181, 'x': 216, 'u': 'https://preview.redd.it/i7fzlrf7zz061.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a23774b706f25d17f485ade97341f3b88f7aa35c'}, {'y': 268, 'x': 320, 'u': 'https://preview.redd.it/i7fzlrf7zz061.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=14823990907acf5335a644b69308994619f4580e'}], 's': {'y': 526, 'x': 626, 'u': 'https://preview.redd.it/i7fzlrf7zz061.png?width=626&amp;format=png&amp;auto=webp&amp;s=eeb91b0faf48f0fc65746021fd898015fe25557a'}, 'id': 'i7fzlrf7zz061'}}",,,,
214,,pytorch," Could someone explain to me why pytorch has both **init** and forward function in a model definition, and what each one does, and why do we need 2 function rather than just one?",t2_2jezkxap,False,,0,False,Why does a model definition have both a __init__ and forward function?,[],r/pytorch,False,6,,0,,,False,t3_jyneu4,False,dark,0.58,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1606039229.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Could someone explain to me why pytorch has both &lt;strong&gt;init&lt;/strong&gt; and forward function in a model definition, and what each one does, and why do we need 2 function rather than just one?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jyneu4,True,,Stanley_C,,6,True,all_ads,False,[],False,,/r/pytorch/comments/jyneu4/why_does_a_model_definition_have_both_a_init_and/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jyneu4/why_does_a_model_definition_have_both_a_init_and/,7135,1606010429.0,1,,False,,,,,,,,
215,,pytorch,[https://analyticsindiamag.com/deep-dive-in-datasets-for-machine-translation-in-nlp-using-tensorflow-and-pytorch/](https://analyticsindiamag.com/deep-dive-in-datasets-for-machine-translation-in-nlp-using-tensorflow-and-pytorch/),t2_40d0zt4s,False,,0,False,Deep Dive in Datasets for Machine translation in NLP Using TensorFlow and PyTorch,[],r/pytorch,False,6,,0,,,False,t3_jy87s7,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1605980786.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://analyticsindiamag.com/deep-dive-in-datasets-for-machine-translation-in-nlp-using-tensorflow-and-pytorch/""&gt;https://analyticsindiamag.com/deep-dive-in-datasets-for-machine-translation-in-nlp-using-tensorflow-and-pytorch/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xSitVsSWDYoy1Y8nehv7yt5DftRPtf7Ezfpu_UCAUNo.jpg?auto=webp&amp;s=7dbe977cf0efabf857e00168e197da39df21813f', 'width': 512, 'height': 303}, 'resolutions': [{'url': 'https://external-preview.redd.it/xSitVsSWDYoy1Y8nehv7yt5DftRPtf7Ezfpu_UCAUNo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=74f57cc11a53ec58d6aec02f8da80e178e9e5761', 'width': 108, 'height': 63}, {'url': 'https://external-preview.redd.it/xSitVsSWDYoy1Y8nehv7yt5DftRPtf7Ezfpu_UCAUNo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d71f944357151e5e5df6b73a0c28a192e7f74aa8', 'width': 216, 'height': 127}, {'url': 'https://external-preview.redd.it/xSitVsSWDYoy1Y8nehv7yt5DftRPtf7Ezfpu_UCAUNo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e3aad1385e8caef13bbfbe6ad91309b84091f379', 'width': 320, 'height': 189}], 'variants': {}, 'id': '_pUtqJJ7KJmd0yKbGs3oSMhthkX1_MFCmbw8GFBAWZw'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jy87s7,True,,analyticsindiam,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jy87s7/deep_dive_in_datasets_for_machine_translation_in/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jy87s7/deep_dive_in_datasets_for_machine_translation_in/,7135,1605951986.0,0,,False,,,,,,,,
216,,pytorch,"To connect the optimizer and model together, we pass in \`model.parameters()\` as an arguement to the optimizer, so that it can zero out the gradients and perform the step. Whereas I wasn't able to understand how does the backprop weights from the loss function reflect onto the model, when we never connected them ?",t2_5n6xtijf,False,,0,False,How do the loss function and model parameters connect with each other ?,[],r/pytorch,False,6,,0,,,False,t3_jxw02k,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1605930328.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;To connect the optimizer and model together, we pass in `model.parameters()` as an arguement to the optimizer, so that it can zero out the gradients and perform the step. Whereas I wasn&amp;#39;t able to understand how does the backprop weights from the loss function reflect onto the model, when we never connected them ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jxw02k,True,,the-machine-learner,,3,True,all_ads,False,[],False,,/r/pytorch/comments/jxw02k/how_do_the_loss_function_and_model_parameters/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jxw02k/how_do_the_loss_function_and_model_parameters/,7135,1605901528.0,0,,False,,,,,,,,
217,,pytorch,"I have a basic transformer model:

```
class Reconstructor(nn.Module):
    """"""Container module with an encoder, a recurrent or transformer module, and a decoder.""""""

    def __init__(self, input_dim, output_dim, dim_embedding, num_layers=4, nhead=8, dim_feedforward=2048, dropout=0.5):
        super(Reconstructor, self).__init__()

        self.model_type = 'Transformer'
        self.embedding = nn.Linear(input_dim, dim_embedding)
        self.pos_encoder = PositionalEncoding(d_model=dim_embedding, dropout=dropout)
        self.transformer = nn.Transformer(d_model=dim_embedding, nhead=nhead, dim_feedforward=dim_feedforward, num_encoder_layers=num_layers, num_decoder_layers=num_layers)

        self.decoder = nn.Linear(dim_embedding, output_dim)

        self.init_weights()

    def _generate_square_subsequent_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def init_weights(self):
        initrange = 0.1
        nn.init.zeros_(self.decoder.weight)
        nn.init.uniform_(self.decoder.weight, -initrange, initrange)

    def forward(self, src, tgt, tgt_mask=None):
        embedding_inp = self.embedding(src).permute(1, 0, 2)
        embedding_out = self.embedding(tgt).permute(1, 0, 2)

        pe_src = embedding_inp + self.pos_encoder(embedding_inp)  # (seq, batch, features)
        pe_tgt = embedding_out + self.pos_encoder(embedding_out)  # (seq, batch, features)
        transformer_output = self.transformer(pe_src, pe_tgt)
        decoder_output = self.decoder(transformer_output).permute(1, 0, 2)
        decoder_output = self.decoder_act_fn(decoder_output)
        return decoder_output


    def initHidden(self):
        return torch.zeros(1, 1, 128)
```

During training, I have:
```
tgt_mask = gen_nopeek_mask(ground_truth.size(1)).to(DEVICE)
tgt = torch.ones(ground_truth.size()).to(DEVICE) * -1
tgt[:, 1:, :] = ground_truth[:, 0:-1, :]
pred = model(x, tgt, tgt_mask=tgt_mask)
```

which I believe will do the training in parallel.

During inference, I have:
```

        tgt = torch.ones(x.size(0), 1, 128) * -1

        for i in range(x.size(1)):
            print(i)
            tgt = torch.randn(tgt.size())
            pred = reconstruct_spect_model(x, tgt)

            tgt = torch.cat((tgt, pred[:, -1, :].unsqueeze(1)), 1)
```

given that my sequence has 499 steps, it takes A WHILE to go through autoregressively. Is there any way to speed this up?",t2_bdnrw,False,,0,False,When using nn.Transformer for inference is there any way to speed up the autoregressive generation?,[],r/pytorch,False,6,,0,,,False,t3_jxt55g,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1605921437.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a basic transformer model:&lt;/p&gt;

&lt;p&gt;```
class Reconstructor(nn.Module):
    &amp;quot;&amp;quot;&amp;quot;Container module with an encoder, a recurrent or transformer module, and a decoder.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def __init__(self, input_dim, output_dim, dim_embedding, num_layers=4, nhead=8, dim_feedforward=2048, dropout=0.5):
    super(Reconstructor, self).__init__()

    self.model_type = &amp;#39;Transformer&amp;#39;
    self.embedding = nn.Linear(input_dim, dim_embedding)
    self.pos_encoder = PositionalEncoding(d_model=dim_embedding, dropout=dropout)
    self.transformer = nn.Transformer(d_model=dim_embedding, nhead=nhead, dim_feedforward=dim_feedforward, num_encoder_layers=num_layers, num_decoder_layers=num_layers)

    self.decoder = nn.Linear(dim_embedding, output_dim)

    self.init_weights()

def _generate_square_subsequent_mask(self, sz):
    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float(&amp;#39;-inf&amp;#39;)).masked_fill(mask == 1, float(0.0))
    return mask

def init_weights(self):
    initrange = 0.1
    nn.init.zeros_(self.decoder.weight)
    nn.init.uniform_(self.decoder.weight, -initrange, initrange)

def forward(self, src, tgt, tgt_mask=None):
    embedding_inp = self.embedding(src).permute(1, 0, 2)
    embedding_out = self.embedding(tgt).permute(1, 0, 2)

    pe_src = embedding_inp + self.pos_encoder(embedding_inp)  # (seq, batch, features)
    pe_tgt = embedding_out + self.pos_encoder(embedding_out)  # (seq, batch, features)
    transformer_output = self.transformer(pe_src, pe_tgt)
    decoder_output = self.decoder(transformer_output).permute(1, 0, 2)
    decoder_output = self.decoder_act_fn(decoder_output)
    return decoder_output


def initHidden(self):
    return torch.zeros(1, 1, 128)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;During training, I have:
&lt;code&gt;
tgt_mask = gen_nopeek_mask(ground_truth.size(1)).to(DEVICE)
tgt = torch.ones(ground_truth.size()).to(DEVICE) * -1
tgt[:, 1:, :] = ground_truth[:, 0:-1, :]
pred = model(x, tgt, tgt_mask=tgt_mask)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;which I believe will do the training in parallel.&lt;/p&gt;

&lt;p&gt;During inference, I have:
```&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    tgt = torch.ones(x.size(0), 1, 128) * -1

    for i in range(x.size(1)):
        print(i)
        tgt = torch.randn(tgt.size())
        pred = reconstruct_spect_model(x, tgt)

        tgt = torch.cat((tgt, pred[:, -1, :].unsqueeze(1)), 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;given that my sequence has 499 steps, it takes A WHILE to go through autoregressively. Is there any way to speed this up?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jxt55g,True,,shamoons,,3,True,all_ads,False,[],False,,/r/pytorch/comments/jxt55g/when_using_nntransformer_for_inference_is_there/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jxt55g/when_using_nntransformer_for_inference_is_there/,7135,1605892637.0,0,,False,,,,,,,,
218,,pytorch,"Hi Pytorch Community,

I am really new to Pytorch (some hours now). I have two datasets of images (malignant vs benign pictures of breast cancer). I have uploaded them into Google Colab and now I have all my variables for malignant in one variable and for benign in another. How can I ""label"" them, such that I can mix them together afterwards into training and testing data?",t2_77bwjx2r,False,,0,False,How can I label my data?,[],r/pytorch,False,6,,0,,,False,t3_jxv8j5,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1605927815.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Pytorch Community,&lt;/p&gt;

&lt;p&gt;I am really new to Pytorch (some hours now). I have two datasets of images (malignant vs benign pictures of breast cancer). I have uploaded them into Google Colab and now I have all my variables for malignant in one variable and for benign in another. How can I &amp;quot;label&amp;quot; them, such that I can mix them together afterwards into training and testing data?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jxv8j5,True,,AbroadPotential,,9,True,all_ads,False,[],False,,/r/pytorch/comments/jxv8j5/how_can_i_label_my_data/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jxv8j5/how_can_i_label_my_data/,7135,1605899015.0,0,,False,,,,,,,,
219,,pytorch,How is the performance of AMD CPUs vs Intel? Are there any problems faced by AMD due to the use of MKL in pytorch?,,False,,0,False,Intel vs AMD cpu performance,[],r/pytorch,False,6,,0,,,False,t3_jx6qzg,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,,self,False,,,{},,,True,,1605834918.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How is the performance of AMD CPUs vs Intel? Are there any problems faced by AMD due to the use of MKL in pytorch?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jx6qzg,True,,[deleted],,4,True,all_ads,False,[],,dark,/r/pytorch/comments/jx6qzg/intel_vs_amd_cpu_performance/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jx6qzg/intel_vs_amd_cpu_performance/,7135,1605806118.0,0,,False,,,,,,,,
220,,pytorch,,t2_8m9lhs25,False,,0,False,GuitarML/PedalNetRT - PyTorch for emulating guitar amplifiers/effects (for anyone interested in deep learning on audio),[],r/pytorch,False,6,,0,140.0,,False,t3_jx1eip,False,dark,0.95,,public,14,0,{},140.0,,False,[],,False,False,,{},,False,14,,False,https://b.thumbs.redditmedia.com/7Vtya03TNlCUMGV7ADt94RDjC4jUzAMkTrTwsOU-GCw.jpg,False,,[],{},link,,False,,1605815807.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/-jaa8jeoJHela6jKb08tzSvktVDTmm9UgwHZopgEKmY.jpg?auto=webp&amp;s=1ce87e7d3a0ea4ae9281155b7282f3e826a11b81', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/-jaa8jeoJHela6jKb08tzSvktVDTmm9UgwHZopgEKmY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=26495a8980f0b7035ab14eb55df0e886c566123c', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/-jaa8jeoJHela6jKb08tzSvktVDTmm9UgwHZopgEKmY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=76c380c4ef968c6510f6f0ab216cf4b7027daff0', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/-jaa8jeoJHela6jKb08tzSvktVDTmm9UgwHZopgEKmY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a40c4e857fcea960e173a5a8e6f57ed2982f7f8', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'aEtZBAGgDlPkVCBuvw6rvMU8FioDDP6g4klBnxBdRCk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jx1eip,True,,GuitarML,,1,True,all_ads,False,[],False,,/r/pytorch/comments/jx1eip/guitarmlpedalnetrt_pytorch_for_emulating_guitar/,all_ads,False,https://github.com/GuitarML/PedalNetRT,7135,1605787007.0,0,,False,https://github.com/GuitarML/PedalNetRT,,,,,,,
221,,pytorch,"I haven't received my M1, but I see that TensorFlow has optimized  for training on M1, so I am looking forward to the performance of Pytorch on M1, although it may be weaker than on x86.",t2_7n73g0st,False,,0,False,What is the performance of Pytorch running on Apple M1?,[],r/pytorch,False,6,,0,,,False,t3_jwye04,False,dark,0.96,,public,22,0,{},,,False,[],,False,False,,{},,False,22,,False,self,False,,[],{},,,True,,1605799685.0,text,6,,,text,self.pytorch,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I haven&amp;#39;t received my M1, but I see that TensorFlow has optimized  for training on M1, so I am looking forward to the performance of Pytorch on M1, although it may be weaker than on x86.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jwye04,True,,Levi_ww,,2,True,all_ads,False,[],False,,/r/pytorch/comments/jwye04/what_is_the_performance_of_pytorch_running_on/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jwye04/what_is_the_performance_of_pytorch_running_on/,7135,1605770885.0,0,,False,,,,,,,,
222,,pytorch,"Hey there,

I've been working on a concept that allows you to upload your ML models (Tensorflow, Pytorch, etc) and turn them into sharable web apps super quickly, without writing code (for the deployment/ UI part at least).

Here's the landing page I threw together for a more in-depth description of what I'm imagining it could look like: [https://www.getaiko.com/](https://www.getaiko.com/?fbclid=IwAR2eWORlgX2FlBpl9Y42R-_b1wzNuIcfWTaItESr1BP6WmAMmN994SugD78)

Some questions that would help guide me on this project:

* What stands in your way of making your model useful/ getting it out into the world?
* What are your pain points and major goals when it comes to getting your model deployed?",t2_gmwuj,False,,0,False,What stands in the way of making your model useful?,[],r/pytorch,False,6,,0,,,False,t3_jx81zd,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1605838832.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey there,&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been working on a concept that allows you to upload your ML models (Tensorflow, Pytorch, etc) and turn them into sharable web apps super quickly, without writing code (for the deployment/ UI part at least).&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s the landing page I threw together for a more in-depth description of what I&amp;#39;m imagining it could look like: &lt;a href=""https://www.getaiko.com/?fbclid=IwAR2eWORlgX2FlBpl9Y42R-_b1wzNuIcfWTaItESr1BP6WmAMmN994SugD78""&gt;https://www.getaiko.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Some questions that would help guide me on this project:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What stands in your way of making your model useful/ getting it out into the world?&lt;/li&gt;
&lt;li&gt;What are your pain points and major goals when it comes to getting your model deployed?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4fUh-EzSbL1yhOdtEEWX2S40e4dFR6IiSMfdKMPP7og.jpg?auto=webp&amp;s=1e7635e7154f5f207d6ebcfb7e6270d013600ade', 'width': 2732, 'height': 1566}, 'resolutions': [{'url': 'https://external-preview.redd.it/4fUh-EzSbL1yhOdtEEWX2S40e4dFR6IiSMfdKMPP7og.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1bf839fa2aae81a3b393baaf389710976dd0879d', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/4fUh-EzSbL1yhOdtEEWX2S40e4dFR6IiSMfdKMPP7og.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c5e49c655af057f7c1f28e38f1067f080c66acb', 'width': 216, 'height': 123}, {'url': 'https://external-preview.redd.it/4fUh-EzSbL1yhOdtEEWX2S40e4dFR6IiSMfdKMPP7og.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=871f558d804628c156dc78629a3d461934ce2528', 'width': 320, 'height': 183}, {'url': 'https://external-preview.redd.it/4fUh-EzSbL1yhOdtEEWX2S40e4dFR6IiSMfdKMPP7og.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=31517e27af6fbefd593a5982ed58415fc5d1dcfe', 'width': 640, 'height': 366}, {'url': 'https://external-preview.redd.it/4fUh-EzSbL1yhOdtEEWX2S40e4dFR6IiSMfdKMPP7og.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a255426981646fe1bd71ee58ba311f4fda25a154', 'width': 960, 'height': 550}, {'url': 'https://external-preview.redd.it/4fUh-EzSbL1yhOdtEEWX2S40e4dFR6IiSMfdKMPP7og.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=14b42403636e1a21e4bd4036d66cf556fc7f0d58', 'width': 1080, 'height': 619}], 'variants': {}, 'id': 'I6mP4-ETJQSAvUhl9uzx0lvHNypGoFtEGt-NtcXsxu4'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jx81zd,True,,BillCrum,,1,True,all_ads,False,[],False,,/r/pytorch/comments/jx81zd/what_stands_in_the_way_of_making_your_model_useful/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jx81zd/what_stands_in_the_way_of_making_your_model_useful/,7135,1605810032.0,0,,False,,,,,,,,
223,,pytorch,"PyTorch has recently released four new PyTorch prototype features. The first three enable mobile machine-learning developers to execute models on the full set of hardware (HW) engines making up a system-on-chip (SOC) system. This allows developers to optimize their model execution for a unique performance, power, and system-level concurrency.

Summary: [https://www.marktechpost.com/2020/11/18/pytorch-releases-prototype-features-to-execute-machine-learning-models-on-device-hardware-engines/](https://www.marktechpost.com/2020/11/18/pytorch-releases-prototype-features-to-execute-machine-learning-models-on-device-hardware-engines/) 

GitHub: [https://github.com/pytorch/tutorials/tree/master/prototype\_source](https://github.com/pytorch/tutorials/tree/master/prototype_source) 

Source: [https://pytorch.org/blog/prototype-features-now-available-apis-for-hardware-accelerated-mobile-and-arm64-builds/](https://pytorch.org/blog/prototype-features-now-available-apis-for-hardware-accelerated-mobile-and-arm64-builds/)",t2_2wsvqwhg,False,,0,False,PyTorch Releases Prototype Features To Execute Machine Learning Models On-Device Hardware Engines,[],r/pytorch,False,6,,0,,,False,t3_jwyg5z,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},self,,True,,1605799988.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;PyTorch has recently released four new PyTorch prototype features. The first three enable mobile machine-learning developers to execute models on the full set of hardware (HW) engines making up a system-on-chip (SOC) system. This allows developers to optimize their model execution for a unique performance, power, and system-level concurrency.&lt;/p&gt;

&lt;p&gt;Summary: &lt;a href=""https://www.marktechpost.com/2020/11/18/pytorch-releases-prototype-features-to-execute-machine-learning-models-on-device-hardware-engines/""&gt;https://www.marktechpost.com/2020/11/18/pytorch-releases-prototype-features-to-execute-machine-learning-models-on-device-hardware-engines/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;GitHub: &lt;a href=""https://github.com/pytorch/tutorials/tree/master/prototype_source""&gt;https://github.com/pytorch/tutorials/tree/master/prototype_source&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Source: &lt;a href=""https://pytorch.org/blog/prototype-features-now-available-apis-for-hardware-accelerated-mobile-and-arm64-builds/""&gt;https://pytorch.org/blog/prototype-features-now-available-apis-for-hardware-accelerated-mobile-and-arm64-builds/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/VcPVDYJiK_Rqt02Oc_QGLmR5qoeXysS2QLfHrRaWLjw.jpg?auto=webp&amp;s=6cc32e0031a79942bcfdbf35f6e6e5a253a27466', 'width': 984, 'height': 394}, 'resolutions': [{'url': 'https://external-preview.redd.it/VcPVDYJiK_Rqt02Oc_QGLmR5qoeXysS2QLfHrRaWLjw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f75fd7202224cbc4d9c32403df620c0960ed181', 'width': 108, 'height': 43}, {'url': 'https://external-preview.redd.it/VcPVDYJiK_Rqt02Oc_QGLmR5qoeXysS2QLfHrRaWLjw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=31b3f02a2bdf304bdc66a95d3c6f1dc204d228ed', 'width': 216, 'height': 86}, {'url': 'https://external-preview.redd.it/VcPVDYJiK_Rqt02Oc_QGLmR5qoeXysS2QLfHrRaWLjw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=843b4b9467431a14cccf7fc4116171e7a8c0b444', 'width': 320, 'height': 128}, {'url': 'https://external-preview.redd.it/VcPVDYJiK_Rqt02Oc_QGLmR5qoeXysS2QLfHrRaWLjw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d08571625380e6cbef9f077e9f04ba871b27f1d', 'width': 640, 'height': 256}, {'url': 'https://external-preview.redd.it/VcPVDYJiK_Rqt02Oc_QGLmR5qoeXysS2QLfHrRaWLjw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2cda15a09fa75a4cb7382ffa66e7862cfbdfb7b1', 'width': 960, 'height': 384}], 'variants': {}, 'id': '_5QBonUqVKiyoSRF9PVz1OyiCeOQLDwW-ug_shi7C4I'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jwyg5z,True,,ai-lover,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jwyg5z/pytorch_releases_prototype_features_to_execute/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jwyg5z/pytorch_releases_prototype_features_to_execute/,7135,1605771188.0,0,,False,,,,,,,,
224,,pytorch,"My problem is a bit hard to explain.

* I want to update probability distributions recursively in \[0, 1\]\^n.
* I discretized it with 10 bins per dimension, so there are 10\^n cells indexed by an n-tuple.
* Initially, I should have a uniform distribution; but then I take a threshold parameter that dictates how I propagate the distribution: every point that is ""below"" all the n thresholds must receive zero probability.

I would need to access and update `tensor[j, :, ..., :], tensor[:, j, ..., :], ..., tensor[:, :, ..., j]` recursively, and the problem is that I wanted the number of dimensions to be dynamic. How could I implement it?

I have added a question on StackOverflow with a snippet that may help to understand: [https://stackoverflow.com/questions/64885859/sequence-of-for-loops-to-access-different-dimensions-of-a-tensor](https://stackoverflow.com/questions/64885859/sequence-of-for-loops-to-access-different-dimensions-of-a-tensor)",t2_hlbf8r7,False,,0,False,Sequence of for loops to access different dimensions of a tensor,[],r/pytorch,False,6,,0,,,False,t3_jwfj2f,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1605733976.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My problem is a bit hard to explain.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I want to update probability distributions recursively in [0, 1]^n.&lt;/li&gt;
&lt;li&gt;I discretized it with 10 bins per dimension, so there are 10^n cells indexed by an n-tuple.&lt;/li&gt;
&lt;li&gt;Initially, I should have a uniform distribution; but then I take a threshold parameter that dictates how I propagate the distribution: every point that is &amp;quot;below&amp;quot; all the n thresholds must receive zero probability.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I would need to access and update &lt;code&gt;tensor[j, :, ..., :], tensor[:, j, ..., :], ..., tensor[:, :, ..., j]&lt;/code&gt; recursively, and the problem is that I wanted the number of dimensions to be dynamic. How could I implement it?&lt;/p&gt;

&lt;p&gt;I have added a question on StackOverflow with a snippet that may help to understand: &lt;a href=""https://stackoverflow.com/questions/64885859/sequence-of-for-loops-to-access-different-dimensions-of-a-tensor""&gt;https://stackoverflow.com/questions/64885859/sequence-of-for-loops-to-access-different-dimensions-of-a-tensor&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jwfj2f,True,,hazevedosa,,2,True,all_ads,False,[],False,,/r/pytorch/comments/jwfj2f/sequence_of_for_loops_to_access_different/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jwfj2f/sequence_of_for_loops_to_access_different/,7135,1605705176.0,0,,False,,,,,,,,
225,,pytorch,"&amp;#x200B;

Complete newbie to pytorch, Just started pytorch last week, but I've been dabbling in the theory and mathematics of machine learning for a few months, so I understand that part, mostly.

I'm following along with the second course in this tutorial online [https://www.youtube.com/watch?v=GIsg-ZUy0MY&amp;ab\_channel=freeCodeCamp.org](https://www.youtube.com/watch?v=GIsg-ZUy0MY&amp;ab_channel=freeCodeCamp.org) for logistic regression with the MNIST dataset. I decided to try out the same thing for practice on the CIFAR-10 dataset, which can be accessed directly through the pytorch library.

I'm working in a google colab document and wrote a bunch of notes for myself. I worked my way through the tutorial pretty well.

Near the end, where I step through each epoch, I tried (a probably very code inefficent way) to plot the training and validation accuracy w.r.t the amount of epochs and found it to be a flat line, my accuracy starting at around 30% and wavering around there indefinitely.

&amp;#x200B;

https://preview.redd.it/enu9urvrnyz51.png?width=536&amp;format=png&amp;auto=webp&amp;s=ab475d11fffe8220203e3b8ebca424534fee1818

I'm really confused at what this is suggesting!! It isn't at 10%, so that means that it worked a bit (since it's more than just guessing) but why would it stay? I tried a bunch of arbitrary learning rates and found it hover between 10-35% accuracy, but it would never improve much more than 5 percentage points after the initial step.

Here's my google colab page for this document: [https://colab.research.google.com/drive/1lWmBlI2BTLw3B-jW5uum0GfYiFNA5kQv?usp=sharing](https://colab.research.google.com/drive/1lWmBlI2BTLw3B-jW5uum0GfYiFNA5kQv?usp=sharing)

I'm pretty confused - also, some tips on how to better plot learning curves and loss curves would be much appreciated! I feel like my implementation is not efficient.

Thanks, A",t2_3ke4xozp,False,,0,False,Pytorch image recognition logistic regression - CIFAR-10 has flat loss curve (no improvement in accuracy after training),[],r/pytorch,False,6,,0,99.0,,False,t3_jwcaci,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/wygrsoOFBtqsaITzsO9EVsaqDQYRiwy7aWYfYrSLOwA.jpg,False,,[],{},self,,True,,1605717670.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Complete newbie to pytorch, Just started pytorch last week, but I&amp;#39;ve been dabbling in the theory and mathematics of machine learning for a few months, so I understand that part, mostly.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m following along with the second course in this tutorial online &lt;a href=""https://www.youtube.com/watch?v=GIsg-ZUy0MY&amp;amp;ab_channel=freeCodeCamp.org""&gt;https://www.youtube.com/watch?v=GIsg-ZUy0MY&amp;amp;ab_channel=freeCodeCamp.org&lt;/a&gt; for logistic regression with the MNIST dataset. I decided to try out the same thing for practice on the CIFAR-10 dataset, which can be accessed directly through the pytorch library.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working in a google colab document and wrote a bunch of notes for myself. I worked my way through the tutorial pretty well.&lt;/p&gt;

&lt;p&gt;Near the end, where I step through each epoch, I tried (a probably very code inefficent way) to plot the training and validation accuracy w.r.t the amount of epochs and found it to be a flat line, my accuracy starting at around 30% and wavering around there indefinitely.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/enu9urvrnyz51.png?width=536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab475d11fffe8220203e3b8ebca424534fee1818""&gt;https://preview.redd.it/enu9urvrnyz51.png?width=536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab475d11fffe8220203e3b8ebca424534fee1818&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m really confused at what this is suggesting!! It isn&amp;#39;t at 10%, so that means that it worked a bit (since it&amp;#39;s more than just guessing) but why would it stay? I tried a bunch of arbitrary learning rates and found it hover between 10-35% accuracy, but it would never improve much more than 5 percentage points after the initial step.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s my google colab page for this document: &lt;a href=""https://colab.research.google.com/drive/1lWmBlI2BTLw3B-jW5uum0GfYiFNA5kQv?usp=sharing""&gt;https://colab.research.google.com/drive/1lWmBlI2BTLw3B-jW5uum0GfYiFNA5kQv?usp=sharing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m pretty confused - also, some tips on how to better plot learning curves and loss curves would be much appreciated! I feel like my implementation is not efficient.&lt;/p&gt;

&lt;p&gt;Thanks, A&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GwLm_dOukl0ncxHg3-AKQhvIrilFWfH3uwa_X0drfSQ.jpg?auto=webp&amp;s=1d1cd3f2b1f4a5c1867cd0b31d3da86a6609c94f', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/GwLm_dOukl0ncxHg3-AKQhvIrilFWfH3uwa_X0drfSQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=471858eefdffee9e3c241d8254e3ecaf6e0e13e4', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/GwLm_dOukl0ncxHg3-AKQhvIrilFWfH3uwa_X0drfSQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb379be16fce616b417a5527f421ccbeeb4f9671', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/GwLm_dOukl0ncxHg3-AKQhvIrilFWfH3uwa_X0drfSQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=21439f88abef3a9ceb93731dc8c6bdea77e4eda1', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'Tauh0aeoBTT0K_s4Y4fRV4BAix-hE5yoCH0D0TK70VI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jwcaci,True,,kirbyburgers,,3,True,all_ads,False,[],False,,/r/pytorch/comments/jwcaci/pytorch_image_recognition_logistic_regression/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jwcaci/pytorch_image_recognition_logistic_regression/,7135,1605688870.0,0,,False,,,,"{'enu9urvrnyz51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 76, 'x': 108, 'u': 'https://preview.redd.it/enu9urvrnyz51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b302765b3bbee2d7165c27ef36da34eee87fdaa6'}, {'y': 153, 'x': 216, 'u': 'https://preview.redd.it/enu9urvrnyz51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b2621947d9da7ece9aba35af29d80580fa437b7'}, {'y': 226, 'x': 320, 'u': 'https://preview.redd.it/enu9urvrnyz51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6a1b9c595229bca37c1092dc30bc60ca558b42e5'}], 's': {'y': 380, 'x': 536, 'u': 'https://preview.redd.it/enu9urvrnyz51.png?width=536&amp;format=png&amp;auto=webp&amp;s=ab475d11fffe8220203e3b8ebca424534fee1818'}, 'id': 'enu9urvrnyz51'}}",,,,
226,,pytorch,"Hey,

So I got my rtx 3070 on my ubuntu pc.

Is there already support for pytorch with rtx 3070? 

I read that you need cuda 11.1 for it, but pytorch didn't release a version with cuda 11.1 yet",t2_11zpu1,False,,0,False,[Q] Rtx 3000,[],r/pytorch,False,6,,0,,,False,t3_jvycs7,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1605666521.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey,&lt;/p&gt;

&lt;p&gt;So I got my rtx 3070 on my ubuntu pc.&lt;/p&gt;

&lt;p&gt;Is there already support for pytorch with rtx 3070? &lt;/p&gt;

&lt;p&gt;I read that you need cuda 11.1 for it, but pytorch didn&amp;#39;t release a version with cuda 11.1 yet&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jvycs7,True,,HoLeeFaak,,7,True,all_ads,False,[],False,,/r/pytorch/comments/jvycs7/q_rtx_3000/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jvycs7/q_rtx_3000/,7135,1605637721.0,0,,False,,,,,,,,
227,,pytorch,,t2_766u1eio,False,,0,False,How to load checkpoints across different versions of pytorch (1.3.1 and 1.6.x) using ppc64le and x86?,[],r/pytorch,False,6,,0,140.0,,False,t3_jvza7v,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/rysuQbE3ON_LRGfDDy1QJF-F2vtO31Wwm1j3CNXgATw.jpg,False,,[],{},link,,False,,1605669346.0,text,6,,,text,stackoverflow.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jvza7v,True,,No_Ad3397,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jvza7v/how_to_load_checkpoints_across_different_versions/,all_ads,False,https://stackoverflow.com/questions/64141188/how-to-load-checkpoints-across-different-versions-of-pytorch-1-3-1-and-1-6-x-u,7135,1605640546.0,0,,False,https://stackoverflow.com/questions/64141188/how-to-load-checkpoints-across-different-versions-of-pytorch-1-3-1-and-1-6-x-u,,,,,,,
228,,pytorch,"Hello,  


I've started to port some of my Pytorch trainers to Pytorch Lightning. I like the modularity of it but it seems to train a lot slower than regular Pytorch. Have any of you noticed any significant differences in speed between Pytorch and Pytorch Lightning? I'm using the same data loading and network codes in both versions.",t2_28a57i1o,False,,0,False,Pytorch vs Pytorch Lightning speed,[],r/pytorch,False,6,,0,,,False,t3_jvqtbo,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1605637957.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,  &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve started to port some of my Pytorch trainers to Pytorch Lightning. I like the modularity of it but it seems to train a lot slower than regular Pytorch. Have any of you noticed any significant differences in speed between Pytorch and Pytorch Lightning? I&amp;#39;m using the same data loading and network codes in both versions.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jvqtbo,True,,Alex-S-S,,4,True,all_ads,False,[],False,,/r/pytorch/comments/jvqtbo/pytorch_vs_pytorch_lightning_speed/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jvqtbo/pytorch_vs_pytorch_lightning_speed/,7135,1605609157.0,0,,False,,,,,,,,
229,,pytorch,,t2_79p1h62w,False,,0,False,PyTorch 3D: Digging Deeper in Deep Learning,[],r/pytorch,False,6,,0,64.0,,False,t3_jvp39i,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/87PekbcjIBWRhMtwITpJ6H9d1A9QlSCBDAFIL6pj2zA.jpg,False,,[],{},link,,False,,1605628304.0,text,6,,,text,artiba.org,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/XiIAqQFCYV7g9rbftXJB5ajEeukQ5wIzBZJzIM46Qjg.jpg?auto=webp&amp;s=bc0bda5ec823b15806642f47210a8666894a21be', 'width': 800, 'height': 370}, 'resolutions': [{'url': 'https://external-preview.redd.it/XiIAqQFCYV7g9rbftXJB5ajEeukQ5wIzBZJzIM46Qjg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5f441033e00831c6f51b57db95bde012ee30c6aa', 'width': 108, 'height': 49}, {'url': 'https://external-preview.redd.it/XiIAqQFCYV7g9rbftXJB5ajEeukQ5wIzBZJzIM46Qjg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1c71f8de778ee4949f7067973ff0ba9f3f413fc5', 'width': 216, 'height': 99}, {'url': 'https://external-preview.redd.it/XiIAqQFCYV7g9rbftXJB5ajEeukQ5wIzBZJzIM46Qjg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2a413e7c3539c72dd0935ec69a7927c5c818157', 'width': 320, 'height': 148}, {'url': 'https://external-preview.redd.it/XiIAqQFCYV7g9rbftXJB5ajEeukQ5wIzBZJzIM46Qjg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f646e430c691c6380655a0edefbba97eb60cced9', 'width': 640, 'height': 296}], 'variants': {}, 'id': 'IDJRg05Td4mvOgIavkCHAaJjTm0tylHp6aPaMxPEOsQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jvp39i,True,,Shradha_Singh,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jvp39i/pytorch_3d_digging_deeper_in_deep_learning/,all_ads,False,https://www.artiba.org/blog/pytorch-3d-digging-deeper-in-deep-learning,7135,1605599504.0,0,,False,https://www.artiba.org/blog/pytorch-3d-digging-deeper-in-deep-learning,,,,,,,
230,,pytorch,"I have checked pytorch docs to find test accuracy after each epoch and found basic for loop and updating a counter.

This works but is fairly slow on cifar10 dataset. Is there a way to use numpy like vectorizations to make it faster.

I'm new to pytorch. Any help would be appreciated.",t2_wjclo,False,,0,False,Checking test accuracy using vectorized code?,[],r/pytorch,False,6,,0,,,False,t3_jv905w,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1605570545.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have checked pytorch docs to find test accuracy after each epoch and found basic for loop and updating a counter.&lt;/p&gt;

&lt;p&gt;This works but is fairly slow on cifar10 dataset. Is there a way to use numpy like vectorizations to make it faster.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m new to pytorch. Any help would be appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jv905w,True,,crazyb14,,2,True,all_ads,False,[],False,,/r/pytorch/comments/jv905w/checking_test_accuracy_using_vectorized_code/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jv905w/checking_test_accuracy_using_vectorized_code/,7135,1605541745.0,0,,False,,,,,,,,
231,,pytorch,"Hi there,

&amp;#x200B;

I have a Pytorch Bert model was originally trained with 1 GPU. Now i moved it to 4GPU due to memory issue. However, when I moved it to 4GPU. I got an error message as below during the validation part:

\`\`\`

RuntimeError: Caught RuntimeError in replica 1 on device 1.

Original Traceback (most recent call last):

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/parallel\_apply.py"", line 60, in \_worker

output = module(\*input, \*\*kwargs)

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in \_\_call\_\_

result = self.forward(\*input, \*\*kwargs)

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/data\_parallel.py"", line 155, in forward

outputs = self.parallel\_apply(replicas, inputs, kwargs)

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/data\_parallel.py"", line 165, in parallel\_apply

return parallel\_apply(replicas, inputs, kwargs, self.device\_ids\[:len(replicas)\])

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/parallel\_apply.py"", line 85, in parallel\_apply

output.reraise()

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/\_utils.py"", line 395, in reraise

raise self.exc\_type(msg)

RuntimeError: Caught RuntimeError in replica 0 on device 0.

Original Traceback (most recent call last):

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/parallel\_apply.py"", line 60, in \_worker

output = module(\*input, \*\*kwargs)

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in \_\_call\_\_

result = self.forward(\*input, \*\*kwargs)

  File ""/home/ec2-user/SageMaker/Anecdotes/model.py"", line 117, in forward

inputs\_embeds=None,

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in \_\_call\_\_

result = self.forward(\*input, \*\*kwargs)

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling\_bert.py"", line 727, in forward

input\_ids=input\_ids, position\_ids=position\_ids, token\_type\_ids=token\_type\_ids, inputs\_embeds=inputs\_embeds

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in \_\_call\_\_

result = self.forward(\*input, \*\*kwargs)

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling\_bert.py"", line 174, in forward

inputs\_embeds = self.word\_embeddings(input\_ids)

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in \_\_call\_\_

result = self.forward(\*input, \*\*kwargs)

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/sparse.py"", line 114, in forward

self.norm\_type, self.scale\_grad\_by\_freq, self.sparse)

  File ""/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py"", line 1724, in embedding

return torch.embedding(weight, input, padding\_idx, scale\_grad\_by\_freq, sparse)

RuntimeError: arguments are located on different GPUs at /pytorch/aten/src/THC/generic/THCTensorIndex.cu:403

\`\`\`

&amp;#x200B;

My [model.py](https://model.py) is as below:

\`\`\`

import torch

import torch.nn as nn

import torch.nn.functional as F

from torch.nn import MultiheadAttention, EmbeddingBag, CrossEntropyLoss, MultiLabelSoftMarginLoss, BCEWithLogitsLoss

from transformers import BertPreTrainedModel, BertModel, BertTokenizer, AdamW

&amp;#x200B;

from utils import LABEL\_NAME

&amp;#x200B;

&amp;#x200B;

class ReviewClassification(BertPreTrainedModel):

def \_\_init\_\_(self, config,

add\_agent\_text, agent\_text\_heads):

""""""

:param config: Bert configuration, can set up some parameters, like  output\_attention, output\_hidden\_states

:param add\_agent\_text: whether to use the non text feature, and how.

It can have three options: None, ""concat"" and ""attention""

:param agent\_text\_heads: number of the heads in agent attention mechanism. Only useful if add\_agent\_text are set to

""attention""

""""""

super().\_\_init\_\_(config)

\# self.num\_labels = 2

self.add\_agent\_text = add\_agent\_text

&amp;#x200B;

self.bert = BertModel(config)

self.dropout = nn.Dropout(config.hidden\_dropout\_prob)

&amp;#x200B;

embedding\_size = config.hidden\_size

&amp;#x200B;

if self.add\_agent\_text == ""concat"":

embedding\_size = 2 \* embedding\_size

elif self.add\_agent\_text == ""attention"":

self.agent\_attention = nn.MultiheadAttention(embedding\_size, num\_heads=agent\_text\_heads)

else:

\# don't use the information in Agent text

pass

&amp;#x200B;

self.classifier = nn.Linear(embedding\_size, 1) # self.classifier = nn.Linear(embedding\_size, len(LABEL\_NAME)) # bias: If set to False, the layer will not learn an additive bias

self.init\_weights()

&amp;#x200B;

print(

""""""            

add agent text         :{}

agent text multi-head  :{}

"""""".format(self.add\_agent\_text, agent\_text\_heads)

)

&amp;#x200B;

def forward(

self,

review\_input\_ids=None,

review\_attention\_mask=None,

review\_token\_type\_ids=None,

agent\_input\_ids=None,

agent\_attention\_mask=None,

agent\_token\_type\_ids=None,

labels=None,

):

""""""

labels (:obj:\`torch.LongTensor\` of shape :obj:\`(batch\_size,)\`, \`optional\`, defaults to :obj:\`None\`):

Labels for computing the sequence classification/regression loss.

Indices should be in :obj:\`\[0, ..., config.num\_labels - 1\]\`.

If :obj:\`config.num\_labels == 1\` a regression loss is computed (Mean-Square loss),

If :obj:\`config.num\_labels &gt; 1\` a classification loss is computed (Cross-Entropy).

&amp;#x200B;

Returns:

:obj:\`tuple(torch.FloatTensor)\` comprising various elements depending on the configuration (:class:\`\~transformers.BertConfig\`) and inputs:

loss (:obj:\`torch.FloatTensor\` of shape :obj:\`(1,)\`, \`optional\`, returned when :obj:\`label\` is provided):

Classification (or regression if config.num\_labels==1) loss.

logits (:obj:\`torch.FloatTensor\` of shape :obj:\`(batch\_size, config.num\_labels)\`):

Classification (or regression if config.num\_labels==1) scores (before SoftMax).

hidden\_states (:obj:\`tuple(torch.FloatTensor)\`, \`optional\`, returned when \`\`config.output\_hidden\_states=True\`\`):

Tuple of :obj:\`torch.FloatTensor\` (one for the output of the embeddings + one for the output of each layer)

of shape :obj:\`(batch\_size, sequence\_length, hidden\_size)\`.

&amp;#x200B;

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions (:obj:\`tuple(torch.FloatTensor)\`, \`optional\`, returned when \`\`config.output\_attentions=True\`\`):

Tuple of :obj:\`torch.FloatTensor\` (one for each layer) of shape

:obj:\`(batch\_size, num\_heads, sequence\_length, sequence\_length)\`.

&amp;#x200B;

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention

heads.

&amp;#x200B;

Examples::

&amp;#x200B;

from transformers import BertTokenizer, BertForSequenceClassification

import torch

&amp;#x200B;

tokenizer = BertTokenizer.from\_pretrained('bert-base-uncased')

model = BertForSequenceClassification.from\_pretrained('bert-base-uncased')

&amp;#x200B;

input\_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"", add\_special\_tokens=True)).unsqueeze(0)  # Batch size 1

labels = torch.tensor(\[1\]).unsqueeze(0)  # Batch size 1

outputs = model(input\_ids, labels=labels)

&amp;#x200B;

loss, logits = outputs\[:2\]

&amp;#x200B;

""""""

&amp;#x200B;

review\_outputs = self.bert(

review\_input\_ids,

attention\_mask=review\_attention\_mask,

token\_type\_ids=review\_token\_type\_ids,

position\_ids=None,

head\_mask=None,

inputs\_embeds=None,

)

if self.add\_agent\_text is not None:

\# means that self.add\_agent\_text is ""concat"" or ""attention""

\# TODO: we can try that agent\_outputs do not share the same parameter

agent\_outputs = self.bert(

agent\_input\_ids,

attention\_mask=agent\_attention\_mask,

token\_type\_ids=agent\_token\_type\_ids,

position\_ids=None,

head\_mask=None,

inputs\_embeds=None,

)

&amp;#x200B;

if self.add\_agent\_text == ""attention"":

review\_hidden\_states = review\_outputs\[0\].transpose(0, 1)  # before trans: (bs, seq\_len, hidden\_size)

agent\_hidden\_states = agent\_outputs\[0\].mean(axis=1).unsqueeze(dim=0)  # (1, batch\_size, hidden\_size)

&amp;#x200B;

attn\_output, \_ = self.agent\_attention(agent\_hidden\_states, review\_hidden\_states, review\_hidden\_states)

feature = attn\_output.squeeze()  # (batch\_size, seq\_len)

else:

feature = review\_outputs\[1\]  # (batch\_size, seq\_len) -? Should it be (batch\_size, hidden\_size)

&amp;#x200B;

if self.add\_agent\_text == ""concat"":

feature = [torch.cat](https://torch.cat)(\[feature, agent\_outputs\[1\]\], axis=1)



&amp;#x200B;

\# nn.CrossEntropyLoss applies F.log\_softmax and nn.NLLLoss internally on your input,

\# so you should pass the raw logits to it.

&amp;#x200B;

\# torch.nn.functional.binary\_cross\_entropy takes logistic sigmoid values as inputs

\# torch.nn.functional.binary\_cross\_entropy\_with\_logits takes logits as inputs

\# torch.nn.functional.cross\_entropy takes logits as inputs (performs log\_softmax internally)

\# torch.nn.functional.nll\_loss is like cross\_entropy but takes log-probabilities (log-softmax) values as inputs

&amp;#x200B;

\# CrossEntropyLoss takes prediction logits (size: (N,D)) and target labels (size: (N,)) 

\# CrossEntropyLoss expects logits i.e whereas BCELoss expects probability value

logits = self.classifier(feature).squeeze()

&amp;#x200B;

outputs = (logits,)  # + outputs\[2:\]  # add hidden states and attention if they are here

&amp;#x200B;

&amp;#x200B;

if labels is not None:

\##### original

\# loss\_fct = MultiLabelSoftMarginLoss()

\# loss = loss\_fct(logits, labels)

\# outputs = (loss,) + outputs

\#### Version 1 try

\# pos\_weight = dataset.label\_proportion.iloc\[0\]/dataset.label\_proportion.iloc\[1\]

&amp;#x200B;

\# Version 1.1 for weight

\# weight = torch.tensor(\[0.101521, 0.898479\]) # hard code from entire training dataset

\# pos\_weight = weight\[labels.data.view(-1).long()\].view\_as(labels)

\# Version 1.2 for weight

pos\_weight=torch.tensor(1)

\# Version 1.3 for weight

\#weight = torch.tensor(\[1.0, 8.85\]) # hard code from entire training dataset

\#pos\_weight = weight\[labels.data.view(-1).long()\].view\_as(labels)



loss\_fct = nn.BCEWithLogitsLoss(pos\_weight=pos\_weight).cuda() 

loss = loss\_fct(logits, labels)

\# loss = loss\_fct(logits.view(-1, self.num\_labels), labels.view(-1, self.num\_labels))

outputs = (loss,) + outputs

\### Version 2 try

\# loss\_fct = nn.CrossEntropyLoss()

\# loss = loss\_fct(logits.view(-1, self.num\_labels), labels.view(-1))

\# outputs = (loss,) + outputs

&amp;#x200B;

return outputs  # (loss, logits, hidden\_states, attentions)

\`\`\`

&amp;#x200B;

And my train\_valid\_test.py for the training, validation, test process is as below:

\`\`\`

import time

import pickle

from path import Path

import numpy as np

import pandas as pd

&amp;#x200B;

from sklearn.metrics import precision\_recall\_fscore\_support, classification\_report, confusion\_matrix

import torch

import torch.nn as nn

&amp;#x200B;

from utils import LABEL\_NAME, isnotebook, set\_seed, format\_time

&amp;#x200B;

if isnotebook():

from tqdm.notebook import tqdm

else:

from tqdm import tqdm

&amp;#x200B;

&amp;#x200B;

set\_seed(seed=228)

&amp;#x200B;

def model\_train(model, train\_data\_loader, valid\_data\_loader, test\_data\_loader,

logger, optimizer, scheduler, num\_epochs, seed, out\_dir):

\# move model to gpu

device = torch.device('cuda' if torch.cuda.is\_available() else 'cpu')

[model.to](https://model.to)(device)

if torch.cuda.device\_count() &gt; 1:

model = nn.DataParallel(model)

&amp;#x200B;

num\_gpus = torch.cuda.device\_count()

[logger.info](https://logger.info)(""Let's use {} GPUs!"".format(num\_gpus))

&amp;#x200B;

\# Set the seed value all over the place to make this reproducible.

\#     set\_seed(seed=seed)

&amp;#x200B;

\# We'll store a number of quantities such as training and validation loss,

\# validation accuracy, and timings.

training\_stats = \[\]

print\_interval = 100

&amp;#x200B;

\# Measure the total training time for the whole run.

total\_t0 = time.time()

batch\_size = train\_data\_loader.batch\_size

num\_batch = len(train\_data\_loader)

best\_f1\_score = {

""weighted"": 0,

""averaged"": 0

}

best\_test\_f1\_score = 0

&amp;#x200B;

\# For each epoch...

for epoch\_i in range(0, num\_epochs):

&amp;#x200B;

\# ========================================

\#               Training

\# ========================================

&amp;#x200B;

\# Perform one full pass over the training set.

[logger.info](https://logger.info)("""")

[logger.info](https://logger.info)('======== Epoch {:} / {:} ========'.format(epoch\_i + 1, num\_epochs))

[logger.info](https://logger.info)('Training...')

&amp;#x200B;

\# Reset the total loss for this epoch.

total\_train\_loss = 0

&amp;#x200B;

\# Measure how long the training epoch takes.

t\_train = time.time()

&amp;#x200B;

model.train()

&amp;#x200B;

\# For each batch of training data...

for step, batch in tqdm(enumerate(train\_data\_loader), desc=""Training Iteration"", total=num\_batch):

\# Progress update every 100 batches.

if step % print\_interval == 0 and not step == 0:

\# Calculate elapsed time in minutes.

elapsed = format\_time(time.time() - t\_train)

avg\_train\_loss = total\_train\_loss / print\_interval

&amp;#x200B;

\# Report progress.

[logger.info](https://logger.info)('| epoch {:3d} | {:5d}/{:5d} batches | lr {:.3e} | loss {:5.3f} | Elapsed {:s}'.format(

epoch\_i+1, step, num\_batch, scheduler.get\_last\_lr()\[0\], avg\_train\_loss, elapsed)

)

total\_train\_loss = 0

training\_stats.append(

{

'epoch': epoch\_i + 1,

'step': step,

'train loss': avg\_train\_loss,

}

)

&amp;#x200B;

\# Unpack this training batch from our dataloader.

\#

\# As we unpack the batch, we'll also copy each tensor to the GPU using the

\# \`to\` method.

\#

\# \`batch\` contains four pytorch tensors:

\#   ""input\_ids""

\#   ""attention\_mask""

\#   ""token\_type\_ids""

\#   ""binarized\_labels""

&amp;#x200B;

b\_review\_input\_ids = batch\[""review\_input\_ids""\].to(device)

b\_review\_attention\_mask = batch\[""review\_attention\_mask""\].to(device)

b\_review\_token\_type\_ids = batch\[""review\_token\_type\_ids""\].to(device)

b\_agent\_input\_ids = batch\[""agent\_input\_ids""\].to(device)

b\_agent\_attention\_mask = batch\[""agent\_attention\_mask""\].to(device)

b\_agent\_token\_type\_ids = batch\[""agent\_token\_type\_ids""\].to(device)

&amp;#x200B;

b\_binarized\_label = batch\[""binarized\_label""\].to(device)

&amp;#x200B;

model.zero\_grad()

(loss, \_) = model(review\_input\_ids=b\_review\_input\_ids,

review\_attention\_mask=b\_review\_attention\_mask,

review\_token\_type\_ids=b\_review\_token\_type\_ids,

agent\_input\_ids=b\_agent\_input\_ids,

agent\_attention\_mask=b\_agent\_attention\_mask,

agent\_token\_type\_ids=b\_agent\_token\_type\_ids,

&amp;#x200B;

labels=b\_binarized\_label

)

&amp;#x200B;

\# Accumulate the training loss over all of the batches so that we can

\# calculate the average loss at the end. \`loss\` is a Tensor containing a

\# single value; the \`.item()\` function just returns the Python value

\# from the tensor.

&amp;#x200B;

if num\_gpus &gt; 1:

total\_train\_loss += loss.mean().item()

loss.mean().backward()  # use loss.mean().backward() instead of loss.backward() for multiple gpu trainings

else:

total\_train\_loss += loss.item()

loss.backward()

&amp;#x200B;

\# Clip the norm of the gradients to 1.0.

\# This is to help prevent the ""exploding gradients"" problem.

torch.nn.utils.clip\_grad\_norm\_(model.parameters(), 1.0)

&amp;#x200B;

\# Update parameters and take a step using the computed gradient.

\# The optimizer dictates the ""update rule""--how the parameters are

\# modified based on their gradients, the learning rate, etc.

optimizer.step()

scheduler.step()

\# End of training epoch

&amp;#x200B;

\# Measure how long this epoch took.

training\_time = format\_time(time.time() - t\_train)

&amp;#x200B;

[logger.info](https://logger.info)("""")

[logger.info](https://logger.info)(""  Training epoch took: {:s}"".format(training\_time))

&amp;#x200B;

\# evaluate the model after one epoch.

&amp;#x200B;

\# ========================================

\#               Validation

\# ========================================

\# After the completion of each training epoch, measure our performance on

\# our validation set.

&amp;#x200B;

[logger.info](https://logger.info)("""")

[logger.info](https://logger.info)(""Validating..."")

&amp;#x200B;

t\_valid = time.time()

model.eval()

ave\_valid\_loss, valid\_f1\_table, cm\_table, f1\_score = model\_validate(model=model, data\_loader=valid\_data\_loader)

\# Measure how long this epoch took.

validation\_time = format\_time(time.time() - t\_valid)

&amp;#x200B;

[logger.info](https://logger.info)("""")

[logger.info](https://logger.info)('| loss {:5.3f} | Elapsed {:s}'.format(ave\_valid\_loss, validation\_time))

[logger.info](https://logger.info)(""  \\n{:s}"".format(valid\_f1\_table.to\_string()))

[logger.info](https://logger.info)("""")

[logger.info](https://logger.info)(""  \\n{:s}"".format(cm\_table.to\_string()))

&amp;#x200B;

\# need to store the best model

for key in best\_f1\_score.keys():

if best\_f1\_score\[key\] &lt; f1\_score\[key\]:

\# remove the old model:

file\_list = \[f for f in out\_dir.files() if f.name.endswith("".pt"") and f.name.startswith(key)\]

for f in file\_list:

Path.remove(f)

model\_file = out\_dir.joinpath('{:s}\_epoch\_{:02d}-f1\_{:.3f}.pt'.format(

key, epoch\_i + 1, f1\_score\[key\])

)

best\_f1\_score\[key\] = f1\_score\[key\]

if num\_gpus &gt; 1:

[torch.save](https://torch.save)(model.module.state\_dict(), model\_file)

else:

[torch.save](https://torch.save)(model.state\_dict(), model\_file)

&amp;#x200B;

\# ========================================

\#               Test

\# ========================================

[logger.info](https://logger.info)("""")

[logger.info](https://logger.info)(""Testing..."")

&amp;#x200B;

result\_df = model\_test(model=model, data\_loader=test\_data\_loader)



y\_true = np.array(result\_df\[""review\_label""\], dtype=np.bool) # This part may need double check

y\_pred = result\_df\[""Probability""\] &gt; 0.5

&amp;#x200B;

report = classification\_report(y\_true, y\_pred, output\_dict=True)

metrics\_df = pd.DataFrame(report).transpose()

&amp;#x200B;

metrics\_df = metrics\_df.sort\_index()

&amp;#x200B;

weighted\_f1\_score = metrics\_df.loc\['weighted avg', 'f1-score'\]

averaged\_f1\_score = metrics\_df.loc\['macro avg', 'f1-score'\]

&amp;#x200B;

best\_test\_f1\_score = metrics\_df.loc\['weighted avg', 'f1-score'\] \\

if best\_test\_f1\_score &lt; metrics\_df.loc\['weighted avg', 'f1-score'\] else best\_test\_f1\_score

&amp;#x200B;

metrics\_df = metrics\_df.astype(float).round(3)

&amp;#x200B;

\# Calculate confusion matrix

tn, fp, fn, tp  = confusion\_matrix(y\_true, y\_pred).ravel()

cm\_df = pd.DataFrame(columns = \['Predicted No', 'Predicted Yes'\],  

index = \['Actual No', 'Actual Yes'\]) 

\# adding rows to an empty  

\# dataframe at existing index 

cm\_df.loc\['Actual No'\] = \[tn,fp\] 

cm\_df.loc\['Actual Yes'\] = \[fn,tp\]



[logger.info](https://logger.info)(""use model: {} batch / {} step"".format(epoch\_i + 1, step))

[logger.info](https://logger.info)(""\\n"" + ""="" \* 50)

[logger.info](https://logger.info)(""\\n"" + metrics\_df.to\_string())

[logger.info](https://logger.info)(""\\n"" + ""="" \* 50)

[logger.info](https://logger.info)(""\\n"" + cm\_df.to\_string())

[logger.info](https://logger.info)(""best test F1 score: {}"".format(best\_test\_f1\_score))

[logger.info](https://logger.info)(""\\n"" + ""="" \* 50)

\# Below is to save the result files

\#         result\_filename = ""result\_df\_epoch\_"" + str(epoch\_i + 1) + "".xlsx""

\#         result\_df.to\_excel(out\_dir.joinpath(result\_filename), index=False)

&amp;#x200B;

[logger.info](https://logger.info)("""")

[logger.info](https://logger.info)(""Training complete!"")

[logger.info](https://logger.info)(""Total training took {:} (h:mm:ss)"".format(format\_time(time.time() - total\_t0)))

&amp;#x200B;

\# Save training\_stats to csv file

pd.DataFrame(training\_stats).to\_csv(out\_dir.joinpath(""model\_train.log""), index=False)

return model, optimizer, scheduler

&amp;#x200B;

&amp;#x200B;

def model\_validate(model, data\_loader):

\# Put the model in evaluation mode--the dropout layers behave differently

\# during evaluation.

model.eval()

device = torch.device('cuda' if torch.cuda.is\_available() else 'cpu')

[model.to](https://model.to)(device)

if torch.cuda.device\_count() &gt; 1:

model = nn.DataParallel(model)

&amp;#x200B;

label\_prop = data\_loader.dataset.dataset.label\_prop()

&amp;#x200B;

total\_valid\_loss = 0

&amp;#x200B;

batch\_size = data\_loader.batch\_size

num\_batch = len(data\_loader)

&amp;#x200B;

y\_pred, y\_true = \[\], \[\]

&amp;#x200B;

\# Evaluate data

for step, batch in tqdm(enumerate(data\_loader), desc=""Validation..."", total=num\_batch):

b\_review\_input\_ids = batch\[""review\_input\_ids""\].to(device)

b\_review\_attention\_mask = batch\[""review\_attention\_mask""\].to(device)

b\_review\_token\_type\_ids = batch\[""review\_token\_type\_ids""\].to(device)

b\_agent\_input\_ids = batch\[""agent\_input\_ids""\].to(device)

b\_agent\_attention\_mask = batch\[""agent\_attention\_mask""\].to(device)

b\_agent\_token\_type\_ids = batch\[""agent\_token\_type\_ids""\].to(device)

&amp;#x200B;

b\_binarized\_label = batch\[""binarized\_label""\].to(device)

&amp;#x200B;

\# Tell pytorch not to bother with constructing the compute graph during

\# the forward pass, since this is only needed for backprop (training).

with torch.no\_grad():

(loss, logits,) = model(review\_input\_ids=b\_review\_input\_ids,

review\_attention\_mask=b\_review\_attention\_mask,

review\_token\_type\_ids=b\_review\_token\_type\_ids,

agent\_input\_ids=b\_agent\_input\_ids,

agent\_attention\_mask=b\_agent\_attention\_mask,

agent\_token\_type\_ids=b\_agent\_token\_type\_ids,

&amp;#x200B;

labels=b\_binarized\_label)

&amp;#x200B;

total\_valid\_loss += loss.item()

\### The sigmoid function is used for the two-class logistic regression, 

\### whereas the softmax function is used for the multiclass logistic regression



\# Version 1

\# numpy\_probas = logits.detach().cpu().numpy()

\# y\_pred.extend(np.argmax(numpy\_probas, axis=1).flatten())

\# y\_true.extend(b\_binarized\_label.cpu().numpy())

&amp;#x200B;

\# Version 2

\# transfored\_logits = F.log\_softmax(logits,dim=1)

\# numpy\_probas = transfored\_logits.detach().cpu().numpy()

\# y\_pred.extend(np.argmax(numpy\_probas, axis=1).flatten())

\# y\_true.extend(b\_binarized\_label.cpu().numpy())

&amp;#x200B;

\# Version 3

\# transfored\_logits = torch.sigmoid(logits)

\# numpy\_probas = transfored\_logits.detach().cpu().numpy()

\# y\_pred.extend(np.argmax(numpy\_probas, axis=1).flatten())

\# y\_true.extend(b\_binarized\_label.cpu().numpy())

&amp;#x200B;

\# New version - for num\_label = 1

transfored\_logits = torch.sigmoid(logits)

numpy\_probas = transfored\_logits.detach().cpu().numpy()

y\_pred.extend(numpy\_probas)

y\_true.extend(b\_binarized\_label.cpu().numpy())



\# End of an epoch of validation

&amp;#x200B;

\# put model to train mode again.

model.train()

&amp;#x200B;

ave\_loss = total\_valid\_loss / (num\_batch \* batch\_size)

&amp;#x200B;

y\_pred = np.array(y\_pred)

y\_pred\[y\_pred &lt; 0.5\] = 0

y\_pred\[y\_pred &gt;= 0.5\] = 1



\# Below is in case the input and target are not the same data format

y\_pred = np.array(y\_pred, dtype=np.bool)

y\_true = np.array(y\_true, dtype=np.bool)





\# compute the various f1 score for each label

report = classification\_report(y\_true, y\_pred, output\_dict=True)

metrics\_df = pd.DataFrame(report).transpose()

\# metrics\_df = pd.DataFrame(0, index=LABEL\_NAME, columns=\[""Precision"", ""Recall"", ""F1"",""support""\])

\# metrics\_df.Precision = precision\_recall\_fscore\_support(y\_true, y\_pred)\[0\]

\# metrics\_df.Recall = precision\_recall\_fscore\_support(y\_true, y\_pred)\[1\]

\# metrics\_df.F1 = precision\_recall\_fscore\_support(y\_true, y\_pred)\[2\]

\# metrics\_df.support = precision\_recall\_fscore\_support(y\_true, y\_pred)\[3\]

&amp;#x200B;

\# y\_pred = np.array(y\_pred)

\# y\_pred\[y\_pred &lt; 0\] = 0

\# y\_pred\[y\_pred &gt; 0\] = 1

\# y\_pred = np.array(y\_pred, dtype=np.bool)

\# y\_true = np.array(y\_true, dtype=np.bool)

&amp;#x200B;

\# metrics\_df = pd.DataFrame(0, index=LABEL\_NAME, columns=\[""Precision"", ""Recall"", ""F1""\], dtype=np.float)

\# # or\_y\_pred = np.zeros(y\_pred.shape\[0\], dtype=np.bool)

\# # or\_y\_true = np.zeros(y\_true.shape\[0\], dtype=np.bool)

\# for i in range(len(LABEL\_NAME)):

\#     metrics\_df.iloc\[i\] = precision\_recall\_fscore\_support(

\#         y\_true=y\_true\[:, i\], y\_pred=y\_pred\[:, i\], average='binary', zero\_division=0)\[0:3\]

&amp;#x200B;

\# or\_y\_pred = or\_y\_pred | y\_pred\[:, i\]

\# or\_y\_true = or\_y\_true | y\_true\[:, i\]

&amp;#x200B;

metrics\_df = metrics\_df.sort\_index()

\# metrics\_df.loc\['Weighted Average'\] = metrics\_df.transpose().dot(label\_prop)

\# metrics\_df.loc\['Average'\] = metrics\_df.mean()

&amp;#x200B;

\# metrics\_df.loc\['Weighted Average', 'F1'\] = 2 / (1/metrics\_df.loc\['Weighted Average', ""Recall""\] +

\#                                                 1/metrics\_df.loc\['Weighted Average', ""Precision""\])

\# metrics\_df.loc\['Average', 'F1'\] = 2 / (1/metrics\_df.loc\['Average', ""Recall""\] +

\#                                        1/metrics\_df.loc\['Average', ""Precision""\])

&amp;#x200B;

weighted\_f1\_score = metrics\_df.loc\['weighted avg', 'f1-score'\]

averaged\_f1\_score = metrics\_df.loc\['macro avg', 'f1-score'\]

&amp;#x200B;

\# Calculate confusion matrix

tn, fp, fn, tp  = confusion\_matrix(y\_true, y\_pred).ravel()

cm\_df = pd.DataFrame(columns = \['Predicted No', 'Predicted Yes'\],  

index = \['Actual No', 'Actual Yes'\]) 

\# adding rows to an empty  

\# dataframe at existing index 

cm\_df.loc\['Actual No'\] = \[tn,fp\] 

cm\_df.loc\['Actual Yes'\] = \[fn,tp\]

&amp;#x200B;

\# pooled\_f1\_score = f1\_score(y\_pred=or\_y\_pred, y\_true=or\_y\_true)

&amp;#x200B;

return ave\_loss, metrics\_df, cm\_df,{

""weighted"": weighted\_f1\_score,

""averaged"": averaged\_f1\_score,

}

&amp;#x200B;

&amp;#x200B;

def model\_test(model, data\_loader):

\# Put the model in evaluation mode--the dropout layers behave differently

\# during evaluation.

device = torch.device('cuda' if torch.cuda.is\_available() else 'cpu')

model.eval()

[model.to](https://model.to)(device)

if torch.cuda.device\_count() &gt; 1:

model = nn.DataParallel(model)

&amp;#x200B;

num\_batch = len(data\_loader)

\# Below need to modify if change the input

review\_id, review\_label, hmd\_text, head\_cust\_text = \[\], \[\], \[\], \[\]

agent = \[\]

pred\_logits = \[\]

&amp;#x200B;

\# Evaluate data

for step, batch in tqdm(enumerate(data\_loader), desc=""Inference..."", total=num\_batch):

if ""anecdote\_lead\_final"" in batch.keys():

review\_label.extend(batch\[""anecdote\_lead\_final""\])

review\_id.extend(batch\[""\_id""\].tolist())

hmd\_text.extend(batch\[""hmd\_comments""\])

head\_cust\_text.extend(batch\[""head\_cust""\])

agent.extend(batch\[""new\_transcript\_agent""\])

&amp;#x200B;

b\_review\_input\_ids = batch\[""review\_input\_ids""\].to(device)

b\_review\_attention\_mask = batch\[""review\_attention\_mask""\].to(device)

b\_review\_token\_type\_ids = batch\[""review\_token\_type\_ids""\].to(device)

b\_agent\_input\_ids = batch\[""agent\_input\_ids""\].to(device)

b\_agent\_attention\_mask = batch\[""agent\_attention\_mask""\].to(device)

b\_agent\_token\_type\_ids = batch\[""agent\_token\_type\_ids""\].to(device)

&amp;#x200B;

&amp;#x200B;

\# Tell pytorch not to bother with constructing the compute graph during

\# the forward pass, since this is only needed for backprop (training).

with torch.no\_grad():

(logits,) = model(review\_input\_ids=b\_review\_input\_ids,

review\_token\_type\_ids=b\_review\_token\_type\_ids,

review\_attention\_mask=b\_review\_attention\_mask,

agent\_input\_ids=b\_agent\_input\_ids,

agent\_token\_type\_ids=b\_agent\_token\_type\_ids,

agent\_attention\_mask=b\_agent\_attention\_mask

)

&amp;#x200B;

if logits.detach().cpu().numpy().size == 1:

pred\_logits.extend(logits.detach().cpu().numpy().reshape(1,))  

else:

pred\_logits.extend(logits.detach().cpu().numpy())



\# End of an epoch of validation

\# put model to train mode again.

model.train()

pred\_logits = np.array(pred\_logits)

pred\_prob = np.exp(pred\_logits)

pred\_prob = pred\_prob / (1 + pred\_prob)

pred\_label = pred\_prob.copy()

pred\_label\[pred\_label &lt; 0.5\] = 0

pred\_label\[pred\_label &gt;= 0.5\] = 1

\# compute the f1 score for each tags

d = {'Probability':pred\_prob,'Anecdotes Prediction':pred\_label}

pred\_df = pd.DataFrame(d, columns=\['Probability','Anecdotes Prediction'\])

result\_df = pd.DataFrame(

{

""review\_id"": review\_id,

""hmd\_text"": hmd\_text,

""head\_cust\_text"": head\_cust\_text,

""agent"": agent

}

)

if len(review\_label) != 0:

result\_df\[""review\_label""\] =  \[x.item() for x in review\_label\] 

return pd.concat(\[result\_df, pred\_df\], axis=1).set\_index(""review\_id"")

\`\`\`

&amp;#x200B;

Can anyone help me how to fix this issue? Thank you so much!!!",t2_vyx9y,False,,0,False,RuntimeError: arguments are located on different GPUs at /pytorch/aten/src/THC/generic/THCTensorIndex.cu:403,[],r/pytorch,False,6,,0,,,False,t3_jv2s5o,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1605540755.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I have a Pytorch Bert model was originally trained with 1 GPU. Now i moved it to 4GPU due to memory issue. However, when I moved it to 4GPU. I got an error message as below during the validation part:&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;RuntimeError: Caught RuntimeError in replica 1 on device 1.&lt;/p&gt;

&lt;p&gt;Original Traceback (most recent call last):&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py&amp;quot;, line 60, in _worker&lt;/p&gt;

&lt;p&gt;output = module(*input, **kwargs)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py&amp;quot;, line 550, in __call__&lt;/p&gt;

&lt;p&gt;result = self.forward(*input, **kwargs)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py&amp;quot;, line 155, in forward&lt;/p&gt;

&lt;p&gt;outputs = self.parallel_apply(replicas, inputs, kwargs)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py&amp;quot;, line 165, in parallel_apply&lt;/p&gt;

&lt;p&gt;return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py&amp;quot;, line 85, in parallel_apply&lt;/p&gt;

&lt;p&gt;output.reraise()&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/_utils.py&amp;quot;, line 395, in reraise&lt;/p&gt;

&lt;p&gt;raise self.exc_type(msg)&lt;/p&gt;

&lt;p&gt;RuntimeError: Caught RuntimeError in replica 0 on device 0.&lt;/p&gt;

&lt;p&gt;Original Traceback (most recent call last):&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py&amp;quot;, line 60, in _worker&lt;/p&gt;

&lt;p&gt;output = module(*input, **kwargs)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py&amp;quot;, line 550, in __call__&lt;/p&gt;

&lt;p&gt;result = self.forward(*input, **kwargs)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/SageMaker/Anecdotes/model.py&amp;quot;, line 117, in forward&lt;/p&gt;

&lt;p&gt;inputs_embeds=None,&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py&amp;quot;, line 550, in __call__&lt;/p&gt;

&lt;p&gt;result = self.forward(*input, **kwargs)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py&amp;quot;, line 727, in forward&lt;/p&gt;

&lt;p&gt;input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py&amp;quot;, line 550, in __call__&lt;/p&gt;

&lt;p&gt;result = self.forward(*input, **kwargs)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling_bert.py&amp;quot;, line 174, in forward&lt;/p&gt;

&lt;p&gt;inputs_embeds = self.word_embeddings(input_ids)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py&amp;quot;, line 550, in __call__&lt;/p&gt;

&lt;p&gt;result = self.forward(*input, **kwargs)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/sparse.py&amp;quot;, line 114, in forward&lt;/p&gt;

&lt;p&gt;self.norm_type, self.scale_grad_by_freq, self.sparse)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py&amp;quot;, line 1724, in embedding&lt;/p&gt;

&lt;p&gt;return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)&lt;/p&gt;

&lt;p&gt;RuntimeError: arguments are located on different GPUs at /pytorch/aten/src/THC/generic/THCTensorIndex.cu:403&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;My &lt;a href=""https://model.py""&gt;model.py&lt;/a&gt; is as below:&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;import torch&lt;/p&gt;

&lt;p&gt;import torch.nn as nn&lt;/p&gt;

&lt;p&gt;import torch.nn.functional as F&lt;/p&gt;

&lt;p&gt;from torch.nn import MultiheadAttention, EmbeddingBag, CrossEntropyLoss, MultiLabelSoftMarginLoss, BCEWithLogitsLoss&lt;/p&gt;

&lt;p&gt;from transformers import BertPreTrainedModel, BertModel, BertTokenizer, AdamW&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;from utils import LABEL_NAME&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;class ReviewClassification(BertPreTrainedModel):&lt;/p&gt;

&lt;p&gt;def __init__(self, config,&lt;/p&gt;

&lt;p&gt;add_agent_text, agent_text_heads):&lt;/p&gt;

&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;

&lt;p&gt;:param config: Bert configuration, can set up some parameters, like  output_attention, output_hidden_states&lt;/p&gt;

&lt;p&gt;:param add_agent_text: whether to use the non text feature, and how.&lt;/p&gt;

&lt;p&gt;It can have three options: None, &amp;quot;concat&amp;quot; and &amp;quot;attention&amp;quot;&lt;/p&gt;

&lt;p&gt;:param agent_text_heads: number of the heads in agent attention mechanism. Only useful if add_agent_text are set to&lt;/p&gt;

&lt;p&gt;&amp;quot;attention&amp;quot;&lt;/p&gt;

&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;

&lt;p&gt;super().__init__(config)&lt;/p&gt;

&lt;p&gt;# self.num_labels = 2&lt;/p&gt;

&lt;p&gt;self.add_agent_text = add_agent_text&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;self.bert = BertModel(config)&lt;/p&gt;

&lt;p&gt;self.dropout = nn.Dropout(config.hidden_dropout_prob)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;embedding_size = config.hidden_size&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;if self.add_agent_text == &amp;quot;concat&amp;quot;:&lt;/p&gt;

&lt;p&gt;embedding_size = 2 * embedding_size&lt;/p&gt;

&lt;p&gt;elif self.add_agent_text == &amp;quot;attention&amp;quot;:&lt;/p&gt;

&lt;p&gt;self.agent_attention = nn.MultiheadAttention(embedding_size, num_heads=agent_text_heads)&lt;/p&gt;

&lt;p&gt;else:&lt;/p&gt;

&lt;p&gt;# don&amp;#39;t use the information in Agent text&lt;/p&gt;

&lt;p&gt;pass&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;self.classifier = nn.Linear(embedding_size, 1) # self.classifier = nn.Linear(embedding_size, len(LABEL_NAME)) # bias: If set to False, the layer will not learn an additive bias&lt;/p&gt;

&lt;p&gt;self.init_weights()&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;print(&lt;/p&gt;

&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;            &lt;/p&gt;

&lt;p&gt;add agent text         :{}&lt;/p&gt;

&lt;p&gt;agent text multi-head  :{}&lt;/p&gt;

&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;.format(self.add_agent_text, agent_text_heads)&lt;/p&gt;

&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;def forward(&lt;/p&gt;

&lt;p&gt;self,&lt;/p&gt;

&lt;p&gt;review_input_ids=None,&lt;/p&gt;

&lt;p&gt;review_attention_mask=None,&lt;/p&gt;

&lt;p&gt;review_token_type_ids=None,&lt;/p&gt;

&lt;p&gt;agent_input_ids=None,&lt;/p&gt;

&lt;p&gt;agent_attention_mask=None,&lt;/p&gt;

&lt;p&gt;agent_token_type_ids=None,&lt;/p&gt;

&lt;p&gt;labels=None,&lt;/p&gt;

&lt;p&gt;):&lt;/p&gt;

&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;

&lt;p&gt;labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):&lt;/p&gt;

&lt;p&gt;Labels for computing the sequence classification/regression loss.&lt;/p&gt;

&lt;p&gt;Indices should be in :obj:`[0, ..., config.num_labels - 1]`.&lt;/p&gt;

&lt;p&gt;If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),&lt;/p&gt;

&lt;p&gt;If :obj:`config.num_labels &amp;gt; 1` a classification loss is computed (Cross-Entropy).&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Returns:&lt;/p&gt;

&lt;p&gt;:obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:&lt;/p&gt;

&lt;p&gt;loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):&lt;/p&gt;

&lt;p&gt;Classification (or regression if config.num_labels==1) loss.&lt;/p&gt;

&lt;p&gt;logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):&lt;/p&gt;

&lt;p&gt;Classification (or regression if config.num_labels==1) scores (before SoftMax).&lt;/p&gt;

&lt;p&gt;hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):&lt;/p&gt;

&lt;p&gt;Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)&lt;/p&gt;

&lt;p&gt;of shape :obj:`(batch_size, sequence_length, hidden_size)`.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Hidden-states of the model at the output of each layer plus the initial embedding outputs.&lt;/p&gt;

&lt;p&gt;attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):&lt;/p&gt;

&lt;p&gt;Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape&lt;/p&gt;

&lt;p&gt;:obj:`(batch_size, num_heads, sequence_length, sequence_length)`.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Attentions weights after the attention softmax, used to compute the weighted average in the self-attention&lt;/p&gt;

&lt;p&gt;heads.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Examples::&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;from transformers import BertTokenizer, BertForSequenceClassification&lt;/p&gt;

&lt;p&gt;import torch&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;tokenizer = BertTokenizer.from_pretrained(&amp;#39;bert-base-uncased&amp;#39;)&lt;/p&gt;

&lt;p&gt;model = BertForSequenceClassification.from_pretrained(&amp;#39;bert-base-uncased&amp;#39;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;input_ids = torch.tensor(tokenizer.encode(&amp;quot;Hello, my dog is cute&amp;quot;, add_special_tokens=True)).unsqueeze(0)  # Batch size 1&lt;/p&gt;

&lt;p&gt;labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1&lt;/p&gt;

&lt;p&gt;outputs = model(input_ids, labels=labels)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;loss, logits = outputs[:2]&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;review_outputs = self.bert(&lt;/p&gt;

&lt;p&gt;review_input_ids,&lt;/p&gt;

&lt;p&gt;attention_mask=review_attention_mask,&lt;/p&gt;

&lt;p&gt;token_type_ids=review_token_type_ids,&lt;/p&gt;

&lt;p&gt;position_ids=None,&lt;/p&gt;

&lt;p&gt;head_mask=None,&lt;/p&gt;

&lt;p&gt;inputs_embeds=None,&lt;/p&gt;

&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;if self.add_agent_text is not None:&lt;/p&gt;

&lt;p&gt;# means that self.add_agent_text is &amp;quot;concat&amp;quot; or &amp;quot;attention&amp;quot;&lt;/p&gt;

&lt;p&gt;# TODO: we can try that agent_outputs do not share the same parameter&lt;/p&gt;

&lt;p&gt;agent_outputs = self.bert(&lt;/p&gt;

&lt;p&gt;agent_input_ids,&lt;/p&gt;

&lt;p&gt;attention_mask=agent_attention_mask,&lt;/p&gt;

&lt;p&gt;token_type_ids=agent_token_type_ids,&lt;/p&gt;

&lt;p&gt;position_ids=None,&lt;/p&gt;

&lt;p&gt;head_mask=None,&lt;/p&gt;

&lt;p&gt;inputs_embeds=None,&lt;/p&gt;

&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;if self.add_agent_text == &amp;quot;attention&amp;quot;:&lt;/p&gt;

&lt;p&gt;review_hidden_states = review_outputs[0].transpose(0, 1)  # before trans: (bs, seq_len, hidden_size)&lt;/p&gt;

&lt;p&gt;agent_hidden_states = agent_outputs[0].mean(axis=1).unsqueeze(dim=0)  # (1, batch_size, hidden_size)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;attn_output, _ = self.agent_attention(agent_hidden_states, review_hidden_states, review_hidden_states)&lt;/p&gt;

&lt;p&gt;feature = attn_output.squeeze()  # (batch_size, seq_len)&lt;/p&gt;

&lt;p&gt;else:&lt;/p&gt;

&lt;p&gt;feature = review_outputs[1]  # (batch_size, seq_len) -? Should it be (batch_size, hidden_size)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;if self.add_agent_text == &amp;quot;concat&amp;quot;:&lt;/p&gt;

&lt;p&gt;feature = &lt;a href=""https://torch.cat""&gt;torch.cat&lt;/a&gt;([feature, agent_outputs[1]], axis=1)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# nn.CrossEntropyLoss applies F.log_softmax and nn.NLLLoss internally on your input,&lt;/p&gt;

&lt;p&gt;# so you should pass the raw logits to it.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# torch.nn.functional.binary_cross_entropy takes logistic sigmoid values as inputs&lt;/p&gt;

&lt;p&gt;# torch.nn.functional.binary_cross_entropy_with_logits takes logits as inputs&lt;/p&gt;

&lt;p&gt;# torch.nn.functional.cross_entropy takes logits as inputs (performs log_softmax internally)&lt;/p&gt;

&lt;p&gt;# torch.nn.functional.nll_loss is like cross_entropy but takes log-probabilities (log-softmax) values as inputs&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# CrossEntropyLoss takes prediction logits (size: (N,D)) and target labels (size: (N,)) &lt;/p&gt;

&lt;p&gt;# CrossEntropyLoss expects logits i.e whereas BCELoss expects probability value&lt;/p&gt;

&lt;p&gt;logits = self.classifier(feature).squeeze()&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;outputs = (logits,)  # + outputs[2:]  # add hidden states and attention if they are here&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;if labels is not None:&lt;/p&gt;

&lt;p&gt;##### original&lt;/p&gt;

&lt;p&gt;# loss_fct = MultiLabelSoftMarginLoss()&lt;/p&gt;

&lt;p&gt;# loss = loss_fct(logits, labels)&lt;/p&gt;

&lt;p&gt;# outputs = (loss,) + outputs&lt;/p&gt;

&lt;p&gt;#### Version 1 try&lt;/p&gt;

&lt;p&gt;# pos_weight = dataset.label_proportion.iloc[0]/dataset.label_proportion.iloc[1]&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Version 1.1 for weight&lt;/p&gt;

&lt;p&gt;# weight = torch.tensor([0.101521, 0.898479]) # hard code from entire training dataset&lt;/p&gt;

&lt;p&gt;# pos_weight = weight[labels.data.view(-1).long()].view_as(labels)&lt;/p&gt;

&lt;p&gt;# Version 1.2 for weight&lt;/p&gt;

&lt;p&gt;pos_weight=torch.tensor(1)&lt;/p&gt;

&lt;p&gt;# Version 1.3 for weight&lt;/p&gt;

&lt;p&gt;#weight = torch.tensor([1.0, 8.85]) # hard code from entire training dataset&lt;/p&gt;

&lt;p&gt;#pos_weight = weight[labels.data.view(-1).long()].view_as(labels)&lt;/p&gt;

&lt;p&gt;loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weight).cuda() &lt;/p&gt;

&lt;p&gt;loss = loss_fct(logits, labels)&lt;/p&gt;

&lt;p&gt;# loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))&lt;/p&gt;

&lt;p&gt;outputs = (loss,) + outputs&lt;/p&gt;

&lt;p&gt;### Version 2 try&lt;/p&gt;

&lt;p&gt;# loss_fct = nn.CrossEntropyLoss()&lt;/p&gt;

&lt;p&gt;# loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))&lt;/p&gt;

&lt;p&gt;# outputs = (loss,) + outputs&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;return outputs  # (loss, logits, hidden_states, attentions)&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;And my train_valid_test.py for the training, validation, test process is as below:&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;import time&lt;/p&gt;

&lt;p&gt;import pickle&lt;/p&gt;

&lt;p&gt;from path import Path&lt;/p&gt;

&lt;p&gt;import numpy as np&lt;/p&gt;

&lt;p&gt;import pandas as pd&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix&lt;/p&gt;

&lt;p&gt;import torch&lt;/p&gt;

&lt;p&gt;import torch.nn as nn&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;from utils import LABEL_NAME, isnotebook, set_seed, format_time&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;if isnotebook():&lt;/p&gt;

&lt;p&gt;from tqdm.notebook import tqdm&lt;/p&gt;

&lt;p&gt;else:&lt;/p&gt;

&lt;p&gt;from tqdm import tqdm&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;set_seed(seed=228)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;def model_train(model, train_data_loader, valid_data_loader, test_data_loader,&lt;/p&gt;

&lt;p&gt;logger, optimizer, scheduler, num_epochs, seed, out_dir):&lt;/p&gt;

&lt;p&gt;# move model to gpu&lt;/p&gt;

&lt;p&gt;device = torch.device(&amp;#39;cuda&amp;#39; if torch.cuda.is_available() else &amp;#39;cpu&amp;#39;)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://model.to""&gt;model.to&lt;/a&gt;(device)&lt;/p&gt;

&lt;p&gt;if torch.cuda.device_count() &amp;gt; 1:&lt;/p&gt;

&lt;p&gt;model = nn.DataParallel(model)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;num_gpus = torch.cuda.device_count()&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;Let&amp;#39;s use {} GPUs!&amp;quot;.format(num_gpus))&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Set the seed value all over the place to make this reproducible.&lt;/p&gt;

&lt;p&gt;#     set_seed(seed=seed)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# We&amp;#39;ll store a number of quantities such as training and validation loss,&lt;/p&gt;

&lt;p&gt;# validation accuracy, and timings.&lt;/p&gt;

&lt;p&gt;training_stats = []&lt;/p&gt;

&lt;p&gt;print_interval = 100&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Measure the total training time for the whole run.&lt;/p&gt;

&lt;p&gt;total_t0 = time.time()&lt;/p&gt;

&lt;p&gt;batch_size = train_data_loader.batch_size&lt;/p&gt;

&lt;p&gt;num_batch = len(train_data_loader)&lt;/p&gt;

&lt;p&gt;best_f1_score = {&lt;/p&gt;

&lt;p&gt;&amp;quot;weighted&amp;quot;: 0,&lt;/p&gt;

&lt;p&gt;&amp;quot;averaged&amp;quot;: 0&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;best_test_f1_score = 0&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# For each epoch...&lt;/p&gt;

&lt;p&gt;for epoch_i in range(0, num_epochs):&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# ========================================&lt;/p&gt;

&lt;p&gt;#               Training&lt;/p&gt;

&lt;p&gt;# ========================================&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Perform one full pass over the training set.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;&amp;quot;)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;#39;======== Epoch {:} / {:} ========&amp;#39;.format(epoch_i + 1, num_epochs))&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;#39;Training...&amp;#39;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Reset the total loss for this epoch.&lt;/p&gt;

&lt;p&gt;total_train_loss = 0&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Measure how long the training epoch takes.&lt;/p&gt;

&lt;p&gt;t_train = time.time()&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;model.train()&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# For each batch of training data...&lt;/p&gt;

&lt;p&gt;for step, batch in tqdm(enumerate(train_data_loader), desc=&amp;quot;Training Iteration&amp;quot;, total=num_batch):&lt;/p&gt;

&lt;p&gt;# Progress update every 100 batches.&lt;/p&gt;

&lt;p&gt;if step % print_interval == 0 and not step == 0:&lt;/p&gt;

&lt;p&gt;# Calculate elapsed time in minutes.&lt;/p&gt;

&lt;p&gt;elapsed = format_time(time.time() - t_train)&lt;/p&gt;

&lt;p&gt;avg_train_loss = total_train_loss / print_interval&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Report progress.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;#39;| epoch {:3d} | {:5d}/{:5d} batches | lr {:.3e} | loss {:5.3f} | Elapsed {:s}&amp;#39;.format(&lt;/p&gt;

&lt;p&gt;epoch_i+1, step, num_batch, scheduler.get_last_lr()[0], avg_train_loss, elapsed)&lt;/p&gt;

&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;total_train_loss = 0&lt;/p&gt;

&lt;p&gt;training_stats.append(&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;&amp;#39;epoch&amp;#39;: epoch_i + 1,&lt;/p&gt;

&lt;p&gt;&amp;#39;step&amp;#39;: step,&lt;/p&gt;

&lt;p&gt;&amp;#39;train loss&amp;#39;: avg_train_loss,&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Unpack this training batch from our dataloader.&lt;/p&gt;

&lt;p&gt;#&lt;/p&gt;

&lt;p&gt;# As we unpack the batch, we&amp;#39;ll also copy each tensor to the GPU using the&lt;/p&gt;

&lt;p&gt;# `to` method.&lt;/p&gt;

&lt;p&gt;#&lt;/p&gt;

&lt;p&gt;# `batch` contains four pytorch tensors:&lt;/p&gt;

&lt;p&gt;#   &amp;quot;input_ids&amp;quot;&lt;/p&gt;

&lt;p&gt;#   &amp;quot;attention_mask&amp;quot;&lt;/p&gt;

&lt;p&gt;#   &amp;quot;token_type_ids&amp;quot;&lt;/p&gt;

&lt;p&gt;#   &amp;quot;binarized_labels&amp;quot;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;b_review_input_ids = batch[&amp;quot;review_input_ids&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_review_attention_mask = batch[&amp;quot;review_attention_mask&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_review_token_type_ids = batch[&amp;quot;review_token_type_ids&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_agent_input_ids = batch[&amp;quot;agent_input_ids&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_agent_attention_mask = batch[&amp;quot;agent_attention_mask&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_agent_token_type_ids = batch[&amp;quot;agent_token_type_ids&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;b_binarized_label = batch[&amp;quot;binarized_label&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;model.zero_grad()&lt;/p&gt;

&lt;p&gt;(loss, _) = model(review_input_ids=b_review_input_ids,&lt;/p&gt;

&lt;p&gt;review_attention_mask=b_review_attention_mask,&lt;/p&gt;

&lt;p&gt;review_token_type_ids=b_review_token_type_ids,&lt;/p&gt;

&lt;p&gt;agent_input_ids=b_agent_input_ids,&lt;/p&gt;

&lt;p&gt;agent_attention_mask=b_agent_attention_mask,&lt;/p&gt;

&lt;p&gt;agent_token_type_ids=b_agent_token_type_ids,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;labels=b_binarized_label&lt;/p&gt;

&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Accumulate the training loss over all of the batches so that we can&lt;/p&gt;

&lt;p&gt;# calculate the average loss at the end. `loss` is a Tensor containing a&lt;/p&gt;

&lt;p&gt;# single value; the `.item()` function just returns the Python value&lt;/p&gt;

&lt;p&gt;# from the tensor.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;if num_gpus &amp;gt; 1:&lt;/p&gt;

&lt;p&gt;total_train_loss += loss.mean().item()&lt;/p&gt;

&lt;p&gt;loss.mean().backward()  # use loss.mean().backward() instead of loss.backward() for multiple gpu trainings&lt;/p&gt;

&lt;p&gt;else:&lt;/p&gt;

&lt;p&gt;total_train_loss += loss.item()&lt;/p&gt;

&lt;p&gt;loss.backward()&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Clip the norm of the gradients to 1.0.&lt;/p&gt;

&lt;p&gt;# This is to help prevent the &amp;quot;exploding gradients&amp;quot; problem.&lt;/p&gt;

&lt;p&gt;torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Update parameters and take a step using the computed gradient.&lt;/p&gt;

&lt;p&gt;# The optimizer dictates the &amp;quot;update rule&amp;quot;--how the parameters are&lt;/p&gt;

&lt;p&gt;# modified based on their gradients, the learning rate, etc.&lt;/p&gt;

&lt;p&gt;optimizer.step()&lt;/p&gt;

&lt;p&gt;scheduler.step()&lt;/p&gt;

&lt;p&gt;# End of training epoch&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Measure how long this epoch took.&lt;/p&gt;

&lt;p&gt;training_time = format_time(time.time() - t_train)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;&amp;quot;)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;  Training epoch took: {:s}&amp;quot;.format(training_time))&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# evaluate the model after one epoch.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# ========================================&lt;/p&gt;

&lt;p&gt;#               Validation&lt;/p&gt;

&lt;p&gt;# ========================================&lt;/p&gt;

&lt;p&gt;# After the completion of each training epoch, measure our performance on&lt;/p&gt;

&lt;p&gt;# our validation set.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;&amp;quot;)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;Validating...&amp;quot;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;t_valid = time.time()&lt;/p&gt;

&lt;p&gt;model.eval()&lt;/p&gt;

&lt;p&gt;ave_valid_loss, valid_f1_table, cm_table, f1_score = model_validate(model=model, data_loader=valid_data_loader)&lt;/p&gt;

&lt;p&gt;# Measure how long this epoch took.&lt;/p&gt;

&lt;p&gt;validation_time = format_time(time.time() - t_valid)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;&amp;quot;)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;#39;| loss {:5.3f} | Elapsed {:s}&amp;#39;.format(ave_valid_loss, validation_time))&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;  \n{:s}&amp;quot;.format(valid_f1_table.to_string()))&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;&amp;quot;)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;  \n{:s}&amp;quot;.format(cm_table.to_string()))&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# need to store the best model&lt;/p&gt;

&lt;p&gt;for key in best_f1_score.keys():&lt;/p&gt;

&lt;p&gt;if best_f1_score[key] &amp;lt; f1_score[key]:&lt;/p&gt;

&lt;p&gt;# remove the old model:&lt;/p&gt;

&lt;p&gt;file_list = [f for f in out_dir.files() if f.name.endswith(&amp;quot;.pt&amp;quot;) and f.name.startswith(key)]&lt;/p&gt;

&lt;p&gt;for f in file_list:&lt;/p&gt;

&lt;p&gt;Path.remove(f)&lt;/p&gt;

&lt;p&gt;model_file = out_dir.joinpath(&amp;#39;{:s}_epoch_{:02d}-f1_{:.3f}.pt&amp;#39;.format(&lt;/p&gt;

&lt;p&gt;key, epoch_i + 1, f1_score[key])&lt;/p&gt;

&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;best_f1_score[key] = f1_score[key]&lt;/p&gt;

&lt;p&gt;if num_gpus &amp;gt; 1:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://torch.save""&gt;torch.save&lt;/a&gt;(model.module.state_dict(), model_file)&lt;/p&gt;

&lt;p&gt;else:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://torch.save""&gt;torch.save&lt;/a&gt;(model.state_dict(), model_file)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# ========================================&lt;/p&gt;

&lt;p&gt;#               Test&lt;/p&gt;

&lt;p&gt;# ========================================&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;&amp;quot;)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;Testing...&amp;quot;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;result_df = model_test(model=model, data_loader=test_data_loader)&lt;/p&gt;

&lt;p&gt;y_true = np.array(result_df[&amp;quot;review_label&amp;quot;], dtype=np.bool) # This part may need double check&lt;/p&gt;

&lt;p&gt;y_pred = result_df[&amp;quot;Probability&amp;quot;] &amp;gt; 0.5&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;report = classification_report(y_true, y_pred, output_dict=True)&lt;/p&gt;

&lt;p&gt;metrics_df = pd.DataFrame(report).transpose()&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;metrics_df = metrics_df.sort_index()&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;weighted_f1_score = metrics_df.loc[&amp;#39;weighted avg&amp;#39;, &amp;#39;f1-score&amp;#39;]&lt;/p&gt;

&lt;p&gt;averaged_f1_score = metrics_df.loc[&amp;#39;macro avg&amp;#39;, &amp;#39;f1-score&amp;#39;]&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;best_test_f1_score = metrics_df.loc[&amp;#39;weighted avg&amp;#39;, &amp;#39;f1-score&amp;#39;] \&lt;/p&gt;

&lt;p&gt;if best_test_f1_score &amp;lt; metrics_df.loc[&amp;#39;weighted avg&amp;#39;, &amp;#39;f1-score&amp;#39;] else best_test_f1_score&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;metrics_df = metrics_df.astype(float).round(3)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Calculate confusion matrix&lt;/p&gt;

&lt;p&gt;tn, fp, fn, tp  = confusion_matrix(y_true, y_pred).ravel()&lt;/p&gt;

&lt;p&gt;cm_df = pd.DataFrame(columns = [&amp;#39;Predicted No&amp;#39;, &amp;#39;Predicted Yes&amp;#39;],  &lt;/p&gt;

&lt;p&gt;index = [&amp;#39;Actual No&amp;#39;, &amp;#39;Actual Yes&amp;#39;]) &lt;/p&gt;

&lt;p&gt;# adding rows to an empty  &lt;/p&gt;

&lt;p&gt;# dataframe at existing index &lt;/p&gt;

&lt;p&gt;cm_df.loc[&amp;#39;Actual No&amp;#39;] = [tn,fp] &lt;/p&gt;

&lt;p&gt;cm_df.loc[&amp;#39;Actual Yes&amp;#39;] = [fn,tp]&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;use model: {} batch / {} step&amp;quot;.format(epoch_i + 1, step))&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;\n&amp;quot; + &amp;quot;=&amp;quot; * 50)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;\n&amp;quot; + metrics_df.to_string())&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;\n&amp;quot; + &amp;quot;=&amp;quot; * 50)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;\n&amp;quot; + cm_df.to_string())&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;best test F1 score: {}&amp;quot;.format(best_test_f1_score))&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;\n&amp;quot; + &amp;quot;=&amp;quot; * 50)&lt;/p&gt;

&lt;p&gt;# Below is to save the result files&lt;/p&gt;

&lt;p&gt;#         result_filename = &amp;quot;result_df_epoch_&amp;quot; + str(epoch_i + 1) + &amp;quot;.xlsx&amp;quot;&lt;/p&gt;

&lt;p&gt;#         result_df.to_excel(out_dir.joinpath(result_filename), index=False)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;&amp;quot;)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;Training complete!&amp;quot;)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://logger.info""&gt;logger.info&lt;/a&gt;(&amp;quot;Total training took {:} (h:mm:ss)&amp;quot;.format(format_time(time.time() - total_t0)))&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Save training_stats to csv file&lt;/p&gt;

&lt;p&gt;pd.DataFrame(training_stats).to_csv(out_dir.joinpath(&amp;quot;model_train.log&amp;quot;), index=False)&lt;/p&gt;

&lt;p&gt;return model, optimizer, scheduler&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;def model_validate(model, data_loader):&lt;/p&gt;

&lt;p&gt;# Put the model in evaluation mode--the dropout layers behave differently&lt;/p&gt;

&lt;p&gt;# during evaluation.&lt;/p&gt;

&lt;p&gt;model.eval()&lt;/p&gt;

&lt;p&gt;device = torch.device(&amp;#39;cuda&amp;#39; if torch.cuda.is_available() else &amp;#39;cpu&amp;#39;)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://model.to""&gt;model.to&lt;/a&gt;(device)&lt;/p&gt;

&lt;p&gt;if torch.cuda.device_count() &amp;gt; 1:&lt;/p&gt;

&lt;p&gt;model = nn.DataParallel(model)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;label_prop = data_loader.dataset.dataset.label_prop()&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;total_valid_loss = 0&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;batch_size = data_loader.batch_size&lt;/p&gt;

&lt;p&gt;num_batch = len(data_loader)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;y_pred, y_true = [], []&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Evaluate data&lt;/p&gt;

&lt;p&gt;for step, batch in tqdm(enumerate(data_loader), desc=&amp;quot;Validation...&amp;quot;, total=num_batch):&lt;/p&gt;

&lt;p&gt;b_review_input_ids = batch[&amp;quot;review_input_ids&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_review_attention_mask = batch[&amp;quot;review_attention_mask&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_review_token_type_ids = batch[&amp;quot;review_token_type_ids&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_agent_input_ids = batch[&amp;quot;agent_input_ids&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_agent_attention_mask = batch[&amp;quot;agent_attention_mask&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_agent_token_type_ids = batch[&amp;quot;agent_token_type_ids&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;b_binarized_label = batch[&amp;quot;binarized_label&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Tell pytorch not to bother with constructing the compute graph during&lt;/p&gt;

&lt;p&gt;# the forward pass, since this is only needed for backprop (training).&lt;/p&gt;

&lt;p&gt;with torch.no_grad():&lt;/p&gt;

&lt;p&gt;(loss, logits,) = model(review_input_ids=b_review_input_ids,&lt;/p&gt;

&lt;p&gt;review_attention_mask=b_review_attention_mask,&lt;/p&gt;

&lt;p&gt;review_token_type_ids=b_review_token_type_ids,&lt;/p&gt;

&lt;p&gt;agent_input_ids=b_agent_input_ids,&lt;/p&gt;

&lt;p&gt;agent_attention_mask=b_agent_attention_mask,&lt;/p&gt;

&lt;p&gt;agent_token_type_ids=b_agent_token_type_ids,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;labels=b_binarized_label)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;total_valid_loss += loss.item()&lt;/p&gt;

&lt;p&gt;### The sigmoid function is used for the two-class logistic regression, &lt;/p&gt;

&lt;p&gt;### whereas the softmax function is used for the multiclass logistic regression&lt;/p&gt;

&lt;p&gt;# Version 1&lt;/p&gt;

&lt;p&gt;# numpy_probas = logits.detach().cpu().numpy()&lt;/p&gt;

&lt;p&gt;# y_pred.extend(np.argmax(numpy_probas, axis=1).flatten())&lt;/p&gt;

&lt;p&gt;# y_true.extend(b_binarized_label.cpu().numpy())&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Version 2&lt;/p&gt;

&lt;p&gt;# transfored_logits = F.log_softmax(logits,dim=1)&lt;/p&gt;

&lt;p&gt;# numpy_probas = transfored_logits.detach().cpu().numpy()&lt;/p&gt;

&lt;p&gt;# y_pred.extend(np.argmax(numpy_probas, axis=1).flatten())&lt;/p&gt;

&lt;p&gt;# y_true.extend(b_binarized_label.cpu().numpy())&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Version 3&lt;/p&gt;

&lt;p&gt;# transfored_logits = torch.sigmoid(logits)&lt;/p&gt;

&lt;p&gt;# numpy_probas = transfored_logits.detach().cpu().numpy()&lt;/p&gt;

&lt;p&gt;# y_pred.extend(np.argmax(numpy_probas, axis=1).flatten())&lt;/p&gt;

&lt;p&gt;# y_true.extend(b_binarized_label.cpu().numpy())&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# New version - for num_label = 1&lt;/p&gt;

&lt;p&gt;transfored_logits = torch.sigmoid(logits)&lt;/p&gt;

&lt;p&gt;numpy_probas = transfored_logits.detach().cpu().numpy()&lt;/p&gt;

&lt;p&gt;y_pred.extend(numpy_probas)&lt;/p&gt;

&lt;p&gt;y_true.extend(b_binarized_label.cpu().numpy())&lt;/p&gt;

&lt;p&gt;# End of an epoch of validation&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# put model to train mode again.&lt;/p&gt;

&lt;p&gt;model.train()&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;ave_loss = total_valid_loss / (num_batch * batch_size)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;y_pred = np.array(y_pred)&lt;/p&gt;

&lt;p&gt;y_pred[y_pred &amp;lt; 0.5] = 0&lt;/p&gt;

&lt;p&gt;y_pred[y_pred &amp;gt;= 0.5] = 1&lt;/p&gt;

&lt;p&gt;# Below is in case the input and target are not the same data format&lt;/p&gt;

&lt;p&gt;y_pred = np.array(y_pred, dtype=np.bool)&lt;/p&gt;

&lt;p&gt;y_true = np.array(y_true, dtype=np.bool)&lt;/p&gt;

&lt;p&gt;# compute the various f1 score for each label&lt;/p&gt;

&lt;p&gt;report = classification_report(y_true, y_pred, output_dict=True)&lt;/p&gt;

&lt;p&gt;metrics_df = pd.DataFrame(report).transpose()&lt;/p&gt;

&lt;p&gt;# metrics_df = pd.DataFrame(0, index=LABEL_NAME, columns=[&amp;quot;Precision&amp;quot;, &amp;quot;Recall&amp;quot;, &amp;quot;F1&amp;quot;,&amp;quot;support&amp;quot;])&lt;/p&gt;

&lt;p&gt;# metrics_df.Precision = precision_recall_fscore_support(y_true, y_pred)[0]&lt;/p&gt;

&lt;p&gt;# metrics_df.Recall = precision_recall_fscore_support(y_true, y_pred)[1]&lt;/p&gt;

&lt;p&gt;# metrics_df.F1 = precision_recall_fscore_support(y_true, y_pred)[2]&lt;/p&gt;

&lt;p&gt;# metrics_df.support = precision_recall_fscore_support(y_true, y_pred)[3]&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# y_pred = np.array(y_pred)&lt;/p&gt;

&lt;p&gt;# y_pred[y_pred &amp;lt; 0] = 0&lt;/p&gt;

&lt;p&gt;# y_pred[y_pred &amp;gt; 0] = 1&lt;/p&gt;

&lt;p&gt;# y_pred = np.array(y_pred, dtype=np.bool)&lt;/p&gt;

&lt;p&gt;# y_true = np.array(y_true, dtype=np.bool)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# metrics_df = pd.DataFrame(0, index=LABEL_NAME, columns=[&amp;quot;Precision&amp;quot;, &amp;quot;Recall&amp;quot;, &amp;quot;F1&amp;quot;], dtype=np.float)&lt;/p&gt;

&lt;p&gt;# # or_y_pred = np.zeros(y_pred.shape[0], dtype=np.bool)&lt;/p&gt;

&lt;p&gt;# # or_y_true = np.zeros(y_true.shape[0], dtype=np.bool)&lt;/p&gt;

&lt;p&gt;# for i in range(len(LABEL_NAME)):&lt;/p&gt;

&lt;p&gt;#     metrics_df.iloc[i] = precision_recall_fscore_support(&lt;/p&gt;

&lt;p&gt;#         y_true=y_true[:, i], y_pred=y_pred[:, i], average=&amp;#39;binary&amp;#39;, zero_division=0)[0:3]&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# or_y_pred = or_y_pred | y_pred[:, i]&lt;/p&gt;

&lt;p&gt;# or_y_true = or_y_true | y_true[:, i]&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;metrics_df = metrics_df.sort_index()&lt;/p&gt;

&lt;p&gt;# metrics_df.loc[&amp;#39;Weighted Average&amp;#39;] = metrics_df.transpose().dot(label_prop)&lt;/p&gt;

&lt;p&gt;# metrics_df.loc[&amp;#39;Average&amp;#39;] = metrics_df.mean()&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# metrics_df.loc[&amp;#39;Weighted Average&amp;#39;, &amp;#39;F1&amp;#39;] = 2 / (1/metrics_df.loc[&amp;#39;Weighted Average&amp;#39;, &amp;quot;Recall&amp;quot;] +&lt;/p&gt;

&lt;p&gt;#                                                 1/metrics_df.loc[&amp;#39;Weighted Average&amp;#39;, &amp;quot;Precision&amp;quot;])&lt;/p&gt;

&lt;p&gt;# metrics_df.loc[&amp;#39;Average&amp;#39;, &amp;#39;F1&amp;#39;] = 2 / (1/metrics_df.loc[&amp;#39;Average&amp;#39;, &amp;quot;Recall&amp;quot;] +&lt;/p&gt;

&lt;p&gt;#                                        1/metrics_df.loc[&amp;#39;Average&amp;#39;, &amp;quot;Precision&amp;quot;])&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;weighted_f1_score = metrics_df.loc[&amp;#39;weighted avg&amp;#39;, &amp;#39;f1-score&amp;#39;]&lt;/p&gt;

&lt;p&gt;averaged_f1_score = metrics_df.loc[&amp;#39;macro avg&amp;#39;, &amp;#39;f1-score&amp;#39;]&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Calculate confusion matrix&lt;/p&gt;

&lt;p&gt;tn, fp, fn, tp  = confusion_matrix(y_true, y_pred).ravel()&lt;/p&gt;

&lt;p&gt;cm_df = pd.DataFrame(columns = [&amp;#39;Predicted No&amp;#39;, &amp;#39;Predicted Yes&amp;#39;],  &lt;/p&gt;

&lt;p&gt;index = [&amp;#39;Actual No&amp;#39;, &amp;#39;Actual Yes&amp;#39;]) &lt;/p&gt;

&lt;p&gt;# adding rows to an empty  &lt;/p&gt;

&lt;p&gt;# dataframe at existing index &lt;/p&gt;

&lt;p&gt;cm_df.loc[&amp;#39;Actual No&amp;#39;] = [tn,fp] &lt;/p&gt;

&lt;p&gt;cm_df.loc[&amp;#39;Actual Yes&amp;#39;] = [fn,tp]&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# pooled_f1_score = f1_score(y_pred=or_y_pred, y_true=or_y_true)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;return ave_loss, metrics_df, cm_df,{&lt;/p&gt;

&lt;p&gt;&amp;quot;weighted&amp;quot;: weighted_f1_score,&lt;/p&gt;

&lt;p&gt;&amp;quot;averaged&amp;quot;: averaged_f1_score,&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;def model_test(model, data_loader):&lt;/p&gt;

&lt;p&gt;# Put the model in evaluation mode--the dropout layers behave differently&lt;/p&gt;

&lt;p&gt;# during evaluation.&lt;/p&gt;

&lt;p&gt;device = torch.device(&amp;#39;cuda&amp;#39; if torch.cuda.is_available() else &amp;#39;cpu&amp;#39;)&lt;/p&gt;

&lt;p&gt;model.eval()&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://model.to""&gt;model.to&lt;/a&gt;(device)&lt;/p&gt;

&lt;p&gt;if torch.cuda.device_count() &amp;gt; 1:&lt;/p&gt;

&lt;p&gt;model = nn.DataParallel(model)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;num_batch = len(data_loader)&lt;/p&gt;

&lt;p&gt;# Below need to modify if change the input&lt;/p&gt;

&lt;p&gt;review_id, review_label, hmd_text, head_cust_text = [], [], [], []&lt;/p&gt;

&lt;p&gt;agent = []&lt;/p&gt;

&lt;p&gt;pred_logits = []&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Evaluate data&lt;/p&gt;

&lt;p&gt;for step, batch in tqdm(enumerate(data_loader), desc=&amp;quot;Inference...&amp;quot;, total=num_batch):&lt;/p&gt;

&lt;p&gt;if &amp;quot;anecdote_lead_final&amp;quot; in batch.keys():&lt;/p&gt;

&lt;p&gt;review_label.extend(batch[&amp;quot;anecdote_lead_final&amp;quot;])&lt;/p&gt;

&lt;p&gt;review_id.extend(batch[&amp;quot;_id&amp;quot;].tolist())&lt;/p&gt;

&lt;p&gt;hmd_text.extend(batch[&amp;quot;hmd_comments&amp;quot;])&lt;/p&gt;

&lt;p&gt;head_cust_text.extend(batch[&amp;quot;head_cust&amp;quot;])&lt;/p&gt;

&lt;p&gt;agent.extend(batch[&amp;quot;new_transcript_agent&amp;quot;])&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;b_review_input_ids = batch[&amp;quot;review_input_ids&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_review_attention_mask = batch[&amp;quot;review_attention_mask&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_review_token_type_ids = batch[&amp;quot;review_token_type_ids&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_agent_input_ids = batch[&amp;quot;agent_input_ids&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_agent_attention_mask = batch[&amp;quot;agent_attention_mask&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;b_agent_token_type_ids = batch[&amp;quot;agent_token_type_ids&amp;quot;].to(device)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;# Tell pytorch not to bother with constructing the compute graph during&lt;/p&gt;

&lt;p&gt;# the forward pass, since this is only needed for backprop (training).&lt;/p&gt;

&lt;p&gt;with torch.no_grad():&lt;/p&gt;

&lt;p&gt;(logits,) = model(review_input_ids=b_review_input_ids,&lt;/p&gt;

&lt;p&gt;review_token_type_ids=b_review_token_type_ids,&lt;/p&gt;

&lt;p&gt;review_attention_mask=b_review_attention_mask,&lt;/p&gt;

&lt;p&gt;agent_input_ids=b_agent_input_ids,&lt;/p&gt;

&lt;p&gt;agent_token_type_ids=b_agent_token_type_ids,&lt;/p&gt;

&lt;p&gt;agent_attention_mask=b_agent_attention_mask&lt;/p&gt;

&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;if logits.detach().cpu().numpy().size == 1:&lt;/p&gt;

&lt;p&gt;pred_logits.extend(logits.detach().cpu().numpy().reshape(1,))  &lt;/p&gt;

&lt;p&gt;else:&lt;/p&gt;

&lt;p&gt;pred_logits.extend(logits.detach().cpu().numpy())&lt;/p&gt;

&lt;p&gt;# End of an epoch of validation&lt;/p&gt;

&lt;p&gt;# put model to train mode again.&lt;/p&gt;

&lt;p&gt;model.train()&lt;/p&gt;

&lt;p&gt;pred_logits = np.array(pred_logits)&lt;/p&gt;

&lt;p&gt;pred_prob = np.exp(pred_logits)&lt;/p&gt;

&lt;p&gt;pred_prob = pred_prob / (1 + pred_prob)&lt;/p&gt;

&lt;p&gt;pred_label = pred_prob.copy()&lt;/p&gt;

&lt;p&gt;pred_label[pred_label &amp;lt; 0.5] = 0&lt;/p&gt;

&lt;p&gt;pred_label[pred_label &amp;gt;= 0.5] = 1&lt;/p&gt;

&lt;p&gt;# compute the f1 score for each tags&lt;/p&gt;

&lt;p&gt;d = {&amp;#39;Probability&amp;#39;:pred_prob,&amp;#39;Anecdotes Prediction&amp;#39;:pred_label}&lt;/p&gt;

&lt;p&gt;pred_df = pd.DataFrame(d, columns=[&amp;#39;Probability&amp;#39;,&amp;#39;Anecdotes Prediction&amp;#39;])&lt;/p&gt;

&lt;p&gt;result_df = pd.DataFrame(&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;&amp;quot;review_id&amp;quot;: review_id,&lt;/p&gt;

&lt;p&gt;&amp;quot;hmd_text&amp;quot;: hmd_text,&lt;/p&gt;

&lt;p&gt;&amp;quot;head_cust_text&amp;quot;: head_cust_text,&lt;/p&gt;

&lt;p&gt;&amp;quot;agent&amp;quot;: agent&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;if len(review_label) != 0:&lt;/p&gt;

&lt;p&gt;result_df[&amp;quot;review_label&amp;quot;] =  [x.item() for x in review_label] &lt;/p&gt;

&lt;p&gt;return pd.concat([result_df, pred_df], axis=1).set_index(&amp;quot;review_id&amp;quot;)&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Can anyone help me how to fix this issue? Thank you so much!!!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jv2s5o,True,,backpackerice,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jv2s5o/runtimeerror_arguments_are_located_on_different/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jv2s5o/runtimeerror_arguments_are_located_on_different/,7135,1605511955.0,0,,False,,,,,,,,
232,,pytorch,,t2_kj0nv,False,,0,False,"LibreASR – An On-Premises, Streaming Speech Recognition System",[],r/pytorch,False,6,,0,140.0,,False,t3_juotjr,False,dark,0.86,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/2izo57hmpLtvMgv1qQAk9kehnMeo9Js1oIQxMLXNlwc.jpg,False,,[],{},link,,False,,1605487458.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/y3itMGmFAY4MUz3n4kzh2ICQyVTBOLuDUINnT2m_XS4.jpg?auto=webp&amp;s=1d81bd90142148ae1264852cc26f2366ef3f0d5e', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/y3itMGmFAY4MUz3n4kzh2ICQyVTBOLuDUINnT2m_XS4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=11bee2a174509ecb8002bcf74ec1cde9c6184e59', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/y3itMGmFAY4MUz3n4kzh2ICQyVTBOLuDUINnT2m_XS4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d9625be9ccd8e0991ec462defc4ce203e452859e', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/y3itMGmFAY4MUz3n4kzh2ICQyVTBOLuDUINnT2m_XS4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eaa284a3aadf3e2990d6a260e558ece9bd58301b', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'kFfmu_GMYAmQSHcyZ342Nr01BHTz2f6JSxnYUYYojDE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,juotjr,True,,binaryfor,,1,True,all_ads,False,[],False,,/r/pytorch/comments/juotjr/libreasr_an_onpremises_streaming_speech/,all_ads,False,https://github.com/iceychris/LibreASR,7135,1605458658.0,0,,False,https://github.com/iceychris/LibreASR,,,,,,,
233,,pytorch,"[https://github.com/sugarme/gotch](https://github.com/sugarme/gotch)

We are happy to share a new toolkit for developing deep learning in Go - [gotch](https://github.com/sugarme/gotch). 

**Some features are**:

* Comprehensive Pytorch tensor APIs (\~ 1404)
* Fully featured Pytorch dynamic graph computation
* JIT interface to run model trained/saved using PyTorch Python API
* Load pretrained Pytorch models and run inference
* Pure Go APIs to build and train neural network models with both CPU and GPU support
* Most recent image models
* NLP Language models - [Transformer](https://github.com/sugarme/transformer) in separate package built with GoTch and [pure Go Tokenizer](https://github.com/sugarme/tokenizer).",t2_8vuqjc7q,False,,0,False,Introduce Pytorch C++ API Go binding,[],r/pytorch,False,6,,0,,,False,t3_juek7b,False,dark,0.92,,public,10,0,{},,,False,[],,False,False,,{},,False,10,,False,self,False,,[],{},self,,True,,1605436961.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://github.com/sugarme/gotch""&gt;https://github.com/sugarme/gotch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We are happy to share a new toolkit for developing deep learning in Go - &lt;a href=""https://github.com/sugarme/gotch""&gt;gotch&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Some features are&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Comprehensive Pytorch tensor APIs (~ 1404)&lt;/li&gt;
&lt;li&gt;Fully featured Pytorch dynamic graph computation&lt;/li&gt;
&lt;li&gt;JIT interface to run model trained/saved using PyTorch Python API&lt;/li&gt;
&lt;li&gt;Load pretrained Pytorch models and run inference&lt;/li&gt;
&lt;li&gt;Pure Go APIs to build and train neural network models with both CPU and GPU support&lt;/li&gt;
&lt;li&gt;Most recent image models&lt;/li&gt;
&lt;li&gt;NLP Language models - &lt;a href=""https://github.com/sugarme/transformer""&gt;Transformer&lt;/a&gt; in separate package built with GoTch and &lt;a href=""https://github.com/sugarme/tokenizer""&gt;pure Go Tokenizer&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/lZ2Dw9yt3bmVTx0oDLpZertlm1xxmoIC8M0TkspLNgY.jpg?auto=webp&amp;s=6f6906e8944aaf7a290538d37fbc9b2f24b1f1bb', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/lZ2Dw9yt3bmVTx0oDLpZertlm1xxmoIC8M0TkspLNgY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=15a82db47f8bf92a5f8327f43b703d2930aa63a3', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/lZ2Dw9yt3bmVTx0oDLpZertlm1xxmoIC8M0TkspLNgY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f696de3a2d54eae3efad68d5e130a369de57d67', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/lZ2Dw9yt3bmVTx0oDLpZertlm1xxmoIC8M0TkspLNgY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9db0cc4b391aec41d1927290a41ad9ad067bd1cf', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'H0WNFjVg5vDmiMrRrw4IZszF-54X1qRM2qY1yl4-Utg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,juek7b,True,,nikon-sugar,,1,True,all_ads,False,[],False,,/r/pytorch/comments/juek7b/introduce_pytorch_c_api_go_binding/,all_ads,False,https://www.reddit.com/r/pytorch/comments/juek7b/introduce_pytorch_c_api_go_binding/,7135,1605408161.0,0,,False,,,,,,,,
234,,pytorch,For the past year or so I have been plugging away on a side project - [torchutils](https://gitlab.com/avilay/torchutils) - a PyTorch library for quick but systematic experimentation. It'd be great if you could take it for a spin when training your next mL model and give me some feedback. Here are the [detailed docs](https://avilay.gitlab.io/torchutils/).,t2_86w82,False,,0,False,Convenience library for PyTorch training,[],r/pytorch,False,6,,0,,,False,t3_ju38cn,False,dark,0.94,,public,15,0,{},,,False,[],,False,False,,{},,False,15,,False,self,False,,[],{},self,,True,,1605394990.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For the past year or so I have been plugging away on a side project - &lt;a href=""https://gitlab.com/avilay/torchutils""&gt;torchutils&lt;/a&gt; - a PyTorch library for quick but systematic experimentation. It&amp;#39;d be great if you could take it for a spin when training your next mL model and give me some feedback. Here are the &lt;a href=""https://avilay.gitlab.io/torchutils/""&gt;detailed docs&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/z55b_0-_DYGZxjSO4sTVwBNIkMqkFCKvhcexHvELgV4.jpg?auto=webp&amp;s=6b590839e21b0f0e5d45a926d1c383c41316f7fd', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/z55b_0-_DYGZxjSO4sTVwBNIkMqkFCKvhcexHvELgV4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a3dfba29f9d19dc5dd95d58e68cd2ca48d0f6551', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'euUE_k9o_euzwgNONmIZMM026oeSTBH_ZdvLjtUNylg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ju38cn,True,,avilay,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ju38cn/convenience_library_for_pytorch_training/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ju38cn/convenience_library_for_pytorch_training/,7135,1605366190.0,0,,False,,,,,,,,
235,,pytorch,"Need to use a video dataset for Training? Theres not much on the internet about easily and efficiently using video datasets for deep learning. So, I hope this is useful to some people. Would greatly appreciate any feedback!

[https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch](https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch)",t2_jfbrm6n,False,,0,False,PyTorch Dataloading for Videos: A Small but Powerful Helper Repo,[],r/pytorch,False,6,,0,,,False,t3_ju3zjb,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},self,,True,,1605397927.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Need to use a video dataset for Training? Theres not much on the internet about easily and efficiently using video datasets for deep learning. So, I hope this is useful to some people. Would greatly appreciate any feedback!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch""&gt;https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xcXCAXxENhoRydDtR3XybP0agzdAilJoqoQA9W6NQBo.jpg?auto=webp&amp;s=b6b76144cafad4d6731fcf4fda807189f0b5e5c8', 'width': 312, 'height': 312}, 'resolutions': [{'url': 'https://external-preview.redd.it/xcXCAXxENhoRydDtR3XybP0agzdAilJoqoQA9W6NQBo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fea9bd1c937971cd570fcf4972f2386e0838b194', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/xcXCAXxENhoRydDtR3XybP0agzdAilJoqoQA9W6NQBo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b60857446ad8acfe6a1b2d065e654453f834fda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'JVNGgVDqWMdH_z0obpQ9ayVmKwjyJXHR946MkKcKst4'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ju3zjb,True,,RaivoK,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ju3zjb/pytorch_dataloading_for_videos_a_small_but/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ju3zjb/pytorch_dataloading_for_videos_a_small_but/,7135,1605369127.0,0,,False,,,,,,,,
236,,pytorch,,t2_1wi1cv4l,False,,0,False,Facebook AI and OpenMined create PyTorch privacy and machine learning courses,[],r/pytorch,False,6,,0,,,False,t3_jtlyjq,False,dark,0.95,,public,17,0,{},,,False,[],,False,False,,{},,False,17,,False,default,False,,[],{},,,False,,1605323124.0,text,6,,,text,courses.openmined.org,False,,,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jtlyjq,True,,ConfidentMushroom,,1,True,all_ads,False,[],False,,/r/pytorch/comments/jtlyjq/facebook_ai_and_openmined_create_pytorch_privacy/,all_ads,False,https://courses.openmined.org/,7135,1605294324.0,0,,False,https://courses.openmined.org/,,,,,,,
237,,pytorch," Hi everyone, my name is Enoch and I am a researcher studying deep generative models.

I've started this project called Plexiglass, which is a PyTorch toolbox for cybersecurity research and testing against adversarial attacks and deepfakes.

I would very much appreciate any suggestions/ feedbacks and even contributions.

Repo is here: [https://github.com/enochkan/p](https://github.com/enochkan/safetynet)lexiglass",t2_4xdyqvc,False,,0,False,Plexiglass: A PyTorch toolbox for cybersecurity research and testing against adversarial attacks and deepfakes.,[],r/pytorch,False,6,,0,,,False,t3_jtchnd,False,dark,1.0,,public,11,0,{},,,False,[],,False,False,,{},,False,11,,False,self,False,,[],{},self,,True,,1605279197.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, my name is Enoch and I am a researcher studying deep generative models.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve started this project called Plexiglass, which is a PyTorch toolbox for cybersecurity research and testing against adversarial attacks and deepfakes.&lt;/p&gt;

&lt;p&gt;I would very much appreciate any suggestions/ feedbacks and even contributions.&lt;/p&gt;

&lt;p&gt;Repo is here: &lt;a href=""https://github.com/enochkan/safetynet""&gt;https://github.com/enochkan/p&lt;/a&gt;lexiglass&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/XETQCpYjU9-O-vpCz5xiAV8mEAjOYygr8JWIxF5g7UA.jpg?auto=webp&amp;s=2b80f90bdb1d5c4339cc258d9aa98736feb6d973', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/XETQCpYjU9-O-vpCz5xiAV8mEAjOYygr8JWIxF5g7UA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f44a45e7502188a77d5f4d8e8293e09c36c62059', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/XETQCpYjU9-O-vpCz5xiAV8mEAjOYygr8JWIxF5g7UA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=88cbff16722b9b04807ed5ae6321abc0c5d44f14', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/XETQCpYjU9-O-vpCz5xiAV8mEAjOYygr8JWIxF5g7UA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc918ebd44098fe17edc5aba3023607295ac9a7e', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'dqQx_mO-sM4MddTOK5zEprmoDkNRhJos3LWAUujFV-g'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jtchnd,True,,kanxx030,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jtchnd/plexiglass_a_pytorch_toolbox_for_cybersecurity/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jtchnd/plexiglass_a_pytorch_toolbox_for_cybersecurity/,7135,1605250397.0,0,,False,,,,,,,,
238,,pytorch,,t2_44mbtmjy,False,,0,False,Real-world video Super resolution!,[],r/pytorch,False,6,,0,30.0,,False,t3_jte5qa,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/LGGbnx-ba5Ne4y3MCia1cp5kULZP45p5tojpdpVcQsw.jpg,False,,[],{},link,,False,,1605288346.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5lcr4_fDB39dXAhy3zw2I6BImXKxNXWY2wHQ71NI2q8.jpg?auto=webp&amp;s=2dda59303bc6c7eb39010dc9797a6470d13a18f2', 'width': 1378, 'height': 304}, 'resolutions': [{'url': 'https://external-preview.redd.it/5lcr4_fDB39dXAhy3zw2I6BImXKxNXWY2wHQ71NI2q8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=63f91ef9dfeceadc56d463b225bd1a19686a2c6e', 'width': 108, 'height': 23}, {'url': 'https://external-preview.redd.it/5lcr4_fDB39dXAhy3zw2I6BImXKxNXWY2wHQ71NI2q8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=37f22e9ea1a85259a8ed2632fbee42287d423757', 'width': 216, 'height': 47}, {'url': 'https://external-preview.redd.it/5lcr4_fDB39dXAhy3zw2I6BImXKxNXWY2wHQ71NI2q8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=259a59dd95634d11f2ae1686c017b28b1fce601c', 'width': 320, 'height': 70}, {'url': 'https://external-preview.redd.it/5lcr4_fDB39dXAhy3zw2I6BImXKxNXWY2wHQ71NI2q8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3deaa7a5ad312b24af0616910f8ea884ad0f8b83', 'width': 640, 'height': 141}, {'url': 'https://external-preview.redd.it/5lcr4_fDB39dXAhy3zw2I6BImXKxNXWY2wHQ71NI2q8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a549b10a0f95296f4d6ab352502a81169ceefe1', 'width': 960, 'height': 211}, {'url': 'https://external-preview.redd.it/5lcr4_fDB39dXAhy3zw2I6BImXKxNXWY2wHQ71NI2q8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=067a15bda9a5faae4d949eef9a63fe431a0802a4', 'width': 1080, 'height': 238}], 'variants': {}, 'id': 'Q4ZAm4e4SUtQFXn4G3Xwc0lw6_r4OyUrqAWmZft6kF4'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jte5qa,True,,MLtinkerer,,1,True,all_ads,False,[],False,,/r/pytorch/comments/jte5qa/realworld_video_super_resolution/,all_ads,False,/r/LatestInML/comments/jte4t4/realworld_video_super_resolution/,7135,1605259546.0,0,,False,/r/LatestInML/comments/jte4t4/realworld_video_super_resolution/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': '""**DynaVSR: Dynamic Adaptive Blind Video Super-Resolution**""\n\nFor project and expert/code/API request: [click here](https://www.catalyzex.com/paper/arxiv:2011.04482)\n\nhttps://reddit.com/link/jte4t4/video/2vcwk5hq6zy51/player', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Real-world video Super resolution!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 30, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'2vcwk5hq6zy51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/jte4t4/asset/2vcwk5hq6zy51/DASHPlaylist.mpd?a=1618044163%2CYWEwMjY4NDFhYTkzNWM3OGJmYTE5YjEzZDg1NGNkNjA3YzM5NTQwNDg2NTE1MDg2NzBiZjRjOWEzZjI4ZjRmYg%3D%3D&amp;v=1&amp;f=sd', 'x': 480, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/jte4t4/asset/2vcwk5hq6zy51/HLSPlaylist.m3u8?a=1618044163%2CNDAzNWE5YmYxM2ViMWIwMDM1YjRhMzU5NTNlMTE5ZGExY2FmZmRkM2FmZWM1MGMxZWJiOTM5MzU2ZGQ1NjQwMw%3D%3D&amp;v=1&amp;f=sd', 'id': '2vcwk5hq6zy51', 'isGif': False}}, 'name': 't3_jte4t4', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 20, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 20, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/LGGbnx-ba5Ne4y3MCia1cp5kULZP45p5tojpdpVcQsw.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1605288194.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;quot;&lt;strong&gt;DynaVSR: Dynamic Adaptive Blind Video Super-Resolution&lt;/strong&gt;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;For project and expert/code/API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2011.04482""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/jte4t4/video/2vcwk5hq6zy51/player""&gt;https://reddit.com/link/jte4t4/video/2vcwk5hq6zy51/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/5lcr4_fDB39dXAhy3zw2I6BImXKxNXWY2wHQ71NI2q8.jpg?auto=webp&amp;s=2dda59303bc6c7eb39010dc9797a6470d13a18f2', 'width': 1378, 'height': 304}, 'resolutions': [{'url': 'https://external-preview.redd.it/5lcr4_fDB39dXAhy3zw2I6BImXKxNXWY2wHQ71NI2q8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=63f91ef9dfeceadc56d463b225bd1a19686a2c6e', 'width': 108, 'height': 23}, {'url': 'https://external-preview.redd.it/5lcr4_fDB39dXAhy3zw2I6BImXKxNXWY2wHQ71NI2q8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=37f22e9ea1a85259a8ed2632fbee42287d423757', 'width': 216, 'height': 47}, {'url': 'https://external-preview.redd.it/5lcr4_fDB39dXAhy3zw2I6BImXKxNXWY2wHQ71NI2q8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=259a59dd95634d11f2ae1686c017b28b1fce601c', 'width': 320, 'height': 70}, {'url': 'https://external-preview.redd.it/5lcr4_fDB39dXAhy3zw2I6BImXKxNXWY2wHQ71NI2q8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3deaa7a5ad312b24af0616910f8ea884ad0f8b83', 'width': 640, 'height': 141}, {'url': 'https://external-preview.redd.it/5lcr4_fDB39dXAhy3zw2I6BImXKxNXWY2wHQ71NI2q8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a549b10a0f95296f4d6ab352502a81169ceefe1', 'width': 960, 'height': 211}, {'url': 'https://external-preview.redd.it/5lcr4_fDB39dXAhy3zw2I6BImXKxNXWY2wHQ71NI2q8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=067a15bda9a5faae4d949eef9a63fe431a0802a4', 'width': 1080, 'height': 238}], 'variants': {}, 'id': 'Q4ZAm4e4SUtQFXn4G3Xwc0lw6_r4OyUrqAWmZft6kF4'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'jte4t4', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/jte4t4/realworld_video_super_resolution/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/jte4t4/realworld_video_super_resolution/', 'subreddit_subscribers': 6676, 'created_utc': 1605259394.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_jte4t4,,,,,
239,,pytorch,"Could anyone explain the last four lines (*lt, rb, inter,* and the return expression) of the following code ? I do not quite understand how : works inside \[\]  for pytorch

    def box_iou(boxes1, boxes2):
        # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py
        """"""
        Return intersection-over-union (Jaccard index) of boxes.
        Both sets of boxes are expected to be in (x1, y1, x2, y2) format.
        Arguments:
            boxes1 (Tensor[N, 4])
            boxes2 (Tensor[M, 4])
        Returns:
            iou (Tensor[N, M]): the NxM matrix containing the pairwise
                IoU values for every element in boxes1 and boxes2
        """"""
    
        def box_area(box):
            # box = 4xn
            return (box[2] - box[0]) * (box[3] - box[1])
    
        area1 = box_area(boxes1.t())
        area2 = box_area(boxes2.t())
    
        lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]
        rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]
    
        inter = (rb - lt).clamp(min=0).prod(2)  # [N,M]
        return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)",t2_bpftl,False,,0,False,iou computation code,[],r/pytorch,False,6,,0,,,False,t3_jsy47s,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1605228681.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Could anyone explain the last four lines (&lt;em&gt;lt, rb, inter,&lt;/em&gt; and the return expression) of the following code ? I do not quite understand how : works inside []  for pytorch&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def box_iou(boxes1, boxes2):
    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py
    &amp;quot;&amp;quot;&amp;quot;
    Return intersection-over-union (Jaccard index) of boxes.
    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.
    Arguments:
        boxes1 (Tensor[N, 4])
        boxes2 (Tensor[M, 4])
    Returns:
        iou (Tensor[N, M]): the NxM matrix containing the pairwise
            IoU values for every element in boxes1 and boxes2
    &amp;quot;&amp;quot;&amp;quot;

    def box_area(box):
        # box = 4xn
        return (box[2] - box[0]) * (box[3] - box[1])

    area1 = box_area(boxes1.t())
    area2 = box_area(boxes2.t())

    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]
    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]

    inter = (rb - lt).clamp(min=0).prod(2)  # [N,M]
    return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jsy47s,True,,promach,,11,True,all_ads,False,[],False,,/r/pytorch/comments/jsy47s/iou_computation_code/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jsy47s/iou_computation_code/,7135,1605199881.0,0,,False,,,,,,,,
240,,pytorch,,t2_44mbtmjy,False,,0,False,More info on the popular browser extension in AI/ML community!,[],r/pytorch,False,6,,0,70.0,,False,t3_jsm1md,False,dark,1.0,,public,4,0,{},70.0,,False,[],,False,False,,{},,False,4,,False,https://b.thumbs.redditmedia.com/tPTSJTzaygYGO_GVYZ_WjE2xV3A875hWMhGWKnpW8_c.jpg,False,,[],{},link,,False,,1605175985.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?auto=webp&amp;s=b2ec1cfa2b9e720731d4dc172cbef8096fb37c3a', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad5435b4b2d94d9df9309e4d5ceb4bc021b52783', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'TIQjNUpyCsCWmTM49ZEEAiVQDWg07G_PjT8zXCX4wSE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jsm1md,True,,MLtinkerer,,1,True,all_ads,False,[],False,,/r/pytorch/comments/jsm1md/more_info_on_the_popular_browser_extension_in/,all_ads,False,/r/LatestInML/comments/jsl4ss/more_info_on_the_popular_browser_extension_in/,7135,1605147185.0,0,,False,/r/LatestInML/comments/jsl4ss/more_info_on_the_popular_browser_extension_in/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': '**More info on the popular browser extension in AI/ML community!** An explainer video on how to use the browser extension that lets you quickly get code for ML/AI papers across the web! (Google, Arxiv, Twitter, Scholar--etc.)\n\nhttps://reddit.com/link/jsl4ss/video/s6qtdvxzmpy51/player\n\nGet the free ML code finder extension:  \n\nChrome [bit.ly/code\\_finder\\_chrome](https://bit.ly/code_finder_chrome)\n\nFirefox [bit.ly/code\\_finder\\_firefox](https://bit.ly/code_finder_firefox)\n\n&amp;#x200B;\n\n&gt;Andrew Ng likes it too :)', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'More info on the popular browser extension in AI/ML community!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'s6qtdvxzmpy51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/jsl4ss/asset/s6qtdvxzmpy51/DASHPlaylist.mpd?a=1618044163%2CZTY2ZTdjZTg0NWIwZTc4MTAwNTU2YWVkZTYwYTVkZmY1MjdlOTM4OTU4ODExOTE1NjNkN2Q0NDRhMzYxNjgwYg%3D%3D&amp;v=1&amp;f=sd', 'x': 1676, 'y': 1080, 'hlsUrl': 'https://v.redd.it/link/jsl4ss/asset/s6qtdvxzmpy51/HLSPlaylist.m3u8?a=1618044163%2CNjU0ZWQ4ZTI5NmZhZDliYjUzZTMxOWU3MjkzOWNkMTY2NjU3ZTM1NDE1MjhhNmUyZGIyMWZkMGEwNDk1NTkxMw%3D%3D&amp;v=1&amp;f=sd', 'id': 's6qtdvxzmpy51', 'isGif': False}}, 'name': 't3_jsl4ss', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.79, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 17, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 70, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 17, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/tPTSJTzaygYGO_GVYZ_WjE2xV3A875hWMhGWKnpW8_c.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1605172751.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;strong&gt;More info on the popular browser extension in AI/ML community!&lt;/strong&gt; An explainer video on how to use the browser extension that lets you quickly get code for ML/AI papers across the web! (Google, Arxiv, Twitter, Scholar--etc.)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/jsl4ss/video/s6qtdvxzmpy51/player""&gt;https://reddit.com/link/jsl4ss/video/s6qtdvxzmpy51/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Get the free ML code finder extension:  &lt;/p&gt;\n\n&lt;p&gt;Chrome &lt;a href=""https://bit.ly/code_finder_chrome""&gt;bit.ly/code_finder_chrome&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Firefox &lt;a href=""https://bit.ly/code_finder_firefox""&gt;bit.ly/code_finder_firefox&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Andrew Ng likes it too :)&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?auto=webp&amp;s=b2ec1cfa2b9e720731d4dc172cbef8096fb37c3a', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad5435b4b2d94d9df9309e4d5ceb4bc021b52783', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'TIQjNUpyCsCWmTM49ZEEAiVQDWg07G_PjT8zXCX4wSE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'jsl4ss', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/jsl4ss/more_info_on_the_popular_browser_extension_in/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/jsl4ss/more_info_on_the_popular_browser_extension_in/', 'subreddit_subscribers': 6676, 'created_utc': 1605143951.0, 'num_crossposts': 15, 'media': None, 'is_video': False}]",t3_jsl4ss,,,,,
241,,pytorch,,t2_40d0zt4s,False,,0,False,"How To Use UCF101, The Largest Dataset Of Human Actions",[],r/pytorch,False,6,,0,94.0,,False,t3_js7gwr,False,dark,0.9,,public,8,0,{},140.0,,False,[],,False,False,,{},,False,8,,False,https://b.thumbs.redditmedia.com/Yz011z3Z8zY9R75is2IyNlvOSxjsMZGmMC9du6auaAU.jpg,False,,[],{},link,,False,,1605127460.0,text,6,,,text,analyticsindiamag.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/qjGUGCA4yHFn3NvEoK8eSa4phFATgM7k9wrGsH7klEk.jpg?auto=webp&amp;s=b0fadae031b4331726f376e525b703e68b731089', 'width': 1340, 'height': 906}, 'resolutions': [{'url': 'https://external-preview.redd.it/qjGUGCA4yHFn3NvEoK8eSa4phFATgM7k9wrGsH7klEk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f96ad3f8207e300b2d96619bfb21d5fdf59a0d7d', 'width': 108, 'height': 73}, {'url': 'https://external-preview.redd.it/qjGUGCA4yHFn3NvEoK8eSa4phFATgM7k9wrGsH7klEk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b23dac45c1965863b9aee819e2cc5f3a77d6db3c', 'width': 216, 'height': 146}, {'url': 'https://external-preview.redd.it/qjGUGCA4yHFn3NvEoK8eSa4phFATgM7k9wrGsH7klEk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c5ddd4abed9f0d6f40920643897210a6bbab9f13', 'width': 320, 'height': 216}, {'url': 'https://external-preview.redd.it/qjGUGCA4yHFn3NvEoK8eSa4phFATgM7k9wrGsH7klEk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=853cfc25ab025e9d37eca61d4c113d529afbebdc', 'width': 640, 'height': 432}, {'url': 'https://external-preview.redd.it/qjGUGCA4yHFn3NvEoK8eSa4phFATgM7k9wrGsH7klEk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a99aee8f3ed7c8947d939fb89d11fd61133263c5', 'width': 960, 'height': 649}, {'url': 'https://external-preview.redd.it/qjGUGCA4yHFn3NvEoK8eSa4phFATgM7k9wrGsH7klEk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cfbfe1444e050aac5698e3409868fc408d587ff9', 'width': 1080, 'height': 730}], 'variants': {}, 'id': 'jqRb8-uOOlC9Vw3OPQKnWaEVRrssSmyMviK0FAmYWLQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,js7gwr,True,,analyticsindiam,,0,True,all_ads,False,[],False,,/r/pytorch/comments/js7gwr/how_to_use_ucf101_the_largest_dataset_of_human/,all_ads,False,https://analyticsindiamag.com/how-to-use-ucf101-the-largest-dataset-of-human-actions/,7135,1605098660.0,0,,False,https://analyticsindiamag.com/how-to-use-ucf101-the-largest-dataset-of-human-actions/,,,,,,,
242,,pytorch,"Hello All

I have a tensor of size (1, 8, 1024). I wish to append this tensor to itself to make it (77, 8, 1024). How can I do it ?",t2_5n6xtijf,False,,0,False,Appending tensor to itself ?,[],r/pytorch,False,6,,0,,,False,t3_jrsvk6,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1605068953.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello All&lt;/p&gt;

&lt;p&gt;I have a tensor of size (1, 8, 1024). I wish to append this tensor to itself to make it (77, 8, 1024). How can I do it ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jrsvk6,True,,the-machine-learner,,7,True,all_ads,False,[],False,,/r/pytorch/comments/jrsvk6/appending_tensor_to_itself/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jrsvk6/appending_tensor_to_itself/,7135,1605040153.0,0,,False,,,,,,,,
243,,pytorch,"Hi all,

I bought a new Palit GeForce RTX 3070 GPU, to speed up my deep learning projects. My laptop is a Dell Latitude 5491 with an Nvidia GeForce MX130 and Intel UHD Graphics 630. I am using the GeForce RTX 3070 in a Razer Core X via Thunderbolt 3.0.

I would like to make my pytorch training reproducible, so I am using:
torch.manual_seed(1)
np.random.seed(1)
random.seed(1)
torch.cuda.manual_seed(1)
torch.cuda.manual_seed_all(1)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

Symptom: When the device=“cuda:0” its addressing the MX130, and the seeds are working, I got the same result every time. When the device=“cuda:1” its addressing the RTX 3070 and I dont get the same results. Seems like with the external GPU the random seed is not working. When device=“cuda” its automatically uses the RTX 3070 and no reproducibility.
I am working with num_workers=0 and worker_init_fn=np.random.seed(1) in the dataloder. So practically changing the executor GPU has effect on the random seed. I dont want to, and I am not using both GPU-s in parallel.

How can I make the work with external GPU reproducible? I would very appreciate any help. Thanks in advance!

Pytorch version: 1.7.0
Cuda toolkit: 11.0.221
Anaconda version: 2020.07

According to NVIDIA-SMI:
Cuda version: 11.1
Driver version: 457.09",t2_ldb1dg8,False,,0,False,Random seed with external GPU,[],r/pytorch,False,6,,0,,,False,t3_jri43u,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1605029792.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I bought a new Palit GeForce RTX 3070 GPU, to speed up my deep learning projects. My laptop is a Dell Latitude 5491 with an Nvidia GeForce MX130 and Intel UHD Graphics 630. I am using the GeForce RTX 3070 in a Razer Core X via Thunderbolt 3.0.&lt;/p&gt;

&lt;p&gt;I would like to make my pytorch training reproducible, so I am using:
torch.manual_seed(1)
np.random.seed(1)
random.seed(1)
torch.cuda.manual_seed(1)
torch.cuda.manual_seed_all(1)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False&lt;/p&gt;

&lt;p&gt;Symptom: When the device=“cuda:0” its addressing the MX130, and the seeds are working, I got the same result every time. When the device=“cuda:1” its addressing the RTX 3070 and I dont get the same results. Seems like with the external GPU the random seed is not working. When device=“cuda” its automatically uses the RTX 3070 and no reproducibility.
I am working with num_workers=0 and worker_init_fn=np.random.seed(1) in the dataloder. So practically changing the executor GPU has effect on the random seed. I dont want to, and I am not using both GPU-s in parallel.&lt;/p&gt;

&lt;p&gt;How can I make the work with external GPU reproducible? I would very appreciate any help. Thanks in advance!&lt;/p&gt;

&lt;p&gt;Pytorch version: 1.7.0
Cuda toolkit: 11.0.221
Anaconda version: 2020.07&lt;/p&gt;

&lt;p&gt;According to NVIDIA-SMI:
Cuda version: 11.1
Driver version: 457.09&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jri43u,True,,karolypoka,,4,True,all_ads,False,[],False,,/r/pytorch/comments/jri43u/random_seed_with_external_gpu/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jri43u/random_seed_with_external_gpu/,7135,1605000992.0,0,,False,,,,,,,,
244,,pytorch,"A recent [research paper published by InterDigital AI Lab](https://arxiv.org/abs/2011.03029) introduces CompressAI. CompressAI is a platform that provides custom operations, layers, models, and tools to research, develop, and evaluate end-to-end image and video compression codecs. It uses pre-trained models and evaluation tools to compare learned methods with traditional codecs. Various models have been trained on learned end-to-end compression from scratch and re-implemented in PyTorch. Artificial Neural Network (ANN) based codecs have shown remarkable outcomes for compressing images. This framework currently implements models only for still-picture compression; however, it is believed to soon extend over to the video compression domain.

Summary: [https://www.marktechpost.com/2020/11/09/compressai-a-pytorch-library-for-end-to-end-compression-research/](https://www.marktechpost.com/2020/11/09/compressai-a-pytorch-library-for-end-to-end-compression-research/)

Paper: [https://arxiv.org/abs/2011.03029](https://arxiv.org/abs/2011.03029) 

&amp;#x200B;

https://preview.redd.it/ef0i6odlr8y51.png?width=696&amp;format=png&amp;auto=webp&amp;s=cffc12b8130db42d4190081bef2fc3eb14e315cd",t2_2wsvqwhg,False,,0,False,CompressAI: A PyTorch Library For End-To-End Compression Research,[],r/pytorch,False,6,,0,115.0,,False,t3_jr0u11,False,dark,0.94,,public,12,0,{},140.0,,False,[],,False,False,,{},,False,12,,False,https://b.thumbs.redditmedia.com/EayuHXOoYE7TeTzQ2lucky8LnwBXFHM_kK1xaYwI-sk.jpg,False,,[],{},,,True,,1604968342.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A recent &lt;a href=""https://arxiv.org/abs/2011.03029""&gt;research paper published by InterDigital AI Lab&lt;/a&gt; introduces CompressAI. CompressAI is a platform that provides custom operations, layers, models, and tools to research, develop, and evaluate end-to-end image and video compression codecs. It uses pre-trained models and evaluation tools to compare learned methods with traditional codecs. Various models have been trained on learned end-to-end compression from scratch and re-implemented in PyTorch. Artificial Neural Network (ANN) based codecs have shown remarkable outcomes for compressing images. This framework currently implements models only for still-picture compression; however, it is believed to soon extend over to the video compression domain.&lt;/p&gt;

&lt;p&gt;Summary: &lt;a href=""https://www.marktechpost.com/2020/11/09/compressai-a-pytorch-library-for-end-to-end-compression-research/""&gt;https://www.marktechpost.com/2020/11/09/compressai-a-pytorch-library-for-end-to-end-compression-research/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/abs/2011.03029""&gt;https://arxiv.org/abs/2011.03029&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/ef0i6odlr8y51.png?width=696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cffc12b8130db42d4190081bef2fc3eb14e315cd""&gt;https://preview.redd.it/ef0i6odlr8y51.png?width=696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cffc12b8130db42d4190081bef2fc3eb14e315cd&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jr0u11,True,,ai-lover,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jr0u11/compressai_a_pytorch_library_for_endtoend/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jr0u11/compressai_a_pytorch_library_for_endtoend/,7135,1604939542.0,0,,False,,,,"{'ef0i6odlr8y51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 89, 'x': 108, 'u': 'https://preview.redd.it/ef0i6odlr8y51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d78e27574854c25f862d97689241afaf9d9b18d'}, {'y': 178, 'x': 216, 'u': 'https://preview.redd.it/ef0i6odlr8y51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b789ce89afe5cbbc36bc4d347e2b7e41ec6e97c7'}, {'y': 263, 'x': 320, 'u': 'https://preview.redd.it/ef0i6odlr8y51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=151558fda7956a71c4f624cf37a42fad237ded5e'}, {'y': 527, 'x': 640, 'u': 'https://preview.redd.it/ef0i6odlr8y51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b02ba3ef01df290dfff3f55aec286d74a189827'}], 's': {'y': 574, 'x': 696, 'u': 'https://preview.redd.it/ef0i6odlr8y51.png?width=696&amp;format=png&amp;auto=webp&amp;s=cffc12b8130db42d4190081bef2fc3eb14e315cd'}, 'id': 'ef0i6odlr8y51'}}",,,,
245,,pytorch,"Hello everyone!

This might sound as a very basic question, but I'm new to distributed training in Pytorch.

&amp;#x200B;

So I have two models (net1 and net2) and I have two dataloaders for training and testing (train\_dl and test\_dl). I also have two available GPUs (""cuda:0"" and ""cuda:1""). There is a function called ""training"" that takes a model and two dataloders and returns the testing accuracy. How do I use this function a the same time?

[net1.to](https://net1.to)(""cuda:0"")

[net2.to](https://net2.to)(""cuda:1"")

accuracy1 = training(net1, train\_dl, test\_dl)

accuracy2 = training(net2, train\_dl, test\_dl)

&amp;#x200B;

What do I need to do to make the last two lines to execute at the same time? Simultaneously, not sequentially. I read something about ""spawn"" in the Pytorch distributed packages, but I have no clue of how to implement it. The simplest solution would be appreciated.

&amp;#x200B;

Thank you!

&amp;#x200B;

**EDIT:** I should mention that this is for a Neuroevolution project. This means, I have, say, 10 networks saved in python list. I need to train them all, and I have two GPUs. To speed up the process, I'm going to take the networks in groups of two networks to train them. This means that I need to be able to train them at the same time inside the same program/script.",t2_56lj6uum,False,,0,False,How do I train two different models on two different GPUs at the same time?,[],r/pytorch,False,6,,0,,,False,t3_jqzfsy,False,dark,0.9,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,1604940012.0,,[],{},,,True,,1604963909.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;

&lt;p&gt;This might sound as a very basic question, but I&amp;#39;m new to distributed training in Pytorch.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So I have two models (net1 and net2) and I have two dataloaders for training and testing (train_dl and test_dl). I also have two available GPUs (&amp;quot;cuda:0&amp;quot; and &amp;quot;cuda:1&amp;quot;). There is a function called &amp;quot;training&amp;quot; that takes a model and two dataloders and returns the testing accuracy. How do I use this function a the same time?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://net1.to""&gt;net1.to&lt;/a&gt;(&amp;quot;cuda:0&amp;quot;)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://net2.to""&gt;net2.to&lt;/a&gt;(&amp;quot;cuda:1&amp;quot;)&lt;/p&gt;

&lt;p&gt;accuracy1 = training(net1, train_dl, test_dl)&lt;/p&gt;

&lt;p&gt;accuracy2 = training(net2, train_dl, test_dl)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What do I need to do to make the last two lines to execute at the same time? Simultaneously, not sequentially. I read something about &amp;quot;spawn&amp;quot; in the Pytorch distributed packages, but I have no clue of how to implement it. The simplest solution would be appreciated.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; I should mention that this is for a Neuroevolution project. This means, I have, say, 10 networks saved in python list. I need to train them all, and I have two GPUs. To speed up the process, I&amp;#39;m going to take the networks in groups of two networks to train them. This means that I need to be able to train them at the same time inside the same program/script.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jqzfsy,True,,Historical-Carpenter,,9,True,all_ads,False,[],False,,/r/pytorch/comments/jqzfsy/how_do_i_train_two_different_models_on_two/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jqzfsy/how_do_i_train_two_different_models_on_two/,7135,1604935109.0,0,,False,,,,,,,,
246,,pytorch,"Let's say I have a dataset X of size n x m (n rows, m columns) and Y of size n x 1. 

I have a model that uses X_i as an input and makes a prediction Y_hat_i.

I suppose this means that I have a differentiable function:

y_hat_i = F(X_i) 

Assuming my model is accurate and precise,

I want to find the values of X_i that minimize Y_hat_i. Where X_i is a 1 x m vector and Y_hat_i is a scaler value, under a set of constraints. 

I have looked into many ways to this including SciPy's Optimize.minimize as well as linear programming but either I'm doing something wrong or it simply doesn't work for this use case. 

What is a good way to approach this? 

Essentially I have a function that models a business and I want to find optimal operational values (inventory, costs etc) that minimize (or maximize) a given performance metric. 

Simply predicting is not enough here, I want to actually ""optimize"" the business so I am looking for some insights to this.",t2_ebu4m,False,,0,False,Constrained Optimization,[],r/pytorch,False,6,,0,,,False,t3_jr3gpy,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1604976185.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let&amp;#39;s say I have a dataset X of size n x m (n rows, m columns) and Y of size n x 1. &lt;/p&gt;

&lt;p&gt;I have a model that uses X_i as an input and makes a prediction Y_hat_i.&lt;/p&gt;

&lt;p&gt;I suppose this means that I have a differentiable function:&lt;/p&gt;

&lt;p&gt;y_hat_i = F(X_i) &lt;/p&gt;

&lt;p&gt;Assuming my model is accurate and precise,&lt;/p&gt;

&lt;p&gt;I want to find the values of X_i that minimize Y_hat_i. Where X_i is a 1 x m vector and Y_hat_i is a scaler value, under a set of constraints. &lt;/p&gt;

&lt;p&gt;I have looked into many ways to this including SciPy&amp;#39;s Optimize.minimize as well as linear programming but either I&amp;#39;m doing something wrong or it simply doesn&amp;#39;t work for this use case. &lt;/p&gt;

&lt;p&gt;What is a good way to approach this? &lt;/p&gt;

&lt;p&gt;Essentially I have a function that models a business and I want to find optimal operational values (inventory, costs etc) that minimize (or maximize) a given performance metric. &lt;/p&gt;

&lt;p&gt;Simply predicting is not enough here, I want to actually &amp;quot;optimize&amp;quot; the business so I am looking for some insights to this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jr3gpy,True,,Pepipasta,,7,True,all_ads,False,[],False,,/r/pytorch/comments/jr3gpy/constrained_optimization/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jr3gpy/constrained_optimization/,7135,1604947385.0,0,,False,,,,,,,,
247,,pytorch,"Hello! 

  
I have a question. Let's say I have this network example.   
class NetExple(nn.Module):  
def \_\_init\_\_(self):  
super(NetExple,self).\_\_init\_\_()  
self.fc1 = nn.Linear(784,128)  
self.fc2 = nn.Linear(128,64)  
self.fc3 = nn.Linear(64,10)

   def forward(self,x,mask1, mask2):  
x = F.relu(self.fc1(x))   
x = x \* mask1  
x = F.relu(self.fc2(x))  
x = x \* mask2  
x = F.softmax(self.fc3(x),dim=1)

return x  


A custom mask is multiplied (This mask is customized). In this case, how does Pytorch handles backpropagation? Does it still apply the mask1/mask2 on the gradients during BackProp?  


Thanks a lot!",t2_56160evf,False,,0,False,How does Pytorch handles BackPropagation in this case?,[],r/pytorch,False,6,,0,,,False,t3_jqw27b,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1604950517.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! &lt;/p&gt;

&lt;p&gt;I have a question. Let&amp;#39;s say I have this network example.&lt;br/&gt;
class NetExple(nn.Module):&lt;br/&gt;
def __init__(self):&lt;br/&gt;
super(NetExple,self).__init__()&lt;br/&gt;
self.fc1 = nn.Linear(784,128)&lt;br/&gt;
self.fc2 = nn.Linear(128,64)&lt;br/&gt;
self.fc3 = nn.Linear(64,10)&lt;/p&gt;

&lt;p&gt;def forward(self,x,mask1, mask2):&lt;br/&gt;
x = F.relu(self.fc1(x))&lt;br/&gt;
x = x * mask1&lt;br/&gt;
x = F.relu(self.fc2(x))&lt;br/&gt;
x = x * mask2&lt;br/&gt;
x = F.softmax(self.fc3(x),dim=1)&lt;/p&gt;

&lt;p&gt;return x  &lt;/p&gt;

&lt;p&gt;A custom mask is multiplied (This mask is customized). In this case, how does Pytorch handles backpropagation? Does it still apply the mask1/mask2 on the gradients during BackProp?  &lt;/p&gt;

&lt;p&gt;Thanks a lot!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jqw27b,True,,Light_Mindless,,3,True,all_ads,False,[],False,,/r/pytorch/comments/jqw27b/how_does_pytorch_handles_backpropagation_in_this/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jqw27b/how_does_pytorch_handles_backpropagation_in_this/,7135,1604921717.0,0,,False,,,,,,,,
248,,pytorch,"Hi there,

&amp;#x200B;

I am using my customized bert script to train a model. However, everything even I keep the same setting for lr, AdamW weight decay and epoch, and run on the same platform (cuda on SageMaker) with same torch (1.5.0) and transformers (2.11.0) versions,  the results still change a lot in terms of the loss. This make my different experiments not comparable.

&amp;#x200B;

Can someone who has experienced this before or have any ideas please advice me on what should I do? I really want to solve this inreproducible issue so that I can continue on my experiments. Super appreciated for your help!

&amp;#x200B;

&amp;#x200B;

Details as below:

&amp;#x200B;

For example, if I set epoch = 4, lr = 1e-5, decay for AdamW as 0.01.

For one run I got this result for the first epoch only showing the last complete 100 batches result:

`2020-10-19 03:45:29,032 - utils - INFO - | epoch   1 |  1300/ 1320 batches | lr 2.261e-05 | loss 0.267 | Elapsed 0:12:29`

`2020-10-19 03:45:40,550 - utils - INFO -   Training epoch took: 0:12:41`

`2020-10-19 03:45:40,550 - utils - INFO - Validating...`

`2020-10-19 03:46:14,588 - utils - INFO - | loss 0.019 | Elapsed 0:00:34`

`precision    recall  f1-score      support`

`False          0.906472  0.979875  0.941745  2087.000000`

`True           0.475000  0.152610  0.231003   249.000000`

`accuracy       0.891695  0.891695  0.891695     0.891695`

`macro avg      0.690736  0.566243  0.586374  2336.000000`

`weighted avg   0.860480  0.891695  0.865986  2336.000000`

`2020-10-19 03:46:15,403 - utils - INFO - Testing...`

`2020-10-19 03:46:55,182 - utils - INFO - use model: 1 batch / 1319 step`

`precision  recall  f1-score   support`

`False             0.906   0.984     0.944  2344.000`

`True              0.413   0.098     0.159   265.000`

`accuracy          0.894   0.894     0.894     0.894`

`macro avg         0.659   0.541     0.551  2609.000`

`weighted avg      0.856   0.894     0.864  2609.000`

`2020-10-19 03:46:55,188 - utils - INFO - best test F1 score: 0.8638224640164368`

&amp;#x200B;

And for the second attempt I got this for the first epoch:

`2020-11-07 17:08:08,821 - utils - INFO - | epoch   1 |  1300/ 1320 batches | lr 2.261e-05 | loss 0.286 | Elapsed 0:12:25`

`2020-11-07 17:08:20,487 - utils - INFO -   Training epoch took: 0:12:37`

`2020-11-07 17:08:20,487 - utils - INFO - Validating...`

`2020-11-07 17:08:54,609 - utils - INFO - | loss 0.018 | Elapsed 0:00:34`

`precision    recall  f1-score      support`

`False          0.893408  1.000000  0.943703  2087.000000`

`True           0.000000  0.000000  0.000000   249.000000`

`accuracy       0.893408  0.893408  0.893408     0.893408`

`macro avg      0.446704  0.500000  0.471852  2336.000000`

`weighted avg   0.798177  0.893408  0.843112  2336.000000`

`2020-11-07 17:08:55,313 - utils - INFO - Testing...`

`2020-11-07 17:09:34,934 - utils - INFO - use model: 1 batch / 1319 step`

`precision  recall  f1-score   support`

`False             0.898   1.000     0.946  2344.000`

`True              0.000   0.000     0.000   265.000`

`accuracy          0.898   0.898     0.898     0.898`

`macro avg         0.449   0.500     0.473  2609.000`

`weighted avg      0.807   0.898     0.850  2609.000`

`2020-11-07 17:09:34,938 - utils - INFO - best test F1 score: 0.8503599608647853`

&amp;#x200B;

Note that, the last used lr rate per 100 batches are the same, while the average loss per 100 batches are slightly different. But this result in the predictions for the validation and testing data set very different.

&amp;#x200B;

I already set the seed during my model with this function below:

&amp;#x200B;

`def set_seed(seed):`

`"""""" Set all seeds to make results reproducible (deterministic mode).`

`When seed is a false-y value or not supplied, disables deterministic mode. """"""`

`random.seed(seed)`

`np.random.seed(seed)`

`torch.manual_seed(seed)`

`torch.cuda.manual_seed_all(seed)`

`torch.backends.cudnn.deterministic = True`

`torch.backends.cudnn.benchmark = False`

&amp;#x200B;

The model is just using the bert layers + nn.linear layer on the top. ",t2_vyx9y,False,,0,False,In-reproducible loss/result for Bert model training with same settings but &gt;=2 times,[],r/pytorch,False,6,,0,,,False,t3_jq3agg,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1604800996.0,,[],{},,,True,,1604829502.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am using my customized bert script to train a model. However, everything even I keep the same setting for lr, AdamW weight decay and epoch, and run on the same platform (cuda on SageMaker) with same torch (1.5.0) and transformers (2.11.0) versions,  the results still change a lot in terms of the loss. This make my different experiments not comparable.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Can someone who has experienced this before or have any ideas please advice me on what should I do? I really want to solve this inreproducible issue so that I can continue on my experiments. Super appreciated for your help!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Details as below:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;For example, if I set epoch = 4, lr = 1e-5, decay for AdamW as 0.01.&lt;/p&gt;

&lt;p&gt;For one run I got this result for the first epoch only showing the last complete 100 batches result:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2020-10-19 03:45:29,032 - utils - INFO - | epoch   1 |  1300/ 1320 batches | lr 2.261e-05 | loss 0.267 | Elapsed 0:12:29&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2020-10-19 03:45:40,550 - utils - INFO -   Training epoch took: 0:12:41&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2020-10-19 03:45:40,550 - utils - INFO - Validating...&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2020-10-19 03:46:14,588 - utils - INFO - | loss 0.019 | Elapsed 0:00:34&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;precision    recall  f1-score      support&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;False          0.906472  0.979875  0.941745  2087.000000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;True           0.475000  0.152610  0.231003   249.000000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;accuracy       0.891695  0.891695  0.891695     0.891695&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;macro avg      0.690736  0.566243  0.586374  2336.000000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;weighted avg   0.860480  0.891695  0.865986  2336.000000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2020-10-19 03:46:15,403 - utils - INFO - Testing...&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2020-10-19 03:46:55,182 - utils - INFO - use model: 1 batch / 1319 step&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;precision  recall  f1-score   support&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;False             0.906   0.984     0.944  2344.000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;True              0.413   0.098     0.159   265.000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;accuracy          0.894   0.894     0.894     0.894&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;macro avg         0.659   0.541     0.551  2609.000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;weighted avg      0.856   0.894     0.864  2609.000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2020-10-19 03:46:55,188 - utils - INFO - best test F1 score: 0.8638224640164368&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;And for the second attempt I got this for the first epoch:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2020-11-07 17:08:08,821 - utils - INFO - | epoch   1 |  1300/ 1320 batches | lr 2.261e-05 | loss 0.286 | Elapsed 0:12:25&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2020-11-07 17:08:20,487 - utils - INFO -   Training epoch took: 0:12:37&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2020-11-07 17:08:20,487 - utils - INFO - Validating...&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2020-11-07 17:08:54,609 - utils - INFO - | loss 0.018 | Elapsed 0:00:34&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;precision    recall  f1-score      support&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;False          0.893408  1.000000  0.943703  2087.000000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;True           0.000000  0.000000  0.000000   249.000000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;accuracy       0.893408  0.893408  0.893408     0.893408&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;macro avg      0.446704  0.500000  0.471852  2336.000000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;weighted avg   0.798177  0.893408  0.843112  2336.000000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2020-11-07 17:08:55,313 - utils - INFO - Testing...&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2020-11-07 17:09:34,934 - utils - INFO - use model: 1 batch / 1319 step&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;precision  recall  f1-score   support&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;False             0.898   1.000     0.946  2344.000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;True              0.000   0.000     0.000   265.000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;accuracy          0.898   0.898     0.898     0.898&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;macro avg         0.449   0.500     0.473  2609.000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;weighted avg      0.807   0.898     0.850  2609.000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2020-11-07 17:09:34,938 - utils - INFO - best test F1 score: 0.8503599608647853&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Note that, the last used lr rate per 100 batches are the same, while the average loss per 100 batches are slightly different. But this result in the predictions for the validation and testing data set very different.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I already set the seed during my model with this function below:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def set_seed(seed):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;quot;&amp;quot;&amp;quot; Set all seeds to make results reproducible (deterministic mode).&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;When seed is a false-y value or not supplied, disables deterministic mode. &amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;random.seed(seed)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;np.random.seed(seed)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;torch.manual_seed(seed)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;torch.cuda.manual_seed_all(seed)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;torch.backends.cudnn.deterministic = True&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;torch.backends.cudnn.benchmark = False&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The model is just using the bert layers + nn.linear layer on the top. &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jq3agg,True,,backpackerice,,12,True,all_ads,False,[],False,,/r/pytorch/comments/jq3agg/inreproducible_lossresult_for_bert_model_training/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jq3agg/inreproducible_lossresult_for_bert_model_training/,7135,1604800702.0,0,,False,,,,,,,,
249,,pytorch,"Hey all :) 

There are good instructions on how train a model using torchtext.  


But how do I preper it to production?  
**How to create a PREDICT pipeline?**

Cuz the eval function is simple - I have everything ready to go.  
**How to preper some new data in the same way?**

For example, here is some standard training process:  
tokenize =&gt; padding =&gt; split=&gt; iterator

&amp;#x200B;

    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    
    # Model parameter
    MAX_SEQ_LEN = 32
    PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
    UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)
    
    # Fields
    id_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)
    label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)
    text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,
                       fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)
    fields = [('id',id_field), ('message', text_field),('label', label_field),]
    
    # TabularDataset
    train, valid, test = TabularDataset.splits(path=data_dir, train='train.csv', validation='valid.csv',test='test.csv', format='CSV', fields=fields, skip_header=True)
    
    # Iterators
    train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.message),
                                device=device, train=True, sort=True, sort_within_batch=True)
    valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.message),
                                device=device, train=True, sort=True, sort_within_batch=True)
    test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)

  

**How to prepare a simple list of strings for the model to predict?**

    list_of_sentences=
    ['not sure, still in progress',
     'Yes',
     'How can I increase my mbps',
     'I have had to call every single month',
     'Hi! Can you help me get started with my new phone?']
    ]
    
    ....?
    ....?
    ....?
    
    model(please_predict_this)

Thanks :)",t2_81q4lnfj,False,,0,False,Prepare strings for prediction using torchtext,[],r/pytorch,False,6,,0,,,False,t3_jp27ha,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1604683904.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all :) &lt;/p&gt;

&lt;p&gt;There are good instructions on how train a model using torchtext.  &lt;/p&gt;

&lt;p&gt;But how do I preper it to production?&lt;br/&gt;
&lt;strong&gt;How to create a PREDICT pipeline?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Cuz the eval function is simple - I have everything ready to go.&lt;br/&gt;
&lt;strong&gt;How to preper some new data in the same way?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For example, here is some standard training process:&lt;br/&gt;
tokenize =&amp;gt; padding =&amp;gt; split=&amp;gt; iterator&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tokenizer = BertTokenizer.from_pretrained(&amp;#39;bert-base-uncased&amp;#39;)

# Model parameter
MAX_SEQ_LEN = 32
PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)

# Fields
id_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)
label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)
text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,
                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)
fields = [(&amp;#39;id&amp;#39;,id_field), (&amp;#39;message&amp;#39;, text_field),(&amp;#39;label&amp;#39;, label_field),]

# TabularDataset
train, valid, test = TabularDataset.splits(path=data_dir, train=&amp;#39;train.csv&amp;#39;, validation=&amp;#39;valid.csv&amp;#39;,test=&amp;#39;test.csv&amp;#39;, format=&amp;#39;CSV&amp;#39;, fields=fields, skip_header=True)

# Iterators
train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.message),
                            device=device, train=True, sort=True, sort_within_batch=True)
valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.message),
                            device=device, train=True, sort=True, sort_within_batch=True)
test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;How to prepare a simple list of strings for the model to predict?&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;list_of_sentences=
[&amp;#39;not sure, still in progress&amp;#39;,
 &amp;#39;Yes&amp;#39;,
 &amp;#39;How can I increase my mbps&amp;#39;,
 &amp;#39;I have had to call every single month&amp;#39;,
 &amp;#39;Hi! Can you help me get started with my new phone?&amp;#39;]
]

....?
....?
....?

model(please_predict_this)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thanks :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jp27ha,True,,SaharMilis,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jp27ha/prepare_strings_for_prediction_using_torchtext/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jp27ha/prepare_strings_for_prediction_using_torchtext/,7135,1604655104.0,0,,False,,,,,,,,
250,,pytorch,"We have used OpenCV with C++ and Python API and now we have a surprise for you. In this blog, we will show an example of how it can be used in a 3rd language - Java - using OpenCV Java API.

https://www.learnopencv.com/image-classification-with-opencv-java-2/

Here is what we will do in the blog post:

1. Convert the MobileNet classification model trained in PyTorch to ONNX

2. Check the model prediction on a simple example

3. Construct a Java pipeline for image classification

https://preview.redd.it/9utokc01ujx51.png?width=600&amp;format=png&amp;auto=webp&amp;s=86e2a73805fbd10245ab289674c0cb3caeca8089",t2_cvc9f,False,,0,False,Image Classification with OpenCV Java,[],r/pytorch,False,6,,0,93.0,,False,t3_joyr9z,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/qBFqxsXJKD4FH3sGlQc89uwlvcgHXbqMSB_CYL40d-w.jpg,False,,[],{},,,True,,1604667152.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We have used OpenCV with C++ and Python API and now we have a surprise for you. In this blog, we will show an example of how it can be used in a 3rd language - Java - using OpenCV Java API.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.learnopencv.com/image-classification-with-opencv-java-2/""&gt;https://www.learnopencv.com/image-classification-with-opencv-java-2/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here is what we will do in the blog post:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Convert the MobileNet classification model trained in PyTorch to ONNX&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Check the model prediction on a simple example&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Construct a Java pipeline for image classification&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/9utokc01ujx51.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=86e2a73805fbd10245ab289674c0cb3caeca8089""&gt;https://preview.redd.it/9utokc01ujx51.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=86e2a73805fbd10245ab289674c0cb3caeca8089&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,joyr9z,True,,spmallick,,0,True,all_ads,False,[],False,,/r/pytorch/comments/joyr9z/image_classification_with_opencv_java/,all_ads,False,https://www.reddit.com/r/pytorch/comments/joyr9z/image_classification_with_opencv_java/,7135,1604638352.0,0,,False,,,,"{'9utokc01ujx51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 72, 'x': 108, 'u': 'https://preview.redd.it/9utokc01ujx51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a50f47e3fc6a337e7ebfb317cdbc767658f6318'}, {'y': 144, 'x': 216, 'u': 'https://preview.redd.it/9utokc01ujx51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d2e7f76dcb3bb13024bd084e53d1cef302ca30c7'}, {'y': 213, 'x': 320, 'u': 'https://preview.redd.it/9utokc01ujx51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=18af6eed42a8af6e35f7424a8cd210a746d2207b'}], 's': {'y': 400, 'x': 600, 'u': 'https://preview.redd.it/9utokc01ujx51.png?width=600&amp;format=png&amp;auto=webp&amp;s=86e2a73805fbd10245ab289674c0cb3caeca8089'}, 'id': '9utokc01ujx51'}}",,,,
251,,pytorch,What would you rather get?,t2_2x0723cu,False,,0,False,2x 2080 Ti vs 1x 3080?,[],r/pytorch,False,6,,0,,,False,t3_jomks9,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1604624979.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What would you rather get?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jomks9,True,,desmap,,10,True,all_ads,False,[],False,,/r/pytorch/comments/jomks9/2x_2080_ti_vs_1x_3080/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jomks9/2x_2080_ti_vs_1x_3080/,7135,1604596179.0,0,,False,,,,,,,,
252,,pytorch,"I am trying to develop a loss function by combining dice loss and cross-entropy loss for semantic segmentation (Multiclass). Got the idea from [this](https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch#BCE-Dice-Loss) (Look at DiceBCELoss class for PyTorch),  but it's for single class. I want an exact definition for multiclass for which I have written this code in my forward method, (`inputs` are predictions from model &amp; `targets` is ground truth tensor one-hot encoded from masks. Both are of shape `(batch_size, C, H, W)`)

    inputs = F.softmax(inputs, 1)
            
    CE = F.cross_entropy(inputs, torch.argmax(targets, 1), reduction='mean')
    
    inputs = inputs.view(-1)
    targets = targets.view(-1)
    
    intersection = (inputs * targets).sum()                            
    dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  
    
    Dice_CE = CE + dice_loss
    
    return Dice_CE

The problem is cross-entropy requires `targets` tensor to contain values b.w. 0 to C-1. And in dice loss, targets need to be one-hot encoded. So I chose to apply `torch.argmax` on targets (which is already one-hot encoded) to find cross-entropy loss. But I saw somewhere that `torch.argmax` is not differentiable, though I only use it on `targets`. So I am confused, is this loss function differentiable? If not how can I make it differentiable? Thanks for your time.",t2_3q4w2o4,False,,0,False,Is this loss function differentiable?,[],r/pytorch,False,6,,0,,,False,t3_jojoau,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,1604587016.0,,[],{},self,,True,,1604615297.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to develop a loss function by combining dice loss and cross-entropy loss for semantic segmentation (Multiclass). Got the idea from &lt;a href=""https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch#BCE-Dice-Loss""&gt;this&lt;/a&gt; (Look at DiceBCELoss class for PyTorch),  but it&amp;#39;s for single class. I want an exact definition for multiclass for which I have written this code in my forward method, (&lt;code&gt;inputs&lt;/code&gt; are predictions from model &amp;amp; &lt;code&gt;targets&lt;/code&gt; is ground truth tensor one-hot encoded from masks. Both are of shape &lt;code&gt;(batch_size, C, H, W)&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;inputs = F.softmax(inputs, 1)

CE = F.cross_entropy(inputs, torch.argmax(targets, 1), reduction=&amp;#39;mean&amp;#39;)

inputs = inputs.view(-1)
targets = targets.view(-1)

intersection = (inputs * targets).sum()                            
dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  

Dice_CE = CE + dice_loss

return Dice_CE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The problem is cross-entropy requires &lt;code&gt;targets&lt;/code&gt; tensor to contain values b.w. 0 to C-1. And in dice loss, targets need to be one-hot encoded. So I chose to apply &lt;code&gt;torch.argmax&lt;/code&gt; on targets (which is already one-hot encoded) to find cross-entropy loss. But I saw somewhere that &lt;code&gt;torch.argmax&lt;/code&gt; is not differentiable, though I only use it on &lt;code&gt;targets&lt;/code&gt;. So I am confused, is this loss function differentiable? If not how can I make it differentiable? Thanks for your time.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/3pwSy4C0OWiBJzXfUfnFygBU9SZNHmXZo-5T6LVgmUg.jpg?auto=webp&amp;s=80185fc68be6081a4d830cc24777396528527a29', 'width': 100, 'height': 100}, 'resolutions': [], 'variants': {}, 'id': 'XWOEd64309mcrHY_OqldxFXbkpSYVLdfYNbpBvwhEtY'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jojoau,True,,hp2304,,6,True,all_ads,False,[],False,,/r/pytorch/comments/jojoau/is_this_loss_function_differentiable/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jojoau/is_this_loss_function_differentiable/,7135,1604586497.0,0,,False,,,,,,,,
253,,pytorch,"I'm new to pytorch and I'm trying to build a dense cnn architecture.
So I made 4 classes DenseUnit, DenseBlock, BottleNeck and Output.

    class DenseUnit(nn.Module):
      def __init__(self,in_channels):
    
      def forward(self, x):
  
        return x
    class DenseBlock(nn.Module):
      def __init__(self,in_channels):
    
      def forward(self, x):
          for _ in range(12):
            
        return x
    class BottleNeck(nn.Module):
      def __init__(self,in_channels):
    
      def forward(self, x):
      
        return x
    class Output(nn.Module):
      def __init__(self,in_channels):
    
      def forward(self, x):
      
        return x
and finally I'm using these class objects in my model's `__init__()` and `forward()` to build my model.

I have sent a random input and it gives me output without errors, but when used model summary it shows 0 trainable params for denseblocks.

I'm wondering am I using pytorch correctly? If yes, how can I get params info for class objects too.",t2_wjclo,False,,0,False,Is this a good way to use pytorch to build dense cnn?,[],r/pytorch,False,6,,0,,,False,t3_jn8wsa,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1604432929.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m new to pytorch and I&amp;#39;m trying to build a dense cnn architecture.
So I made 4 classes DenseUnit, DenseBlock, BottleNeck and Output.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class DenseUnit(nn.Module):
  def __init__(self,in_channels):

  def forward(self, x):

    return x
class DenseBlock(nn.Module):
  def __init__(self,in_channels):

  def forward(self, x):
      for _ in range(12):

    return x
class BottleNeck(nn.Module):
  def __init__(self,in_channels):

  def forward(self, x):

    return x
class Output(nn.Module):
  def __init__(self,in_channels):

  def forward(self, x):

    return x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and finally I&amp;#39;m using these class objects in my model&amp;#39;s &lt;code&gt;__init__()&lt;/code&gt; and &lt;code&gt;forward()&lt;/code&gt; to build my model.&lt;/p&gt;

&lt;p&gt;I have sent a random input and it gives me output without errors, but when used model summary it shows 0 trainable params for denseblocks.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m wondering am I using pytorch correctly? If yes, how can I get params info for class objects too.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jn8wsa,True,,crazyb14,,2,True,all_ads,False,[],False,,/r/pytorch/comments/jn8wsa/is_this_a_good_way_to_use_pytorch_to_build_dense/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jn8wsa/is_this_a_good_way_to_use_pytorch_to_build_dense/,7135,1604404129.0,0,,False,,,,,,,,
254,,pytorch,"I am trying to write a c++ extension using CUDA libraries in windows 10 following the tutorial \[here\]([https://pytorch.org/tutorials/advanced/cpp\_extension.html](https://pytorch.org/tutorials/advanced/cpp_extension.html))

I have python 3.6.11, pytorch 1.8.0.dev20201021, rtx 3080 gpu, cuda 11.1.

I ended up getting pytorch c++ 1.7 (stable/debug) with cuda 11.0 working in Microsoft Visual Studio 2019 v 16.6.5 with the cl.exe compiler, and I am wondering what is the best way to write code with syntax completion, debugging abilities so that when I run python [setup.py](https://setup.py) install, that I can be sure it will work.

Questions are what is your suggested environment/debugging tools for windows 10 ?

Do I need to use Nvidia Nsight Compute to debug the Cuda Code?

What flags will I need to pass in to the nvcc compiler (c++11 and maybe --gpu-architecture=compute\_86 --gpu-code=sm\_86)

Should I use debug/release pytorch c++ ?

&amp;#x200B;

My question with code is in the discuss pytorch forum here

[https://discuss.pytorch.org/t/suggested-environment-for-developing-c-cuda-extensions-in-windows-10/101426](https://discuss.pytorch.org/t/suggested-environment-for-developing-c-cuda-extensions-in-windows-10/101426)

&amp;#x200B;

Any tips or insights very much appreciated, would be happy to provide more information.",t2_bai5lf,False,,0,False,Suggested Environment for Developing C++/Cuda Extensions in Windows 10,[],r/pytorch,False,6,,0,,,False,t3_jn1erz,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1604368542.0,,[],{},,,True,,1604397030.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to write a c++ extension using CUDA libraries in windows 10 following the tutorial [here](&lt;a href=""https://pytorch.org/tutorials/advanced/cpp_extension.html""&gt;https://pytorch.org/tutorials/advanced/cpp_extension.html&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;I have python 3.6.11, pytorch 1.8.0.dev20201021, rtx 3080 gpu, cuda 11.1.&lt;/p&gt;

&lt;p&gt;I ended up getting pytorch c++ 1.7 (stable/debug) with cuda 11.0 working in Microsoft Visual Studio 2019 v 16.6.5 with the cl.exe compiler, and I am wondering what is the best way to write code with syntax completion, debugging abilities so that when I run python &lt;a href=""https://setup.py""&gt;setup.py&lt;/a&gt; install, that I can be sure it will work.&lt;/p&gt;

&lt;p&gt;Questions are what is your suggested environment/debugging tools for windows 10 ?&lt;/p&gt;

&lt;p&gt;Do I need to use Nvidia Nsight Compute to debug the Cuda Code?&lt;/p&gt;

&lt;p&gt;What flags will I need to pass in to the nvcc compiler (c++11 and maybe --gpu-architecture=compute_86 --gpu-code=sm_86)&lt;/p&gt;

&lt;p&gt;Should I use debug/release pytorch c++ ?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;My question with code is in the discuss pytorch forum here&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://discuss.pytorch.org/t/suggested-environment-for-developing-c-cuda-extensions-in-windows-10/101426""&gt;https://discuss.pytorch.org/t/suggested-environment-for-developing-c-cuda-extensions-in-windows-10/101426&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Any tips or insights very much appreciated, would be happy to provide more information.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jn1erz,True,,JayDupesCSMAN,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jn1erz/suggested_environment_for_developing_ccuda/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jn1erz/suggested_environment_for_developing_ccuda/,7135,1604368230.0,0,,False,,,,,,,,
255,,pytorch,"I want to test the performance of a power supply unit by accelerating multiple GPU usage. I have 6 parallel GTX 1050 ti. Looking for a deep learning program which will take long time to train and put pressure on the GPUs. I will use the CUDA platform, can anyone help me out with this?",t2_29ziagh6,False,,0,False,Multi GPU Training,[],r/pytorch,False,6,,0,,,False,t3_jn0vz5,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1604395177.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to test the performance of a power supply unit by accelerating multiple GPU usage. I have 6 parallel GTX 1050 ti. Looking for a deep learning program which will take long time to train and put pressure on the GPUs. I will use the CUDA platform, can anyone help me out with this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jn0vz5,True,,tanshifat,,1,True,all_ads,False,[],False,,/r/pytorch/comments/jn0vz5/multi_gpu_training/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jn0vz5/multi_gpu_training/,7135,1604366377.0,0,,False,,,,,,,,
256,,pytorch,"Is is normal that the weights 'resets' after each kfold run ? Because the loss value seems to be poor at the beginning of each training iteration. Or do I have to load the best weights for every kfold  in some way?

&amp;#x200B;

 

for n in range(EPOCHS):  
        num\_epochs\_run=n  
        train\_loss= eng.train(train\_loader)  
        valid\_loss= eng.validate(valid\_loader)  
        score +=train\_loss  
        score\_v +=valid\_loss  
 \#Early stopping checking if model validation loss does imporve other wise stop after n steps.  
 \#Bstops if no improves is seen   
 if valid\_loss &lt; best\_loss:  
            epochs\_no\_improve=0  
            best\_loss= valid\_loss  
 print( f""Best loss: {best\_loss} at epoch: {n}"")  
 if n==EPOCHS:  
                torch.save(model.state\_dict(),f""fold{fold}-epoch{num\_epochs\_run}.pth"")  
 else:  
            epochs\_no\_improve += 1  
 if epochs\_no\_improve == early\_stopping:  
 print(f"" Early stopping at epoch: {n}"")  
                early\_stop= True  
                torch.save(model.state\_dict(),weights\_path+'best\_pytorch\_weights.pth')  
 break  
 else:  
 continue  
 if early\_stop:  
 print('Stopped')  
 \#torch.save(model.state\_dict(),weights\_path+'best\_pytorch\_weights.pth')  
 break",t2_128ob4,False,,0,False,Weights resets after each kfold?,[],r/pytorch,False,6,,0,,,False,t3_jmcx8l,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1604303658.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is is normal that the weights &amp;#39;resets&amp;#39; after each kfold run ? Because the loss value seems to be poor at the beginning of each training iteration. Or do I have to load the best weights for every kfold  in some way?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;for n in range(EPOCHS):&lt;br/&gt;
        num_epochs_run=n&lt;br/&gt;
        train_loss= eng.train(train_loader)&lt;br/&gt;
        valid_loss= eng.validate(valid_loader)&lt;br/&gt;
        score +=train_loss&lt;br/&gt;
        score_v +=valid_loss&lt;br/&gt;
 #Early stopping checking if model validation loss does imporve other wise stop after n steps.&lt;br/&gt;
 #Bstops if no improves is seen &lt;br/&gt;
 if valid_loss &amp;lt; best_loss:&lt;br/&gt;
            epochs_no_improve=0&lt;br/&gt;
            best_loss= valid_loss&lt;br/&gt;
 print( f&amp;quot;Best loss: {best_loss} at epoch: {n}&amp;quot;)&lt;br/&gt;
 if n==EPOCHS:&lt;br/&gt;
                torch.save(model.state_dict(),f&amp;quot;fold{fold}-epoch{num_epochs_run}.pth&amp;quot;)&lt;br/&gt;
 else:&lt;br/&gt;
            epochs_no_improve += 1&lt;br/&gt;
 if epochs_no_improve == early_stopping:&lt;br/&gt;
 print(f&amp;quot; Early stopping at epoch: {n}&amp;quot;)&lt;br/&gt;
                early_stop= True&lt;br/&gt;
                torch.save(model.state_dict(),weights_path+&amp;#39;best_pytorch_weights.pth&amp;#39;)&lt;br/&gt;
 break&lt;br/&gt;
 else:&lt;br/&gt;
 continue&lt;br/&gt;
 if early_stop:&lt;br/&gt;
 print(&amp;#39;Stopped&amp;#39;)&lt;br/&gt;
 #torch.save(model.state_dict(),weights_path+&amp;#39;best_pytorch_weights.pth&amp;#39;)&lt;br/&gt;
 break&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jmcx8l,True,,darvidas,,1,True,all_ads,False,[],False,,/r/pytorch/comments/jmcx8l/weights_resets_after_each_kfold/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jmcx8l/weights_resets_after_each_kfold/,7135,1604274858.0,0,,False,,,,,,,,
257,,pytorch,"Is is normal that the weights 'resets' after each kfold run ? Because the loss value seems to be poor at the beginning of each training iteration. Or do I have to load the best weights for every kfold  in some way?

https://preview.redd.it/sajzy1pwrpw51.png?width=336&amp;format=png&amp;auto=webp&amp;s=582ce271f99915c8fb1b3829bfc06ae9cb7f63c0

 This is the model training code

for n in range(EPOCHS):  
num\_epochs\_run=n  
train\_loss= eng.train(train\_loader)  
valid\_loss= eng.validate(valid\_loader)  
score +=train\_loss  
score\_v +=valid\_loss  
 \#Early stopping checking if model validation loss does imporve other wise stop after n steps.  
 \#Bstops if no improves is seen   
 if valid\_loss &lt; best\_loss:  
epochs\_no\_improve=0  
best\_loss= valid\_loss  
 print( f""Best loss: {best\_loss} at epoch: {n}"")  
 if n==EPOCHS:  
torch.save(model.state\_dict(),f""fold{fold}-epoch{num\_epochs\_run}.pth"")  
 else:  
epochs\_no\_improve += 1  
 if epochs\_no\_improve == early\_stopping:  
 print(f"" Early stopping at epoch: {n}"")  
early\_stop= True  
torch.save(model.state\_dict(),weights\_path+'best\_pytorch\_weights.pth')  
 break  
 else:  
 continue  
 if early\_stop:  
 print('Stopped')  
 \#torch.save(model.state\_dict(),weights\_path+'best\_pytorch\_weights.pth')  
 break  
",t2_128ob4,False,,0,False,Weights resets after each epoch?,[],r/pytorch,False,6,,0,113.0,,False,t3_jmcmin,False,dark,0.75,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/XTeaCwBYd_6ktqWynEsjTU8lYrG3-YhU86OgKoLrPfA.jpg,1604274705.0,,[],{},,,True,,1604302538.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is is normal that the weights &amp;#39;resets&amp;#39; after each kfold run ? Because the loss value seems to be poor at the beginning of each training iteration. Or do I have to load the best weights for every kfold  in some way?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/sajzy1pwrpw51.png?width=336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=582ce271f99915c8fb1b3829bfc06ae9cb7f63c0""&gt;https://preview.redd.it/sajzy1pwrpw51.png?width=336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=582ce271f99915c8fb1b3829bfc06ae9cb7f63c0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is the model training code&lt;/p&gt;

&lt;p&gt;for n in range(EPOCHS):&lt;br/&gt;
num_epochs_run=n&lt;br/&gt;
train_loss= eng.train(train_loader)&lt;br/&gt;
valid_loss= eng.validate(valid_loader)&lt;br/&gt;
score +=train_loss&lt;br/&gt;
score_v +=valid_loss&lt;br/&gt;
 #Early stopping checking if model validation loss does imporve other wise stop after n steps.&lt;br/&gt;
 #Bstops if no improves is seen &lt;br/&gt;
 if valid_loss &amp;lt; best_loss:&lt;br/&gt;
epochs_no_improve=0&lt;br/&gt;
best_loss= valid_loss&lt;br/&gt;
 print( f&amp;quot;Best loss: {best_loss} at epoch: {n}&amp;quot;)&lt;br/&gt;
 if n==EPOCHS:&lt;br/&gt;
torch.save(model.state_dict(),f&amp;quot;fold{fold}-epoch{num_epochs_run}.pth&amp;quot;)&lt;br/&gt;
 else:&lt;br/&gt;
epochs_no_improve += 1&lt;br/&gt;
 if epochs_no_improve == early_stopping:&lt;br/&gt;
 print(f&amp;quot; Early stopping at epoch: {n}&amp;quot;)&lt;br/&gt;
early_stop= True&lt;br/&gt;
torch.save(model.state_dict(),weights_path+&amp;#39;best_pytorch_weights.pth&amp;#39;)&lt;br/&gt;
 break&lt;br/&gt;
 else:&lt;br/&gt;
 continue&lt;br/&gt;
 if early_stop:&lt;br/&gt;
 print(&amp;#39;Stopped&amp;#39;)&lt;br/&gt;
 #torch.save(model.state_dict(),weights_path+&amp;#39;best_pytorch_weights.pth&amp;#39;)&lt;br/&gt;
 break  &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jmcmin,True,,darvidas,,4,True,all_ads,False,[],False,,/r/pytorch/comments/jmcmin/weights_resets_after_each_epoch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jmcmin/weights_resets_after_each_epoch/,7135,1604273738.0,0,,False,,,,"{'sajzy1pwrpw51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 87, 'x': 108, 'u': 'https://preview.redd.it/sajzy1pwrpw51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=677a25bdcf356fe0a08019656ff4e22b54128e40'}, {'y': 175, 'x': 216, 'u': 'https://preview.redd.it/sajzy1pwrpw51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6eed3e02f47af17390366ea15c7eea7bedb90c8c'}, {'y': 260, 'x': 320, 'u': 'https://preview.redd.it/sajzy1pwrpw51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e7be03e6469f71c1d9ccc802be684f637667cec6'}], 's': {'y': 273, 'x': 336, 'u': 'https://preview.redd.it/sajzy1pwrpw51.png?width=336&amp;format=png&amp;auto=webp&amp;s=582ce271f99915c8fb1b3829bfc06ae9cb7f63c0'}, 'id': 'sajzy1pwrpw51'}}",,,,
258,,pytorch,"I have a question on how to use PyTorch’s DataLoader together with skorch’s GridSearchCV, which I have posted here in stackoverflow: https://stackoverflow.com/questions/64628130/how-to-use-pytorch-s-dataloader-together-with-skorch-s-gridsearchcv

Would really appreciate any help on this. Many thanks in advance.",t2_zmqho4m,False,,0,False,How to use PyTorch’s DataLoader together with skorch’s GridSearchCV,[],r/pytorch,False,6,,0,,,False,t3_jlvrmt,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},self,,True,,1604229961.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a question on how to use PyTorch’s DataLoader together with skorch’s GridSearchCV, which I have posted here in stackoverflow: &lt;a href=""https://stackoverflow.com/questions/64628130/how-to-use-pytorch-s-dataloader-together-with-skorch-s-gridsearchcv""&gt;https://stackoverflow.com/questions/64628130/how-to-use-pytorch-s-dataloader-together-with-skorch-s-gridsearchcv&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Would really appreciate any help on this. Many thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jlvrmt,True,,leockl,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jlvrmt/how_to_use_pytorchs_dataloader_together_with/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jlvrmt/how_to_use_pytorchs_dataloader_together_with/,7135,1604201161.0,0,,False,,,,,,,,
259,,pytorch,"Hello,

i am trying to create 3d CNN using pytorch.

the problem that the accuracy and loss are increasing and decreasing (accuracy  values are between 37% 60%)  

NOTE: if I delete dropout layer the accuracy and loss values remain unchanged for all epochs

Do you know what I am doing wrong here?

Thanks in advance!

    import numpy as np
    import torch
    torch.autograd.set_detect_anomaly(True)
    REBUILD_DATA = True # set to true to one once, then back to false unless you want to change something in your training data.
    import time
    start_time = time.time()
    import os
    import cv2
    import numpy as np
    import nibabel as nib
    from nibabel.testing import data_path
    from sklearn.utils import shuffle
    import matplotlib.pyplot as plt
    import cv2
    
    from sklearn.metrics import ConfusionMatrixDisplay# Build the confusion matrix of our 2-class classification problem  
    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import accuracy_score # for evaluating the model
    import torch # PyTorch libraries and modules
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.optim import *
    from sklearn.metrics import classification_report
    import seaborn as sns # color of graphe
    '''from  owlready2 import *
    import textdistance
    '''
    import numpy as np
    
    
    
    #Channels ordering : first channel (taille , shape of each element ) to ==&gt; last channel ( shape, size )
    def changechannel(data, a, b):
        data = np.asarray(data)
        data = np.rollaxis(data, a, b)
        return(data)
    
    # convert (240,240,155) to ======&gt; (120, 120, 120) #
    def resize3Dimages(data):
        train_x = []
        for i in range(len(data)):
            image = data[i] 
            width = 120
            height = 120
            img_zeros = np.zeros((len(image), width, height)) #### len(image) means the first dim /// in other words shape[0]
    
            for idx in range(len(image)):
                img = data[i][idx, :, :]
                img_sm = cv2.resize(img, (width, height), interpolation=cv2.INTER_CUBIC) #une interpolation bicubique sur un voisinage de 4 × 4 voxels 
                img_zeros[idx, :, :] = img_sm
    #  convert (240,120,120) to ======&gt; (120,120,120)
            img_zeros = img_zeros[::2, :, :] ### 240/2 =120 
            train_x.append(img_zeros)
    ############################# save images in list ################################
        return(np.asarray(train_x)) ## convert list to nd array 
    # end ...
    # 1 channel to 3 channel 
    def channel1to3 (data): 
        data = np.stack((data,) * 3, axis=-1)
        return(data)
    print("" preprocessing  --- %s seconds ---"" % (time.time() - start_time))
    
    print('Building of CNN')
    import os
    # for reading and displaying images
    import matplotlib.pyplot as plt
    # for evaluating the model
    from sklearn.metrics import accuracy_score
    # PyTorch libraries and modules
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.optim import *
    
    num_classes = 2
    
    # Create CNN Model
    class CNNModel(nn.Module):
        def __init__(self):
            super(CNNModel, self).__init__() 
            
            self.conv_layer1 = self._conv_layer_set(3, 32) 
                                                           
            self.conv_layer2 = self._conv_layer_set(32, 64) 
            self.conv_layer3 = self._conv_layer_set(64, 128)
            self.conv_layer4 = self._conv_layer_set(128, 256)
            self.conv_layer5 = self._conv_layer_set(256, 512)
    
          
            self.fc1 = nn.Linear(512, 128)
            self.fc2 = nn.Linear(128, num_classes)
            self.relu = nn.ReLU()
            self.batch=nn.BatchNorm1d(128)
            self.drop=nn.Dropout(p=0.6, inplace = True)   
            
        def _conv_layer_set(self, in_c, out_c):
            conv_layer = nn.Sequential(
            nn.Conv3d(in_c, out_c, kernel_size=(3, 3, 3), padding=0),
            nn.ReLU(),
            nn.MaxPool3d((2, 2, 2)),
            )
            return conv_layer
        
    
        def forward(self, x):
            # Set 1
            out = self.conv_layer1(x)
            out = self.conv_layer2(out)
            out = self.conv_layer3(out)
            out = self.conv_layer4(out)
            out = self.conv_layer5(out)
            out = out.view(out.size(0), -1)
            out = self.fc1(out)
            out = self.relu(out)
            out = self.batch(out) # batchnormalization 
            out = self.drop(out)
            out = self.fc2(out)
            #out = F.softmax(out, dim=1)
            return out
    #Definition of hyperparameters
    n_iters = 2
    num_epochs =25
    # Create CNN
    model = CNNModel()
    model.cuda() #  GPU
    print(model)
    # Cross Entropy Loss 
    for param in model.parameters():
        param.requires_grad = False # prendre false si ""you want to freeze model weights"" TRUE si le poids change 
        error = nn.CrossEntropyLoss()
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)
    
    ###################################################accuracy function ##################################
    def accuracyCalc (predicted, targets):
        correct = 0
        p = predicted.tolist()
        t = targets.flatten().tolist() # flatten [[0],[1]] ===&gt; [1,0] tolist 
        for i in range(len(p)):
            if (p[i] == t[i]):
                correct +=1
        accuracy = 100 * correct / targets.shape[0]
        return(accuracy)
    #######################################################################################################
    print("" build model --- %s seconds ---"" % (time.time() - start_time))
    #######################################################{{{{{{{training}}}}}}}##################################
    print('data preparation ')
    training_data = np.load(""Datasets/brats/Train/training_data.npy"", allow_pickle=True)
    
    targets = np.load(""Datasets/brats/Train/targets.npy"", allow_pickle=True)
    
    
    from sklearn.utils import shuffle
    training_data, targets = shuffle(training_data, targets)
    
    training_data = changechannel(training_data, 1, 5) #Channels ordering : first channel to ==&gt; last channel'
    training_data  = resize3Dimages(training_data) #resize images
    training_data = channel1to3(training_data,)#1 channel to 3 channel ===&gt; RGB
    training_data = changechannel(training_data, 4, 1)# last to first
    
    #Definition of hyperparameters
    loss_list_train = []
    accuracy_list_train = []
    for epoch in range(num_epochs): 
        outputs = []
        outputs= torch.tensor(outputs).cuda()
        for fold in range(0, len(training_data), 4): # we will take 4 images
            xtrain = training_data[fold : fold+4]
            xtrain =torch.tensor(xtrain).float().cuda() #  GPU
            xtrain = xtrain.view(4, 3, 120, 120, 120) 
            # Clear gradients
            # Forward propagation
            optimizer.zero_grad() 
            v = model(xtrain)
            outputs = torch.cat((outputs,v.detach()),dim=0)
            # Calculate softmax and ross entropy loss
        targets = torch.Tensor(targets)
        labels = targets.cuda()
        outputs = torch.tensor(outputs,  requires_grad=True) 
        _, predicted = torch.max(outputs, 1) 
        accuracy = accuracyCalc(predicted, targets)
        labels = labels.long() 
        labels=labels.view(-1) #
        loss = nn.CrossEntropyLoss()
        loss = loss(outputs, labels)    
        # Calculating gradients
        loss.backward()
        # Update parameters
        optimizer.step()
        loss_list_train.append(loss.data)
        accuracy_list_train.append(accuracy/100)
        np.save('Datasets/brats/accuracy_list_train.npy', np.array(accuracy_list_train))
        np.save('Datasets/brats/loss_list_train.npy', np.array(loss_list_train)) 
        print('Iteration: {}/{}  Loss: {}  Accuracy: {} %'.format(epoch+1,  num_epochs, loss.data, accuracy))
    print('Model training  : Finished')

&amp;#x200B;

result is :

    Iteration: 1/25 Loss: 0.8488530516624451 Accuracy: 48.0 %
    Iteration: 2/25 Loss: 0.7767133116722107 Accuracy: 48.0 %
    Iteration: 3/25 Loss: 0.7962564826011658 Accuracy: 52.0 %
    Iteration: 4/25 Loss: 0.7540275454521179 Accuracy: 49.333333333333336 %
    Iteration: 5/25 Loss: 0.9554114937782288 Accuracy: 38.666666666666664 %
    Iteration: 6/25 Loss: 0.8776708245277405 Accuracy: 45.333333333333336 %
    Iteration: 7/25 Loss: 0.9581964015960693 Accuracy: 37.333333333333336 %
    Iteration: 8/25 Loss: 0.8645199537277222 Accuracy: 52.0 %
    Iteration: 9/25 Loss: 0.862994909286499 Accuracy: 50.666666666666664 %
    Iteration: 10/25 Loss: 0.6595868468284607 Accuracy: 60.0 %
    Iteration: 11/25 Loss: 0.8252826929092407 Accuracy: 45.333333333333336 %

&amp;#x200B;",t2_762btifd,False,,0,False,CNN: accuracy and loss are increasing and decreasing,[],r/pytorch,False,6,,0,,,False,t3_jlgwrw,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1604144477.0,,[],{},,,True,,1604172994.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;i am trying to create 3d CNN using pytorch.&lt;/p&gt;

&lt;p&gt;the problem that the accuracy and loss are increasing and decreasing (accuracy  values are between 37% 60%)  &lt;/p&gt;

&lt;p&gt;NOTE: if I delete dropout layer the accuracy and loss values remain unchanged for all epochs&lt;/p&gt;

&lt;p&gt;Do you know what I am doing wrong here?&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np
import torch
torch.autograd.set_detect_anomaly(True)
REBUILD_DATA = True # set to true to one once, then back to false unless you want to change something in your training data.
import time
start_time = time.time()
import os
import cv2
import numpy as np
import nibabel as nib
from nibabel.testing import data_path
from sklearn.utils import shuffle
import matplotlib.pyplot as plt
import cv2

from sklearn.metrics import ConfusionMatrixDisplay# Build the confusion matrix of our 2-class classification problem  
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score # for evaluating the model
import torch # PyTorch libraries and modules
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import *
from sklearn.metrics import classification_report
import seaborn as sns # color of graphe
&amp;#39;&amp;#39;&amp;#39;from  owlready2 import *
import textdistance
&amp;#39;&amp;#39;&amp;#39;
import numpy as np



#Channels ordering : first channel (taille , shape of each element ) to ==&amp;gt; last channel ( shape, size )
def changechannel(data, a, b):
    data = np.asarray(data)
    data = np.rollaxis(data, a, b)
    return(data)

# convert (240,240,155) to ======&amp;gt; (120, 120, 120) #
def resize3Dimages(data):
    train_x = []
    for i in range(len(data)):
        image = data[i] 
        width = 120
        height = 120
        img_zeros = np.zeros((len(image), width, height)) #### len(image) means the first dim /// in other words shape[0]

        for idx in range(len(image)):
            img = data[i][idx, :, :]
            img_sm = cv2.resize(img, (width, height), interpolation=cv2.INTER_CUBIC) #une interpolation bicubique sur un voisinage de 4 × 4 voxels 
            img_zeros[idx, :, :] = img_sm
#  convert (240,120,120) to ======&amp;gt; (120,120,120)
        img_zeros = img_zeros[::2, :, :] ### 240/2 =120 
        train_x.append(img_zeros)
############################# save images in list ################################
    return(np.asarray(train_x)) ## convert list to nd array 
# end ...
# 1 channel to 3 channel 
def channel1to3 (data): 
    data = np.stack((data,) * 3, axis=-1)
    return(data)
print(&amp;quot; preprocessing  --- %s seconds ---&amp;quot; % (time.time() - start_time))

print(&amp;#39;Building of CNN&amp;#39;)
import os
# for reading and displaying images
import matplotlib.pyplot as plt
# for evaluating the model
from sklearn.metrics import accuracy_score
# PyTorch libraries and modules
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import *

num_classes = 2

# Create CNN Model
class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__() 

        self.conv_layer1 = self._conv_layer_set(3, 32) 

        self.conv_layer2 = self._conv_layer_set(32, 64) 
        self.conv_layer3 = self._conv_layer_set(64, 128)
        self.conv_layer4 = self._conv_layer_set(128, 256)
        self.conv_layer5 = self._conv_layer_set(256, 512)


        self.fc1 = nn.Linear(512, 128)
        self.fc2 = nn.Linear(128, num_classes)
        self.relu = nn.ReLU()
        self.batch=nn.BatchNorm1d(128)
        self.drop=nn.Dropout(p=0.6, inplace = True)   

    def _conv_layer_set(self, in_c, out_c):
        conv_layer = nn.Sequential(
        nn.Conv3d(in_c, out_c, kernel_size=(3, 3, 3), padding=0),
        nn.ReLU(),
        nn.MaxPool3d((2, 2, 2)),
        )
        return conv_layer


    def forward(self, x):
        # Set 1
        out = self.conv_layer1(x)
        out = self.conv_layer2(out)
        out = self.conv_layer3(out)
        out = self.conv_layer4(out)
        out = self.conv_layer5(out)
        out = out.view(out.size(0), -1)
        out = self.fc1(out)
        out = self.relu(out)
        out = self.batch(out) # batchnormalization 
        out = self.drop(out)
        out = self.fc2(out)
        #out = F.softmax(out, dim=1)
        return out
#Definition of hyperparameters
n_iters = 2
num_epochs =25
# Create CNN
model = CNNModel()
model.cuda() #  GPU
print(model)
# Cross Entropy Loss 
for param in model.parameters():
    param.requires_grad = False # prendre false si &amp;quot;you want to freeze model weights&amp;quot; TRUE si le poids change 
    error = nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)

###################################################accuracy function ##################################
def accuracyCalc (predicted, targets):
    correct = 0
    p = predicted.tolist()
    t = targets.flatten().tolist() # flatten [[0],[1]] ===&amp;gt; [1,0] tolist 
    for i in range(len(p)):
        if (p[i] == t[i]):
            correct +=1
    accuracy = 100 * correct / targets.shape[0]
    return(accuracy)
#######################################################################################################
print(&amp;quot; build model --- %s seconds ---&amp;quot; % (time.time() - start_time))
#######################################################{{{{{{{training}}}}}}}##################################
print(&amp;#39;data preparation &amp;#39;)
training_data = np.load(&amp;quot;Datasets/brats/Train/training_data.npy&amp;quot;, allow_pickle=True)

targets = np.load(&amp;quot;Datasets/brats/Train/targets.npy&amp;quot;, allow_pickle=True)


from sklearn.utils import shuffle
training_data, targets = shuffle(training_data, targets)

training_data = changechannel(training_data, 1, 5) #Channels ordering : first channel to ==&amp;gt; last channel&amp;#39;
training_data  = resize3Dimages(training_data) #resize images
training_data = channel1to3(training_data,)#1 channel to 3 channel ===&amp;gt; RGB
training_data = changechannel(training_data, 4, 1)# last to first

#Definition of hyperparameters
loss_list_train = []
accuracy_list_train = []
for epoch in range(num_epochs): 
    outputs = []
    outputs= torch.tensor(outputs).cuda()
    for fold in range(0, len(training_data), 4): # we will take 4 images
        xtrain = training_data[fold : fold+4]
        xtrain =torch.tensor(xtrain).float().cuda() #  GPU
        xtrain = xtrain.view(4, 3, 120, 120, 120) 
        # Clear gradients
        # Forward propagation
        optimizer.zero_grad() 
        v = model(xtrain)
        outputs = torch.cat((outputs,v.detach()),dim=0)
        # Calculate softmax and ross entropy loss
    targets = torch.Tensor(targets)
    labels = targets.cuda()
    outputs = torch.tensor(outputs,  requires_grad=True) 
    _, predicted = torch.max(outputs, 1) 
    accuracy = accuracyCalc(predicted, targets)
    labels = labels.long() 
    labels=labels.view(-1) #
    loss = nn.CrossEntropyLoss()
    loss = loss(outputs, labels)    
    # Calculating gradients
    loss.backward()
    # Update parameters
    optimizer.step()
    loss_list_train.append(loss.data)
    accuracy_list_train.append(accuracy/100)
    np.save(&amp;#39;Datasets/brats/accuracy_list_train.npy&amp;#39;, np.array(accuracy_list_train))
    np.save(&amp;#39;Datasets/brats/loss_list_train.npy&amp;#39;, np.array(loss_list_train)) 
    print(&amp;#39;Iteration: {}/{}  Loss: {}  Accuracy: {} %&amp;#39;.format(epoch+1,  num_epochs, loss.data, accuracy))
print(&amp;#39;Model training  : Finished&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;result is :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Iteration: 1/25 Loss: 0.8488530516624451 Accuracy: 48.0 %
Iteration: 2/25 Loss: 0.7767133116722107 Accuracy: 48.0 %
Iteration: 3/25 Loss: 0.7962564826011658 Accuracy: 52.0 %
Iteration: 4/25 Loss: 0.7540275454521179 Accuracy: 49.333333333333336 %
Iteration: 5/25 Loss: 0.9554114937782288 Accuracy: 38.666666666666664 %
Iteration: 6/25 Loss: 0.8776708245277405 Accuracy: 45.333333333333336 %
Iteration: 7/25 Loss: 0.9581964015960693 Accuracy: 37.333333333333336 %
Iteration: 8/25 Loss: 0.8645199537277222 Accuracy: 52.0 %
Iteration: 9/25 Loss: 0.862994909286499 Accuracy: 50.666666666666664 %
Iteration: 10/25 Loss: 0.6595868468284607 Accuracy: 60.0 %
Iteration: 11/25 Loss: 0.8252826929092407 Accuracy: 45.333333333333336 %
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jlgwrw,True,,HyahyAi,,11,True,all_ads,False,[],False,,/r/pytorch/comments/jlgwrw/cnn_accuracy_and_loss_are_increasing_and/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jlgwrw/cnn_accuracy_and_loss_are_increasing_and/,7135,1604144194.0,0,,False,,,,,,,,
260,,pytorch,"I have searched a lot, and I am still confused. Is there any easy guide to write a neural network in pytorch? I work on the classic example with digits - image classification).",,False,,0,False,A template to write my first neural network (that classifies digits) with pytorch??,[],r/pytorch,False,6,,0,,,False,t3_jlf0ku,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,,self,False,,,{},,,True,,1604162586.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have searched a lot, and I am still confused. Is there any easy guide to write a neural network in pytorch? I work on the classic example with digits - image classification).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jlf0ku,True,,[deleted],,3,True,all_ads,False,[],,dark,/r/pytorch/comments/jlf0ku/a_template_to_write_my_first_neural_network_that/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jlf0ku/a_template_to_write_my_first_neural_network_that/,7135,1604133786.0,0,,False,,,,,,,,
261,,pytorch,"Hi i am building a new computer specifically for pytorch ML and looking to make a purchase around December. First question, are AMD rx 6000 series compatible with pytorch?

I hear the nvidia rtx 3080/3090 are not very well optimized for pytorch at the moment, when is it likely developers will make full use of these cards? How do these cards compare to the rtx 2000 series in terms of performance at the moment with torch?",t2_13p42q,False,,0,False,current GPU recommendation for pytorch,[],r/pytorch,False,6,,0,,,False,t3_jl1a15,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,1604081727.0,,[],{},,,True,,1604108053.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi i am building a new computer specifically for pytorch ML and looking to make a purchase around December. First question, are AMD rx 6000 series compatible with pytorch?&lt;/p&gt;

&lt;p&gt;I hear the nvidia rtx 3080/3090 are not very well optimized for pytorch at the moment, when is it likely developers will make full use of these cards? How do these cards compare to the rtx 2000 series in terms of performance at the moment with torch?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jl1a15,True,,Qtbby69,,7,True,all_ads,False,[],False,,/r/pytorch/comments/jl1a15/current_gpu_recommendation_for_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jl1a15/current_gpu_recommendation_for_pytorch/,7135,1604079253.0,0,,False,,,,,,,,
262,,pytorch,I spend the last few weeks getting my PyTorch code working on TPUs. I made a short thread on points that might be helpful ([https://twitter.com/KaliTessera/status/1321454533671870466](https://twitter.com/KaliTessera/status/1321454533671870466)).,t2_p98c3j5,False,,0,False,Tips for training PyTorch models on TPUs,[],r/pytorch,False,6,,0,,,False,t3_jjok7x,False,dark,1.0,,public,14,0,{},,,False,[],,False,False,,{},,False,14,,False,self,False,,[],{},self,,True,,1603923444.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I spend the last few weeks getting my PyTorch code working on TPUs. I made a short thread on points that might be helpful (&lt;a href=""https://twitter.com/KaliTessera/status/1321454533671870466""&gt;https://twitter.com/KaliTessera/status/1321454533671870466&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/pU8sax6zV2sxvutXETiCwE3ny7PoPVTJ21Glw-MSNto.jpg?auto=webp&amp;s=541b447568b00b38e3f243a60dbcdd0dd6a7a474', 'width': 140, 'height': 140}, 'resolutions': [{'url': 'https://external-preview.redd.it/pU8sax6zV2sxvutXETiCwE3ny7PoPVTJ21Glw-MSNto.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d7bebf6de24bd5e9491eab264183888414af4bc', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'Ibn0grtbTc34nOroZoPSyeCMuzaUR6YHzCFkinT20yA'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jjok7x,True,,ktessera,,1,True,all_ads,False,[],False,,/r/pytorch/comments/jjok7x/tips_for_training_pytorch_models_on_tpus/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jjok7x/tips_for_training_pytorch_models_on_tpus/,7135,1603894644.0,0,,False,,,,,,,,
263,,pytorch,,t2_7eslkpz,False,,0,False,"PyTorch 1.7 released w/ CUDA 11, New APIs for FFTs, Windows support for Distributed training and more",[],r/pytorch,False,6,,0,140.0,,False,t3_jj8dbc,False,dark,0.98,,public,30,0,{},140.0,,False,[],,False,False,,{},,False,30,,False,https://b.thumbs.redditmedia.com/p4uzn6eQTfOK6ssscXbQI3osXiabGIlyHAgUZCKT5vs.jpg,False,,[],{},link,,False,,1603856856.0,text,6,,,text,pytorch.org,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?auto=webp&amp;s=d0da5dbb7aff983a5173aba7d2bdd36fa28abe08', 'width': 2500, 'height': 2500}, 'resolutions': [{'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fef05ec37ddf39032979380cc7acc403e05834be', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e1de8e408f33ef3c93425e30e0bb5afffd5c631', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89562dde9cea0af5ebaaba50a9437e484d732513', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d6125bcd02c2fed8d84dac49cc8d73dbe205d81', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=40d7ca8d88566f27054acb632514012b5026acc9', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=757464a70e69532931dcbf0fbd397b9d433cf205', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'Mg0HAz_PmYkYWkaGTVyhCEzp15PpLW_TWWqmn9VNVFs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jj8dbc,True,,Marha01,,1,False,all_ads,False,[],False,,/r/pytorch/comments/jj8dbc/pytorch_17_released_w_cuda_11_new_apis_for_ffts/,all_ads,False,https://pytorch.org/blog/pytorch-1.7-released/,7135,1603828056.0,0,,False,https://pytorch.org/blog/pytorch-1.7-released/,,,,,,,
264,,pytorch,"Today we have an exciting post on Classifying Knee MRI images using Deep Learning. In this post, you will learn  


1. What an MRI dataset looks like.
2. What is Stanford MRNet Challenge
3. How to create an AI model for MRI data classification
4. Results
5. Suggestions for alternative approaches.

[https://www.learnopencv.com/stanford-mrnet-challenge-classifying-knee-mris/](https://click.convertkit-mail.com/v8u6ek0726urhm5krpcw/9qhzhdu277vrwkf9/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL3N0YW5mb3JkLW1ybmV0LWNoYWxsZW5nZS1jbGFzc2lmeWluZy1rbmVlLW1yaXMv)  


and the **PyTorch code** is at  


[https://github.com/spmallick/learnopencv/tree/master/MRNet-Single-Model](https://click.convertkit-mail.com/v8u6ek0726urhm5krpcw/3ohphdudww5gmoar/aHR0cHM6Ly9naXRodWIuY29tL3NwbWFsbGljay9sZWFybm9wZW5jdi90cmVlL21hc3Rlci9NUk5ldC1TaW5nbGUtTW9kZWw=) 

https://preview.redd.it/xmovkhbp4pv51.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=641c1e7b4ed9d23edb12b8e7c95939822aa943d0",t2_cvc9f,False,,0,False,MRI Image Classification : A step by step guide,[],r/pytorch,False,6,,0,93.0,,False,t3_jj91ge,False,dark,0.84,,public,4,0,{},140.0,,False,[],,False,False,,{},,False,4,,False,https://b.thumbs.redditmedia.com/VIlV-P13Lyi9aocjgeSxG6NZYxruxMnWCqPmseHtdTs.jpg,False,,[],{},,,True,,1603858881.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Today we have an exciting post on Classifying Knee MRI images using Deep Learning. In this post, you will learn  &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What an MRI dataset looks like.&lt;/li&gt;
&lt;li&gt;What is Stanford MRNet Challenge&lt;/li&gt;
&lt;li&gt;How to create an AI model for MRI data classification&lt;/li&gt;
&lt;li&gt;Results&lt;/li&gt;
&lt;li&gt;Suggestions for alternative approaches.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=""https://click.convertkit-mail.com/v8u6ek0726urhm5krpcw/9qhzhdu277vrwkf9/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL3N0YW5mb3JkLW1ybmV0LWNoYWxsZW5nZS1jbGFzc2lmeWluZy1rbmVlLW1yaXMv""&gt;https://www.learnopencv.com/stanford-mrnet-challenge-classifying-knee-mris/&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;and the &lt;strong&gt;PyTorch code&lt;/strong&gt; is at  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://click.convertkit-mail.com/v8u6ek0726urhm5krpcw/3ohphdudww5gmoar/aHR0cHM6Ly9naXRodWIuY29tL3NwbWFsbGljay9sZWFybm9wZW5jdi90cmVlL21hc3Rlci9NUk5ldC1TaW5nbGUtTW9kZWw=""&gt;https://github.com/spmallick/learnopencv/tree/master/MRNet-Single-Model&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/xmovkhbp4pv51.jpg?width=600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=641c1e7b4ed9d23edb12b8e7c95939822aa943d0""&gt;https://preview.redd.it/xmovkhbp4pv51.jpg?width=600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=641c1e7b4ed9d23edb12b8e7c95939822aa943d0&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jj91ge,True,,spmallick,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jj91ge/mri_image_classification_a_step_by_step_guide/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jj91ge/mri_image_classification_a_step_by_step_guide/,7135,1603830081.0,0,,False,,,,"{'xmovkhbp4pv51': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 72, 'x': 108, 'u': 'https://preview.redd.it/xmovkhbp4pv51.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ae0513db18c648fcb4eb1ed1cf5fa33587629266'}, {'y': 144, 'x': 216, 'u': 'https://preview.redd.it/xmovkhbp4pv51.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9242b25724e78a1bfeca7cd68d8bf8f3750fe61c'}, {'y': 213, 'x': 320, 'u': 'https://preview.redd.it/xmovkhbp4pv51.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5477e3b0b0338a447d06181d8178c97cb1adc913'}], 's': {'y': 400, 'x': 600, 'u': 'https://preview.redd.it/xmovkhbp4pv51.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=641c1e7b4ed9d23edb12b8e7c95939822aa943d0'}, 'id': 'xmovkhbp4pv51'}}",,,,
265,,pytorch,"Hallo,

I recently trying to migrate from **MATLAB** to **python**. I am trying to find a decent **IDE**. I work with **pytorch** mostly. I find **Spyder** very appealing due to variable explorer (reminds me MATLAB). However I read that **Visual Studio** is more widely used. Also I see that **jupyter** is good for the ""portability"". 

Any guidance here?",t2_3u539kf9,False,,0,False,IDE for python (pytorch),[],r/pytorch,False,6,,0,,,False,t3_jj2cub,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1603838451.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hallo,&lt;/p&gt;

&lt;p&gt;I recently trying to migrate from &lt;strong&gt;MATLAB&lt;/strong&gt; to &lt;strong&gt;python&lt;/strong&gt;. I am trying to find a decent &lt;strong&gt;IDE&lt;/strong&gt;. I work with &lt;strong&gt;pytorch&lt;/strong&gt; mostly. I find &lt;strong&gt;Spyder&lt;/strong&gt; very appealing due to variable explorer (reminds me MATLAB). However I read that &lt;strong&gt;Visual Studio&lt;/strong&gt; is more widely used. Also I see that &lt;strong&gt;jupyter&lt;/strong&gt; is good for the &amp;quot;portability&amp;quot;. &lt;/p&gt;

&lt;p&gt;Any guidance here?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jj2cub,True,,Slifernet,,10,True,all_ads,False,[],False,,/r/pytorch/comments/jj2cub/ide_for_python_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jj2cub/ide_for_python_pytorch/,7135,1603809651.0,0,,False,,,,,,,,
266,,pytorch,"Hello,

I have a ""train"" method as shown below.  When I run it, I get an error that my input and weight type should be the same - which I understand as meaning that my network has been pushed to the GPU but my batch of images has not been.  Indeed, when I print out the device of my batch of images it says it is still on the CPU even though doing basically the same thing to the network moves it to the GPU.  Do you know what I am doing wrong here?

Thanks in advance!

&amp;#x200B;

    def train(net, trainset, testset, n, bs):
        device = torch.device('cuda:0')
        net.to(device)
        o = optim.Adam(net.parameters(), lr=0.01)
        trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs)
        testloader = torch.utils.data.DataLoader(testset, batch_size=bs)
        
        best_net = net
        best_loss =  float('inf')
        
        #This is the epoch loop - one execution = one epoch
        for e in range(n):
            total_loss = 0
            total_accuracy = 0
            validation_loss = 0
            validation_accuracy = 0
            
            #Will need gradient while training...
            net.requires_grad = True
            
            #Get all batches...
            for batch in trainloader:
                images, labels = batch
                images.to(device)
                labels.to(device)
                
                for l in net.parameters():
                    print(l.device)
                print(images.device)
                
                #Get predictions from the network
                preds = net(images)

output from print statements:

    cuda:0
    cuda:0
    cuda:0
    cuda:0
    cuda:0
    cuda:0
    cuda:0
    cuda:0
    cuda:0
    cuda:0
    cpu

Error Message:

    RuntimeError                              Traceback (most recent call last)
    &lt;ipython-input-107-764051b26d04&gt; in &lt;module&gt;
    ----&gt; 1 bnet = train(net, train_set, test_set, 10, 100)
    
    &lt;ipython-input-106-c6b2a1bc7e39&gt; in train(net, trainset, testset, n, bs)
         30 
         31             #Get predictions from the network
    ---&gt; 32             preds = net(images)
         33 
         34             #Find the loss
    
    ~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
        720             result = self._slow_forward(*input, **kwargs)
        721         else:
    --&gt; 722             result = self.forward(*input, **kwargs)
        723         for hook in itertools.chain(
        724                 _global_forward_hooks.values(),
    
    &lt;ipython-input-10-fd501e1e41e9&gt; in forward(self, t)
         14 
         15         #Layer 1
    ---&gt; 16         t = self.conv1(t)
         17         t = F.relu(t)
         18         t = F.max_pool2d(t, kernel_size=2, stride=2)
    
    ~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
        720             result = self._slow_forward(*input, **kwargs)
        721         else:
    --&gt; 722             result = self.forward(*input, **kwargs)
        723         for hook in itertools.chain(
        724                 _global_forward_hooks.values(),
    
    ~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py in forward(self, input)
        417 
        418     def forward(self, input: Tensor) -&gt; Tensor:
    --&gt; 419         return self._conv_forward(input, self.weight)
        420 
        421 class Conv3d(_ConvNd):
    
    ~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight)
        414                             _pair(0), self.dilation, self.groups)
        415         return F.conv2d(input, weight, self.bias, self.stride,
    --&gt; 416                         self.padding, self.dilation, self.groups)
        417 
        418     def forward(self, input: Tensor) -&gt; Tensor:
    
    RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same

&amp;#x200B;",t2_16fs1f,False,,0,False,Cannot move batch of images to gpu,[],r/pytorch,False,6,,0,,,False,t3_jirz7v,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1603762771.0,,[],{},,,True,,1603791019.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I have a &amp;quot;train&amp;quot; method as shown below.  When I run it, I get an error that my input and weight type should be the same - which I understand as meaning that my network has been pushed to the GPU but my batch of images has not been.  Indeed, when I print out the device of my batch of images it says it is still on the CPU even though doing basically the same thing to the network moves it to the GPU.  Do you know what I am doing wrong here?&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def train(net, trainset, testset, n, bs):
    device = torch.device(&amp;#39;cuda:0&amp;#39;)
    net.to(device)
    o = optim.Adam(net.parameters(), lr=0.01)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs)
    testloader = torch.utils.data.DataLoader(testset, batch_size=bs)

    best_net = net
    best_loss =  float(&amp;#39;inf&amp;#39;)

    #This is the epoch loop - one execution = one epoch
    for e in range(n):
        total_loss = 0
        total_accuracy = 0
        validation_loss = 0
        validation_accuracy = 0

        #Will need gradient while training...
        net.requires_grad = True

        #Get all batches...
        for batch in trainloader:
            images, labels = batch
            images.to(device)
            labels.to(device)

            for l in net.parameters():
                print(l.device)
            print(images.device)

            #Get predictions from the network
            preds = net(images)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;output from print statements:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cpu
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Error Message:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RuntimeError                              Traceback (most recent call last)
&amp;lt;ipython-input-107-764051b26d04&amp;gt; in &amp;lt;module&amp;gt;
----&amp;gt; 1 bnet = train(net, train_set, test_set, 10, 100)

&amp;lt;ipython-input-106-c6b2a1bc7e39&amp;gt; in train(net, trainset, testset, n, bs)
     30 
     31             #Get predictions from the network
---&amp;gt; 32             preds = net(images)
     33 
     34             #Find the loss

~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    720             result = self._slow_forward(*input, **kwargs)
    721         else:
--&amp;gt; 722             result = self.forward(*input, **kwargs)
    723         for hook in itertools.chain(
    724                 _global_forward_hooks.values(),

&amp;lt;ipython-input-10-fd501e1e41e9&amp;gt; in forward(self, t)
     14 
     15         #Layer 1
---&amp;gt; 16         t = self.conv1(t)
     17         t = F.relu(t)
     18         t = F.max_pool2d(t, kernel_size=2, stride=2)

~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    720             result = self._slow_forward(*input, **kwargs)
    721         else:
--&amp;gt; 722             result = self.forward(*input, **kwargs)
    723         for hook in itertools.chain(
    724                 _global_forward_hooks.values(),

~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py in forward(self, input)
    417 
    418     def forward(self, input: Tensor) -&amp;gt; Tensor:
--&amp;gt; 419         return self._conv_forward(input, self.weight)
    420 
    421 class Conv3d(_ConvNd):

~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight)
    414                             _pair(0), self.dilation, self.groups)
    415         return F.conv2d(input, weight, self.bias, self.stride,
--&amp;gt; 416                         self.padding, self.dilation, self.groups)
    417 
    418     def forward(self, input: Tensor) -&amp;gt; Tensor:

RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jirz7v,True,,yourtalllife,,5,True,all_ads,False,[],False,,/r/pytorch/comments/jirz7v/cannot_move_batch_of_images_to_gpu/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jirz7v/cannot_move_batch_of_images_to_gpu/,7135,1603762219.0,0,,False,,,,,,,,
267,,pytorch,"Hi all,

I made a post on the Pytorch forums about my issue, please could someone help?

[https://discuss.pytorch.org/t/loss-flattens-out/100699](https://discuss.pytorch.org/t/loss-flattens-out/100699)

\--------------------------------------------------

I have worked on a couple of projects now where I have constructed a neural net for binary classification and something like this has happened. This leads me to believe it's something to do with how I am programming it rather than the data itself.

I have tried all kinds of things, even posted here before but I'm not sure why. I did not recycle my own code or anything so I don't know how this problem persists across projects.

&amp;#x200B;

The model is simple:

`# A simple binary classification model`

`class BinaryClassifier(nn.Module):`

`def __init__(self, input_size, hidden_size, num_classes=1):`

`super().__init__()`

`self.input_size = input_size`

`self.num_classes = num_classes`

`self.linear1 = nn.Linear(input_size, hidden_size)`

`self.linear2 = nn.Linear(hidden_size, num_classes)`

`self.relu = nn.ReLU()`

`def forward(self, x):`

`x = self.linear1(x)`

`x = self.relu(x)`

`x = self.linear2(x)`

`return x`

I have set num\_classes as 1 in order to get a probability that the label is 1. I wasn't sure how else to structure this binary classification.

I use BCEWithLogitsLoss, Adam with a StepLR scheduler

`model = BinaryClassifier(X_tensor.shape[1], 512)`

`criterion = nn.BCEWithLogitsLoss()`

`optimizer = torch.optim.Adam(model.parameters(), lr=lr)`

`scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)`

It's worth mentioning here I only added the scheduler to fix the problem and it did improve the loss but not the flattening out effect.

This is how I train the model:

`def sub_train_(model, dataloader):`

`model.train()`

`losses = list()`

`for idx, (X, y) in enumerate(dataloader):`

`out = model(X)`

`loss = criterion(out, y.unsqueeze(1))`

`optimizer.zero_grad()`

`loss.backward()`

`optimizer.step()`

`losses.append(loss.item())`

`return np.mean(losses), model`

&amp;#x200B;

`def train(model, trainloader, testloader, scheduler, n_epochs):`

`best_model = model`

`best_loss = math.inf`

`ts = time.time()`

`losses = list()`

`for epoch in range(n_epochs):`

`train_loss, model = sub_train_(model, trainloader)`

`test_loss = sub_valid_(model, testloader)`

`scheduler.step()`

`losses.append(train_loss)`

`if train_loss &lt; best_loss:`

`best_loss = train_loss`

`best_model = model`

`print('Epoch: {}, train_loss: {}, test_loss: {}'.format(`

`epoch, train_loss, test_loss`

`))`

`te = time.time()`

`fig, ax = plt.subplots()`

`ax.plot(range(n_epochs), losses)`

[`plt.show`](https://plt.show)`()`

`mins = int((te-ts) / 60)`

`secs = int((te-ts) % 60)`

`print('Training completed in {} minutes, {} seconds.'.format(mins, secs))`

`return losses, best_model`

And every time it yields a loss plot like this:

https://preview.redd.it/ocwt26vg0iv51.png?width=457&amp;format=png&amp;auto=webp&amp;s=dcc9935fa17cc008a488d659ffc9a66dbda20069

&amp;#x200B;

What am I doing wrong here? How can I avoid this happening?

Thank you in advance to anyone who helps me out with this!",t2_ebu4m,False,,0,False,Loss flattens out,[],r/pytorch,False,6,,0,99.0,,False,t3_jimfls,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/FjeLKmNBc7u237R3ZvZfK1S6N5lOYaUCevERaO3EbLE.jpg,1603743997.0,,[],{},self,,True,,1603772613.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I made a post on the Pytorch forums about my issue, please could someone help?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://discuss.pytorch.org/t/loss-flattens-out/100699""&gt;https://discuss.pytorch.org/t/loss-flattens-out/100699&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;--------------------------------------------------&lt;/p&gt;

&lt;p&gt;I have worked on a couple of projects now where I have constructed a neural net for binary classification and something like this has happened. This leads me to believe it&amp;#39;s something to do with how I am programming it rather than the data itself.&lt;/p&gt;

&lt;p&gt;I have tried all kinds of things, even posted here before but I&amp;#39;m not sure why. I did not recycle my own code or anything so I don&amp;#39;t know how this problem persists across projects.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The model is simple:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# A simple binary classification model&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;class BinaryClassifier(nn.Module):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def __init__(self, input_size, hidden_size, num_classes=1):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;super().__init__()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.input_size = input_size&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.num_classes = num_classes&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.linear1 = nn.Linear(input_size, hidden_size)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.linear2 = nn.Linear(hidden_size, num_classes)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.relu = nn.ReLU()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def forward(self, x):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = self.linear1(x)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = self.relu(x)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = self.linear2(x)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;return x&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I have set num_classes as 1 in order to get a probability that the label is 1. I wasn&amp;#39;t sure how else to structure this binary classification.&lt;/p&gt;

&lt;p&gt;I use BCEWithLogitsLoss, Adam with a StepLR scheduler&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model = BinaryClassifier(X_tensor.shape[1], 512)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;criterion = nn.BCEWithLogitsLoss()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;optimizer = torch.optim.Adam(model.parameters(), lr=lr)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It&amp;#39;s worth mentioning here I only added the scheduler to fix the problem and it did improve the loss but not the flattening out effect.&lt;/p&gt;

&lt;p&gt;This is how I train the model:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def sub_train_(model, dataloader):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model.train()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;losses = list()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for idx, (X, y) in enumerate(dataloader):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;out = model(X)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;loss = criterion(out, y.unsqueeze(1))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;optimizer.zero_grad()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;loss.backward()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;optimizer.step()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;losses.append(loss.item())&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;return np.mean(losses), model&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def train(model, trainloader, testloader, scheduler, n_epochs):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;best_model = model&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;best_loss = math.inf&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ts = time.time()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;losses = list()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for epoch in range(n_epochs):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;train_loss, model = sub_train_(model, trainloader)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;test_loss = sub_valid_(model, testloader)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;scheduler.step()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;losses.append(train_loss)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;if train_loss &amp;lt; best_loss:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;best_loss = train_loss&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;best_model = model&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(&amp;#39;Epoch: {}, train_loss: {}, test_loss: {}&amp;#39;.format(&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;epoch, train_loss, test_loss&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;te = time.time()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;fig, ax = plt.subplots()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ax.plot(range(n_epochs), losses)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://plt.show""&gt;&lt;code&gt;plt.show&lt;/code&gt;&lt;/a&gt;&lt;code&gt;()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;mins = int((te-ts) / 60)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;secs = int((te-ts) % 60)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(&amp;#39;Training completed in {} minutes, {} seconds.&amp;#39;.format(mins, secs))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;return losses, best_model&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And every time it yields a loss plot like this:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/ocwt26vg0iv51.png?width=457&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dcc9935fa17cc008a488d659ffc9a66dbda20069""&gt;https://preview.redd.it/ocwt26vg0iv51.png?width=457&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dcc9935fa17cc008a488d659ffc9a66dbda20069&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What am I doing wrong here? How can I avoid this happening?&lt;/p&gt;

&lt;p&gt;Thank you in advance to anyone who helps me out with this!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/_wpd8QNJiKOC1PHnlE_HXQ4D2tAMxbP0r1_YMS2XyRE.jpg?auto=webp&amp;s=fe2304bfa5a3962af5f283dea4480e78eb3995c4', 'width': 467, 'height': 331}, 'resolutions': [{'url': 'https://external-preview.redd.it/_wpd8QNJiKOC1PHnlE_HXQ4D2tAMxbP0r1_YMS2XyRE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa1cd6e551bab65ca6f5f6b431ed8f54f81b7553', 'width': 108, 'height': 76}, {'url': 'https://external-preview.redd.it/_wpd8QNJiKOC1PHnlE_HXQ4D2tAMxbP0r1_YMS2XyRE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7ca75f488dad66348d6a1e42f1c2f1900aba3aaa', 'width': 216, 'height': 153}, {'url': 'https://external-preview.redd.it/_wpd8QNJiKOC1PHnlE_HXQ4D2tAMxbP0r1_YMS2XyRE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e73875caa52826dca026222fb97ef45929d33df3', 'width': 320, 'height': 226}], 'variants': {}, 'id': 'rzMrJiqq4itTmk1K0-G5q7-XK38peFHMlcEX9cbpBqs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jimfls,True,,Pepipasta,,4,True,all_ads,False,[],False,,/r/pytorch/comments/jimfls/loss_flattens_out/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jimfls/loss_flattens_out/,7135,1603743813.0,0,,False,,,,"{'ocwt26vg0iv51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 73, 'x': 108, 'u': 'https://preview.redd.it/ocwt26vg0iv51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=41430f5418a6c1e3a8efaf814ff26e9a73b94afa'}, {'y': 147, 'x': 216, 'u': 'https://preview.redd.it/ocwt26vg0iv51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=114f554d01ec3da318a5e9c767941f1d0f97a82a'}, {'y': 218, 'x': 320, 'u': 'https://preview.redd.it/ocwt26vg0iv51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6a033c9700773dd85382a7fff25f7d7ed3e99309'}], 's': {'y': 312, 'x': 457, 'u': 'https://preview.redd.it/ocwt26vg0iv51.png?width=457&amp;format=png&amp;auto=webp&amp;s=dcc9935fa17cc008a488d659ffc9a66dbda20069'}, 'id': 'ocwt26vg0iv51'}}",,,,
268,,pytorch,"I'm trying to train a simple classifier with PyTorch, and in an attempt to do something other than just follow along a tutorial I am training it to classify lists into two categories: ""repeating"" and ""increasing"".

So I tried to build some code to work with DataLoader, as I'm generating these lists. It looks like this:

    class ListDataset (Dataset):
        def __init__(self, num_lists = num_data, verbose = 0):
            x_data = []
            y_data = []
            # Generate the appropriate number of lists
            for i in range (num_lists):
                x, y = generate_list(verbose=verbose)
                x_data.append(x)
                y_data.append(y)
            # Record, though I could have done this with less code
            self.x_data = x_data
            self.y_data = y_data
            self.length = len(self.x_data)
    
            print(self.y_data)
            print(self.y_data[0])

It's not working properly, when I try to call `label =` [`label.to`](https://label.to)`(self.device)` I get the error `AttributeError: 'tuple' object has no attribute 'to'` and printing the label before that gives something which looks like this: `(""increasing"", ""repeating"" .... ""repeating"", ""increasing"")` The problem is not with this part of the code, as it was pretty much lifted from a tutorial. 

I'm sure that the problem is with the data code, but I'm not quite sure, is it that my labels are strings? What should they be? 1 and 0 ints?",t2_26lp84x0,False,,0,False,Does DataLoader accept string values as labels? What values does it accept?,[],r/pytorch,False,6,,0,,,False,t3_jhqlny,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1603646616.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to train a simple classifier with PyTorch, and in an attempt to do something other than just follow along a tutorial I am training it to classify lists into two categories: &amp;quot;repeating&amp;quot; and &amp;quot;increasing&amp;quot;.&lt;/p&gt;

&lt;p&gt;So I tried to build some code to work with DataLoader, as I&amp;#39;m generating these lists. It looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class ListDataset (Dataset):
    def __init__(self, num_lists = num_data, verbose = 0):
        x_data = []
        y_data = []
        # Generate the appropriate number of lists
        for i in range (num_lists):
            x, y = generate_list(verbose=verbose)
            x_data.append(x)
            y_data.append(y)
        # Record, though I could have done this with less code
        self.x_data = x_data
        self.y_data = y_data
        self.length = len(self.x_data)

        print(self.y_data)
        print(self.y_data[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;#39;s not working properly, when I try to call &lt;code&gt;label =&lt;/code&gt; &lt;a href=""https://label.to""&gt;&lt;code&gt;label.to&lt;/code&gt;&lt;/a&gt;&lt;code&gt;(self.device)&lt;/code&gt; I get the error &lt;code&gt;AttributeError: &amp;#39;tuple&amp;#39; object has no attribute &amp;#39;to&amp;#39;&lt;/code&gt; and printing the label before that gives something which looks like this: &lt;code&gt;(&amp;quot;increasing&amp;quot;, &amp;quot;repeating&amp;quot; .... &amp;quot;repeating&amp;quot;, &amp;quot;increasing&amp;quot;)&lt;/code&gt; The problem is not with this part of the code, as it was pretty much lifted from a tutorial. &lt;/p&gt;

&lt;p&gt;I&amp;#39;m sure that the problem is with the data code, but I&amp;#39;m not quite sure, is it that my labels are strings? What should they be? 1 and 0 ints?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jhqlny,True,,RichKat666,,4,True,all_ads,False,[],False,,/r/pytorch/comments/jhqlny/does_dataloader_accept_string_values_as_labels/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jhqlny/does_dataloader_accept_string_values_as_labels/,7135,1603617816.0,0,,False,,,,,,,,
269,,pytorch,,t2_766u1eio,False,,0,False,How does one install custom binaries for the Knights Landing CPU architecture for PyTorch?,[],r/pytorch,False,6,,0,140.0,,False,t3_jhsy7w,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/y1f8kHZL3_ZPNgIMnsnA56oJurnNSy8u3OiXDP7bXfk.jpg,False,,[],{},link,,False,,1603659592.0,text,6,,,text,discuss.pytorch.org,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jhsy7w,True,,No_Ad3397,,1,True,all_ads,False,[],False,,/r/pytorch/comments/jhsy7w/how_does_one_install_custom_binaries_for_the/,all_ads,False,https://discuss.pytorch.org/t/custom-cpu-binaries-for-intel-knights-landing-for-pytorch/100563,7135,1603630792.0,0,,False,https://discuss.pytorch.org/t/custom-cpu-binaries-for-intel-knights-landing-for-pytorch/100563,,,,,,,
270,,pytorch,"For your reference, [this](https://pytorch2020.devpost.com/project-gallery) is the link to the hackathon.

According to the [rules](https://imgur.com/a/oFyw8Ia), there are honorable mention prizes for those who are scoring 3rd to 8th. Although it is not a big deal to most, I find that people should be correctly awarded the right prize given the circumstance.

Edit: As you can clearly see, there are no honorable mention prizes given to participants

Thus I asked twice, [once](https://imgur.com/a/pbAw9mF) in the official slack channel where I got straight up ignored, and on the [official](https://imgur.com/Zv5er2d) devpost q/a section, where the manager was irresponsive.

This is a literal shot in the dark but what should I do to solve this?? To whom do I even contact at this point?",t2_tz5p0rl,False,,0,False,"Pytorch hackathon went against its own rules, who should I contact?",[],r/pytorch,False,6,,0,,,False,t3_jhwdoe,False,dark,0.47,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1603672666.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For your reference, &lt;a href=""https://pytorch2020.devpost.com/project-gallery""&gt;this&lt;/a&gt; is the link to the hackathon.&lt;/p&gt;

&lt;p&gt;According to the &lt;a href=""https://imgur.com/a/oFyw8Ia""&gt;rules&lt;/a&gt;, there are honorable mention prizes for those who are scoring 3rd to 8th. Although it is not a big deal to most, I find that people should be correctly awarded the right prize given the circumstance.&lt;/p&gt;

&lt;p&gt;Edit: As you can clearly see, there are no honorable mention prizes given to participants&lt;/p&gt;

&lt;p&gt;Thus I asked twice, &lt;a href=""https://imgur.com/a/pbAw9mF""&gt;once&lt;/a&gt; in the official slack channel where I got straight up ignored, and on the &lt;a href=""https://imgur.com/Zv5er2d""&gt;official&lt;/a&gt; devpost q/a section, where the manager was irresponsive.&lt;/p&gt;

&lt;p&gt;This is a literal shot in the dark but what should I do to solve this?? To whom do I even contact at this point?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/UdK9j8C3_FZ71oYDVKb78CW4hvWhLNtKQZbJ8DgAYZM.jpg?auto=webp&amp;s=12a89a44aed271bd7fe9a2bac842402a95b87ea4', 'width': 1616, 'height': 1024}, 'resolutions': [{'url': 'https://external-preview.redd.it/UdK9j8C3_FZ71oYDVKb78CW4hvWhLNtKQZbJ8DgAYZM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9547c9b0734640094789aac7b0696d74492c39da', 'width': 108, 'height': 68}, {'url': 'https://external-preview.redd.it/UdK9j8C3_FZ71oYDVKb78CW4hvWhLNtKQZbJ8DgAYZM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3cdd064f8b2947f17fa675dcc33cd9f4fb9dca6e', 'width': 216, 'height': 136}, {'url': 'https://external-preview.redd.it/UdK9j8C3_FZ71oYDVKb78CW4hvWhLNtKQZbJ8DgAYZM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f7216732f4fc74dfae29cd86bff701e5def0c0c', 'width': 320, 'height': 202}, {'url': 'https://external-preview.redd.it/UdK9j8C3_FZ71oYDVKb78CW4hvWhLNtKQZbJ8DgAYZM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e950400c620627f07e727793c2dedbf6ad700b81', 'width': 640, 'height': 405}, {'url': 'https://external-preview.redd.it/UdK9j8C3_FZ71oYDVKb78CW4hvWhLNtKQZbJ8DgAYZM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=db5bcac6c4e84fc7d30e9bf3ba581558f30b41de', 'width': 960, 'height': 608}, {'url': 'https://external-preview.redd.it/UdK9j8C3_FZ71oYDVKb78CW4hvWhLNtKQZbJ8DgAYZM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4aa010544e03ec344955ea14d1036e1b28fab9c4', 'width': 1080, 'height': 684}], 'variants': {}, 'id': 'FhUxRcODmib-w3aIvUCmhtUlfW2Q0jeJNOX3uTe2d2g'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jhwdoe,True,,verysad1997,,4,True,all_ads,False,[],False,,/r/pytorch/comments/jhwdoe/pytorch_hackathon_went_against_its_own_rules_who/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jhwdoe/pytorch_hackathon_went_against_its_own_rules_who/,7135,1603643866.0,0,,False,,,,,,,,
271,,pytorch,"Hi guys, I hope you are well and healthy!  I have put together a compact, concise, and customizable deep learning computer vision library called ""glasses"". 

Github: [https://github.com/FrancescoSaverioZuppichini/glasses](https://github.com/FrancescoSaverioZuppichini/glasses)

Website: [https://francescosaveriozuppichini.github.io/glasses-webapp/](https://francescosaveriozuppichini.github.io/glasses-webapp/)

It is the first beta so there are a lot of missing models and features that I will add in the future.

I do computer vision for a living and I wanted to have a flexible easy to use tool for my daily work. Most of the current libraries are very badly written with tons of code repetition and not very easy to customize. Moreover, it is hard to understand how most models are implemented and I hope my library will make it easier for new people in the field.

Unfortunately, I still haven't found a good place to host the pre-trained models so they are not available at the moment (any suggestions?)

I also would like to know if anyone wants to help me out with some feedback or in the coding!",t2_gw9fw2x,False,,0,False,A new deep learning computer vision library!,[],r/pytorch,False,6,,0,,,False,t3_jhc7od,False,dark,1.0,,public,19,0,{},,,False,[],,False,False,,{},,False,19,,False,self,False,,[],{},self,,True,,1603587000.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys, I hope you are well and healthy!  I have put together a compact, concise, and customizable deep learning computer vision library called &amp;quot;glasses&amp;quot;. &lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/FrancescoSaverioZuppichini/glasses""&gt;https://github.com/FrancescoSaverioZuppichini/glasses&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Website: &lt;a href=""https://francescosaveriozuppichini.github.io/glasses-webapp/""&gt;https://francescosaveriozuppichini.github.io/glasses-webapp/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It is the first beta so there are a lot of missing models and features that I will add in the future.&lt;/p&gt;

&lt;p&gt;I do computer vision for a living and I wanted to have a flexible easy to use tool for my daily work. Most of the current libraries are very badly written with tons of code repetition and not very easy to customize. Moreover, it is hard to understand how most models are implemented and I hope my library will make it easier for new people in the field.&lt;/p&gt;

&lt;p&gt;Unfortunately, I still haven&amp;#39;t found a good place to host the pre-trained models so they are not available at the moment (any suggestions?)&lt;/p&gt;

&lt;p&gt;I also would like to know if anyone wants to help me out with some feedback or in the coding!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Q-JAuaWn7tEVLUPPkF8J8227Fg3tAEhevtFSSD9r258.jpg?auto=webp&amp;s=216586bc287819a827c24f43928311298e61789f', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/Q-JAuaWn7tEVLUPPkF8J8227Fg3tAEhevtFSSD9r258.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9953e753606e4aa521afe1e82b359a58ac514d1f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/Q-JAuaWn7tEVLUPPkF8J8227Fg3tAEhevtFSSD9r258.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ff18d28c781a8310d67c4a14b20661b024e71c79', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/Q-JAuaWn7tEVLUPPkF8J8227Fg3tAEhevtFSSD9r258.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c4c082ef14d123cef45f58f1031721d2366ff64a', 'width': 320, 'height': 320}], 'variants': {}, 'id': '3eXNmZroA3SZgT4gLOPU_65FRKHmR6FbW808CFu-8vs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jhc7od,True,,FrancescoSZ,,4,True,all_ads,False,[],False,,/r/pytorch/comments/jhc7od/a_new_deep_learning_computer_vision_library/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jhc7od/a_new_deep_learning_computer_vision_library/,7135,1603558200.0,0,,False,,,,,,,,
272,,pytorch,,t2_44mbtmjy,False,,0,False,Dynamic Sky Replacement and Harmonization in Videos,[],r/pytorch,False,6,,0,104.0,,False,t3_jh1uxv,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/1WO_qN-kUsQZlrZ1gp2PFSk7kXTtxhKVcCrRPltBedI.jpg,False,,[],{},link,,False,,1603538629.0,text,6,,,text,self.LatestInML,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/qEb34SXKMzgHdUpjardQMBC4I7nDCz81FW9m99Bmuws.jpg?auto=webp&amp;s=175881828ac4155675f6283ecb09f5d5855c05dd', 'width': 550, 'height': 412}, 'resolutions': [{'url': 'https://external-preview.redd.it/qEb34SXKMzgHdUpjardQMBC4I7nDCz81FW9m99Bmuws.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1862d1421fde20de23060073b1e098feff692f70', 'width': 108, 'height': 80}, {'url': 'https://external-preview.redd.it/qEb34SXKMzgHdUpjardQMBC4I7nDCz81FW9m99Bmuws.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=422fec0a0920518fb5cf6d6497a63a124421ac25', 'width': 216, 'height': 161}, {'url': 'https://external-preview.redd.it/qEb34SXKMzgHdUpjardQMBC4I7nDCz81FW9m99Bmuws.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c280cb24b3aeb425b7e99044cda5cf757bd3c1f', 'width': 320, 'height': 239}], 'variants': {}, 'id': 'uZVOFw10-mSgqLolCEIiz_PG4m12LL_kcRCnxFSox5c'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jh1uxv,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jh1uxv/dynamic_sky_replacement_and_harmonization_in/,all_ads,False,/r/LatestInML/comments/jh15yw/dynamic_sky_replacement_and_harmonization_in/,7135,1603509829.0,0,,False,/r/LatestInML/comments/jh15yw/dynamic_sky_replacement_and_harmonization_in/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/expert/API requests: [click here](https://www.catalyzex.com/paper/arxiv:2010.11800)\n\nhttps://reddit.com/link/jh15yw/video/upftd71ofyu51/player', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Dynamic Sky Replacement and Harmonization in Videos', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 104, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'upftd71ofyu51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/jh15yw/asset/upftd71ofyu51/DASHPlaylist.mpd?a=1618044168%2CNmMyZGNhMjM1NjI0ZTk5ZDZmZGM2YzBiOTY2Y2NmOTBmNjc3ODgyZmMzMDY1ZDI3NDBiMjYzZGVjZDM1ZDMwMw%3D%3D&amp;v=1&amp;f=sd', 'x': 426, 'y': 206, 'hlsUrl': 'https://v.redd.it/link/jh15yw/asset/upftd71ofyu51/HLSPlaylist.m3u8?a=1618044168%2CN2ViZDBmMTlhOGI1NmU0ZjZkMWFjZjAwZjExZWE1NTc4ZTZiYjA3MTI0ODdlMjUyMzYzZGZhNzI4YmU1NDY4MA%3D%3D&amp;v=1&amp;f=sd', 'id': 'upftd71ofyu51', 'isGif': False}}, 'name': 't3_jh15yw', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 11, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/1WO_qN-kUsQZlrZ1gp2PFSk7kXTtxhKVcCrRPltBedI.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1603535671.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/expert/API requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2010.11800""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/jh15yw/video/upftd71ofyu51/player""&gt;https://reddit.com/link/jh15yw/video/upftd71ofyu51/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/qEb34SXKMzgHdUpjardQMBC4I7nDCz81FW9m99Bmuws.jpg?auto=webp&amp;s=175881828ac4155675f6283ecb09f5d5855c05dd', 'width': 550, 'height': 412}, 'resolutions': [{'url': 'https://external-preview.redd.it/qEb34SXKMzgHdUpjardQMBC4I7nDCz81FW9m99Bmuws.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1862d1421fde20de23060073b1e098feff692f70', 'width': 108, 'height': 80}, {'url': 'https://external-preview.redd.it/qEb34SXKMzgHdUpjardQMBC4I7nDCz81FW9m99Bmuws.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=422fec0a0920518fb5cf6d6497a63a124421ac25', 'width': 216, 'height': 161}, {'url': 'https://external-preview.redd.it/qEb34SXKMzgHdUpjardQMBC4I7nDCz81FW9m99Bmuws.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c280cb24b3aeb425b7e99044cda5cf757bd3c1f', 'width': 320, 'height': 239}], 'variants': {}, 'id': 'uZVOFw10-mSgqLolCEIiz_PG4m12LL_kcRCnxFSox5c'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'jh15yw', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/jh15yw/dynamic_sky_replacement_and_harmonization_in/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/jh15yw/dynamic_sky_replacement_and_harmonization_in/', 'subreddit_subscribers': 6676, 'created_utc': 1603506871.0, 'num_crossposts': 11, 'media': None, 'is_video': False}]",t3_jh15yw,,,,,
273,,pytorch,"I have memory issues with the scatter\_add function.

The standard implementation computes, e.g. self\[i\]\[index\[i\]\[j\]\[k\]\[m\]\]\[k\]\[m\] += src\[i\]\[j\]\[k\]\[m\], where the output, will have shape I x Index\_Max x K x M. This is often a large tensor, causing memory issues. I then sum over certain dimensions., e.g. if choose dimension 0,3 it will then compute a tensor shape 1x Index\_Max x K x 1.

I am looking for an option, or workaround, to choose some of the output dimensions to have shape 1. In the example above, the scatter\_add function would compute

self\[0\]\[index\[i\]\[j\]\[k\]\[m\]\]\[k\]\[0\] += src\[i\]\[j\]\[k\]\[m\]

(or ignore dimensions 0,3 altogether), not computing the entire output tensor.

Thank you for the suggestions!",t2_1j1kx1ex,False,,0,False,scatter_add reduce output dimensions/shape,[],r/pytorch,False,6,,0,,,False,t3_jgs94p,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1603504814.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have memory issues with the scatter_add function.&lt;/p&gt;

&lt;p&gt;The standard implementation computes, e.g. self[i][index[i][j][k][m]][k][m] += src[i][j][k][m], where the output, will have shape I x Index_Max x K x M. This is often a large tensor, causing memory issues. I then sum over certain dimensions., e.g. if choose dimension 0,3 it will then compute a tensor shape 1x Index_Max x K x 1.&lt;/p&gt;

&lt;p&gt;I am looking for an option, or workaround, to choose some of the output dimensions to have shape 1. In the example above, the scatter_add function would compute&lt;/p&gt;

&lt;p&gt;self[0][index[i][j][k][m]][k][0] += src[i][j][k][m]&lt;/p&gt;

&lt;p&gt;(or ignore dimensions 0,3 altogether), not computing the entire output tensor.&lt;/p&gt;

&lt;p&gt;Thank you for the suggestions!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jgs94p,True,,sbb_ml,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jgs94p/scatter_add_reduce_output_dimensionsshape/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jgs94p/scatter_add_reduce_output_dimensionsshape/,7135,1603476014.0,0,,False,,,,,,,,
274,,pytorch,"I am ""translating"" a notebook made in Pytorch to one made in Keras.  And they use that app to pack the data from a tensor into the dataset that will be used for the network. But I can't find something that fulfills that function. I would greatly appreciate the help! 

Pytorch documentation says that torch.utils.data.TensorDataset (* tensors) does: 

""Dataset wrapping tensors. 
Each sample will be retrieved by indexing Tensor a along the first dimension.""

Thank you everybody!",t2_8g5s5gb,False,,0,False,Is there something like torch.utils.data.TensorDataset(*tensors) for TensorFlow/Keras?,[],r/pytorch,False,6,,0,,,False,t3_jgii9y,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1603467054.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am &amp;quot;translating&amp;quot; a notebook made in Pytorch to one made in Keras.  And they use that app to pack the data from a tensor into the dataset that will be used for the network. But I can&amp;#39;t find something that fulfills that function. I would greatly appreciate the help! &lt;/p&gt;

&lt;p&gt;Pytorch documentation says that torch.utils.data.TensorDataset (* tensors) does: &lt;/p&gt;

&lt;p&gt;&amp;quot;Dataset wrapping tensors. 
Each sample will be retrieved by indexing Tensor a along the first dimension.&amp;quot;&lt;/p&gt;

&lt;p&gt;Thank you everybody!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jgii9y,True,,aguillarcanus97,,1,True,all_ads,False,[],False,,/r/pytorch/comments/jgii9y/is_there_something_like/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jgii9y/is_there_something_like/,7135,1603438254.0,0,,False,,,,,,,,
275,,pytorch,"I'm training a model that applies softmax across an axis, in the following way:

 x = F.softmax(x.float(), dim=-1)

However, some rows in x are only -inf, leading to an output of NaN.  As such, I would like these rows to be outputted with something else, like a zero vector, for example.  So in affect, I want an if statement that only applies softmax when the values aren't all -inf.  How does one do this in pytorch?",t2_141zks,False,,0,False,How to apply a safe softmax,[],r/pytorch,False,6,,0,,,False,t3_jgfloz,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1603454106.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m training a model that applies softmax across an axis, in the following way:&lt;/p&gt;

&lt;p&gt;x = F.softmax(x.float(), dim=-1)&lt;/p&gt;

&lt;p&gt;However, some rows in x are only -inf, leading to an output of NaN.  As such, I would like these rows to be outputted with something else, like a zero vector, for example.  So in affect, I want an if statement that only applies softmax when the values aren&amp;#39;t all -inf.  How does one do this in pytorch?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jgfloz,True,,rajicon17,,3,True,all_ads,False,[],False,,/r/pytorch/comments/jgfloz/how_to_apply_a_safe_softmax/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jgfloz/how_to_apply_a_safe_softmax/,7135,1603425306.0,0,,False,,,,,,,,
276,,pytorch,"Say I wanted to run a hyperparameter search over a model. If I was running on a CPU theres options like GridSearchCV that allows me to parallelise this process over a single CPU, resulting in a speedup.

However, situations like this don't seem to exist on a GPU. I guess I could spawn several workers on a GPU and send a job to each of them. However, people seem sceptical of this approach (https://discuss.pytorch.org/t/split-single-gpu/18651/8) saying that it will not result in a performance increase.

Can someone explain to me why? I thought GPUs were ""good"" at parallelising ML problems. They can vectorise some problems and parallelise them, so why not my neural network? What am I missing here?",t2_dt0g2,False,,0,False,Can someone explain problems of running multiple jobs on 1 gpu?,[],r/pytorch,False,6,,0,,,False,t3_jfxi2n,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},self,,True,,1603393298.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Say I wanted to run a hyperparameter search over a model. If I was running on a CPU theres options like GridSearchCV that allows me to parallelise this process over a single CPU, resulting in a speedup.&lt;/p&gt;

&lt;p&gt;However, situations like this don&amp;#39;t seem to exist on a GPU. I guess I could spawn several workers on a GPU and send a job to each of them. However, people seem sceptical of this approach (&lt;a href=""https://discuss.pytorch.org/t/split-single-gpu/18651/8""&gt;https://discuss.pytorch.org/t/split-single-gpu/18651/8&lt;/a&gt;) saying that it will not result in a performance increase.&lt;/p&gt;

&lt;p&gt;Can someone explain to me why? I thought GPUs were &amp;quot;good&amp;quot; at parallelising ML problems. They can vectorise some problems and parallelise them, so why not my neural network? What am I missing here?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jfxi2n,True,,vaaalbara,,2,True,all_ads,False,[],False,,/r/pytorch/comments/jfxi2n/can_someone_explain_problems_of_running_multiple/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jfxi2n/can_someone_explain_problems_of_running_multiple/,7135,1603364498.0,0,,False,,,,,,,,
277,,pytorch,"Training a machine learning model is an iterative process. You first implement a baseline solution and measure its quality.  
Often, quite a few experiments need to be performed before a good solution is obtained. Once you obtain a good solution, it is important that you have all the information necessary to reproduce the same result. That’s why tracking the best hyperparameter set is so important.

https://preview.redd.it/w2y64aca1ou51.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=14345263d51671f89f187cce474e92c1690b3881

It is also cumbersome without the right tools!  
In today's post, we will learn how to log your experiments like a pro using these two tools.

1. **Tensorboard**
2. **Weights and Biases Developer Tools**

To learn the details please check out the post below.

[https://www.learnopencv.com/experiment-logging-with-tensorboard-and-wandb/](https://click.convertkit-mail.com/xmu2qe4ng2a6hk4exlbg/e0hph0unxznz08i8/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL2V4cGVyaW1lbnQtbG9nZ2luZy13aXRoLXRlbnNvcmJvYXJkLWFuZC13YW5kYi8=)",t2_cvc9f,False,,0,False,Experiment Logging with TensorBoard and wandb,[],r/pytorch,False,6,,0,93.0,,False,t3_jg1wdc,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/vfFpISAcbAKeN8hrvwQ3ZeASgRoP73ITlDGnDX80m2M.jpg,False,,[],{},self,,True,,1603409773.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Training a machine learning model is an iterative process. You first implement a baseline solution and measure its quality.&lt;br/&gt;
Often, quite a few experiments need to be performed before a good solution is obtained. Once you obtain a good solution, it is important that you have all the information necessary to reproduce the same result. That’s why tracking the best hyperparameter set is so important.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/w2y64aca1ou51.jpg?width=600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=14345263d51671f89f187cce474e92c1690b3881""&gt;https://preview.redd.it/w2y64aca1ou51.jpg?width=600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=14345263d51671f89f187cce474e92c1690b3881&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It is also cumbersome without the right tools!&lt;br/&gt;
In today&amp;#39;s post, we will learn how to log your experiments like a pro using these two tools.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Tensorboard&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weights and Biases Developer Tools&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To learn the details please check out the post below.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://click.convertkit-mail.com/xmu2qe4ng2a6hk4exlbg/e0hph0unxznz08i8/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL2V4cGVyaW1lbnQtbG9nZ2luZy13aXRoLXRlbnNvcmJvYXJkLWFuZC13YW5kYi8=""&gt;https://www.learnopencv.com/experiment-logging-with-tensorboard-and-wandb/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/r8SHfbqLrVVcL2xxFnGwwHo1hxWMzeBMndboKCYBVfk.jpg?auto=webp&amp;s=c62398c150b43df6859d653dace6bd0b98bfa02d', 'width': 600, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/r8SHfbqLrVVcL2xxFnGwwHo1hxWMzeBMndboKCYBVfk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e65d5c738653a88b4698abb8a52db2f9565c22c2', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/r8SHfbqLrVVcL2xxFnGwwHo1hxWMzeBMndboKCYBVfk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3901a82f7b9aa15c2e351e3d14c09a6d65ba05b2', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/r8SHfbqLrVVcL2xxFnGwwHo1hxWMzeBMndboKCYBVfk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b44b2176170b63980c7e8dd4d7b46c8d8221c19', 'width': 320, 'height': 213}], 'variants': {}, 'id': 'G9caOOyWees1nf2xGUJWNJukZ0YyLngzS1ED9VguORs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jg1wdc,True,,spmallick,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jg1wdc/experiment_logging_with_tensorboard_and_wandb/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jg1wdc/experiment_logging_with_tensorboard_and_wandb/,7135,1603380973.0,0,,False,,,,"{'w2y64aca1ou51': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 72, 'x': 108, 'u': 'https://preview.redd.it/w2y64aca1ou51.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bfd43fdf2dc3f9d1a72b276ea7e70fef6aa0d2cd'}, {'y': 144, 'x': 216, 'u': 'https://preview.redd.it/w2y64aca1ou51.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6937458a72041b6b4222cc578d6599ccfdf90df8'}, {'y': 213, 'x': 320, 'u': 'https://preview.redd.it/w2y64aca1ou51.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ffa332312d1077879e2eecacceeeb353ee19dff3'}], 's': {'y': 400, 'x': 600, 'u': 'https://preview.redd.it/w2y64aca1ou51.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=14345263d51671f89f187cce474e92c1690b3881'}, 'id': 'w2y64aca1ou51'}}",,,,
278,,pytorch,,t2_44mbtmjy,False,,0,False,Image-Driven Furniture Style for Interactive 3D Scene Modeling,[],r/pytorch,False,6,,0,45.0,,False,t3_jfrvag,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/sP1y86YxjgPSIsUUNH7UKr16CvMA1cus8m51GfnxDCU.jpg,False,,[],{},link,,False,,1603365332.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jfrvag,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jfrvag/imagedriven_furniture_style_for_interactive_3d/,all_ads,False,/r/LatestInML/comments/jfruc6/imagedriven_furniture_style_for_interactive_3d/,7135,1603336532.0,0,,False,/r/LatestInML/comments/jfruc6/imagedriven_furniture_style_for_interactive_3d/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and expert/code/API requests: [click here](https://www.catalyzex.com/paper/arxiv:2010.10557)\n\nhttps://preview.redd.it/5yxdv9dtcku51.jpg?width=1836&amp;format=pjpg&amp;auto=webp&amp;s=e8cd0de11a0335a35928bc540bcdd580f7c604ea', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Image-Driven Furniture Style for Interactive 3D Scene Modeling', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 45, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'5yxdv9dtcku51': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 35, 'x': 108, 'u': 'https://preview.redd.it/5yxdv9dtcku51.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=624335551b40e348450ce8999dbe39db0c39ecac'}, {'y': 70, 'x': 216, 'u': 'https://preview.redd.it/5yxdv9dtcku51.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=906dd5901914c2c634ca8e6513ac7d4556e93d0e'}, {'y': 104, 'x': 320, 'u': 'https://preview.redd.it/5yxdv9dtcku51.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d9d36f8617feafd854e62c8b4944845b1de33e3'}, {'y': 209, 'x': 640, 'u': 'https://preview.redd.it/5yxdv9dtcku51.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e5ca5278f1688815a9a17f87613b0d582544fe04'}, {'y': 314, 'x': 960, 'u': 'https://preview.redd.it/5yxdv9dtcku51.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8415f08f2b12fb2132092e66d827bfafc7feaaf8'}, {'y': 354, 'x': 1080, 'u': 'https://preview.redd.it/5yxdv9dtcku51.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5b5869d889b9b61ed493c6f524bd464bb9c59980'}], 's': {'y': 602, 'x': 1836, 'u': 'https://preview.redd.it/5yxdv9dtcku51.jpg?width=1836&amp;format=pjpg&amp;auto=webp&amp;s=e8cd0de11a0335a35928bc540bcdd580f7c604ea'}, 'id': '5yxdv9dtcku51'}}, 'name': 't3_jfruc6', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 14, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 14, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/sP1y86YxjgPSIsUUNH7UKr16CvMA1cus8m51GfnxDCU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1603365219.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and expert/code/API requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2010.10557""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/5yxdv9dtcku51.jpg?width=1836&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e8cd0de11a0335a35928bc540bcdd580f7c604ea""&gt;https://preview.redd.it/5yxdv9dtcku51.jpg?width=1836&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e8cd0de11a0335a35928bc540bcdd580f7c604ea&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'jfruc6', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/jfruc6/imagedriven_furniture_style_for_interactive_3d/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/jfruc6/imagedriven_furniture_style_for_interactive_3d/', 'subreddit_subscribers': 6676, 'created_utc': 1603336419.0, 'num_crossposts': 14, 'media': None, 'is_video': False}]",t3_jfruc6,,,,,
279,,pytorch,"Hey everyone,

I've been working on [ptjs](https://github.com/raghavmecheri/ptjs) as a part of a class I'm taking over the past few weeks -- I envision the end goal being something like what [tfjs](https://www.tensorflow.org/js) is to Tensorflow, and I just wanted to hear from people whether this is something that you would be interested in seeing? As of now, the library has the barebones/MVP functionality built out, so I thought this would be a good time to get some feedback and hear what people think

I've personally been looking for a good PyTorch wrapper for Node for the longest time, which is what led to me building this out so far. Trying to get a feel for what people think/if people have any ideas on how this should be taken forward now though :)",t2_5mz2l8mv,False,,0,False,Running torch models on Nodejs,[],r/pytorch,False,6,,0,,,False,t3_jeqe8a,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1603232351.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been working on &lt;a href=""https://github.com/raghavmecheri/ptjs""&gt;ptjs&lt;/a&gt; as a part of a class I&amp;#39;m taking over the past few weeks -- I envision the end goal being something like what &lt;a href=""https://www.tensorflow.org/js""&gt;tfjs&lt;/a&gt; is to Tensorflow, and I just wanted to hear from people whether this is something that you would be interested in seeing? As of now, the library has the barebones/MVP functionality built out, so I thought this would be a good time to get some feedback and hear what people think&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve personally been looking for a good PyTorch wrapper for Node for the longest time, which is what led to me building this out so far. Trying to get a feel for what people think/if people have any ideas on how this should be taken forward now though :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/MyU7xX0I2vihrWTR6vq0O9ajHvZToJ-tOj7eaJuGiQw.jpg?auto=webp&amp;s=ffd336667a7c12d6bb869d0ac1341f685fc9aec1', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/MyU7xX0I2vihrWTR6vq0O9ajHvZToJ-tOj7eaJuGiQw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=faaaf81ae56d5a56e681a81262bad82e5bbdc47e', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/MyU7xX0I2vihrWTR6vq0O9ajHvZToJ-tOj7eaJuGiQw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a7ca423e4f31b94618b7ad54f69717fc9db38b4', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/MyU7xX0I2vihrWTR6vq0O9ajHvZToJ-tOj7eaJuGiQw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f86c5928bbb6af5ec8a86ebc6a59d60e9eb7e79', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'KiPtgsQP2ikb1Cw0ODtrXJzVXLnxlgTmQEqwgh1JcVU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jeqe8a,True,,mcweggie,,3,True,all_ads,False,[],False,,/r/pytorch/comments/jeqe8a/running_torch_models_on_nodejs/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jeqe8a/running_torch_models_on_nodejs/,7135,1603203551.0,0,,False,,,,,,,,
280,,pytorch,"I am trying to make LSTM network and I am having some problems. Let's say I have 600 sequences each has 90 elements. What should be the size of LSTM input? Documentation imply shape \[600,1,90\] but my LSTM doesn't seem to work and I am trying to find the issue.",t2_4cltnh8,False,,0,False,Size of LSTM input,[],r/pytorch,False,6,,0,,,False,t3_je621p,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1603157090.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to make LSTM network and I am having some problems. Let&amp;#39;s say I have 600 sequences each has 90 elements. What should be the size of LSTM input? Documentation imply shape [600,1,90] but my LSTM doesn&amp;#39;t seem to work and I am trying to find the issue.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,je621p,True,,IDontHaveNicknameToo,,7,True,all_ads,False,[],False,,/r/pytorch/comments/je621p/size_of_lstm_input/,all_ads,False,https://www.reddit.com/r/pytorch/comments/je621p/size_of_lstm_input/,7135,1603128290.0,0,,False,,,,,,,,
281,,pytorch,"Hi everyone,
I'm confused about a matter regarding custom datasets and indexing. I create a custom dataset class inheriting from Dataset class. I recently noticed that I implemented the following method for getting samples:

def get(self, idx):
   ...

while people usually implement the following, as far as I've seen from the online tutorials:

def _ _ getitem _ _(self, idx):
   ...

Is there any difference between them? I tested both functions and they seem to have the same functionality. But I couldn't find anything on web to justify it.

Thank you very much.",t2_8i536u9h,False,,0,False,"get(self, idx) vs __getitem__(self, idx) for custom datasets",[],r/pytorch,False,6,,0,,,False,t3_jdzo1y,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,1603106916.0,,[],{},,,True,,1603134593.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,
I&amp;#39;m confused about a matter regarding custom datasets and indexing. I create a custom dataset class inheriting from Dataset class. I recently noticed that I implemented the following method for getting samples:&lt;/p&gt;

&lt;p&gt;def get(self, idx):
   ...&lt;/p&gt;

&lt;p&gt;while people usually implement the following, as far as I&amp;#39;ve seen from the online tutorials:&lt;/p&gt;

&lt;p&gt;def _ _ getitem _ _(self, idx):
   ...&lt;/p&gt;

&lt;p&gt;Is there any difference between them? I tested both functions and they seem to have the same functionality. But I couldn&amp;#39;t find anything on web to justify it.&lt;/p&gt;

&lt;p&gt;Thank you very much.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jdzo1y,True,,leikem,,3,True,all_ads,False,[],False,,/r/pytorch/comments/jdzo1y/getself_idx_vs_getitem_self_idx_for_custom/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jdzo1y/getself_idx_vs_getitem_self_idx_for_custom/,7135,1603105793.0,0,,False,,,,,,,,
282,,pytorch,"I am using  embedding- lstm -  and confused in

  lstm\_out, (hidden, cell) = nn.LSTM()(..)  

&amp;#x200B;

what should i take in those of three in final layer, I am not including fully connected layer here Anybody help!!",t2_6hhee0ag,False,,0,False,Which state should i pass in siamese network?,[],r/pytorch,False,6,,0,,,False,t3_jdwxxu,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1603119618.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am using  embedding- lstm -  and confused in&lt;/p&gt;

&lt;p&gt;lstm_out, (hidden, cell) = nn.LSTM()(..)  &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;what should i take in those of three in final layer, I am not including fully connected layer here Anybody help!!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jdwxxu,True,,Psycho-logical-being,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jdwxxu/which_state_should_i_pass_in_siamese_network/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jdwxxu/which_state_should_i_pass_in_siamese_network/,7135,1603090818.0,0,,False,,,,,,,,
283,,pytorch,"I am training LSTM network and wondering whether I should pass any hidden cell state along with input. Documentation doesn't say much about it, even in example they pass it in one case but not in the other. So, do I have to pass hidden cell state with every input or not? If yes then what does it change in training process?",t2_4cltnh8,False,,0,False,Should I pass hidden state to LSTM with every input?,[],r/pytorch,False,6,,0,,,False,t3_jdpk27,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1603088148.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am training LSTM network and wondering whether I should pass any hidden cell state along with input. Documentation doesn&amp;#39;t say much about it, even in example they pass it in one case but not in the other. So, do I have to pass hidden cell state with every input or not? If yes then what does it change in training process?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jdpk27,True,,IDontHaveNicknameToo,,3,True,all_ads,False,[],False,,/r/pytorch/comments/jdpk27/should_i_pass_hidden_state_to_lstm_with_every/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jdpk27/should_i_pass_hidden_state_to_lstm_with_every/,7135,1603059348.0,0,,False,,,,,,,,
284,,pytorch,"https://discuss.pytorch.org/t/how-to-implement-pytorch-equivalent-of-keras-kernel-weight-regulariser/99773?u=paganpasta


Posting the forum link to the problem as it contains a description of the problem and keras code.

Any help is appreciated.",t2_4304nx3z,False,,0,False,Need help in implementing a pytorch equivalent.,[],r/pytorch,False,6,,0,,,False,t3_jdfx1m,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1603054764.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://discuss.pytorch.org/t/how-to-implement-pytorch-equivalent-of-keras-kernel-weight-regulariser/99773?u=paganpasta""&gt;https://discuss.pytorch.org/t/how-to-implement-pytorch-equivalent-of-keras-kernel-weight-regulariser/99773?u=paganpasta&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Posting the forum link to the problem as it contains a description of the problem and keras code.&lt;/p&gt;

&lt;p&gt;Any help is appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jdfx1m,True,,PaganPasta,,3,True,all_ads,False,[],False,,/r/pytorch/comments/jdfx1m/need_help_in_implementing_a_pytorch_equivalent/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jdfx1m/need_help_in_implementing_a_pytorch_equivalent/,7135,1603025964.0,0,,False,,,,,,,,
285,,pytorch,"for reference i used this code right here : [https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more\_advanced/Seq2Seq/seq2seq.py](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/Seq2Seq/seq2seq.py)

it has been a WEEK since i am trying to do a seq2seq model in Pytorch. MOST the example uses NLP but i need TIME SERIES FORECASTING. i know the teory and enough practice but i still can't make the model works.  What should have been an easy task done in a day or two has turned into a nightmare week full of frustration where now i am at the point where i just wanna quit everything and go plant potato.

&amp;#x200B;

Edit. so i tried to remove the embedding layer and use Linear layer instead. and obviously use MSE loss instead of CrossEntropy.

the problem is in the decoder when you do target\[0\] wich should be the first word (&lt;sos&gt;), but in my case it should be the fist number of every batch . Ex suppose we have batch two so the  input is: (\[1,2,3,4\],\[9,10,11,12\]) and target is (\[5,6,7,8\],\[13,14,15,16\]). if i do target\[0\] i get\[5\] wich has shape 1x1 instead target\[0\] should be (\[5,13\]) wich has shape 1xbatch\_size i need this shape for the LSTM in the decoder. obviously the same problem is for the next target\[1\] and the other number in the sequence.

another problem is the LSTM that require 3 dimensions : in the tutorial is used embedding  and the input form shape (batch\_size,seq\_length) goes to (batch\_size,seq\_length,embedding\_features)  but i just do x.unsqueeze(0) is it the same thing or is gonna be a problem ?

PS. my discord is  Bisd#9079",t2_8iqyddru,False,,0,False,HELP with encoder_decoder \ seq2seq model ?,[],r/pytorch,False,6,,0,,,False,t3_jd14hu,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1602971242.0,,[],{},self,,True,,1602990531.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;for reference i used this code right here : &lt;a href=""https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/Seq2Seq/seq2seq.py""&gt;https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/Seq2Seq/seq2seq.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;it has been a WEEK since i am trying to do a seq2seq model in Pytorch. MOST the example uses NLP but i need TIME SERIES FORECASTING. i know the teory and enough practice but i still can&amp;#39;t make the model works.  What should have been an easy task done in a day or two has turned into a nightmare week full of frustration where now i am at the point where i just wanna quit everything and go plant potato.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Edit. so i tried to remove the embedding layer and use Linear layer instead. and obviously use MSE loss instead of CrossEntropy.&lt;/p&gt;

&lt;p&gt;the problem is in the decoder when you do target[0] wich should be the first word (&amp;lt;sos&amp;gt;), but in my case it should be the fist number of every batch . Ex suppose we have batch two so the  input is: ([1,2,3,4],[9,10,11,12]) and target is ([5,6,7,8],[13,14,15,16]). if i do target[0] i get[5] wich has shape 1x1 instead target[0] should be ([5,13]) wich has shape 1xbatch_size i need this shape for the LSTM in the decoder. obviously the same problem is for the next target[1] and the other number in the sequence.&lt;/p&gt;

&lt;p&gt;another problem is the LSTM that require 3 dimensions : in the tutorial is used embedding  and the input form shape (batch_size,seq_length) goes to (batch_size,seq_length,embedding_features)  but i just do x.unsqueeze(0) is it the same thing or is gonna be a problem ?&lt;/p&gt;

&lt;p&gt;PS. my discord is  Bisd#9079&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/mDcqi3On6VUel5csbahhTM8CZz1DGLOdCUjjHWUdziU.jpg?auto=webp&amp;s=59c99788a44248c0ee0b47f54c50baefbdb10a27', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/mDcqi3On6VUel5csbahhTM8CZz1DGLOdCUjjHWUdziU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d240057ac4ca2b717df9127d9171447069ab0c1', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/mDcqi3On6VUel5csbahhTM8CZz1DGLOdCUjjHWUdziU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1aab41ac116446be643a0cc8569fdbcd282baefd', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/mDcqi3On6VUel5csbahhTM8CZz1DGLOdCUjjHWUdziU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0933398701984f2141d074432ffa5683769fa114', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'us_buo_D8JWUM-FxyqQ2uZ2YwL4_3yfDwltFRvLbQxQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jd14hu,True,,IDKIDC77,,2,True,all_ads,False,[],False,,/r/pytorch/comments/jd14hu/help_with_encoder_decoder_seq2seq_model/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jd14hu/help_with_encoder_decoder_seq2seq_model/,7135,1602961731.0,0,,False,,,,,,,,
286,,pytorch,"I trained my model and after some epochs loss was very low. I tried to to test it and it spits out the same number again and again, if I add `optimizer.step()` to the testing code it works fine. Where's the problem?

&amp;#x200B;

Sorry for such basic question but I really can't think of any solution",t2_4cltnh8,False,,0,False,Neural net spits out same number in testing,[],r/pytorch,False,6,,0,,,False,t3_jczcyj,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1602984688.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I trained my model and after some epochs loss was very low. I tried to to test it and it spits out the same number again and again, if I add &lt;code&gt;optimizer.step()&lt;/code&gt; to the testing code it works fine. Where&amp;#39;s the problem?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Sorry for such basic question but I really can&amp;#39;t think of any solution&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jczcyj,True,,IDontHaveNicknameToo,,6,True,all_ads,False,[],False,,/r/pytorch/comments/jczcyj/neural_net_spits_out_same_number_in_testing/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jczcyj/neural_net_spits_out_same_number_in_testing/,7135,1602955888.0,0,,False,,,,,,,,
287,,pytorch,,t2_44mbtmjy,False,,0,False,Groundbreaking research from UWashington researchers: Remove any background noise/voice when in a video call! (See video),[],r/pytorch,False,6,,0,140.0,,False,t3_jcnndw,False,dark,0.69,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/pgWrcw35V8jNDmIEwWSr_XZlYtVgexbffnGo_jk2eSU.jpg,False,,[],{},link,,False,,1602932112.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jcnndw,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jcnndw/groundbreaking_research_from_uwashington/,all_ads,False,/r/LatestInML/comments/jcn81a/groundbreaking_research_from_uwashington/,7135,1602903312.0,0,,False,/r/LatestInML/comments/jcn81a/groundbreaking_research_from_uwashington/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'Groundbreaking research from UWashington researchers: Remove any background noise/voice when in a video call! (See video)\n\nFor project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2010.06007)\n\n&amp;#x200B;\n\nhttps://reddit.com/link/jcn81a/video/8r72gm0dfkt51/player\n\nExperiments demonstrate state-of-the-art performance for both source separation and source localization, particularly in high levels of background noise.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Groundbreaking research from UWashington researchers: Remove any background noise/voice when in a video call! (See video)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'8r72gm0dfkt51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/jcn81a/asset/8r72gm0dfkt51/DASHPlaylist.mpd?a=1618044171%2CNDRhNjNhMjRjYzc0YTRjYzcyYWZhMjlmYjU4MTU4NDRhMzRkMTkyOGM3NTVmMWUyNjg4NGIxNWNiMjUzOTgyMg%3D%3D&amp;v=1&amp;f=sd', 'x': 640, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/jcn81a/asset/8r72gm0dfkt51/HLSPlaylist.m3u8?a=1618044171%2CODNmNzA0MjQ2OTQ0MWY1OTJhYTNmODY1MWYzNmM2ZmViNmU0OTdkNDZmODU4YjZhNzZkOTMyMjM1NThiYWYxMg%3D%3D&amp;v=1&amp;f=sd', 'id': '8r72gm0dfkt51', 'isGif': False}}, 'name': 't3_jcn81a', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 55, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 55, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/pgWrcw35V8jNDmIEwWSr_XZlYtVgexbffnGo_jk2eSU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1602930223.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Groundbreaking research from UWashington researchers: Remove any background noise/voice when in a video call! (See video)&lt;/p&gt;\n\n&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2010.06007""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/jcn81a/video/8r72gm0dfkt51/player""&gt;https://reddit.com/link/jcn81a/video/8r72gm0dfkt51/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Experiments demonstrate state-of-the-art performance for both source separation and source localization, particularly in high levels of background noise.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'jcn81a', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/jcn81a/groundbreaking_research_from_uwashington/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/jcn81a/groundbreaking_research_from_uwashington/', 'subreddit_subscribers': 6676, 'created_utc': 1602901423.0, 'num_crossposts': 13, 'media': None, 'is_video': False}]",t3_jcn81a,,,,,
288,,pytorch,"Hi everyone,
Since I'm having memory issues for training on larger batches, I thought of a trick but I'm not sure it would work.

So, normally we do the following:

For each batch:

    ¬Calculate loss 


    ¬Back prop


    ¬Optimizer step


    ¬Optimizer zero grad


Instead, I want to do the following, for instance a batch size of 1024:

Set batch size equal to 1. 


For each batch:


    ¬Calculate loss 


    ¬Back prop


    If (batch_idx+1) % 1024 == 0:


         ¬Optimizer step


         ¬Optimizer zero grad

Would that be equivalent to training on batches of 1024 samples, since I accumulate the gradients for 1024 samples and update once for every 1024 samples?

Thank you.",t2_8i536u9h,False,,0,False,A trick for training on large batches,[],r/pytorch,False,6,,0,,,False,t3_jc97jl,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1602881417.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,
Since I&amp;#39;m having memory issues for training on larger batches, I thought of a trick but I&amp;#39;m not sure it would work.&lt;/p&gt;

&lt;p&gt;So, normally we do the following:&lt;/p&gt;

&lt;p&gt;For each batch:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;¬Calculate loss 


¬Back prop


¬Optimizer step


¬Optimizer zero grad
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instead, I want to do the following, for instance a batch size of 1024:&lt;/p&gt;

&lt;p&gt;Set batch size equal to 1. &lt;/p&gt;

&lt;p&gt;For each batch:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;¬Calculate loss 


¬Back prop


If (batch_idx+1) % 1024 == 0:


     ¬Optimizer step


     ¬Optimizer zero grad
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Would that be equivalent to training on batches of 1024 samples, since I accumulate the gradients for 1024 samples and update once for every 1024 samples?&lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jc97jl,True,,leikem,,5,True,all_ads,False,[],False,,/r/pytorch/comments/jc97jl/a_trick_for_training_on_large_batches/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jc97jl/a_trick_for_training_on_large_batches/,7135,1602852617.0,0,,False,,,,,,,,
289,,pytorch,"I dont have nvidia gpu, what is best cloud environment where can i write scripts and train models.?",t2_7oa8c7r2,False,,0,False,Cloud traning,[],r/pytorch,False,6,,0,,,False,t3_jcbxtq,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1602891105.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I dont have nvidia gpu, what is best cloud environment where can i write scripts and train models.?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jcbxtq,True,,tabpavle,,4,True,all_ads,False,[],False,,/r/pytorch/comments/jcbxtq/cloud_traning/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jcbxtq/cloud_traning/,7135,1602862305.0,0,,False,,,,,,,,
290,,pytorch,,t2_60s8ciet,False,,0,False,Tutorial on using PyTorch to emulate guitar amps and effects in a plugin.,[],r/pytorch,False,6,,0,105.0,,False,t3_jaz1nu,False,dark,0.94,,public,27,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/EaIWi4TMYHA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Tutorial - Using Machine Learning for Emulation of Guitar Amps and Pedals', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/EaIWi4TMYHA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Keith Bloemer', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/EaIWi4TMYHA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCQ2QJfNG0G6y9kH4D6G-GiA'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/EaIWi4TMYHA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/jaz1nu', 'height': 338}",,False,27,,False,https://a.thumbs.redditmedia.com/GTKHv8LNcpsdwAO1IbbpYbG8A3atCGyEF14mOFdXt54.jpg,False,,[],{},rich:video,,False,,1602704383.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/O9NyVYcUapSbJIlIh_Mt0uz6HInoJv-LqEO1COHPPEU.jpg?auto=webp&amp;s=301d2d546ba349f040f43c8e4a6338a3fd0605ea', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/O9NyVYcUapSbJIlIh_Mt0uz6HInoJv-LqEO1COHPPEU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=20fe454280a260bb3f1c200e7201668c8f42744f', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/O9NyVYcUapSbJIlIh_Mt0uz6HInoJv-LqEO1COHPPEU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=753ae3b49e07c47532ded19358a3596fe723c069', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/O9NyVYcUapSbJIlIh_Mt0uz6HInoJv-LqEO1COHPPEU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c51fc7e0fe519e10b21508e78143784f8b43f660', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'UAbF4fTZbK9g2LNhrk-uXNPuCnTDPtD6s2DFB9KTp0k'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jaz1nu,True,,keyth72,,3,False,all_ads,False,[],False,,/r/pytorch/comments/jaz1nu/tutorial_on_using_pytorch_to_emulate_guitar_amps/,all_ads,False,https://youtu.be/EaIWi4TMYHA,7135,1602675583.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Tutorial - Using Machine Learning for Emulation of Guitar Amps and Pedals', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/EaIWi4TMYHA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Keith Bloemer', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/EaIWi4TMYHA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCQ2QJfNG0G6y9kH4D6G-GiA'}}",False,https://youtu.be/EaIWi4TMYHA,,,,,,,
291,,pytorch,"I have seen some charts of the popularity of pytorch going up(maybe surpassing tensorflow in the future)  should I learn pytorch or tensorflow if I want to get a data scientist job / do freelancing ?
Or does it not matter ?",t2_128ob4,False,,0,False,Pytorch vs tensorflow for job opurtunities/freelancing,[],r/pytorch,False,6,,0,,,False,t3_jarkh8,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1602669555.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have seen some charts of the popularity of pytorch going up(maybe surpassing tensorflow in the future)  should I learn pytorch or tensorflow if I want to get a data scientist job / do freelancing ?
Or does it not matter ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jarkh8,True,,darvidas,,8,True,all_ads,False,[],False,,/r/pytorch/comments/jarkh8/pytorch_vs_tensorflow_for_job/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jarkh8/pytorch_vs_tensorflow_for_job/,7135,1602640755.0,0,,False,,,,,,,,
292,,pytorch,"I've been  die hard fan of Pytorch since i encounter it. I am linux user, although I am also not fan of microsoft or facebook, but anyway, I heard microsoft joined Pytorch?  what does it means? Do they track us or do survellience on us, and put our privacy at risk?

&amp;#x200B;

We would be happy if Pytorch would fan of FSF community. 

Thanks",t2_6hhee0ag,False,,0,False,"Microsoft Joined pytorch, What about our privacy? GNU when?",[],r/pytorch,False,6,,0,,,False,t3_jax1ux,False,dark,0.62,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1602694312.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been  die hard fan of Pytorch since i encounter it. I am linux user, although I am also not fan of microsoft or facebook, but anyway, I heard microsoft joined Pytorch?  what does it means? Do they track us or do survellience on us, and put our privacy at risk?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;We would be happy if Pytorch would fan of FSF community. &lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jax1ux,True,,Psycho-logical-being,,3,True,all_ads,False,[],False,,/r/pytorch/comments/jax1ux/microsoft_joined_pytorch_what_about_our_privacy/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jax1ux/microsoft_joined_pytorch_what_about_our_privacy/,7135,1602665512.0,0,,False,,,,,,,,
293,,pytorch,"

[View Poll](https://www.reddit.com/poll/jabsey)",t2_74seeqo6,False,,0,False,How often do you use pytorch lightning?,[],r/pytorch,False,6,,0,,,False,t3_jabsey,False,dark,1.0,,public,9,0,{},,,False,[],,False,False,,{},,False,9,,False,self,False,,[],{},,,True,,1602618561.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.reddit.com/poll/jabsey""&gt;View Poll&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jabsey,True,,spenceowen,,8,True,all_ads,False,[],False,,/r/pytorch/comments/jabsey/how_often_do_you_use_pytorch_lightning/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jabsey/how_often_do_you_use_pytorch_lightning/,7135,1602589761.0,0,,False,,,,,,,"{'user_won_amount': None, 'tournament_id': None, 'voting_end_timestamp': 1602848961752, 'options': [{'text': 'I always use pytorch lightning.', 'vote_count': 9, 'id': '3468719'}, {'text': 'I sometimes use pytorch lightning.', 'vote_count': 25, 'id': '3468720'}, {'text': ""I don't use pytorch lightning at all."", 'vote_count': 57, 'id': '3468721'}, {'text': 'Never heard of it', 'vote_count': 66, 'id': '3468722'}], 'user_selection': None, 'is_prediction': False, 'resolved_option_id': None, 'total_vote_count': 157, 'total_stake_amount': None}",
294,,pytorch,"What is the most beautifully organized pytorch codebase you've seen. 

I came across codebase the other day and it inspired this post:  
[https://github.com/devzhk/Implicit-Competitive-Regularization](https://github.com/devzhk/Implicit-Competitive-Regularization)

In building my own tooling, I'd love to see some good examples. My impression these days is that machine learning codebase structure is still a bit of a Wild West.   
Thanks! - Danny",t2_79lnqx00,False,,0,False,**--))Beautiful Code ((--**,[],r/pytorch,False,6,,0,,,False,t3_jaiwrd,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1602641333.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What is the most beautifully organized pytorch codebase you&amp;#39;ve seen. &lt;/p&gt;

&lt;p&gt;I came across codebase the other day and it inspired this post:&lt;br/&gt;
&lt;a href=""https://github.com/devzhk/Implicit-Competitive-Regularization""&gt;https://github.com/devzhk/Implicit-Competitive-Regularization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In building my own tooling, I&amp;#39;d love to see some good examples. My impression these days is that machine learning codebase structure is still a bit of a Wild West.&lt;br/&gt;
Thanks! - Danny&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xkUBfyLvAfVypUHwo54QlWBkmf817oy4OgbpmJ5hZAI.jpg?auto=webp&amp;s=67ee2e4312753cbe68cef4f13d470c1de64ddb61', 'width': 300, 'height': 300}, 'resolutions': [{'url': 'https://external-preview.redd.it/xkUBfyLvAfVypUHwo54QlWBkmf817oy4OgbpmJ5hZAI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=32f595a660f77fc3e1155b6322eccd3a5d8bb4b2', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/xkUBfyLvAfVypUHwo54QlWBkmf817oy4OgbpmJ5hZAI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8a4b95994ec67e5c6b62f32499537f5cee1553e4', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'OroYm2dMjQvSFb4jzVPIJx7bw6XOiq9tTXwaE9LvBCc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,jaiwrd,True,,Advanced_Cry_9953,,0,True,all_ads,False,[],False,,/r/pytorch/comments/jaiwrd/beautiful_code/,all_ads,False,https://www.reddit.com/r/pytorch/comments/jaiwrd/beautiful_code/,7135,1602612533.0,0,,False,,,,,,,,
295,,pytorch,"I'm trying to make a DCGAN which can make pictures, so I'm passing pictures to the generator.   


These pictures are taken from jpg images and then transformed through the following:  
`my_transforms = transforms.Compose([`

`transforms.Resize(image_size),`

`transforms.ToTensor(),`

`transforms.Normalize((0.5,),(0.5,)),`

`])`  
This is the start of my training loop:

`for epoch in range(num_epochs):`

`for batch_idx, data in enumerate(dataloader):`

`data =` `data.to``(device)`

`batch_size = data.shape[0]`

`netD.zero_grad()`

`label = (torch.ones(batch_size)*0.9).to(device)`

`output = netD(data).reshape(-1)`

`lossD_real = criterion(output, label)`

`D_x = output.mean().item()`  


The calculation of lossD\_real is where it breaks. I've set the criterion to nn.NLLLoss() after I tried with nn.BCELoss() and that gave me a ""target and input must have the same number of elements"" error.   


Any help would be much appreciated, and I'll gladly provide more info if asked.",t2_t5qwu,False,,0,False,"Problem with nn.NLLLoss() ""Expected 2 or more dimensions (got 1)""",[],r/pytorch,False,6,,0,,,False,t3_j9yilk,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1602562245.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to make a DCGAN which can make pictures, so I&amp;#39;m passing pictures to the generator.   &lt;/p&gt;

&lt;p&gt;These pictures are taken from jpg images and then transformed through the following:&lt;br/&gt;
&lt;code&gt;my_transforms = transforms.Compose([&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;transforms.Resize(image_size),&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;transforms.ToTensor(),&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;transforms.Normalize((0.5,),(0.5,)),&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;])&lt;/code&gt;&lt;br/&gt;
This is the start of my training loop:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for epoch in range(num_epochs):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for batch_idx, data in enumerate(dataloader):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;data =&lt;/code&gt; &lt;code&gt;data.to&lt;/code&gt;&lt;code&gt;(device)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;batch_size = data.shape[0]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;netD.zero_grad()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;label = (torch.ones(batch_size)*0.9).to(device)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;output = netD(data).reshape(-1)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;lossD_real = criterion(output, label)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;D_x = output.mean().item()&lt;/code&gt;  &lt;/p&gt;

&lt;p&gt;The calculation of lossD_real is where it breaks. I&amp;#39;ve set the criterion to nn.NLLLoss() after I tried with nn.BCELoss() and that gave me a &amp;quot;target and input must have the same number of elements&amp;quot; error.   &lt;/p&gt;

&lt;p&gt;Any help would be much appreciated, and I&amp;#39;ll gladly provide more info if asked.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j9yilk,True,,OndrikB,,5,True,all_ads,False,[],False,,/r/pytorch/comments/j9yilk/problem_with_nnnllloss_expected_2_or_more/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j9yilk/problem_with_nnnllloss_expected_2_or_more/,7135,1602533445.0,0,,False,,,,,,,,
296,,pytorch,"hello,

after spending days on this issue i'm trying again starting from scratch with a new environment. i'm going to put all details since i'm not formally educated with computers so maybe i make some dumb mistakes that i'm not aware of.

1. created a conda environment myenv with python 3.8, pytorch 1.6 and tensorboard 2.3.0
2. open pycharm, create new project, create new python module, named [test.py](https://test.py)   .
3. copy paste the code from tutorial in [https://pytorch.org/docs/stable/tensorboard.html](https://pytorch.org/docs/stable/tensorboard.html) 
4. run the code, check that folder was created and event files were generated. (folder path is C:\\Users\\david\\PycharmProjects\\TBtutorial\\runs.    inside that folder is another folder named after the moment it was generated and in it is one event file of type 0 file)
5. from windows toolbar enter anaconda prompt, activate myenv, change directory to /PycharmProjects/TBtutorial/runs.   
6. type in: tensorboard --logdir==runs
7. copy paste link printed below on browser, get tensorboard blank page.
8. tried the same directly from pycharm terminal (typing tensorboard --logdir==runs )

i always get the same result, the tensorboard web page without any image at the image thumbnail. i tried using python 3.6, tensorboard 1.15, pytorch 1.4 - all with one another. also tried to pass the writer other commands other than add\_image, all to no use.

i'm clueless, will be grateful for any advice!",t2_4qcuonpz,False,,0,False,tensorboard not communicating with pytorch,[],r/pytorch,False,6,,0,,,False,t3_j9vt35,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1602554062.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;hello,&lt;/p&gt;

&lt;p&gt;after spending days on this issue i&amp;#39;m trying again starting from scratch with a new environment. i&amp;#39;m going to put all details since i&amp;#39;m not formally educated with computers so maybe i make some dumb mistakes that i&amp;#39;m not aware of.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;created a conda environment myenv with python 3.8, pytorch 1.6 and tensorboard 2.3.0&lt;/li&gt;
&lt;li&gt;open pycharm, create new project, create new python module, named &lt;a href=""https://test.py""&gt;test.py&lt;/a&gt;   .&lt;/li&gt;
&lt;li&gt;copy paste the code from tutorial in &lt;a href=""https://pytorch.org/docs/stable/tensorboard.html""&gt;https://pytorch.org/docs/stable/tensorboard.html&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;run the code, check that folder was created and event files were generated. (folder path is C:\Users\david\PycharmProjects\TBtutorial\runs.    inside that folder is another folder named after the moment it was generated and in it is one event file of type 0 file)&lt;/li&gt;
&lt;li&gt;from windows toolbar enter anaconda prompt, activate myenv, change directory to /PycharmProjects/TBtutorial/runs.&lt;br/&gt;&lt;/li&gt;
&lt;li&gt;type in: tensorboard --logdir==runs&lt;/li&gt;
&lt;li&gt;copy paste link printed below on browser, get tensorboard blank page.&lt;/li&gt;
&lt;li&gt;tried the same directly from pycharm terminal (typing tensorboard --logdir==runs )&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;i always get the same result, the tensorboard web page without any image at the image thumbnail. i tried using python 3.6, tensorboard 1.15, pytorch 1.4 - all with one another. also tried to pass the writer other commands other than add_image, all to no use.&lt;/p&gt;

&lt;p&gt;i&amp;#39;m clueless, will be grateful for any advice!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j9vt35,True,,CantaloupeLeading646,,1,True,all_ads,False,[],False,,/r/pytorch/comments/j9vt35/tensorboard_not_communicating_with_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j9vt35/tensorboard_not_communicating_with_pytorch/,7135,1602525262.0,0,,False,,,,,,,,
297,,pytorch,"Hi, I am new to PyTorch so apologies for the basic question.

I have trained a model from an architecture used in a paper, now I am deploying my model for inference in a web service.

What I am struggling to find information on, is wether I need or can ""fix"" my model to ensure the tensors are no longer trained when I start using it for inference.

Second, is there any other process which can be done on the model to ready it for inference, such as delete any methods / params used during training  - or would this all be model / architecture specific?

Thanks!",t2_2innaoj,False,,0,False,Fixing model for use in production,[],r/pytorch,False,6,,0,,,False,t3_j9n4zq,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1602520307.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am new to PyTorch so apologies for the basic question.&lt;/p&gt;

&lt;p&gt;I have trained a model from an architecture used in a paper, now I am deploying my model for inference in a web service.&lt;/p&gt;

&lt;p&gt;What I am struggling to find information on, is wether I need or can &amp;quot;fix&amp;quot; my model to ensure the tensors are no longer trained when I start using it for inference.&lt;/p&gt;

&lt;p&gt;Second, is there any other process which can be done on the model to ready it for inference, such as delete any methods / params used during training  - or would this all be model / architecture specific?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j9n4zq,True,,ydennisy,,8,True,all_ads,False,[],False,,/r/pytorch/comments/j9n4zq/fixing_model_for_use_in_production/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j9n4zq/fixing_model_for_use_in_production/,7135,1602491507.0,0,,False,,,,,,,,
298,,pytorch,"hello,

I have about 500 networkx graphs and I wanted to perform graph classification on them into 2 classes.

This is how a node looks like.

`{0: {'id': tensor(144), 'residue_name': tensor(8), 'h': tensor([0.0000, 0.0000, 0.0000, 0.0000, 6.0700, 0.1300, 0.1500]), 'coords': tensor([-21.1550, 23.3610, 1.9100]), 'ss': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]), 'asa': tensor([10752.]), 'rsa': tensor([128.])}`

This is how an edge looks like.

`(0, 2, {'id': 322, 'rel_type': tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64), 'norm': tensor(1.)})`

If i try to use stellar graphs, it throws an error that it only accepts numeric features and i am not able to use DGL as none of their examples show to load our own dataset. My graphs dot have features as a graph attribute but as node and edge attributes.

How do I use my data with these libraries or are there any other libraries I can use?

&amp;#x200B;

I create a dataloader after making a list of torch data objects using  

`train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)`  and if i try to iterate over this, i get an error  `RuntimeError: Sizes of tensors must match except in dimension 1. Got 1355 and 1550 in dimension 0 (The offending index is 1)` ",t2_2feck1ws,False,,0,False,Getting Started with Graph Classification,[],r/pytorch,False,6,,0,,,False,t3_j9m35r,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1602486278.0,,[],{},,,True,,1602514574.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;hello,&lt;/p&gt;

&lt;p&gt;I have about 500 networkx graphs and I wanted to perform graph classification on them into 2 classes.&lt;/p&gt;

&lt;p&gt;This is how a node looks like.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;{0: {&amp;#39;id&amp;#39;: tensor(144), &amp;#39;residue_name&amp;#39;: tensor(8), &amp;#39;h&amp;#39;: tensor([0.0000, 0.0000, 0.0000, 0.0000, 6.0700, 0.1300, 0.1500]), &amp;#39;coords&amp;#39;: tensor([-21.1550, 23.3610, 1.9100]), &amp;#39;ss&amp;#39;: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]), &amp;#39;asa&amp;#39;: tensor([10752.]), &amp;#39;rsa&amp;#39;: tensor([128.])}&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is how an edge looks like.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(0, 2, {&amp;#39;id&amp;#39;: 322, &amp;#39;rel_type&amp;#39;: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64), &amp;#39;norm&amp;#39;: tensor(1.)})&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If i try to use stellar graphs, it throws an error that it only accepts numeric features and i am not able to use DGL as none of their examples show to load our own dataset. My graphs dot have features as a graph attribute but as node and edge attributes.&lt;/p&gt;

&lt;p&gt;How do I use my data with these libraries or are there any other libraries I can use?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I create a dataloader after making a list of torch data objects using  &lt;/p&gt;

&lt;p&gt;&lt;code&gt;train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)&lt;/code&gt;  and if i try to iterate over this, i get an error  &lt;code&gt;RuntimeError: Sizes of tensors must match except in dimension 1. Got 1355 and 1550 in dimension 0 (The offending index is 1)&lt;/code&gt; &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j9m35r,True,,ybkhan,,0,True,all_ads,False,[],False,,/r/pytorch/comments/j9m35r/getting_started_with_graph_classification/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j9m35r/getting_started_with_graph_classification/,7135,1602485774.0,0,,False,,,,,,,,
299,,pytorch,"Hi, I am trying to parallelize my pipeline using the 8 TPU cores that kaggle provides for free.  The code works fine with one tpu core but fails with all 8.  I get a data loader and then create a function for running the training, but when I try to parallelize it using `torch_xla.distributed.xla_multiprocessing.spawn` I get an exception thrown such as:

    ---------------------------------------------------------------------------
    Exception                                 Traceback (most recent call last)
    &lt;ipython-input-75-3b5a9d4cb9b4&gt; in &lt;module&gt;
         49 if cfg[""model_params""][""train""]:
         50     if __name__ == '__main__':
    ---&gt; 51         xmp.spawn(_map_fn, args=(), nprocs=cfg['train_data_loader']['num_workers'])
         52 
         53 #     device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
    
    /opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py in spawn(fn, args, nprocs, join, daemon, start_method)
        393         join=join,
        394         daemon=daemon,
    --&gt; 395         start_method=start_method)
        396 
        397 
    
    /opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in start_processes(fn, args, nprocs, join, daemon, start_method)
        155 
        156     # Loop on join until it returns True or raises an exception.
    --&gt; 157     while not context.join():
        158         pass
        159 
    
    /opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in join(self, timeout)
        110                 raise Exception(
        111                     ""process %d terminated with exit code %d"" %
    --&gt; 112                     (error_index, exitcode)
        113                 )
        114 
    
    Exception: process 2 terminated with exit code 1
```
Is there a way to fix this, or at least handle the exception?",t2_4284o4ku,False,,0,False,Exception: process 2 terminated with exit code 1 when using torch_xla,[],r/pytorch,False,6,,0,,,False,t3_j7z6l6,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1602279849.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am trying to parallelize my pipeline using the 8 TPU cores that kaggle provides for free.  The code works fine with one tpu core but fails with all 8.  I get a data loader and then create a function for running the training, but when I try to parallelize it using &lt;code&gt;torch_xla.distributed.xla_multiprocessing.spawn&lt;/code&gt; I get an exception thrown such as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
&amp;lt;ipython-input-75-3b5a9d4cb9b4&amp;gt; in &amp;lt;module&amp;gt;
     49 if cfg[&amp;quot;model_params&amp;quot;][&amp;quot;train&amp;quot;]:
     50     if __name__ == &amp;#39;__main__&amp;#39;:
---&amp;gt; 51         xmp.spawn(_map_fn, args=(), nprocs=cfg[&amp;#39;train_data_loader&amp;#39;][&amp;#39;num_workers&amp;#39;])
     52 
     53 #     device = torch.device(&amp;quot;cuda:0&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;)

/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py in spawn(fn, args, nprocs, join, daemon, start_method)
    393         join=join,
    394         daemon=daemon,
--&amp;gt; 395         start_method=start_method)
    396 
    397 

/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in start_processes(fn, args, nprocs, join, daemon, start_method)
    155 
    156     # Loop on join until it returns True or raises an exception.
--&amp;gt; 157     while not context.join():
    158         pass
    159 

/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in join(self, timeout)
    110                 raise Exception(
    111                     &amp;quot;process %d terminated with exit code %d&amp;quot; %
--&amp;gt; 112                     (error_index, exitcode)
    113                 )
    114 

Exception: process 2 terminated with exit code 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```
Is there a way to fix this, or at least handle the exception?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j7z6l6,True,,hardmemer069,,1,True,all_ads,False,[],False,,/r/pytorch/comments/j7z6l6/exception_process_2_terminated_with_exit_code_1/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j7z6l6/exception_process_2_terminated_with_exit_code_1/,7135,1602251049.0,0,,False,,,,,,,,
300,,pytorch,I cannot wait to run PyTorch with RTX 3000 series :(,t2_4f369v6x,False,,0,False,When will the new pytorch version be released that can be capatible to CUDA 11?,[],r/pytorch,False,6,,0,,,False,t3_j7q8kq,False,dark,1.0,,public,9,0,{},,,False,[],,False,False,,{},,False,9,,False,self,False,,[],{},,,True,,1602236678.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I cannot wait to run PyTorch with RTX 3000 series :(&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j7q8kq,True,,Annoymous9501,,4,True,all_ads,False,[],False,,/r/pytorch/comments/j7q8kq/when_will_the_new_pytorch_version_be_released/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j7q8kq/when_will_the_new_pytorch_version_be_released/,7135,1602207878.0,0,,False,,,,,,,,
301,,pytorch,I'm looking to perform the above task. Any pointers to do this in the best and correct way?,t2_ebu4m,False,,0,False,Tips for deploying a PyTorch model onto Microsoft Azure?,[],r/pytorch,False,6,,0,,,False,t3_j7f369,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1602199652.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking to perform the above task. Any pointers to do this in the best and correct way?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j7f369,True,,Pepipasta,,3,True,all_ads,False,[],False,,/r/pytorch/comments/j7f369/tips_for_deploying_a_pytorch_model_onto_microsoft/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j7f369/tips_for_deploying_a_pytorch_model_onto_microsoft/,7135,1602170852.0,0,,False,,,,,,,,
302,,pytorch,"I'm looking for material (papers, slides, ...), anything useful really, that gives both implementation-wise and mathematical explanations about how autograd works in PyTorch. In particular, I'm having a hard time understanding what is going on under the hood for 2nd order derivatives when, for example, tricks like representing backward passes as forward passes or efficient hessian-vector products are (recognised and) applied. I would like to learn rules-of-thumb that essentially inform me what problems will be run efficiently or not, e.g. because the application of said tricks isn't possible under certain circumstances. If there is no material for PyTorch perhaps there is for other libraries or languages such as C++.

Appreciated!",t2_n757lz,False,,0,False,"Papers, Slides, ... for understanding the tricks behind autograd in PyTorch",[],r/pytorch,False,6,,0,,,False,t3_j79tj5,False,dark,0.73,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,1602175826.0,,[],{},,,True,,1602176910.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for material (papers, slides, ...), anything useful really, that gives both implementation-wise and mathematical explanations about how autograd works in PyTorch. In particular, I&amp;#39;m having a hard time understanding what is going on under the hood for 2nd order derivatives when, for example, tricks like representing backward passes as forward passes or efficient hessian-vector products are (recognised and) applied. I would like to learn rules-of-thumb that essentially inform me what problems will be run efficiently or not, e.g. because the application of said tricks isn&amp;#39;t possible under certain circumstances. If there is no material for PyTorch perhaps there is for other libraries or languages such as C++.&lt;/p&gt;

&lt;p&gt;Appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j79tj5,True,,whiletrue2,,1,True,all_ads,False,[],False,,/r/pytorch/comments/j79tj5/papers_slides_for_understanding_the_tricks_behind/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j79tj5/papers_slides_for_understanding_the_tricks_behind/,7135,1602148110.0,0,,False,,,,,,,,
303,,pytorch,"NVIDIA has developed a universal PyTorch library, **Imaginaire,** with an optimized implementation of various GAN images and video synthesis. 

The **Imaginaire** library currently covers three types of models, providing tutorials for each of them:

* Supervised Image-to-image translation
* Unsupervised Image-to-image translation
* Video-to-video translation 

Summary: https://www.marktechpost.com/2020/10/06/nvidia-releases-imaginaire-a-universal-pytorch-library-designed-for-various-gan-based-tasks-and-methods/

Github: https://github.com/NVlabs/imaginaire#supervised-image-to-image-translation",t2_2wsvqwhg,False,,0,False,NVIDIA Releases Imaginaire: A Universal PyTorch Library Designed For Various GAN-Based Tasks And Methods,[],r/pytorch,False,6,,0,,,False,t3_j6lkna,False,dark,0.98,,public,29,0,{},,,False,[],,False,False,,{},,False,29,,False,self,False,,[],{},self,,True,,1602078888.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;NVIDIA has developed a universal PyTorch library, &lt;strong&gt;Imaginaire,&lt;/strong&gt; with an optimized implementation of various GAN images and video synthesis. &lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Imaginaire&lt;/strong&gt; library currently covers three types of models, providing tutorials for each of them:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Supervised Image-to-image translation&lt;/li&gt;
&lt;li&gt;Unsupervised Image-to-image translation&lt;/li&gt;
&lt;li&gt;Video-to-video translation &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Summary: &lt;a href=""https://www.marktechpost.com/2020/10/06/nvidia-releases-imaginaire-a-universal-pytorch-library-designed-for-various-gan-based-tasks-and-methods/""&gt;https://www.marktechpost.com/2020/10/06/nvidia-releases-imaginaire-a-universal-pytorch-library-designed-for-various-gan-based-tasks-and-methods/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/NVlabs/imaginaire#supervised-image-to-image-translation""&gt;https://github.com/NVlabs/imaginaire#supervised-image-to-image-translation&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/gTTtP_1frudL4qhq_gxNC3Nno8NhFMuiOhLrNnqHZXQ.jpg?auto=webp&amp;s=d10c206332c3246afb279a486fb7ab10234f929e', 'width': 520, 'height': 293}, 'resolutions': [{'url': 'https://external-preview.redd.it/gTTtP_1frudL4qhq_gxNC3Nno8NhFMuiOhLrNnqHZXQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e53f5860cdd9534aef98abb86fde328d7c5c170', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/gTTtP_1frudL4qhq_gxNC3Nno8NhFMuiOhLrNnqHZXQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3fbd5c82f2eb7456b6b30423b84414b025022f46', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/gTTtP_1frudL4qhq_gxNC3Nno8NhFMuiOhLrNnqHZXQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=483973412c5f7c8beb32c989bc2da2d5361ce1b9', 'width': 320, 'height': 180}], 'variants': {}, 'id': 'n2DUVUbo2he4tCnNxSy4KFuuEAdEBoxdhSDN2ZvkeJY'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j6lkna,True,,ai-lover,,1,True,all_ads,False,[],False,,/r/pytorch/comments/j6lkna/nvidia_releases_imaginaire_a_universal_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j6lkna/nvidia_releases_imaginaire_a_universal_pytorch/,7135,1602050088.0,0,,False,,,,,,,,
304,,pytorch,And similarly all other similar functions that exist in both these libraries.,t2_5n6xtijf,False,,0,False,What is the difference between nn.Linear and nn.functional.Linear ? And which one should be used in which case ?,[],r/pytorch,False,6,,0,,,False,t3_j6mikd,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1602083928.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;And similarly all other similar functions that exist in both these libraries.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j6mikd,True,,the-machine-learner,,2,True,all_ads,False,[],False,,/r/pytorch/comments/j6mikd/what_is_the_difference_between_nnlinear_and/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j6mikd/what_is_the_difference_between_nnlinear_and/,7135,1602055128.0,0,,False,,,,,,,,
305,,pytorch,"I'm working to semantic segmentation. I'm trying to get started with pytorch. I have already worked in tensorflow. So I have some basic experience with machine learning. Now in pytorch, I'm using this code to load data

    train_data = torchvision.datasets.ImageFolder(train_dir, transform=TRANSFORM_IMG)
    train_data_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle=True,  num_workers=4)
    test_data = torchvision.datasets.ImageFolder(test_dir, transform=TRANSFORM_IMG)
    test_data_loader  = data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4) 

I had to create sub directories, as this method requires them to be arranged as class.

Now my directory structure looks like this :

    test.ipynb
    train
     |---sat
     |---mask
    test
     |---sat
     |---mask

Now my question is how can I create model out of these data. I have to idea, how to retrieve **sat** or **mask** from *train* or *test* separately.

&amp;#x200B;

Thanks in advance.

&amp;#x200B;

I have tried this :

        for batch_idx, data in enumerate(train_data_loader):
            data, target = Variable(data[""sat""]), Variable(data[""mask""])

But this gives me 

***TypeError: list indices must be integers or slices, not str*** ",t2_1fuhylzi,False,,0,False,ImageFolder sata loading as model for training,[],r/pytorch,False,6,,0,,,False,t3_j5hq93,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1601898267.0,,[],{},,,True,,1601926278.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working to semantic segmentation. I&amp;#39;m trying to get started with pytorch. I have already worked in tensorflow. So I have some basic experience with machine learning. Now in pytorch, I&amp;#39;m using this code to load data&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_data = torchvision.datasets.ImageFolder(train_dir, transform=TRANSFORM_IMG)
train_data_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle=True,  num_workers=4)
test_data = torchvision.datasets.ImageFolder(test_dir, transform=TRANSFORM_IMG)
test_data_loader  = data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I had to create sub directories, as this method requires them to be arranged as class.&lt;/p&gt;

&lt;p&gt;Now my directory structure looks like this :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;test.ipynb
train
 |---sat
 |---mask
test
 |---sat
 |---mask
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now my question is how can I create model out of these data. I have to idea, how to retrieve &lt;strong&gt;sat&lt;/strong&gt; or &lt;strong&gt;mask&lt;/strong&gt; from &lt;em&gt;train&lt;/em&gt; or &lt;em&gt;test&lt;/em&gt; separately.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I have tried this :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    for batch_idx, data in enumerate(train_data_loader):
        data, target = Variable(data[&amp;quot;sat&amp;quot;]), Variable(data[&amp;quot;mask&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But this gives me &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;TypeError: list indices must be integers or slices, not str&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j5hq93,True,,maifee,,1,True,all_ads,False,[],False,,/r/pytorch/comments/j5hq93/imagefolder_sata_loading_as_model_for_training/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j5hq93/imagefolder_sata_loading_as_model_for_training/,7135,1601897478.0,0,,False,,,,,,,,
306,,pytorch,"I need Pytorch for a research project. There are many configurations available online. I'm using python 3.8.2. Should I downgrade python version, some people say to never use latest versions because of dependencies. Also, my GPU is nVidia 940MX (laptop). Should I try with the CUDA version of PyTorch or non-CUDA?",t2_81vn4cma,False,,0,False,Installing PyTorch (GPU and python version),[],r/pytorch,False,6,,0,,,False,t3_j4yi0f,False,dark,1.0,,public,11,0,{},,,False,[],,False,False,,{},,False,11,,False,self,False,,[],{},,,True,,1601844964.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need Pytorch for a research project. There are many configurations available online. I&amp;#39;m using python 3.8.2. Should I downgrade python version, some people say to never use latest versions because of dependencies. Also, my GPU is nVidia 940MX (laptop). Should I try with the CUDA version of PyTorch or non-CUDA?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j4yi0f,True,,hugehughlaurie,,14,True,all_ads,False,[],False,,/r/pytorch/comments/j4yi0f/installing_pytorch_gpu_and_python_version/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j4yi0f/installing_pytorch_gpu_and_python_version/,7135,1601816164.0,0,,False,,,,,,,,
307,,pytorch,,t2_227v2s6x,False,,0,False,[Project] VOC Formatted Dataset Generation for DL Frameworks,[],r/pytorch,False,6,,0,,,False,t3_j56qfq,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},link,,False,,1601874865.0,text,6,,,text,self.MachineLearning,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/gRqsV8B3nLa8oDjqDbt8flSVgNG9dueMtMyAbMjfhGM.jpg?auto=webp&amp;s=b0c846042bad6cac249afd7923e45464fff7395e', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/gRqsV8B3nLa8oDjqDbt8flSVgNG9dueMtMyAbMjfhGM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=404d07af2a7da8dbec1b44f291951993efbbf78b', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/gRqsV8B3nLa8oDjqDbt8flSVgNG9dueMtMyAbMjfhGM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6a5067b52cdbc97e3192aa807fa636ad14f05c07', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/gRqsV8B3nLa8oDjqDbt8flSVgNG9dueMtMyAbMjfhGM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9a7785e1639e5414ff1e92e9cf375fb3126be752', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'h1v_qBmuf3-rvzk0tYiRL0ojdsLb2ld1nL90TzaOWVw'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j56qfq,True,,r42in,,0,True,all_ads,False,[],False,,/r/pytorch/comments/j56qfq/project_voc_formatted_dataset_generation_for_dl/,all_ads,False,/r/MachineLearning/comments/j56oq7/project_voc_formatted_dataset_generation_for_dl/,7135,1601846065.0,0,,False,/r/MachineLearning/comments/j56oq7/project_voc_formatted_dataset_generation_for_dl/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': ' \n\nHi fellow altruists,\n\nThe task of Training a Custom Object Detection Model has been eased to a much greater extent by current well-known deep learning frameworks such as Detectron2 or MMDetection. However, the Dataset Generation sometimes feel daunting. Going through the Docs of Custom Dataset Pipeline provided by the respective frameworks have been helpful, but the fact that both of the frameworks have their configuration files written with respect to COCO and PascalVOC Dataset. So I thought just making the data in the format of Pascal VOC and changing paths of the images and annotations in the respective configuration file is much simpler and time saving way. Hence I have created a script that takes your input folder and generates the PASCAL VOC FORMATTED DATASET for you as the output.\n\nThe Script is available [on my GitHub](https://github.com/Razin-Tailor/make_voc_dataset). Please do check it out. Hope it helps!\n\nAny suggestions are always welcome', 'author_fullname': 't2_227v2s6x', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[Project] VOC Formatted Dataset Generation for DL Frameworks', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'four', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_j56oq7', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1601874702.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi fellow altruists,&lt;/p&gt;\n\n&lt;p&gt;The task of Training a Custom Object Detection Model has been eased to a much greater extent by current well-known deep learning frameworks such as Detectron2 or MMDetection. However, the Dataset Generation sometimes feel daunting. Going through the Docs of Custom Dataset Pipeline provided by the respective frameworks have been helpful, but the fact that both of the frameworks have their configuration files written with respect to COCO and PascalVOC Dataset. So I thought just making the data in the format of Pascal VOC and changing paths of the images and annotations in the respective configuration file is much simpler and time saving way. Hence I have created a script that takes your input folder and generates the PASCAL VOC FORMATTED DATASET for you as the output.&lt;/p&gt;\n\n&lt;p&gt;The Script is available &lt;a href=""https://github.com/Razin-Tailor/make_voc_dataset""&gt;on my GitHub&lt;/a&gt;. Please do check it out. Hope it helps!&lt;/p&gt;\n\n&lt;p&gt;Any suggestions are always welcome&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/gRqsV8B3nLa8oDjqDbt8flSVgNG9dueMtMyAbMjfhGM.jpg?auto=webp&amp;s=b0c846042bad6cac249afd7923e45464fff7395e', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/gRqsV8B3nLa8oDjqDbt8flSVgNG9dueMtMyAbMjfhGM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=404d07af2a7da8dbec1b44f291951993efbbf78b', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/gRqsV8B3nLa8oDjqDbt8flSVgNG9dueMtMyAbMjfhGM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6a5067b52cdbc97e3192aa807fa636ad14f05c07', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/gRqsV8B3nLa8oDjqDbt8flSVgNG9dueMtMyAbMjfhGM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9a7785e1639e5414ff1e92e9cf375fb3126be752', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'h1v_qBmuf3-rvzk0tYiRL0ojdsLb2ld1nL90TzaOWVw'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'j56oq7', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'r42in', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/j56oq7/project_voc_formatted_dataset_generation_for_dl/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/j56oq7/project_voc_formatted_dataset_generation_for_dl/', 'subreddit_subscribers': 1740775, 'created_utc': 1601845902.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_j56oq7,,,,,
308,,pytorch,,t2_44mbtmjy,False,,0,False,"Latest from USC researchers: Given a single neutral scan, researchers generate a complete set of dynamic face model assets, including personalized blendshapes and physically-based dynamic facial skin textures of the input individual!",[],r/pytorch,False,6,,0,57.0,,False,t3_j4owap,False,dark,0.81,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/JHy2vjKukU-QBhXKMcYR0ilwFs5rMAd3r05F2nACvrs.jpg,False,,[],{},link,,False,,1601796082.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/g3DkrbKGB6YEuOu1q5xzu__4ww4tx1sSP1JbfRcCT_Q.jpg?auto=webp&amp;s=e7804f63cc4fe6fa74953a8cd639cbd0a99ae66b', 'width': 1416, 'height': 560}, 'resolutions': [{'url': 'https://external-preview.redd.it/g3DkrbKGB6YEuOu1q5xzu__4ww4tx1sSP1JbfRcCT_Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2cfc05051cfdf58ad5e7535e33f1dec6d3376b8a', 'width': 108, 'height': 42}, {'url': 'https://external-preview.redd.it/g3DkrbKGB6YEuOu1q5xzu__4ww4tx1sSP1JbfRcCT_Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f3782a935836b933c81d338b24c7b9135ad42155', 'width': 216, 'height': 85}, {'url': 'https://external-preview.redd.it/g3DkrbKGB6YEuOu1q5xzu__4ww4tx1sSP1JbfRcCT_Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8acaf13db31258f7a6cbc071230e383ad8c0178c', 'width': 320, 'height': 126}, {'url': 'https://external-preview.redd.it/g3DkrbKGB6YEuOu1q5xzu__4ww4tx1sSP1JbfRcCT_Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0bd96f22422da27e6f16d4ec763dc3d9a57375f8', 'width': 640, 'height': 253}, {'url': 'https://external-preview.redd.it/g3DkrbKGB6YEuOu1q5xzu__4ww4tx1sSP1JbfRcCT_Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ba05363fc63ca621fde524a46ecb5ed7b7a5fdd2', 'width': 960, 'height': 379}, {'url': 'https://external-preview.redd.it/g3DkrbKGB6YEuOu1q5xzu__4ww4tx1sSP1JbfRcCT_Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=047e6ae1f35e0201696d6fc9b6ba4e46286249dc', 'width': 1080, 'height': 427}], 'variants': {}, 'id': 'fIC6MV79UrjbkKAupqmUulZM8sI8ATbNy3zGwXzZXuk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j4owap,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/j4owap/latest_from_usc_researchers_given_a_single/,all_ads,False,/r/LatestInML/comments/j4ov82/latest_from_usc_researchers_given_a_single/,7135,1601767282.0,0,,False,/r/LatestInML/comments/j4ov82/latest_from_usc_researchers_given_a_single/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and expert/code/API request: [click here](https://www.catalyzex.com/paper/arxiv:2010.00560)\n\nhttps://preview.redd.it/cdl5h39mqyq51.jpg?width=1906&amp;format=pjpg&amp;auto=webp&amp;s=a85ce1c1150abd3bef95236ea3f1b74682c87d9c', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Latest from USC researchers: Given a single neutral scan, researchers generate a complete set of dynamic face model assets, including personalized blendshapes and physically-based dynamic facial skin textures of the input individual!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 57, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'cdl5h39mqyq51': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 44, 'x': 108, 'u': 'https://preview.redd.it/cdl5h39mqyq51.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4559f52c05b66ae24e4c9410911a51d765a02ef7'}, {'y': 88, 'x': 216, 'u': 'https://preview.redd.it/cdl5h39mqyq51.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf64c80c29fd0461ce3a88db4c5df22a3af4188d'}, {'y': 130, 'x': 320, 'u': 'https://preview.redd.it/cdl5h39mqyq51.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=689cad57ad863d3c0db038748d1c50c332825ce1'}, {'y': 261, 'x': 640, 'u': 'https://preview.redd.it/cdl5h39mqyq51.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6862fa25ab45bf3107857e46fcf7e30d8c8e09d4'}, {'y': 392, 'x': 960, 'u': 'https://preview.redd.it/cdl5h39mqyq51.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a0f1aae5fe1770f75bfa67606e412a4e73eda0d4'}, {'y': 441, 'x': 1080, 'u': 'https://preview.redd.it/cdl5h39mqyq51.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c8320df62ef0aab032c1b28ebfa803731b55f617'}], 's': {'y': 780, 'x': 1906, 'u': 'https://preview.redd.it/cdl5h39mqyq51.jpg?width=1906&amp;format=pjpg&amp;auto=webp&amp;s=a85ce1c1150abd3bef95236ea3f1b74682c87d9c'}, 'id': 'cdl5h39mqyq51'}}, 'name': 't3_j4ov82', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 23, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 23, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/JHy2vjKukU-QBhXKMcYR0ilwFs5rMAd3r05F2nACvrs.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1601795962.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and expert/code/API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2010.00560""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/cdl5h39mqyq51.jpg?width=1906&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a85ce1c1150abd3bef95236ea3f1b74682c87d9c""&gt;https://preview.redd.it/cdl5h39mqyq51.jpg?width=1906&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a85ce1c1150abd3bef95236ea3f1b74682c87d9c&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/g3DkrbKGB6YEuOu1q5xzu__4ww4tx1sSP1JbfRcCT_Q.jpg?auto=webp&amp;s=e7804f63cc4fe6fa74953a8cd639cbd0a99ae66b', 'width': 1416, 'height': 560}, 'resolutions': [{'url': 'https://external-preview.redd.it/g3DkrbKGB6YEuOu1q5xzu__4ww4tx1sSP1JbfRcCT_Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2cfc05051cfdf58ad5e7535e33f1dec6d3376b8a', 'width': 108, 'height': 42}, {'url': 'https://external-preview.redd.it/g3DkrbKGB6YEuOu1q5xzu__4ww4tx1sSP1JbfRcCT_Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f3782a935836b933c81d338b24c7b9135ad42155', 'width': 216, 'height': 85}, {'url': 'https://external-preview.redd.it/g3DkrbKGB6YEuOu1q5xzu__4ww4tx1sSP1JbfRcCT_Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8acaf13db31258f7a6cbc071230e383ad8c0178c', 'width': 320, 'height': 126}, {'url': 'https://external-preview.redd.it/g3DkrbKGB6YEuOu1q5xzu__4ww4tx1sSP1JbfRcCT_Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0bd96f22422da27e6f16d4ec763dc3d9a57375f8', 'width': 640, 'height': 253}, {'url': 'https://external-preview.redd.it/g3DkrbKGB6YEuOu1q5xzu__4ww4tx1sSP1JbfRcCT_Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ba05363fc63ca621fde524a46ecb5ed7b7a5fdd2', 'width': 960, 'height': 379}, {'url': 'https://external-preview.redd.it/g3DkrbKGB6YEuOu1q5xzu__4ww4tx1sSP1JbfRcCT_Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=047e6ae1f35e0201696d6fc9b6ba4e46286249dc', 'width': 1080, 'height': 427}], 'variants': {}, 'id': 'fIC6MV79UrjbkKAupqmUulZM8sI8ATbNy3zGwXzZXuk'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'j4ov82', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/j4ov82/latest_from_usc_researchers_given_a_single/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/j4ov82/latest_from_usc_researchers_given_a_single/', 'subreddit_subscribers': 6676, 'created_utc': 1601767162.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_j4ov82,,,,,
309,,pytorch,"Hello everyone!

I'm having problems with my network (DQN) in Sumo Simulation . I want to generate a routefile for each episode in my simulation (Reinforcement Learning Problem) but I'm not sure how to connect the traffic lights because that's my agent. Therefore, it's kind to confusing. If you are able to help me, please mesagge me. I develop a big code but the rewards are the same for each episode. SOS uu",t2_65rszqsr,False,,0,False,DQN issues using SUMO SIMULATOR,[],r/pytorch,False,6,,0,,,False,t3_j4m0k1,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1601785227.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m having problems with my network (DQN) in Sumo Simulation . I want to generate a routefile for each episode in my simulation (Reinforcement Learning Problem) but I&amp;#39;m not sure how to connect the traffic lights because that&amp;#39;s my agent. Therefore, it&amp;#39;s kind to confusing. If you are able to help me, please mesagge me. I develop a big code but the rewards are the same for each episode. SOS uu&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j4m0k1,True,,Theron96G,,0,True,all_ads,False,[],False,,/r/pytorch/comments/j4m0k1/dqn_issues_using_sumo_simulator/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j4m0k1/dqn_issues_using_sumo_simulator/,7135,1601756427.0,0,,False,,,,,,,,
310,,pytorch,"Imagine you have trained an awesome neural network model using PyTorch and now want to use it for inference. You don't have the same computational power as you had during training and re-architecting and rewriting source code is not a feasible solution for speeding up inference. Fortunately, this is all possible using the Inference Engine provided Intel's OpenVINO Toolkit.   


In many cases, you get a considerable performance increase without hugely scarifying the inference accuracy. Additionally, the model conversion procedure is simple and fast.  


In today's post, we walk you through the process step by step with code. **In our example, we have accelerated the inference step by approximately 2.2 times!** Click on the link below for a detailed tutorial with code  


[How to speed up Deep Learning Inference Using OpenVINO Toolkit](https://click.convertkit-mail.com/5qukmnm54ks7h3x6xzc6/g3hnh5hzvqwrkmsr/aHR0cHM6Ly9vcGVuY3Yub3JnL2hvdy10by1zcGVlZC11cC1kZWVwLWxlYXJuaW5nLWluZmVyZW5jZS11c2luZy1vcGVudmluby10b29sa2l0LTIv)

https://preview.redd.it/bzv73h0kixq51.jpg?width=256&amp;format=pjpg&amp;auto=webp&amp;s=5c2decedcdfe6e2b4b12cff7cb2228bc71e55167",t2_cvc9f,False,,0,False,How to Speed Up Deep Learning Inference Using OpenVINO Toolkit,[],r/pytorch,False,6,,0,124.0,,False,t3_j4kvq1,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/44GOxqBkmyQ6sCsCY3QNEBgii_tGIHlktJFD5nZp59M.jpg,False,,[],{},,,True,,1601781147.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Imagine you have trained an awesome neural network model using PyTorch and now want to use it for inference. You don&amp;#39;t have the same computational power as you had during training and re-architecting and rewriting source code is not a feasible solution for speeding up inference. Fortunately, this is all possible using the Inference Engine provided Intel&amp;#39;s OpenVINO Toolkit.   &lt;/p&gt;

&lt;p&gt;In many cases, you get a considerable performance increase without hugely scarifying the inference accuracy. Additionally, the model conversion procedure is simple and fast.  &lt;/p&gt;

&lt;p&gt;In today&amp;#39;s post, we walk you through the process step by step with code. &lt;strong&gt;In our example, we have accelerated the inference step by approximately 2.2 times!&lt;/strong&gt; Click on the link below for a detailed tutorial with code  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://click.convertkit-mail.com/5qukmnm54ks7h3x6xzc6/g3hnh5hzvqwrkmsr/aHR0cHM6Ly9vcGVuY3Yub3JnL2hvdy10by1zcGVlZC11cC1kZWVwLWxlYXJuaW5nLWluZmVyZW5jZS11c2luZy1vcGVudmluby10b29sa2l0LTIv""&gt;How to speed up Deep Learning Inference Using OpenVINO Toolkit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/bzv73h0kixq51.jpg?width=256&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5c2decedcdfe6e2b4b12cff7cb2228bc71e55167""&gt;https://preview.redd.it/bzv73h0kixq51.jpg?width=256&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5c2decedcdfe6e2b4b12cff7cb2228bc71e55167&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j4kvq1,True,,spmallick,,0,True,all_ads,False,[],False,,/r/pytorch/comments/j4kvq1/how_to_speed_up_deep_learning_inference_using/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j4kvq1/how_to_speed_up_deep_learning_inference_using/,7135,1601752347.0,0,,False,,,,"{'bzv73h0kixq51': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 95, 'x': 108, 'u': 'https://preview.redd.it/bzv73h0kixq51.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0a01b0d4ebb61fa79459c6ade22eb6ae602e025'}, {'y': 191, 'x': 216, 'u': 'https://preview.redd.it/bzv73h0kixq51.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac7822237289ddd19450b16a2e5d5837337ff004'}], 's': {'y': 227, 'x': 256, 'u': 'https://preview.redd.it/bzv73h0kixq51.jpg?width=256&amp;format=pjpg&amp;auto=webp&amp;s=5c2decedcdfe6e2b4b12cff7cb2228bc71e55167'}, 'id': 'bzv73h0kixq51'}}",,,,
311,,pytorch,"I am trying to make a skip connection. Let's say I want to pass a tensor X of shape (1, 256, 256) to another layer whose output Y is (16, 64, 64). 

I need a resulting tensor Z of shape (17, 256, 256), so my guess is that I'd need to add some zeros (like padding) at the borders of Y until I get a size of 256x256. How can I do this? 

Thank you in advance!",t2_56lj6uum,False,,0,False,How can I concat two tensors with different width and height?,[],r/pytorch,False,6,,0,,,False,t3_j4530u,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1601709100.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to make a skip connection. Let&amp;#39;s say I want to pass a tensor X of shape (1, 256, 256) to another layer whose output Y is (16, 64, 64). &lt;/p&gt;

&lt;p&gt;I need a resulting tensor Z of shape (17, 256, 256), so my guess is that I&amp;#39;d need to add some zeros (like padding) at the borders of Y until I get a size of 256x256. How can I do this? &lt;/p&gt;

&lt;p&gt;Thank you in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j4530u,True,,Historical-Carpenter,,10,True,all_ads,False,[],False,,/r/pytorch/comments/j4530u/how_can_i_concat_two_tensors_with_different_width/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j4530u/how_can_i_concat_two_tensors_with_different_width/,7135,1601680300.0,0,,False,,,,,,,,
312,,pytorch,"Hey there,
First of all, this might be a quite specific question, I would be happy to be shown any kind of lead. 

I am trying to use the SAGEConv module in torch_geometric: https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html

The thing is, even though in the paper there are three different aggregator architectures and a hyperparameter ""k"", in the module implementation I don't see any option to specify them. Have they implemented just for the mean aggregator and k=1? Is there a mistake in my reasoning?

Thank you.",t2_1cui2d3k,False,,0,False,SAGEConv in torch_geometric,[],r/pytorch,False,6,,0,,,False,t3_j3yg72,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1601687079.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey there,
First of all, this might be a quite specific question, I would be happy to be shown any kind of lead. &lt;/p&gt;

&lt;p&gt;I am trying to use the SAGEConv module in torch_geometric: &lt;a href=""https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html""&gt;https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The thing is, even though in the paper there are three different aggregator architectures and a hyperparameter &amp;quot;k&amp;quot;, in the module implementation I don&amp;#39;t see any option to specify them. Have they implemented just for the mean aggregator and k=1? Is there a mistake in my reasoning?&lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j3yg72,True,,cheeky_bastard__,,2,True,all_ads,False,[],False,,/r/pytorch/comments/j3yg72/sageconv_in_torch_geometric/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j3yg72/sageconv_in_torch_geometric/,7135,1601658279.0,0,,False,,,,,,,,
313,,pytorch,"&amp;#x200B;

[Hello in this page \\""CUDA\\"" section has only 9.2 10.1 10.2 but i have cuda 11.1 what should i do?](https://preview.redd.it/fyet1rrj5nq51.png?width=947&amp;format=png&amp;auto=webp&amp;s=40e60cdeb81ef88b9043668a0e547081c3edbba3)",t2_7vk91q70,False,,0,False,about pytorch install-cuda,[],r/pytorch,False,6,,0,47.0,,False,t3_j3qt9l,False,dark,0.75,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/BbBnlV-FvQ97bvIm1x-fFlWu0zvHOuCNP7BhqB9k1bY.jpg,False,,[],{},,,True,,1601655755.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/fyet1rrj5nq51.png?width=947&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=40e60cdeb81ef88b9043668a0e547081c3edbba3""&gt;Hello in this page \&amp;quot;CUDA\&amp;quot; section has only 9.2 10.1 10.2 but i have cuda 11.1 what should i do?&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j3qt9l,True,,Impossible_Banana368,,3,True,all_ads,False,[],False,,/r/pytorch/comments/j3qt9l/about_pytorch_installcuda/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j3qt9l/about_pytorch_installcuda/,7135,1601626955.0,0,,False,,,,"{'fyet1rrj5nq51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 36, 'x': 108, 'u': 'https://preview.redd.it/fyet1rrj5nq51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=67779c6b247fff5c14fb67008b300ba0a701a23e'}, {'y': 73, 'x': 216, 'u': 'https://preview.redd.it/fyet1rrj5nq51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=400f5f12d7f53d35110cb61a4e36ecdddf2b4c7a'}, {'y': 108, 'x': 320, 'u': 'https://preview.redd.it/fyet1rrj5nq51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=01ca2403202461322a87c3a6107a19b7b71aa3d4'}, {'y': 217, 'x': 640, 'u': 'https://preview.redd.it/fyet1rrj5nq51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2b29d131977c31b3c99689a093670cde94a47223'}], 's': {'y': 322, 'x': 947, 'u': 'https://preview.redd.it/fyet1rrj5nq51.png?width=947&amp;format=png&amp;auto=webp&amp;s=40e60cdeb81ef88b9043668a0e547081c3edbba3'}, 'id': 'fyet1rrj5nq51'}}",,,,
314,,pytorch,"QA automation cuts down almost half of the time spent to complete annotation projects. 

[This article](https://blog.superannotate.com/how-to-detect-mislabeled-annotations) presents the tool developed to speed up the QA to a significant degree.",t2_810pw8v7,False,,0,False,How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance,[],r/pytorch,False,6,,0,,,False,t3_j3erqx,False,dark,0.78,,public,9,0,{},,,False,[],,False,False,,{},,False,9,,False,self,False,,[],{},,,True,,1601607252.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;QA automation cuts down almost half of the time spent to complete annotation projects. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://blog.superannotate.com/how-to-detect-mislabeled-annotations""&gt;This article&lt;/a&gt; presents the tool developed to speed up the QA to a significant degree.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j3erqx,True,,WeekendClassic,,0,True,all_ads,False,[],False,,/r/pytorch/comments/j3erqx/how_to_detect_93_of_mislabeled_annotations_while/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j3erqx/how_to_detect_93_of_mislabeled_annotations_while/,7135,1601578452.0,0,,False,,,,,,,,
315,,pytorch,,t2_44mbtmjy,False,,0,False,Propagate the style from a few selected keyframes to the rest of the sequence!,[],r/pytorch,False,6,,0,47.0,,False,t3_j31uu8,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/PAbkIR1KQwKA2mMzkR2kJnf_mEalkdrHGfPdXdvSENc.jpg,False,,[],{},link,,False,,1601555572.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/dZGIVnHHzZR2OOut7VFLB8BvhJIsY0Y5XLYkBQKKSbM.jpg?auto=webp&amp;s=a1901fe5d002f3795486f202f76033c9fa770d49', 'width': 1414, 'height': 482}, 'resolutions': [{'url': 'https://external-preview.redd.it/dZGIVnHHzZR2OOut7VFLB8BvhJIsY0Y5XLYkBQKKSbM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1aae6966d035e1ce69b62de5a0338365ad1d3f68', 'width': 108, 'height': 36}, {'url': 'https://external-preview.redd.it/dZGIVnHHzZR2OOut7VFLB8BvhJIsY0Y5XLYkBQKKSbM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ce8b7500eb54c767d5daf1338f845c63068b2909', 'width': 216, 'height': 73}, {'url': 'https://external-preview.redd.it/dZGIVnHHzZR2OOut7VFLB8BvhJIsY0Y5XLYkBQKKSbM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d968b41b08b87ffc9aa88729faf35fe478f0d0f', 'width': 320, 'height': 109}, {'url': 'https://external-preview.redd.it/dZGIVnHHzZR2OOut7VFLB8BvhJIsY0Y5XLYkBQKKSbM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b515b34d98ffc21224a7984e87376dabe96e210', 'width': 640, 'height': 218}, {'url': 'https://external-preview.redd.it/dZGIVnHHzZR2OOut7VFLB8BvhJIsY0Y5XLYkBQKKSbM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=75a57604e58af2ccb25f18b8007f126c33e27ceb', 'width': 960, 'height': 327}, {'url': 'https://external-preview.redd.it/dZGIVnHHzZR2OOut7VFLB8BvhJIsY0Y5XLYkBQKKSbM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7ef95d09ff935038d2dbb8ad3e83bc9ed5e09722', 'width': 1080, 'height': 368}], 'variants': {}, 'id': 'KEnKpXFf_MD3D5ZvBjPeLVTjjENXzaW_Yblhd1RQ0rM'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j31uu8,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/j31uu8/propagate_the_style_from_a_few_selected_keyframes/,all_ads,False,/r/LatestInML/comments/j31ppz/propagate_the_style_from_a_few_selected_keyframes/,7135,1601526772.0,0,,False,/r/LatestInML/comments/j31ppz/propagate_the_style_from_a_few_selected_keyframes/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2004.14489)\n\nhttps://reddit.com/link/j31ppz/video/yad0ut0yteq51/player', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Propagate the style from a few selected keyframes to the rest of the sequence!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 47, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'yad0ut0yteq51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/j31ppz/asset/yad0ut0yteq51/DASHPlaylist.mpd?a=1618044180%2CNzliNjEwOThhM2U3N2UwNjgzNzAwZDZhYjM3YmFjMDg5N2JhYjc4NWMzYmRmNmQ4MjA4YmNmYWRmZDkxNjE3Yg%3D%3D&amp;v=1&amp;f=sd', 'x': 640, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/j31ppz/asset/yad0ut0yteq51/HLSPlaylist.m3u8?a=1618044180%2CMTljN2UwMTdlY2UxNzhiZDQ2YjY2NWQ1ZTkwYzY4OTI3ODAxZTdhYzcxMDZmZWFlNjQ3ODRlNjkzOGQ5OGQzMQ%3D%3D&amp;v=1&amp;f=sd', 'id': 'yad0ut0yteq51', 'isGif': False}}, 'name': 't3_j31ppz', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 20, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 20, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/PAbkIR1KQwKA2mMzkR2kJnf_mEalkdrHGfPdXdvSENc.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1601554945.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2004.14489""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/j31ppz/video/yad0ut0yteq51/player""&gt;https://reddit.com/link/j31ppz/video/yad0ut0yteq51/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/dZGIVnHHzZR2OOut7VFLB8BvhJIsY0Y5XLYkBQKKSbM.jpg?auto=webp&amp;s=a1901fe5d002f3795486f202f76033c9fa770d49', 'width': 1414, 'height': 482}, 'resolutions': [{'url': 'https://external-preview.redd.it/dZGIVnHHzZR2OOut7VFLB8BvhJIsY0Y5XLYkBQKKSbM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1aae6966d035e1ce69b62de5a0338365ad1d3f68', 'width': 108, 'height': 36}, {'url': 'https://external-preview.redd.it/dZGIVnHHzZR2OOut7VFLB8BvhJIsY0Y5XLYkBQKKSbM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ce8b7500eb54c767d5daf1338f845c63068b2909', 'width': 216, 'height': 73}, {'url': 'https://external-preview.redd.it/dZGIVnHHzZR2OOut7VFLB8BvhJIsY0Y5XLYkBQKKSbM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d968b41b08b87ffc9aa88729faf35fe478f0d0f', 'width': 320, 'height': 109}, {'url': 'https://external-preview.redd.it/dZGIVnHHzZR2OOut7VFLB8BvhJIsY0Y5XLYkBQKKSbM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b515b34d98ffc21224a7984e87376dabe96e210', 'width': 640, 'height': 218}, {'url': 'https://external-preview.redd.it/dZGIVnHHzZR2OOut7VFLB8BvhJIsY0Y5XLYkBQKKSbM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=75a57604e58af2ccb25f18b8007f126c33e27ceb', 'width': 960, 'height': 327}, {'url': 'https://external-preview.redd.it/dZGIVnHHzZR2OOut7VFLB8BvhJIsY0Y5XLYkBQKKSbM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7ef95d09ff935038d2dbb8ad3e83bc9ed5e09722', 'width': 1080, 'height': 368}], 'variants': {}, 'id': 'KEnKpXFf_MD3D5ZvBjPeLVTjjENXzaW_Yblhd1RQ0rM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'j31ppz', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/j31ppz/propagate_the_style_from_a_few_selected_keyframes/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/j31ppz/propagate_the_style_from_a_few_selected_keyframes/', 'subreddit_subscribers': 6676, 'created_utc': 1601526145.0, 'num_crossposts': 10, 'media': None, 'is_video': False}]",t3_j31ppz,,,,,
316,,pytorch,"hi every one, i used torch and python on jupyter to text classification, but i got the error below and couldnt solve it yet, please help me:

 **TypeError**: object of type 'TextClassificationDataset' has no len() 

&amp;#x200B;

    class TextClassificationDataset(Dataset):
    def __init__(self, path, tokenizer,
    split='train',
    vocab_path='vocab.pkl',
    max_len=100, min_count=10):
    self.path=path
    assert split in['train','test']
    self.split=split
    self.vocab_path=vocab_path
    self.tokenizer=tokenizer
    self.max_len=max_len
    self.min_count=min_count
    self.cash={}
    self.vocab=None
    self.classes=[]
    self.class_to_index={}
    self.text_files=[]
    split_path=f'{path}/{split}'
    for cls_idx,lable in enumerate(os.listdir(split_path)):
    text_files=[(fname,cls_idx)for fname in glob(f'{split_path}/{lable}/*.txt')]
    self.text_files+=text_files
    self.classes+=[lable]
    self.class_to_index[lable]=cls_idx
    self.num_classes=len(self.classes)
    #build vocabulary from training &amp; validation texts
    self.build_vocab()
    def _getitem_(self,index):
    #read the tokenized text file &amp; its lable (neg=0 , pos=1)
    fname,class_idx=self.text_files[index]
    if fname in self.cash:
    return self.cash[fname],class_idx
    #read text file
    text=open(fname).read()
    #tokenize the text file
    tokens=self.tokenizer(text.lower())
    #padding &amp; trimming
    if(len(tokens)&lt;self.max_len):
    num_pads=self.max_len-len(tokens)
    tokens=[PAD]*num_pads+tokens
    elif len(tokens)&gt;self.max_len:
    tokens=tokens[:self.max_len]
    #numericalizing
    
    ids=torch.LongTensor(self.max_len)
    for i,word in enumerate(tokens):
    if word not in self.vocab.word2index:
    #unknown words
    ids[i]=self.vocab.word2index[UNK]
    elif word !=PAD and self.vocab.word2count[word]&lt;self.min_count:
    ids[i]=self_vocab.word2index[UNK] #rare words
    else:
    ids[i]=self.vocab.word2index[word]
    #save in cash for future use
    self.cash[fname]=ids
    return ids,class_idx
    def _len_(self):
    return (len(self.text_files))
    def build_vocab(self):
    if not os.path.exists(self.vocab_path):
    vocab=Vocabulary(self.tokenizer)
    filenames=glob(f'{data_dir}/*/*/*.txt')
    for filename in tqdm(filenames,desc='Building Vocab'):
    with open(filename, encoding=""utf-8"", errors=""ignore"") as f:
    #lines = f.readlines()
    #lines = [line.strip(""\n"") for line in lines]
    for line in f:
    #for line in lines:
    vocab.add_sentence(line.lower())
    #sort words by their frequencies
    words=[(0,PAD),(0,UNK)]
    words+=sorted([(c,w) for w,c in vocab.word2count.items()],reverse=True)
    self.vocab=Vocabulary(self.tokenizer)
    for i,(count,word)in enumerate(words):
    self.vocab.word2index[word]=i
    self.vocab.word2count[word]=count
    self.vocab.index2word[i]=word
    self.vocab.count+=1
    pickle.dump(self.vocab,open(self.vocab_path,'wb'))
    else:
    self.vocab=pickle.load(open(self.vocab_path,'rb'))
    

&amp;#x200B;

&amp;#x200B;

    train_ds= TextClassificationDataset(data_dir,tokenizer,'train', vocab_path, max_len, min_count)
    train_dl= DataLoader(train_ds,batch_size=batch_size,shuffle=True)
    

&amp;#x200B;

TypeError                                 Traceback (most recent call last) &lt;ipython-input-11-28061f6a874c&gt; in &lt;module&gt;       1 train\_ds= TextClassificationDataset\*\*(data\_dir,tokenizer,'train',\*\* vocab\_path\*\*,\*\* max\_len\*\*,\*\* min\_count\*\*)\*\* ----&gt; 2 train\_dl= DataLoader\*\*(train\_ds,batch\_size=batch\_size,shuffle=True)\*\*       3       4 valid\_ds= TextClassificationDataset\*\*(data\_dir,tokenizer,'test',vocab\_path,max\_len,min\_count)\*\*       5 valid\_dl= DataLoader\*\*(valid\_ds,batch\_size=batch\_size,shuffle=False)\*\* C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py in \_\_init\_\_(self, dataset, batch\_size, shuffle, sampler, batch\_sampler, num\_workers, collate\_fn, pin\_memory, drop\_last, timeout, worker\_init\_fn, multiprocessing\_context, generator)     222 else: # map-style     223 if shuffle\*\*:\*\* --&gt; 224 sampler = RandomSampler\*\*(dataset,\*\* generator=generator\*\*)\*\*     225 else:     226                     sampler = SequentialSampler\*\*(dataset)\*\* C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\sampler.py in \_\_init\_\_(self, data\_source, replacement, num\_samples, generator)      92                              ""since a random permute will be performed."")      93 ---&gt; 94 if not isinstance\*\*(self.num\_samples,\*\* int\*\*)\*\* or self\*\*.num\_samples &lt;= 0:      95             raise ValueError(""num\_samples should be a positive integer ""      96                              ""value, but got num\_samples={}"".format(self.num\_samples))  C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\sampler.py in num\_samples(self)\*\*     100 # dataset size might change at runtime     101 if self\*\*.\_num\_samples is None: --&gt; 102 return len(self.data\_source)\*\*     103 return self\*\*.\*\*\_num\_samples     104 TypeError: object of type 'TextClassificationDataset' has no len()",t2_8ajm2akc,False,,0,False,TypeError: object of type 'TextClassificationDataset' has no len(),[],r/pytorch,False,6,,0,,,False,t3_j2odi8,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1601508839.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;hi every one, i used torch and python on jupyter to text classification, but i got the error below and couldnt solve it yet, please help me:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TypeError&lt;/strong&gt;: object of type &amp;#39;TextClassificationDataset&amp;#39; has no len() &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class TextClassificationDataset(Dataset):
def __init__(self, path, tokenizer,
split=&amp;#39;train&amp;#39;,
vocab_path=&amp;#39;vocab.pkl&amp;#39;,
max_len=100, min_count=10):
self.path=path
assert split in[&amp;#39;train&amp;#39;,&amp;#39;test&amp;#39;]
self.split=split
self.vocab_path=vocab_path
self.tokenizer=tokenizer
self.max_len=max_len
self.min_count=min_count
self.cash={}
self.vocab=None
self.classes=[]
self.class_to_index={}
self.text_files=[]
split_path=f&amp;#39;{path}/{split}&amp;#39;
for cls_idx,lable in enumerate(os.listdir(split_path)):
text_files=[(fname,cls_idx)for fname in glob(f&amp;#39;{split_path}/{lable}/*.txt&amp;#39;)]
self.text_files+=text_files
self.classes+=[lable]
self.class_to_index[lable]=cls_idx
self.num_classes=len(self.classes)
#build vocabulary from training &amp;amp; validation texts
self.build_vocab()
def _getitem_(self,index):
#read the tokenized text file &amp;amp; its lable (neg=0 , pos=1)
fname,class_idx=self.text_files[index]
if fname in self.cash:
return self.cash[fname],class_idx
#read text file
text=open(fname).read()
#tokenize the text file
tokens=self.tokenizer(text.lower())
#padding &amp;amp; trimming
if(len(tokens)&amp;lt;self.max_len):
num_pads=self.max_len-len(tokens)
tokens=[PAD]*num_pads+tokens
elif len(tokens)&amp;gt;self.max_len:
tokens=tokens[:self.max_len]
#numericalizing

ids=torch.LongTensor(self.max_len)
for i,word in enumerate(tokens):
if word not in self.vocab.word2index:
#unknown words
ids[i]=self.vocab.word2index[UNK]
elif word !=PAD and self.vocab.word2count[word]&amp;lt;self.min_count:
ids[i]=self_vocab.word2index[UNK] #rare words
else:
ids[i]=self.vocab.word2index[word]
#save in cash for future use
self.cash[fname]=ids
return ids,class_idx
def _len_(self):
return (len(self.text_files))
def build_vocab(self):
if not os.path.exists(self.vocab_path):
vocab=Vocabulary(self.tokenizer)
filenames=glob(f&amp;#39;{data_dir}/*/*/*.txt&amp;#39;)
for filename in tqdm(filenames,desc=&amp;#39;Building Vocab&amp;#39;):
with open(filename, encoding=&amp;quot;utf-8&amp;quot;, errors=&amp;quot;ignore&amp;quot;) as f:
#lines = f.readlines()
#lines = [line.strip(&amp;quot;\n&amp;quot;) for line in lines]
for line in f:
#for line in lines:
vocab.add_sentence(line.lower())
#sort words by their frequencies
words=[(0,PAD),(0,UNK)]
words+=sorted([(c,w) for w,c in vocab.word2count.items()],reverse=True)
self.vocab=Vocabulary(self.tokenizer)
for i,(count,word)in enumerate(words):
self.vocab.word2index[word]=i
self.vocab.word2count[word]=count
self.vocab.index2word[i]=word
self.vocab.count+=1
pickle.dump(self.vocab,open(self.vocab_path,&amp;#39;wb&amp;#39;))
else:
self.vocab=pickle.load(open(self.vocab_path,&amp;#39;rb&amp;#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_ds= TextClassificationDataset(data_dir,tokenizer,&amp;#39;train&amp;#39;, vocab_path, max_len, min_count)
train_dl= DataLoader(train_ds,batch_size=batch_size,shuffle=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;TypeError                                 Traceback (most recent call last) &amp;lt;ipython-input-11-28061f6a874c&amp;gt; in &amp;lt;module&amp;gt;       1 train_ds= TextClassificationDataset**(data_dir,tokenizer,&amp;#39;train&amp;#39;,** vocab_path**,** max_len**,** min_count**)** ----&amp;gt; 2 train_dl= DataLoader**(train_ds,batch_size=batch_size,shuffle=True)**       3       4 valid_ds= TextClassificationDataset**(data_dir,tokenizer,&amp;#39;test&amp;#39;,vocab_path,max_len,min_count)**       5 valid_dl= DataLoader**(valid_ds,batch_size=batch_size,shuffle=False)** C:\ProgramData\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py in __init__(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator)     222 else: # map-style     223 if shuffle**:** --&amp;gt; 224 sampler = RandomSampler**(dataset,** generator=generator**)**     225 else:     226                     sampler = SequentialSampler**(dataset)** C:\ProgramData\Anaconda3\lib\site-packages\torch\utils\data\sampler.py in __init__(self, data_source, replacement, num_samples, generator)      92                              &amp;quot;since a random permute will be performed.&amp;quot;)      93 ---&amp;gt; 94 if not isinstance**(self.num_samples,** int**)** or self**.num_samples &amp;lt;= 0:      95             raise ValueError(&amp;quot;num_samples should be a positive integer &amp;quot;      96                              &amp;quot;value, but got num_samples={}&amp;quot;.format(self.num_samples))  C:\ProgramData\Anaconda3\lib\site-packages\torch\utils\data\sampler.py in num_samples(self)**     100 # dataset size might change at runtime     101 if self**._num_samples is None: --&amp;gt; 102 return len(self.data_source)**     103 return self**.**_num_samples     104 TypeError: object of type &amp;#39;TextClassificationDataset&amp;#39; has no len()&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j2odi8,True,,elisoo334,,3,True,all_ads,False,[],False,,/r/pytorch/comments/j2odi8/typeerror_object_of_type/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j2odi8/typeerror_object_of_type/,7135,1601480039.0,0,,False,,,,,,,,
317,,pytorch,"Hello, I have an automatically generated dataset that consists of sums of two signals (clean signal + a sine-like smooth curve) as an input and  those sine-like curves as an output. So, my task is to decompose an original signal. I’m trying to build a model for that, but nothing works well. I’ve already tried simple architecture like (Conv1D - MaxPooling - Dense)*3 but the accuracy is really low. Is it better to use LSTM’s for task like this? What would you suggest to try?",t2_55uv1pxw,False,,0,False,Signal to signal processing,[],r/pytorch,False,6,,0,,,False,t3_j2ff0b,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1601469011.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I have an automatically generated dataset that consists of sums of two signals (clean signal + a sine-like smooth curve) as an input and  those sine-like curves as an output. So, my task is to decompose an original signal. I’m trying to build a model for that, but nothing works well. I’ve already tried simple architecture like (Conv1D - MaxPooling - Dense)*3 but the accuracy is really low. Is it better to use LSTM’s for task like this? What would you suggest to try?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j2ff0b,True,,kivicode,,6,True,all_ads,False,[],False,,/r/pytorch/comments/j2ff0b/signal_to_signal_processing/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j2ff0b/signal_to_signal_processing/,7135,1601440211.0,1,,False,,,,,,,,
318,,pytorch,"Let say I have to add multiple dropouts in my neural network model how do I add dropouts to it.

Should I initialize dropout object once and use at multiple layers like this 

    dropout = nn.Dropout(0.2)


    x = dropout(x)
    .
    x = some layers(x)
    .
    x = dropout(x)
    .
    x = some layers(x)
    .
    x = dropout(x)

or I have to initialize different dropouts for different layers


    dropout1 = nn.Dropout(0.2)
    dropout2 = nn.Dropout(0.2)
    dropout3 = nn.Dropout(0.2)

    x = dropout1(x)
    .
    x = some layers(x)
    .
    x = dropout2(x)
    .
    x = some layers(x)
    .
    x = dropout3(x)",t2_es9xwhl,False,,0,False,How do we add multiple dropouts?,[],r/pytorch,False,6,,0,,,False,t3_j2f103,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1601467363.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let say I have to add multiple dropouts in my neural network model how do I add dropouts to it.&lt;/p&gt;

&lt;p&gt;Should I initialize dropout object once and use at multiple layers like this &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dropout = nn.Dropout(0.2)


x = dropout(x)
.
x = some layers(x)
.
x = dropout(x)
.
x = some layers(x)
.
x = dropout(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or I have to initialize different dropouts for different layers&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dropout1 = nn.Dropout(0.2)
dropout2 = nn.Dropout(0.2)
dropout3 = nn.Dropout(0.2)

x = dropout1(x)
.
x = some layers(x)
.
x = dropout2(x)
.
x = some layers(x)
.
x = dropout3(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j2f103,True,,begooboi,,2,True,all_ads,False,[],False,,/r/pytorch/comments/j2f103/how_do_we_add_multiple_dropouts/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j2f103/how_do_we_add_multiple_dropouts/,7135,1601438563.0,0,,False,,,,,,,,
319,,pytorch,"With annotation project revolving around aerial images, bounding boxes are very last season. 

Smart segmentation is claimed to be the way to go when it comes to annotating aerial images. 

[This article](https://blog.superannotate.com/annotations-for-aerial-imagery-why-pixel-precision-will-be-the-new-norm) presents the pros and cons of the old-fashioned methods as well as how the offered solution addresses all of those problems.",t2_810pw8v7,False,,0,False,Annotations for Aerial Imagery: Why Pixel Precision Will Be the New Norm,[],r/pytorch,False,6,,0,,,False,t3_j1fnxo,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1601340527.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;With annotation project revolving around aerial images, bounding boxes are very last season. &lt;/p&gt;

&lt;p&gt;Smart segmentation is claimed to be the way to go when it comes to annotating aerial images. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://blog.superannotate.com/annotations-for-aerial-imagery-why-pixel-precision-will-be-the-new-norm""&gt;This article&lt;/a&gt; presents the pros and cons of the old-fashioned methods as well as how the offered solution addresses all of those problems.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BMcMl-rrastxTM0sLWvoz9qJjPHKOWCj9HzFdGcWorY.jpg?auto=webp&amp;s=80615dff0f9743197bf75b7dc719022f5785a29a', 'width': 1600, 'height': 637}, 'resolutions': [{'url': 'https://external-preview.redd.it/BMcMl-rrastxTM0sLWvoz9qJjPHKOWCj9HzFdGcWorY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6bb2b39935a68522ce012791a378fbd56c8df21', 'width': 108, 'height': 42}, {'url': 'https://external-preview.redd.it/BMcMl-rrastxTM0sLWvoz9qJjPHKOWCj9HzFdGcWorY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a788b25961fcf01babbaabb706dd685d30ee9528', 'width': 216, 'height': 85}, {'url': 'https://external-preview.redd.it/BMcMl-rrastxTM0sLWvoz9qJjPHKOWCj9HzFdGcWorY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f4bc448bdb431db994fed725b3c445d12abf642e', 'width': 320, 'height': 127}, {'url': 'https://external-preview.redd.it/BMcMl-rrastxTM0sLWvoz9qJjPHKOWCj9HzFdGcWorY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=09faf285a69dcc0a026357d1567c5eb501c9f43f', 'width': 640, 'height': 254}, {'url': 'https://external-preview.redd.it/BMcMl-rrastxTM0sLWvoz9qJjPHKOWCj9HzFdGcWorY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e33f5f808e88b721fe32d24f0b0c5ef2cee623c', 'width': 960, 'height': 382}, {'url': 'https://external-preview.redd.it/BMcMl-rrastxTM0sLWvoz9qJjPHKOWCj9HzFdGcWorY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f98de6581cc5e916205c677456b9dd7ef5f41a12', 'width': 1080, 'height': 429}], 'variants': {}, 'id': 't-jzc8q1nAaxIfCHtCS_ObYNjyt-0WviKJxKiuUvMKY'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j1fnxo,True,,WeekendClassic,,0,True,all_ads,False,[],False,,/r/pytorch/comments/j1fnxo/annotations_for_aerial_imagery_why_pixel/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j1fnxo/annotations_for_aerial_imagery_why_pixel/,7135,1601311727.0,0,,False,,,,,,,,
320,,pytorch,"For a further education I have analyzed PyTorch for image classification (with Kaggle Dogs vs. Cats). In particular I investigated what influences the quality/accuracy of the results.

I claim that the following points are most important (sorted by importance):

- amount of data (I recommend at least 20’000 images for categorizations with two types, DogsVsCats example have 25’000)
- data quality
- division Train/Test-Data (ideal results at 90%/10%)
- model used
- script modifications (e.g. optimizer, transforms)

The following properties have a small influence on the accuracy:

- Size of the images
- Alignment of the images

Furthermore I claim that the hardware used (CPU/CUDA) does not influence the quality of the results, but clearly the duration.

One more question for long-time users: Have new versions of PyTorch significantly improved the quality/accuracy or is that just model related? Unfortunately I cannot test this.

Would you accept this as a professor or have I forgotten important points?

Cheers",t2_36mdykf6,False,,0,False,What affects quality/accuracy of image classification results?,[],r/pytorch,False,6,,0,,,False,t3_j0reeo,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1601243326.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For a further education I have analyzed PyTorch for image classification (with Kaggle Dogs vs. Cats). In particular I investigated what influences the quality/accuracy of the results.&lt;/p&gt;

&lt;p&gt;I claim that the following points are most important (sorted by importance):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;amount of data (I recommend at least 20’000 images for categorizations with two types, DogsVsCats example have 25’000)&lt;/li&gt;
&lt;li&gt;data quality&lt;/li&gt;
&lt;li&gt;division Train/Test-Data (ideal results at 90%/10%)&lt;/li&gt;
&lt;li&gt;model used&lt;/li&gt;
&lt;li&gt;script modifications (e.g. optimizer, transforms)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following properties have a small influence on the accuracy:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Size of the images&lt;/li&gt;
&lt;li&gt;Alignment of the images&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Furthermore I claim that the hardware used (CPU/CUDA) does not influence the quality of the results, but clearly the duration.&lt;/p&gt;

&lt;p&gt;One more question for long-time users: Have new versions of PyTorch significantly improved the quality/accuracy or is that just model related? Unfortunately I cannot test this.&lt;/p&gt;

&lt;p&gt;Would you accept this as a professor or have I forgotten important points?&lt;/p&gt;

&lt;p&gt;Cheers&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j0reeo,True,,F0relli,,4,True,all_ads,False,[],False,,/r/pytorch/comments/j0reeo/what_affects_qualityaccuracy_of_image/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j0reeo/what_affects_qualityaccuracy_of_image/,7135,1601214526.0,0,,False,,,,,,,,
321,,pytorch,"Is the **Libtorch C++ frontend** a good choice for both learning/training and running inference in PyTorch? I bet ya, yes. It is now easier to use than ever.

https://i.redd.it/n8nh5filgjp51.gif

Today I am publishing **TorchRayLib++** [https://github.com/QuantScientist/TorchRayLib](https://github.com/QuantScientist/TorchRayLib): A CMake based integration of the RayLib ([https://www.raylib.com/examples.html](https://www.raylib.com/examples.html)) GUI library and the Libtorch C++ Deep Learning Library. The story: for weeks I have been scavenging the internet for a lovely-looking GUI to play with my AI experiments which are mostly in Libtorch C++/ PyTorch. I was looking for something which is CMake based, does not have many dependencies, multi-platform, has a small footprint, and hopefully can be ported to WASM. I left no stone unturned in my quest for perfection, as I was familiar with many of the available GUI options I started integrating PyTorch into them to see how they work.

&amp;#x200B;

[TorchRayLib++](https://preview.redd.it/hc79eyb44qp51.png?width=920&amp;format=png&amp;auto=webp&amp;s=b8ba0f5063aa87a48a3422475e37ed4f1cdcd6cc)

The list included (there were many more but these are the better ones):

**Juce** – a well respected C++ based audio library with a magnificent UI

**Imgui**\- One of the most popular gaming UI’s today

**NanoGUI** – an old one which was revived lately be the mistuba2 rendering group

**Raylib**\- A C based library which until not long ago, did not have proper CMake support

To make a long story short, I tested and integrated and debugged compilers (LLVM, VC, GCC you name it ..) with libtorch, at times almost losing my mind overs small compilation issues which even the experts can not solve.

At the last minute, I ditched NanoGUI and finally integrated raylib. Written in C/C++, Raylib is a beautiful and widely used UI library which not only has a very large user base but binding for dozens of languages. It can also be easily deployed to the web using web assembly (see [https://www.raylib.com/examples.html](https://www.raylib.com/examples.html))

&amp;#x200B;

https://i.redd.it/d2e42bqt8jp51.gif

I am now in the process of porting many of the original raylib examples to TorchRayLib and hopefully, it will be adopted by others in the community.

See also:

[https://github.com/prabhuomkar/pytorch-cpp](https://github.com/prabhuomkar/pytorch-cpp)

[https://github.com/koba-jon/pytorch\_cpp](https://github.com/koba-jon/pytorch_cpp)

u/pytorch u/raylib",t2_4uymcujd,False,,0,False,TorchRayLib++: A RayLib based UI for the Libtorch C++ Deep Learning Library.,[],r/pytorch,False,6,,0,70.0,,False,t3_j0at3h,False,dark,0.82,,public,7,0,{},70.0,,False,[],,False,False,,{},,False,7,,False,https://b.thumbs.redditmedia.com/IHNpULdGJ_0J1tP_9grB2uKLgQ8DUtD63d_fFQEcv5c.jpg,1601315103.0,,[],{},self,,True,,1601172553.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is the &lt;strong&gt;Libtorch C++ frontend&lt;/strong&gt; a good choice for both learning/training and running inference in PyTorch? I bet ya, yes. It is now easier to use than ever.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/n8nh5filgjp51.gif""&gt;https://i.redd.it/n8nh5filgjp51.gif&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Today I am publishing &lt;strong&gt;TorchRayLib++&lt;/strong&gt; &lt;a href=""https://github.com/QuantScientist/TorchRayLib""&gt;https://github.com/QuantScientist/TorchRayLib&lt;/a&gt;: A CMake based integration of the RayLib (&lt;a href=""https://www.raylib.com/examples.html""&gt;https://www.raylib.com/examples.html&lt;/a&gt;) GUI library and the Libtorch C++ Deep Learning Library. The story: for weeks I have been scavenging the internet for a lovely-looking GUI to play with my AI experiments which are mostly in Libtorch C++/ PyTorch. I was looking for something which is CMake based, does not have many dependencies, multi-platform, has a small footprint, and hopefully can be ported to WASM. I left no stone unturned in my quest for perfection, as I was familiar with many of the available GUI options I started integrating PyTorch into them to see how they work.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/hc79eyb44qp51.png?width=920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b8ba0f5063aa87a48a3422475e37ed4f1cdcd6cc""&gt;TorchRayLib++&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The list included (there were many more but these are the better ones):&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Juce&lt;/strong&gt; – a well respected C++ based audio library with a magnificent UI&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Imgui&lt;/strong&gt;- One of the most popular gaming UI’s today&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NanoGUI&lt;/strong&gt; – an old one which was revived lately be the mistuba2 rendering group&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Raylib&lt;/strong&gt;- A C based library which until not long ago, did not have proper CMake support&lt;/p&gt;

&lt;p&gt;To make a long story short, I tested and integrated and debugged compilers (LLVM, VC, GCC you name it ..) with libtorch, at times almost losing my mind overs small compilation issues which even the experts can not solve.&lt;/p&gt;

&lt;p&gt;At the last minute, I ditched NanoGUI and finally integrated raylib. Written in C/C++, Raylib is a beautiful and widely used UI library which not only has a very large user base but binding for dozens of languages. It can also be easily deployed to the web using web assembly (see &lt;a href=""https://www.raylib.com/examples.html""&gt;https://www.raylib.com/examples.html&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/d2e42bqt8jp51.gif""&gt;https://i.redd.it/d2e42bqt8jp51.gif&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I am now in the process of porting many of the original raylib examples to TorchRayLib and hopefully, it will be adopted by others in the community.&lt;/p&gt;

&lt;p&gt;See also:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/prabhuomkar/pytorch-cpp""&gt;https://github.com/prabhuomkar/pytorch-cpp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/koba-jon/pytorch_cpp""&gt;https://github.com/koba-jon/pytorch_cpp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""/u/pytorch""&gt;u/pytorch&lt;/a&gt; &lt;a href=""/u/raylib""&gt;u/raylib&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/U5V1eTEOH_nfEAFb-m7BnT6XpzSDGxCci6DggPnX-Hk.jpg?auto=webp&amp;s=46c91a7aa0739b3cd225b0bb1b52b11d091c4ee9', 'width': 102, 'height': 102}, 'resolutions': [], 'variants': {}, 'id': 'VNUEFVr_6xBiW1vqMrh4hgE-OZ4i3ZeXK_LSNnp-3ec'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j0at3h,True,,QuanstScientist,,0,True,all_ads,False,[],False,,/r/pytorch/comments/j0at3h/torchraylib_a_raylib_based_ui_for_the_libtorch_c/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j0at3h/torchraylib_a_raylib_based_ui_for_the_libtorch_c/,7135,1601143753.0,4,,False,,,,"{'n8nh5filgjp51': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 88, 'x': 108, 'u': 'https://preview.redd.it/n8nh5filgjp51.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=5dd333a75376f59d0e4c7f7c9a436af948657aa5'}, {'y': 177, 'x': 216, 'u': 'https://preview.redd.it/n8nh5filgjp51.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=cd77fff55c1067f5855821df5d3a236a1429da9d'}, {'y': 262, 'x': 320, 'u': 'https://preview.redd.it/n8nh5filgjp51.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=fab5de3b781417ffc4d9a4d07df8e064fc7749b6'}], 's': {'y': 493, 'gif': 'https://i.redd.it/n8nh5filgjp51.gif', 'mp4': 'https://preview.redd.it/n8nh5filgjp51.gif?format=mp4&amp;s=3df991c480b95381b195d2d7a8c2408a75a20c43', 'x': 600}, 'id': 'n8nh5filgjp51'}, 'd2e42bqt8jp51': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 65, 'x': 108, 'u': 'https://preview.redd.it/d2e42bqt8jp51.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=cde66cb24f6ee11b5f6177d68fcc414459cde6a7'}, {'y': 130, 'x': 216, 'u': 'https://preview.redd.it/d2e42bqt8jp51.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=a6a935818f19f17e0a1e06a23e80966b60ce3d1c'}, {'y': 193, 'x': 320, 'u': 'https://preview.redd.it/d2e42bqt8jp51.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=1baad889607ed65bb1919d28b97866d794c4c467'}], 's': {'y': 362, 'gif': 'https://i.redd.it/d2e42bqt8jp51.gif', 'mp4': 'https://preview.redd.it/d2e42bqt8jp51.gif?format=mp4&amp;s=ffd5ad52806934f9499f929287f3dd08c572b28f', 'x': 600}, 'id': 'd2e42bqt8jp51'}, 'hc79eyb44qp51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 81, 'x': 108, 'u': 'https://preview.redd.it/hc79eyb44qp51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f430d996d001ce16bb22402d8cdd016ddabd4de'}, {'y': 162, 'x': 216, 'u': 'https://preview.redd.it/hc79eyb44qp51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3048f9d0b32776c77881169b8680ef6c650e3792'}, {'y': 240, 'x': 320, 'u': 'https://preview.redd.it/hc79eyb44qp51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8040127b6a11d9a854d072c16cba36f1a56d9e41'}, {'y': 480, 'x': 640, 'u': 'https://preview.redd.it/hc79eyb44qp51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f2544eaefe514128bdc6bb9517ddd06f6d8de5'}], 's': {'y': 690, 'x': 920, 'u': 'https://preview.redd.it/hc79eyb44qp51.png?width=920&amp;format=png&amp;auto=webp&amp;s=b8ba0f5063aa87a48a3422475e37ed4f1cdcd6cc'}, 'id': 'hc79eyb44qp51'}}",,,,
322,,pytorch,"If I have a budget of $100 what would be the best online course, book or other learning material to help me learn AI with Pytorch?  I have a solid understanding of programming but only have a basic understanding of Python and it’s been 20 years since I’ve taken a math class.  I was going to just go on Udemy and find a course but figured I’d check this group first.  Thanks!",t2_e47u2,False,,0,False,What is the best path for me to learn Pytorch and AI?,[],r/pytorch,False,6,,0,,,False,t3_j05ca7,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1601152809.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If I have a budget of $100 what would be the best online course, book or other learning material to help me learn AI with Pytorch?  I have a solid understanding of programming but only have a basic understanding of Python and it’s been 20 years since I’ve taken a math class.  I was going to just go on Udemy and find a course but figured I’d check this group first.  Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j05ca7,True,,geo1999,,11,True,all_ads,False,[],False,,/r/pytorch/comments/j05ca7/what_is_the_best_path_for_me_to_learn_pytorch_and/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j05ca7/what_is_the_best_path_for_me_to_learn_pytorch_and/,7135,1601124009.0,0,,False,,,,,,,,
323,,pytorch,"I found torchtext to be really intuitive, hence I started with a sample binary classification problem.

It consists of a list of text and their corresponding label.
But upon loading it onto a torchtext dataset, I see the only way of using out of the box features --- torchtext.data.TabularDataset-- only allows us to load json/csv files .

Is there a way in which I could have the same functionality while using list as the input instead of files ?",t2_86cibicd,False,,0,False,Using list of text as input for torchtext.data.DataSet,[],r/pytorch,False,6,,0,,,False,t3_j04gci,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1601148642.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I found torchtext to be really intuitive, hence I started with a sample binary classification problem.&lt;/p&gt;

&lt;p&gt;It consists of a list of text and their corresponding label.
But upon loading it onto a torchtext dataset, I see the only way of using out of the box features --- torchtext.data.TabularDataset-- only allows us to load json/csv files .&lt;/p&gt;

&lt;p&gt;Is there a way in which I could have the same functionality while using list as the input instead of files ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,j04gci,True,,saphireforreal,,1,True,all_ads,False,[],False,,/r/pytorch/comments/j04gci/using_list_of_text_as_input_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/j04gci/using_list_of_text_as_input_for/,7135,1601119842.0,0,,False,,,,,,,,
324,,pytorch,"Hi,

I am building a model to predict a continuous variable from an input signal of a mixture of encoded categorical and continuous variables.

I am using some linear layers with LeakyReLUs, Batch Norm and dropouts in between.

The problem I’m facing with this model is that it is learning very  slowly and I’m not sure why. Not that it just requires many epochs to  train but that even then it plateaus and gets somewhat stuck. Further,  since it requires extensive training, it overfits and validation losses  increase.

Moreover, I have tried over models like Random Forest and XGBoost and it seems that they arrive at a similar accuracy in this case although I'm not sure if this is related. I was hoping to beat these models with a neural network.

I have followed some guides to improve model complexity (hence added  layers, batchnorm, dropouts and leaky relus), and while they have helped  my loss is still not as low as I would like as it doesn’t decrease enough each iteration. I have tried different learning rates but it doesn't seem as ""*smooth""* as I've experienced in the past.

I thought this model was relatively simple and so I’m confused as to why it isn’t learning effectively.

I’d really appreciate a second eye as I may have made a mistake somewhere. I have attached the relevant code below.

Thanks in advance!

Model:

    class Net(nn.Module):
        
        def __init__(self, input_size, hidden_size, output_size=1):
            super().__init__()
            self.fc1 = nn.Linear(input_size, 2048)
            self.fc2 = nn.Linear(2048, 1024)
            self.fc3 = nn.Linear(1024, hidden_size)
            self.fc4 = nn.Linear(hidden_size, output_size)
            
            self.relu = nn.LeakyReLU()
            self.dropout = nn.Dropout(p=0.4)
            
            self.norm1 = nn.BatchNorm1d(2048)
            self.norm2 = nn.BatchNorm1d(1024)
            self.norm3 = nn.BatchNorm1d(hidden_size)
    
        def forward(self, x):
    
            x = self.fc1(x)
            x = self.relu(x)
            x = self.dropout(x)
            x = self.norm1(x)
            x = self.fc2(x)
            x = self.relu(x)
            x = self.dropout(x)
            x = self.norm2(x)
            x = self.fc3(x)
            x = self.relu(x)
            x = self.norm3(x)
            x = self.fc4(x)
    
            return x

Train:

    def sub_train_(model, dataloader):
        model.train()
        losses = list()
        for idx, (X, y) in enumerate(dataloader):
                
            X, y = X.to(device), y.to(device)
                
            optimizer.zero_grad()
            out = model(X)
            loss = criterion(out.squeeze(), y)
            
            loss.backward()
            optimizer.step()
            losses.append(loss.item())
        return np.mean(losses), model  
    
    def train(model, trainloader, testloader, num_epochs):
        
        best_model = model
        best_loss = math.inf
        ts = time.time()
        losses = list()
        for epoch in range(num_epochs):
            
            train_loss, model = sub_train_(model, trainloader)
            test_loss = sub_valid_(model, testloader)
            losses.append(train_loss)
            
            if test_loss &lt; best_loss:
                best_loss = test_loss
                best_model = model
                
            if epoch % 20 == 0:
                print('Epoch: {}, train_loss: {}, test_loss: {}'.format(
                    epoch, train_loss, test_loss))
                
        te = time.time()
        
        # Plot Losses
        fig, ax = plt.subplots()
        ax.plot(range(num_epochs), losses)
        plt.show()
        
        mins = int((te-ts) / 60)
        secs = int((te-ts) % 60)
        
        print('Training completed in {} minutes, {} seconds.'.format(mins, secs))
        return losses, best_model

Initiailization:

    model = FundedDateNN(input_size, hidden_size).to(device)
    criterion = nn.MSELoss().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

This is my most recent training loss plot over 100,000 epochs...

https://preview.redd.it/w4nn6ou80fp51.png?width=390&amp;format=png&amp;auto=webp&amp;s=bb6ede54be6ae4c93ea6f5f2dbdf49732bc36cad

Thank you again to anyone who helps me out. I really really appreciate it.

&amp;#x200B;

Edit:

Validation accuracy values:

&amp;#x200B;

https://preview.redd.it/ar3zhc0ojfp51.png?width=603&amp;format=png&amp;auto=webp&amp;s=e101c07bd9df1ebc9db2104f5d142d1181d9b65d

&amp;#x200B;",t2_ebu4m,False,,0,False,"How can I improve my model training, loss gets stuck.",[],r/pytorch,False,6,,0,88.0,,False,t3_izyri1,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://a.thumbs.redditmedia.com/NPH__a3jGW155BmpK4lHWHFIBl2suUPYB31yHrPFTV8.jpg,1601108531.0,,[],{},,,True,,1601121271.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I am building a model to predict a continuous variable from an input signal of a mixture of encoded categorical and continuous variables.&lt;/p&gt;

&lt;p&gt;I am using some linear layers with LeakyReLUs, Batch Norm and dropouts in between.&lt;/p&gt;

&lt;p&gt;The problem I’m facing with this model is that it is learning very  slowly and I’m not sure why. Not that it just requires many epochs to  train but that even then it plateaus and gets somewhat stuck. Further,  since it requires extensive training, it overfits and validation losses  increase.&lt;/p&gt;

&lt;p&gt;Moreover, I have tried over models like Random Forest and XGBoost and it seems that they arrive at a similar accuracy in this case although I&amp;#39;m not sure if this is related. I was hoping to beat these models with a neural network.&lt;/p&gt;

&lt;p&gt;I have followed some guides to improve model complexity (hence added  layers, batchnorm, dropouts and leaky relus), and while they have helped  my loss is still not as low as I would like as it doesn’t decrease enough each iteration. I have tried different learning rates but it doesn&amp;#39;t seem as &amp;quot;&lt;em&gt;smooth&amp;quot;&lt;/em&gt; as I&amp;#39;ve experienced in the past.&lt;/p&gt;

&lt;p&gt;I thought this model was relatively simple and so I’m confused as to why it isn’t learning effectively.&lt;/p&gt;

&lt;p&gt;I’d really appreciate a second eye as I may have made a mistake somewhere. I have attached the relevant code below.&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;

&lt;p&gt;Model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Net(nn.Module):

    def __init__(self, input_size, hidden_size, output_size=1):
        super().__init__()
        self.fc1 = nn.Linear(input_size, 2048)
        self.fc2 = nn.Linear(2048, 1024)
        self.fc3 = nn.Linear(1024, hidden_size)
        self.fc4 = nn.Linear(hidden_size, output_size)

        self.relu = nn.LeakyReLU()
        self.dropout = nn.Dropout(p=0.4)

        self.norm1 = nn.BatchNorm1d(2048)
        self.norm2 = nn.BatchNorm1d(1024)
        self.norm3 = nn.BatchNorm1d(hidden_size)

    def forward(self, x):

        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.norm1(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.norm2(x)
        x = self.fc3(x)
        x = self.relu(x)
        x = self.norm3(x)
        x = self.fc4(x)

        return x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Train:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def sub_train_(model, dataloader):
    model.train()
    losses = list()
    for idx, (X, y) in enumerate(dataloader):

        X, y = X.to(device), y.to(device)

        optimizer.zero_grad()
        out = model(X)
        loss = criterion(out.squeeze(), y)

        loss.backward()
        optimizer.step()
        losses.append(loss.item())
    return np.mean(losses), model  

def train(model, trainloader, testloader, num_epochs):

    best_model = model
    best_loss = math.inf
    ts = time.time()
    losses = list()
    for epoch in range(num_epochs):

        train_loss, model = sub_train_(model, trainloader)
        test_loss = sub_valid_(model, testloader)
        losses.append(train_loss)

        if test_loss &amp;lt; best_loss:
            best_loss = test_loss
            best_model = model

        if epoch % 20 == 0:
            print(&amp;#39;Epoch: {}, train_loss: {}, test_loss: {}&amp;#39;.format(
                epoch, train_loss, test_loss))

    te = time.time()

    # Plot Losses
    fig, ax = plt.subplots()
    ax.plot(range(num_epochs), losses)
    plt.show()

    mins = int((te-ts) / 60)
    secs = int((te-ts) % 60)

    print(&amp;#39;Training completed in {} minutes, {} seconds.&amp;#39;.format(mins, secs))
    return losses, best_model
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Initiailization:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = FundedDateNN(input_size, hidden_size).to(device)
criterion = nn.MSELoss().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is my most recent training loss plot over 100,000 epochs...&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/w4nn6ou80fp51.png?width=390&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb6ede54be6ae4c93ea6f5f2dbdf49732bc36cad""&gt;https://preview.redd.it/w4nn6ou80fp51.png?width=390&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb6ede54be6ae4c93ea6f5f2dbdf49732bc36cad&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you again to anyone who helps me out. I really really appreciate it.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Edit:&lt;/p&gt;

&lt;p&gt;Validation accuracy values:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/ar3zhc0ojfp51.png?width=603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e101c07bd9df1ebc9db2104f5d142d1181d9b65d""&gt;https://preview.redd.it/ar3zhc0ojfp51.png?width=603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e101c07bd9df1ebc9db2104f5d142d1181d9b65d&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,izyri1,True,,Pepipasta,,14,True,all_ads,False,[],False,,/r/pytorch/comments/izyri1/how_can_i_improve_my_model_training_loss_gets/,all_ads,False,https://www.reddit.com/r/pytorch/comments/izyri1/how_can_i_improve_my_model_training_loss_gets/,7135,1601092471.0,0,,False,,,,"{'w4nn6ou80fp51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 68, 'x': 108, 'u': 'https://preview.redd.it/w4nn6ou80fp51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fbced31856c5ef451aa3ac9c22771edda606adb7'}, {'y': 136, 'x': 216, 'u': 'https://preview.redd.it/w4nn6ou80fp51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d13b61239adfe692a69388d1bc584dce8954c00'}, {'y': 201, 'x': 320, 'u': 'https://preview.redd.it/w4nn6ou80fp51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3e8fd14a110e6c9531a3fabffe9d1e763f1fc458'}], 's': {'y': 246, 'x': 390, 'u': 'https://preview.redd.it/w4nn6ou80fp51.png?width=390&amp;format=png&amp;auto=webp&amp;s=bb6ede54be6ae4c93ea6f5f2dbdf49732bc36cad'}, 'id': 'w4nn6ou80fp51'}, 'ar3zhc0ojfp51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 68, 'x': 108, 'u': 'https://preview.redd.it/ar3zhc0ojfp51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb6662c4b3df67efd2e4bb8f79b6dcb2bad44b46'}, {'y': 137, 'x': 216, 'u': 'https://preview.redd.it/ar3zhc0ojfp51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cdf41094c09e1980141a9971f4bbf61937a7ba7c'}, {'y': 203, 'x': 320, 'u': 'https://preview.redd.it/ar3zhc0ojfp51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b1f9671e10c4b24aea0c5bc31d41d0eaf50c5121'}], 's': {'y': 383, 'x': 603, 'u': 'https://preview.redd.it/ar3zhc0ojfp51.png?width=603&amp;format=png&amp;auto=webp&amp;s=e101c07bd9df1ebc9db2104f5d142d1181d9b65d'}, 'id': 'ar3zhc0ojfp51'}}",,,,
325,,pytorch,"Goal

There  have been many great NLP frameworks published in last years e.g.   AllenNLP, fasttext, torchtext, fastai, pytorch-nlp etc.However, this   brought a big complexity and chaos in selection and usage of these   frameworks. They all have advantages and disadvantages against each   other. One may not easily exploit them together easily and has to make a   decision. In this project, I aim to provide **a low level project structure**  in which any of these frameworks' advantages can be exploited easily.

Additionally  it ease run different experiments and monitor their results with  tensorboard.  You can also deploy the model of choice with just a  command.

If interested please check [the link to the repo](https://github.com/ahmetgunduz/pytorch-nlp-project-template).",t2_5ktgycvz,False,,0,False,A Light and Modular PyTorch NLP Project Template,[],r/pytorch,False,6,,0,,,False,t3_iy6qfb,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},self,,True,,1600881060.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Goal&lt;/p&gt;

&lt;p&gt;There  have been many great NLP frameworks published in last years e.g.   AllenNLP, fasttext, torchtext, fastai, pytorch-nlp etc.However, this   brought a big complexity and chaos in selection and usage of these   frameworks. They all have advantages and disadvantages against each   other. One may not easily exploit them together easily and has to make a   decision. In this project, I aim to provide &lt;strong&gt;a low level project structure&lt;/strong&gt;  in which any of these frameworks&amp;#39; advantages can be exploited easily.&lt;/p&gt;

&lt;p&gt;Additionally  it ease run different experiments and monitor their results with  tensorboard.  You can also deploy the model of choice with just a  command.&lt;/p&gt;

&lt;p&gt;If interested please check &lt;a href=""https://github.com/ahmetgunduz/pytorch-nlp-project-template""&gt;the link to the repo&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/iW0OcBeECSoA8GjnCyC81wICLgvarAUm4xZ7x3w5C5M.jpg?auto=webp&amp;s=aa896dfdd1871352dc7022195ce710a7fd6a4ed3', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/iW0OcBeECSoA8GjnCyC81wICLgvarAUm4xZ7x3w5C5M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=25151b8691474fdfe1e6d96a176a91ac45047223', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/iW0OcBeECSoA8GjnCyC81wICLgvarAUm4xZ7x3w5C5M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be0ce6a958a10389d94355ad1370bd18cdf787e0', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/iW0OcBeECSoA8GjnCyC81wICLgvarAUm4xZ7x3w5C5M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f8114f00f2589917480cc832b04a90156241c72b', 'width': 320, 'height': 320}], 'variants': {}, 'id': '6umJJcCN39Q9kC0gB_oLElpwbbEi3VwVjz55OFT-XFo'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iy6qfb,True,,Weary-Worldliness-48,,2,True,all_ads,False,[],False,,/r/pytorch/comments/iy6qfb/a_light_and_modular_pytorch_nlp_project_template/,all_ads,False,https://www.reddit.com/r/pytorch/comments/iy6qfb/a_light_and_modular_pytorch_nlp_project_template/,7135,1600852260.0,0,,False,,,,,,,,
326,,pytorch,Im new in deep learning how can i use lstm with softmax or crossentropyloss or nlloss im trying to do text classification,t2_3or5yy00,False,,0,False,How to use LSTM with softmax?,[],r/pytorch,False,6,,0,,,False,t3_ixnzh9,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1600812176.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Im new in deep learning how can i use lstm with softmax or crossentropyloss or nlloss im trying to do text classification&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ixnzh9,True,,dugidin0saur,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ixnzh9/how_to_use_lstm_with_softmax/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ixnzh9/how_to_use_lstm_with_softmax/,7135,1600783376.0,0,,False,,,,,,,,
327,,pytorch,"I am curious if Pytorch would support for Julia as like C++.

As all other, I also personally found c++ as jibrish, instead Julia which is as fast as c and as pythonic as python.

I would like to see if it happens in future.",t2_7iurgttg,False,,0,False,Pytorch support for Julia,[],r/pytorch,False,6,,0,,,False,t3_ix20wp,False,dark,0.45,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1600729111.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am curious if Pytorch would support for Julia as like C++.&lt;/p&gt;

&lt;p&gt;As all other, I also personally found c++ as jibrish, instead Julia which is as fast as c and as pythonic as python.&lt;/p&gt;

&lt;p&gt;I would like to see if it happens in future.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ix20wp,True,,TopFar5998,,7,True,all_ads,False,[],False,,/r/pytorch/comments/ix20wp/pytorch_support_for_julia/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ix20wp/pytorch_support_for_julia/,7135,1600700311.0,0,,False,,,,,,,,
328,,pytorch,"Hey guys! I was recently working on neural network attacks and defenses, and will like to share with you an article I wrote on how to implement them in PyTorch. Feel free to have a look:

[https://towardsdatascience.com/adversarial-attack-and-defense-on-neural-networks-in-pytorch-82b5bcd9171](https://towardsdatascience.com/adversarial-attack-and-defense-on-neural-networks-in-pytorch-82b5bcd9171)",t2_jn2eq6v,False,,0,False,Using PyTorch to build neural network attacks,[],r/pytorch,False,6,,0,,,False,t3_iwecms,False,dark,0.79,,public,8,0,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},self,,True,,1600639239.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys! I was recently working on neural network attacks and defenses, and will like to share with you an article I wrote on how to implement them in PyTorch. Feel free to have a look:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://towardsdatascience.com/adversarial-attack-and-defense-on-neural-networks-in-pytorch-82b5bcd9171""&gt;https://towardsdatascience.com/adversarial-attack-and-defense-on-neural-networks-in-pytorch-82b5bcd9171&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/7YUtbNt1YrG6O2oWZOL6dQp5Wzexc3ZknEgG2c5-11c.jpg?auto=webp&amp;s=4beb551d64f2e008479b0b02f8783cb555be297a', 'width': 700, 'height': 280}, 'resolutions': [{'url': 'https://external-preview.redd.it/7YUtbNt1YrG6O2oWZOL6dQp5Wzexc3ZknEgG2c5-11c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=14de295fe92e18b3f61a91ed0d5192e5177bfa69', 'width': 108, 'height': 43}, {'url': 'https://external-preview.redd.it/7YUtbNt1YrG6O2oWZOL6dQp5Wzexc3ZknEgG2c5-11c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7bde9343d70a8e292da0e3138e0f9af5aaa94b11', 'width': 216, 'height': 86}, {'url': 'https://external-preview.redd.it/7YUtbNt1YrG6O2oWZOL6dQp5Wzexc3ZknEgG2c5-11c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f275449d154927e883abab6616027767ca0fccf', 'width': 320, 'height': 128}, {'url': 'https://external-preview.redd.it/7YUtbNt1YrG6O2oWZOL6dQp5Wzexc3ZknEgG2c5-11c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6367c2bd94000f7a17057a232071a84c82ac639', 'width': 640, 'height': 256}], 'variants': {}, 'id': 'd2JG0vF9DVA0kwASjA5tSLSSbgOVL2Uifc2WH0bN0iU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iwecms,True,,tt12343,,0,True,all_ads,False,[],False,,/r/pytorch/comments/iwecms/using_pytorch_to_build_neural_network_attacks/,all_ads,False,https://www.reddit.com/r/pytorch/comments/iwecms/using_pytorch_to_build_neural_network_attacks/,7135,1600610439.0,0,,False,,,,,,,,
329,,pytorch,,t2_63ay634e,False,,0,False,"A new browser extension for finding code for ML research papers on the internet (on Google, Arxiv, Scholar, Twitter, Github)",[],r/pytorch,False,6,,0,140.0,,False,t3_ivz4yp,False,dark,1.0,,public,10,0,{},140.0,,False,[],,False,False,,{},,False,10,,False,https://b.thumbs.redditmedia.com/GkF9zjZNnjH1Zqtjmw3jmxuy2IPnXcnIlmShYOx4YLU.jpg,False,,[],{},link,,False,,1600572677.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?auto=webp&amp;s=b2ec1cfa2b9e720731d4dc172cbef8096fb37c3a', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad5435b4b2d94d9df9309e4d5ceb4bc021b52783', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'TIQjNUpyCsCWmTM49ZEEAiVQDWg07G_PjT8zXCX4wSE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ivz4yp,True,,cv2020br,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ivz4yp/a_new_browser_extension_for_finding_code_for_ml/,all_ads,False,/r/LatestInML/comments/ivyqat/a_new_browser_extension_for_finding_code_for_ml/,7135,1600543877.0,0,,False,/r/LatestInML/comments/ivyqat/a_new_browser_extension_for_finding_code_for_ml/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'ICYMI: A New browser extension that finds code implementations for machine learning papers anywhere on the web (Google, Arxiv, Twitter, Scholar, and other sites)!\n\n[link to chrome extension](https://chrome.google.com/webstore/detail/mlai-code-implementation/aikkeehnlfpamidigaffhfmgbkdeheil)  \nor  \n[link to firefox extension](https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/)\n\nhttps://preview.redd.it/kyvp5fh5l5o51.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=1155e81c4ec11447bd5fb37dedebed35e0afc902', 'author_fullname': 't2_63ay634e', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'A new browser extension for finding code for ML research papers on the internet (on Google, Arxiv, Scholar, Twitter, Github)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'kyvp5fh5l5o51': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 135, 'x': 108, 'u': 'https://preview.redd.it/kyvp5fh5l5o51.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e90f2437ec633614dd436753a0394287c84b486'}, {'y': 270, 'x': 216, 'u': 'https://preview.redd.it/kyvp5fh5l5o51.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7e51347d47fd494890176eefb790edf5a4812a43'}, {'y': 400, 'x': 320, 'u': 'https://preview.redd.it/kyvp5fh5l5o51.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c2100caef681ce7eda4784cc52d1dfcd4be075b1'}, {'y': 800, 'x': 640, 'u': 'https://preview.redd.it/kyvp5fh5l5o51.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=50e85a74ecf8b64bca87197ba5d619a08908ead2'}, {'y': 1200, 'x': 960, 'u': 'https://preview.redd.it/kyvp5fh5l5o51.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bdc420c293c48a594ad2247cd4579322e6f41214'}, {'y': 1350, 'x': 1080, 'u': 'https://preview.redd.it/kyvp5fh5l5o51.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6f088383ae7ce766c8f2378b2ba6acf76e9099d3'}], 's': {'y': 1350, 'x': 1080, 'u': 'https://preview.redd.it/kyvp5fh5l5o51.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=1155e81c4ec11447bd5fb37dedebed35e0afc902'}, 'id': 'kyvp5fh5l5o51'}}, 'name': 't3_ivyqat', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 40, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 40, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/GkF9zjZNnjH1Zqtjmw3jmxuy2IPnXcnIlmShYOx4YLU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1600571335.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;ICYMI: A New browser extension that finds code implementations for machine learning papers anywhere on the web (Google, Arxiv, Twitter, Scholar, and other sites)!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://chrome.google.com/webstore/detail/mlai-code-implementation/aikkeehnlfpamidigaffhfmgbkdeheil""&gt;link to chrome extension&lt;/a&gt;&lt;br/&gt;\nor&lt;br/&gt;\n&lt;a href=""https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/""&gt;link to firefox extension&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/kyvp5fh5l5o51.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1155e81c4ec11447bd5fb37dedebed35e0afc902""&gt;https://preview.redd.it/kyvp5fh5l5o51.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1155e81c4ec11447bd5fb37dedebed35e0afc902&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?auto=webp&amp;s=b2ec1cfa2b9e720731d4dc172cbef8096fb37c3a', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad5435b4b2d94d9df9309e4d5ceb4bc021b52783', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'TIQjNUpyCsCWmTM49ZEEAiVQDWg07G_PjT8zXCX4wSE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ivyqat', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'cv2020br', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/ivyqat/a_new_browser_extension_for_finding_code_for_ml/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/ivyqat/a_new_browser_extension_for_finding_code_for_ml/', 'subreddit_subscribers': 6676, 'created_utc': 1600542535.0, 'num_crossposts': 19, 'media': None, 'is_video': False}]",t3_ivyqat,,,,,
330,,pytorch," Hello,

The FashionMNIST dataset has 10 different output classes. How can I get a subset of this dataset with only specific classes? In my case, I only want images of sneaker, pullover, sandal and shirt classes (their classes are 7,2,5 and 6 respectively).

This is how I load my dataset.  
`train_dataset_full = torchvision.datasets.FashionMNIST(data_folder, train = True, download = True, transform = transforms.ToTensor())`

The approach I’ve followed is below.  
Iterate through the dataset, one by one, then compare the 1st element (i.e. class) in the returned tuple to my required class. I’m stuck here. If the value returned is true, how can I append/add this observation to an empty dataset?

`sneaker = 0` 

`pullover = 0` 

`sandal = 0` 

`shirt = 0` 

`for i in range(60000):`     

	`if train_dataset_full[i][1] == 7:`         

		`sneaker += 1 # assign train_dataset_full[i] to another empty dataset`     

	`elif train_dataset_full[i][1] == 2:`         

		`pullover += 1 # assign train_dataset_full[i] to another empty dataset`     

	`elif train_dataset_full[i][1] == 5:`         

		`sandal += 1 # assign train_dataset_full[i] to another empty dataset`     

	`elif train_dataset_full[i][1] == 6:`         

		`shirt += 1 # assign train_dataset_full[i] to another empty dataset` 

If the above approach is incorrect, please suggest another method.",t2_6ctrt80i,False,,0,False,How to get only specific classes from FashionMNIST?,[],r/pytorch,False,6,,0,,,False,t3_iw5utt,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1600598381.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;The FashionMNIST dataset has 10 different output classes. How can I get a subset of this dataset with only specific classes? In my case, I only want images of sneaker, pullover, sandal and shirt classes (their classes are 7,2,5 and 6 respectively).&lt;/p&gt;

&lt;p&gt;This is how I load my dataset.&lt;br/&gt;
&lt;code&gt;train_dataset_full = torchvision.datasets.FashionMNIST(data_folder, train = True, download = True, transform = transforms.ToTensor())&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The approach I’ve followed is below.&lt;br/&gt;
Iterate through the dataset, one by one, then compare the 1st element (i.e. class) in the returned tuple to my required class. I’m stuck here. If the value returned is true, how can I append/add this observation to an empty dataset?&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sneaker = 0&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;pullover = 0&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;sandal = 0&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;shirt = 0&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;for i in range(60000):&lt;/code&gt;     &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`if train_dataset_full[i][1] == 7:`         

    `sneaker += 1 # assign train_dataset_full[i] to another empty dataset`     

`elif train_dataset_full[i][1] == 2:`         

    `pullover += 1 # assign train_dataset_full[i] to another empty dataset`     

`elif train_dataset_full[i][1] == 5:`         

    `sandal += 1 # assign train_dataset_full[i] to another empty dataset`     

`elif train_dataset_full[i][1] == 6:`         

    `shirt += 1 # assign train_dataset_full[i] to another empty dataset` 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the above approach is incorrect, please suggest another method.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iw5utt,True,,focal_fossa,,0,True,all_ads,False,[],False,,/r/pytorch/comments/iw5utt/how_to_get_only_specific_classes_from_fashionmnist/,all_ads,False,https://www.reddit.com/r/pytorch/comments/iw5utt/how_to_get_only_specific_classes_from_fashionmnist/,7135,1600569581.0,0,,False,,,,,,,,
331,,pytorch,"Probably was asked before, but I can't find a good answer.

I have a Tensor of shape A,B, C and one A,B,C,D. 

A,B and C are identical between them. 

I want to add the numbers fpr A,B and C and leave the values in D intact and the resulting Tensor should have shape A,B,C,D

Sounds so simple yet there is no easy way to do this? Do I have to go with loops over the entries in each tensor for this?",t2_cjsqo,False,,0,False,What's the best way to add Tensors of differing sizes?,[],r/pytorch,False,6,,0,,,False,t3_ivwh1u,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1600564159.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Probably was asked before, but I can&amp;#39;t find a good answer.&lt;/p&gt;

&lt;p&gt;I have a Tensor of shape A,B, C and one A,B,C,D. &lt;/p&gt;

&lt;p&gt;A,B and C are identical between them. &lt;/p&gt;

&lt;p&gt;I want to add the numbers fpr A,B and C and leave the values in D intact and the resulting Tensor should have shape A,B,C,D&lt;/p&gt;

&lt;p&gt;Sounds so simple yet there is no easy way to do this? Do I have to go with loops over the entries in each tensor for this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ivwh1u,True,,ReasonablyBadass,,4,True,all_ads,False,[],False,,/r/pytorch/comments/ivwh1u/whats_the_best_way_to_add_tensors_of_differing/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ivwh1u/whats_the_best_way_to_add_tensors_of_differing/,7135,1600535359.0,0,,False,,,,,,,,
332,,pytorch,,t2_7q1hpywu,False,,0,False,"Chrome/Firefox Extension for showing *code* for ML/AI research papers directly on Google, Arxiv, Scholar, Twitter, Github, and more!!",[],r/pytorch,False,6,,0,140.0,,False,t3_ivc0ej,False,dark,1.0,,public,10,0,{},140.0,,False,[],,False,False,,{},,False,10,,False,https://a.thumbs.redditmedia.com/CQbsSjB7RX0gi5bTwglGZP1zk4zWgxVFTqbT7RR5bb0.jpg,False,,[],{},link,,False,,1600483649.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?auto=webp&amp;s=b2ec1cfa2b9e720731d4dc172cbef8096fb37c3a', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad5435b4b2d94d9df9309e4d5ceb4bc021b52783', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'TIQjNUpyCsCWmTM49ZEEAiVQDWg07G_PjT8zXCX4wSE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ivc0ej,True,,fullerhouse570,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ivc0ej/chromefirefox_extension_for_showing_code_for_mlai/,all_ads,False,/r/LatestInML/comments/ivbug3/chromefirefox_extension_for_showing_code_for_mlai/,7135,1600454849.0,0,,False,/r/LatestInML/comments/ivbug3/chromefirefox_extension_for_showing_code_for_mlai/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'Browser extension that finds code implementations for machine learning papers anywhere on the web (Google, Arxiv, Twitter, Scholar, and other sites)!\n\n[link to chrome extension](https://chrome.google.com/webstore/detail/mlai-code-implementation/aikkeehnlfpamidigaffhfmgbkdeheil)  \nor  \n[link to firefox extension](https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/)\n\nhttps://preview.redd.it/oq2z2chvayn51.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=f53f186c6a813a9c1d165c609874b60713a89975', 'author_fullname': 't2_7q1hpywu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Chrome/Firefox Extension for showing *code* for ML/AI research papers directly on Google, Arxiv, Scholar, Twitter, Github, and more!!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'oq2z2chvayn51': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 135, 'x': 108, 'u': 'https://preview.redd.it/oq2z2chvayn51.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=11dca807971a97d5bc6e94e21662efd84290f318'}, {'y': 270, 'x': 216, 'u': 'https://preview.redd.it/oq2z2chvayn51.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f191a7bd0706c9b9ea8b2261773cb1d259da52f7'}, {'y': 400, 'x': 320, 'u': 'https://preview.redd.it/oq2z2chvayn51.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8831e496af858d10c905fafd100d47bf2f5975f'}, {'y': 800, 'x': 640, 'u': 'https://preview.redd.it/oq2z2chvayn51.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=63129605d40ff04ffc91d508081333c3f66b7fc9'}, {'y': 1200, 'x': 960, 'u': 'https://preview.redd.it/oq2z2chvayn51.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=25aa76ad000c3c1f8b97c1b110e078015102c370'}, {'y': 1350, 'x': 1080, 'u': 'https://preview.redd.it/oq2z2chvayn51.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b69d8689bb02e42fb36a623e45a326d743e035b'}], 's': {'y': 1350, 'x': 1080, 'u': 'https://preview.redd.it/oq2z2chvayn51.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=f53f186c6a813a9c1d165c609874b60713a89975'}, 'id': 'oq2z2chvayn51'}}, 'name': 't3_ivbug3', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 34, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 34, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/CQbsSjB7RX0gi5bTwglGZP1zk4zWgxVFTqbT7RR5bb0.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1600483133.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Browser extension that finds code implementations for machine learning papers anywhere on the web (Google, Arxiv, Twitter, Scholar, and other sites)!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://chrome.google.com/webstore/detail/mlai-code-implementation/aikkeehnlfpamidigaffhfmgbkdeheil""&gt;link to chrome extension&lt;/a&gt;&lt;br/&gt;\nor&lt;br/&gt;\n&lt;a href=""https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/""&gt;link to firefox extension&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/oq2z2chvayn51.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f53f186c6a813a9c1d165c609874b60713a89975""&gt;https://preview.redd.it/oq2z2chvayn51.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f53f186c6a813a9c1d165c609874b60713a89975&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?auto=webp&amp;s=b2ec1cfa2b9e720731d4dc172cbef8096fb37c3a', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/IxRliQNcpXlsEmjVCgBan5GuWfN25RXCua0vjZmtOP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad5435b4b2d94d9df9309e4d5ceb4bc021b52783', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'TIQjNUpyCsCWmTM49ZEEAiVQDWg07G_PjT8zXCX4wSE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ivbug3', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'fullerhouse570', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/ivbug3/chromefirefox_extension_for_showing_code_for_mlai/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/ivbug3/chromefirefox_extension_for_showing_code_for_mlai/', 'subreddit_subscribers': 6676, 'created_utc': 1600454333.0, 'num_crossposts': 13, 'media': None, 'is_video': False}]",t3_ivbug3,,,,,
333,,pytorch,"The fastest annotation tool ever built is accessible to everyone from student and researchers to CV engineers. In partnershsip with OpenCV, SuperAnnotate released the fastest and all free-to-use desktop annotation software equipped with all the functionalities designed to increase the speed, the accuracy and the efficiency of the annotation task.

[SuperAnnotate Desktop](https://blog.superannotate.com/superannotate-desktop-a-better-alternative-to-free-annotation-tools) closes the gap between free and commercial annotation tools providing CV engineers with an all-inclusive software to complete their annotation projects",t2_40op27sm,False,,0,False,SuperAnnotate Desktop: A better alternative to free annotation tools,[],r/pytorch,False,6,,0,,,False,t3_iv2pbv,False,dark,1.0,,public,16,0,{},,,False,[],,False,False,,{},,False,16,,False,self,1600421458.0,,[],{},self,,True,,1600448922.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The fastest annotation tool ever built is accessible to everyone from student and researchers to CV engineers. In partnershsip with OpenCV, SuperAnnotate released the fastest and all free-to-use desktop annotation software equipped with all the functionalities designed to increase the speed, the accuracy and the efficiency of the annotation task.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://blog.superannotate.com/superannotate-desktop-a-better-alternative-to-free-annotation-tools""&gt;SuperAnnotate Desktop&lt;/a&gt; closes the gap between free and commercial annotation tools providing CV engineers with an all-inclusive software to complete their annotation projects&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/rcZzQ4MYZSCng2yCWGu9pOUqvl6lN9zcD1RQjCvKSfw.jpg?auto=webp&amp;s=05a40dde4793c57f79e9fef1df1ca804e6e76deb', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/rcZzQ4MYZSCng2yCWGu9pOUqvl6lN9zcD1RQjCvKSfw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f6d552a6b9d5962897aac1511a614d30d4af80dd', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/rcZzQ4MYZSCng2yCWGu9pOUqvl6lN9zcD1RQjCvKSfw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=acf7ffc246d1ad549c80a482186f57031f9d185f', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/rcZzQ4MYZSCng2yCWGu9pOUqvl6lN9zcD1RQjCvKSfw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a9e313142f9ff81b1578746c01e1ea6329bc9696', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/rcZzQ4MYZSCng2yCWGu9pOUqvl6lN9zcD1RQjCvKSfw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8b6b74f799f1ab959195b9d1dd4e6454983f6302', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/rcZzQ4MYZSCng2yCWGu9pOUqvl6lN9zcD1RQjCvKSfw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b5e1f195cdb867ec7fb5941181307c2cac0395bf', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/rcZzQ4MYZSCng2yCWGu9pOUqvl6lN9zcD1RQjCvKSfw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=46a2146eec29ca079f5cbaced967b9e9e71fe591', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'b8Ga88eDPJbVvw7zY24mlewh50jCNwWazMuyHhRQuSo'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iv2pbv,True,,burgerswithbacon818,,1,True,all_ads,False,[],False,,/r/pytorch/comments/iv2pbv/superannotate_desktop_a_better_alternative_to/,all_ads,False,https://www.reddit.com/r/pytorch/comments/iv2pbv/superannotate_desktop_a_better_alternative_to/,7135,1600420122.0,0,,False,,,,,,,,
334,,pytorch,"This one : [https://pytorch.org/tutorials/beginner/deep\_learning\_60min\_blitz.html](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)

So I first saw 3blue1brown's 4 videos about neural nets and was super interested about neural nets. So googled and found pytorch.      

In the 60 mins blitz tutorial, was able to complete ""what is pytorch"". In the ""autograd"" tutorial, was able to understand the math but don't really get the big picture. The 3rd tutorial was too hard to follow as it has a lot of neural net jargon.        

Looks like im trying to walk before learning to crawl haha. 

 What should I read about before doing this tutorial?             

Goal : Just want to make a simple image classifier using CNN for now.      

 I'm not really in the computer field, but trying to learn about neural nets to see if it can be applied in aerospace :D

Thank you :)",t2_3hvzsc5a,False,,0,False,What are the prerequisites to do the pytorch's 60 mins blitz tutorial?,[],r/pytorch,False,6,,0,,,False,t3_iv5idk,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1600462391.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This one : &lt;a href=""https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html""&gt;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So I first saw 3blue1brown&amp;#39;s 4 videos about neural nets and was super interested about neural nets. So googled and found pytorch.      &lt;/p&gt;

&lt;p&gt;In the 60 mins blitz tutorial, was able to complete &amp;quot;what is pytorch&amp;quot;. In the &amp;quot;autograd&amp;quot; tutorial, was able to understand the math but don&amp;#39;t really get the big picture. The 3rd tutorial was too hard to follow as it has a lot of neural net jargon.        &lt;/p&gt;

&lt;p&gt;Looks like im trying to walk before learning to crawl haha. &lt;/p&gt;

&lt;p&gt;What should I read about before doing this tutorial?             &lt;/p&gt;

&lt;p&gt;Goal : Just want to make a simple image classifier using CNN for now.      &lt;/p&gt;

&lt;p&gt;I&amp;#39;m not really in the computer field, but trying to learn about neural nets to see if it can be applied in aerospace :D&lt;/p&gt;

&lt;p&gt;Thank you :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/dwWUgXR923pqdnQ0gcF-F1xgWoSFPu0MBhoTEw8jgyk.jpg?auto=webp&amp;s=dceaa97df75e0122852500963dba1b4536c88c33', 'width': 400, 'height': 280}, 'resolutions': [{'url': 'https://external-preview.redd.it/dwWUgXR923pqdnQ0gcF-F1xgWoSFPu0MBhoTEw8jgyk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb58f36779189e7d4e91b39d81b4c360352e61c1', 'width': 108, 'height': 75}, {'url': 'https://external-preview.redd.it/dwWUgXR923pqdnQ0gcF-F1xgWoSFPu0MBhoTEw8jgyk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=971e16049a4e320c52aaabdf395b903d4fb0f8cf', 'width': 216, 'height': 151}, {'url': 'https://external-preview.redd.it/dwWUgXR923pqdnQ0gcF-F1xgWoSFPu0MBhoTEw8jgyk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=54445cf24737676234453c27c24dffd9bdce7e13', 'width': 320, 'height': 224}], 'variants': {}, 'id': 'XM32XSvb19ZX_NxNJc72ZtPs0WurvzGk88MLvycGhcE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iv5idk,True,,fluidmechanicsdoubts,,2,True,all_ads,False,[],False,,/r/pytorch/comments/iv5idk/what_are_the_prerequisites_to_do_the_pytorchs_60/,all_ads,False,https://www.reddit.com/r/pytorch/comments/iv5idk/what_are_the_prerequisites_to_do_the_pytorchs_60/,7135,1600433591.0,0,,False,,,,,,,,
335,,pytorch,,t2_44mbtmjy,False,,0,False,Retiming and duplicating people in video (using neural rendering)!,[],r/pytorch,False,6,,0,57.0,,False,t3_iuyv8l,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/lBiLI7qpyr-YBxCwiIC-pq3MFfWRBeMldJ0U7R1GvdA.jpg,False,,[],{},link,,False,,1600429755.0,text,6,,,text,self.LatestInML,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/nw2vuhKYtQZOavWnIdKw59MWQY67PfY2BxPwhPO2N8E.jpg?auto=webp&amp;s=89d0b3b0a371f93b0d42dddf16e66ee1cf905c12', 'width': 1412, 'height': 578}, 'resolutions': [{'url': 'https://external-preview.redd.it/nw2vuhKYtQZOavWnIdKw59MWQY67PfY2BxPwhPO2N8E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=44ab01afbaff1a592b17fb636bbfb91bdd22188e', 'width': 108, 'height': 44}, {'url': 'https://external-preview.redd.it/nw2vuhKYtQZOavWnIdKw59MWQY67PfY2BxPwhPO2N8E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7301389cd5e3068441fd4a10be10c1b56506dbc', 'width': 216, 'height': 88}, {'url': 'https://external-preview.redd.it/nw2vuhKYtQZOavWnIdKw59MWQY67PfY2BxPwhPO2N8E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=15c998a4ec53c1c6ad514bc35f539ddf4f5d6ead', 'width': 320, 'height': 130}, {'url': 'https://external-preview.redd.it/nw2vuhKYtQZOavWnIdKw59MWQY67PfY2BxPwhPO2N8E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=22803eda6be407920eb55283fd7414ac21783012', 'width': 640, 'height': 261}, {'url': 'https://external-preview.redd.it/nw2vuhKYtQZOavWnIdKw59MWQY67PfY2BxPwhPO2N8E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=61c1a9ade782e2118c222a0580e8e150d15fced5', 'width': 960, 'height': 392}, {'url': 'https://external-preview.redd.it/nw2vuhKYtQZOavWnIdKw59MWQY67PfY2BxPwhPO2N8E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28f41374dc04746bdb372204431c28e586b41f1a', 'width': 1080, 'height': 442}], 'variants': {}, 'id': 'MIVqFHr7nrkqun5cwJ50ea8EDiZHa6Mnkd8cwbFxkdY'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iuyv8l,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/iuyv8l/retiming_and_duplicating_people_in_video_using/,all_ads,False,/r/LatestInML/comments/iuymg2/retiming_and_duplicating_people_in_video_using/,7135,1600400955.0,0,,False,/r/LatestInML/comments/iuymg2/retiming_and_duplicating_people_in_video_using/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2009.07833)\n\n&amp;#x200B;\n\nhttps://reddit.com/link/iuymg2/video/sb6o61yxstn51/player', 'author_fullname': 't2_63ay634e', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Retiming and duplicating people in video (using neural rendering)!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 57, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'sb6o61yxstn51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/iuymg2/asset/sb6o61yxstn51/DASHPlaylist.mpd?a=1618044184%2CMDY0YWRjNWY1MDE2OGJlNTlhNDg4OGJhYzRjY2NlM2RiMWY5YzI0MGY2MTNkODRjMDdlODk0ZTE3MTJhNDkyNQ%3D%3D&amp;v=1&amp;f=sd', 'x': 630, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/iuymg2/asset/sb6o61yxstn51/HLSPlaylist.m3u8?a=1618044184%2COTY5MTA2MGE3ZDA3ODY2MjFjZDhlZGZmOTRhMjAzMDc0ZmZhMGYzZDc2YWMxYWNiNGY2NGRiYzUyMzc1YjRhOQ%3D%3D&amp;v=1&amp;f=sd', 'id': 'sb6o61yxstn51', 'isGif': False}}, 'name': 't3_iuymg2', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 40, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 40, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/lBiLI7qpyr-YBxCwiIC-pq3MFfWRBeMldJ0U7R1GvdA.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1600428712.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2009.07833""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/iuymg2/video/sb6o61yxstn51/player""&gt;https://reddit.com/link/iuymg2/video/sb6o61yxstn51/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/nw2vuhKYtQZOavWnIdKw59MWQY67PfY2BxPwhPO2N8E.jpg?auto=webp&amp;s=89d0b3b0a371f93b0d42dddf16e66ee1cf905c12', 'width': 1412, 'height': 578}, 'resolutions': [{'url': 'https://external-preview.redd.it/nw2vuhKYtQZOavWnIdKw59MWQY67PfY2BxPwhPO2N8E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=44ab01afbaff1a592b17fb636bbfb91bdd22188e', 'width': 108, 'height': 44}, {'url': 'https://external-preview.redd.it/nw2vuhKYtQZOavWnIdKw59MWQY67PfY2BxPwhPO2N8E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7301389cd5e3068441fd4a10be10c1b56506dbc', 'width': 216, 'height': 88}, {'url': 'https://external-preview.redd.it/nw2vuhKYtQZOavWnIdKw59MWQY67PfY2BxPwhPO2N8E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=15c998a4ec53c1c6ad514bc35f539ddf4f5d6ead', 'width': 320, 'height': 130}, {'url': 'https://external-preview.redd.it/nw2vuhKYtQZOavWnIdKw59MWQY67PfY2BxPwhPO2N8E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=22803eda6be407920eb55283fd7414ac21783012', 'width': 640, 'height': 261}, {'url': 'https://external-preview.redd.it/nw2vuhKYtQZOavWnIdKw59MWQY67PfY2BxPwhPO2N8E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=61c1a9ade782e2118c222a0580e8e150d15fced5', 'width': 960, 'height': 392}, {'url': 'https://external-preview.redd.it/nw2vuhKYtQZOavWnIdKw59MWQY67PfY2BxPwhPO2N8E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28f41374dc04746bdb372204431c28e586b41f1a', 'width': 1080, 'height': 442}], 'variants': {}, 'id': 'MIVqFHr7nrkqun5cwJ50ea8EDiZHa6Mnkd8cwbFxkdY'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'iuymg2', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'cv2020br', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/iuymg2/retiming_and_duplicating_people_in_video_using/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/iuymg2/retiming_and_duplicating_people_in_video_using/', 'subreddit_subscribers': 6676, 'created_utc': 1600399912.0, 'num_crossposts': 15, 'media': None, 'is_video': False}]",t3_iuymg2,,,,,
336,,pytorch,"I have trained and saved some NER models using

    torch.save(model) 

I need to load these model files (extension .pt  
) for evaluation using

    torch.load('PATH_TO_MODEL.pt') 

And I get the following error: 'BertConfig' object has no attribute 'return\_dict'

For the same, I updated my transformer package to the latest one, but the error persists.

This is the stack trace:

    Traceback (most recent call last): File ""/home/systematicReviews/train_mtl_3.py"", line 523, in &lt;module&gt; test_loss, test_cr, test_cr_fine = evaluate_i(test_model, optimizer, scheduler, validation_dataloader, args, device) File ""/home/systematicReviews/train_mtl_3.py"", line 180, in evaluate_i e_loss_coarse, e_output, e_labels, e_loss_fine, e_f_output, e_f_labels, mask, e_cumulative_loss  = defModel(args, e_input_ids, attention_mask=e_input_mask, P_labels=e_labels, P_f_labels=e_f_labels) File ""/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__ result = self.forward(*input, **kwargs) File ""/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 150, in forward return self.module(*inputs[0], **kwargs[0]) File ""/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__ result = self.forward(*input, **kwargs) File ""/home/systematicReviews/models/mtl/model.py"", line 122, in forward attention_mask = attention_mask File ""/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__ result = self.forward(*input, **kwargs) File ""/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/transformers/modeling_bert.py"", line 784, in forward return_dict = return_dict if return_dict is not None else self.config.use_return_dict File ""/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/transformers/configuration_utils.py"", line 219, in use_return_dict return self.return_dict and not self.torchscript AttributeError: 'BertConfig' object has no attribute 'return_dict' 

Here is some more information about my system:

    - `transformers` version: 3.1.0 - Platform: Linux-4.4.0-186-generic-x86_64-with-debian-stretch-sid - Python version: 3.6.9 - PyTorch version (GPU?): 1.3.1 (True) - Tensorflow version (GPU?): not installed (NA) - Using GPU in script?: Yes - Using distributed or parallel set-up in script?: No 

It worked pretty fine until now, but suddenly this bug appears. Any help or hint is appreciated.

&amp;#x200B;

Original question [here](https://stackoverflow.com/questions/63939072/loading-saved-ner-transformers-model-causes-attributeerror).",t2_9ptho1m,False,,0,False,Loading saved NER transformers model causes AttributeError?,[],r/pytorch,False,6,,0,,,False,t3_iujqz9,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1600379122.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have trained and saved some NER models using&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;torch.save(model) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I need to load these model files (extension .pt&lt;br/&gt;
) for evaluation using&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;torch.load(&amp;#39;PATH_TO_MODEL.pt&amp;#39;) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And I get the following error: &amp;#39;BertConfig&amp;#39; object has no attribute &amp;#39;return_dict&amp;#39;&lt;/p&gt;

&lt;p&gt;For the same, I updated my transformer package to the latest one, but the error persists.&lt;/p&gt;

&lt;p&gt;This is the stack trace:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Traceback (most recent call last): File &amp;quot;/home/systematicReviews/train_mtl_3.py&amp;quot;, line 523, in &amp;lt;module&amp;gt; test_loss, test_cr, test_cr_fine = evaluate_i(test_model, optimizer, scheduler, validation_dataloader, args, device) File &amp;quot;/home/systematicReviews/train_mtl_3.py&amp;quot;, line 180, in evaluate_i e_loss_coarse, e_output, e_labels, e_loss_fine, e_f_output, e_f_labels, mask, e_cumulative_loss  = defModel(args, e_input_ids, attention_mask=e_input_mask, P_labels=e_labels, P_f_labels=e_f_labels) File &amp;quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py&amp;quot;, line 541, in __call__ result = self.forward(*input, **kwargs) File &amp;quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py&amp;quot;, line 150, in forward return self.module(*inputs[0], **kwargs[0]) File &amp;quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py&amp;quot;, line 541, in __call__ result = self.forward(*input, **kwargs) File &amp;quot;/home/systematicReviews/models/mtl/model.py&amp;quot;, line 122, in forward attention_mask = attention_mask File &amp;quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/torch/nn/modules/module.py&amp;quot;, line 541, in __call__ result = self.forward(*input, **kwargs) File &amp;quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/transformers/modeling_bert.py&amp;quot;, line 784, in forward return_dict = return_dict if return_dict is not None else self.config.use_return_dict File &amp;quot;/home/anaconda3/envs/systreviewclassifi/lib/python3.6/site-packages/transformers/configuration_utils.py&amp;quot;, line 219, in use_return_dict return self.return_dict and not self.torchscript AttributeError: &amp;#39;BertConfig&amp;#39; object has no attribute &amp;#39;return_dict&amp;#39; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is some more information about my system:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- `transformers` version: 3.1.0 - Platform: Linux-4.4.0-186-generic-x86_64-with-debian-stretch-sid - Python version: 3.6.9 - PyTorch version (GPU?): 1.3.1 (True) - Tensorflow version (GPU?): not installed (NA) - Using GPU in script?: Yes - Using distributed or parallel set-up in script?: No 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It worked pretty fine until now, but suddenly this bug appears. Any help or hint is appreciated.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Original question &lt;a href=""https://stackoverflow.com/questions/63939072/loading-saved-ner-transformers-model-causes-attributeerror""&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iujqz9,True,,freaky_eater,,3,True,all_ads,False,[],False,,/r/pytorch/comments/iujqz9/loading_saved_ner_transformers_model_causes/,all_ads,False,https://www.reddit.com/r/pytorch/comments/iujqz9/loading_saved_ner_transformers_model_causes/,7135,1600350322.0,0,,False,,,,,,,,
337,,pytorch,"Hi guys! I'm really worried about my major project. I need to create a DQN and use SUMO to determinate the behaviour of the traffic lights (my agent in the Reinforcement Learning problem) using one measure like for example the duration time of the phases, the average waiting time of the vehicles because we want to minimize the traffic flow in the lanes and also respond to the acceptable levels of PMx in the air. I have to create six scenarios with different traffic flows and with traffic lights or no traffic lights. I have also to use PYTORCH functions, trying to work also with a server to run the project. I mean of course I have work done about this, but I'm having some issues about what i really need. I took a lot of time to configure and realize what exactly do. If you are able to help me with the project development i will be forever grateful and even i could give you a monetary retribution if you ask for it. I really need to finish it the short time possible but in different parts like to finally conect the whole thing. And also if i ask for help is because i really take a long time to figure it out how to create the project due some time issues due this pandemic, but now i have to do it. Please let me know if you could help me, thank you and stay safe! :)",t2_65rszqsr,False,,0,False,SUMO Simulation problem - Traffic Lights - Pytorch - DQN - S.O.S,[],r/pytorch,False,6,,0,,,False,t3_iu155w,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1600308949.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys! I&amp;#39;m really worried about my major project. I need to create a DQN and use SUMO to determinate the behaviour of the traffic lights (my agent in the Reinforcement Learning problem) using one measure like for example the duration time of the phases, the average waiting time of the vehicles because we want to minimize the traffic flow in the lanes and also respond to the acceptable levels of PMx in the air. I have to create six scenarios with different traffic flows and with traffic lights or no traffic lights. I have also to use PYTORCH functions, trying to work also with a server to run the project. I mean of course I have work done about this, but I&amp;#39;m having some issues about what i really need. I took a lot of time to configure and realize what exactly do. If you are able to help me with the project development i will be forever grateful and even i could give you a monetary retribution if you ask for it. I really need to finish it the short time possible but in different parts like to finally conect the whole thing. And also if i ask for help is because i really take a long time to figure it out how to create the project due some time issues due this pandemic, but now i have to do it. Please let me know if you could help me, thank you and stay safe! :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iu155w,True,,Theron96G,,0,True,all_ads,False,[],False,,/r/pytorch/comments/iu155w/sumo_simulation_problem_traffic_lights_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/iu155w/sumo_simulation_problem_traffic_lights_pytorch/,7135,1600280149.0,0,,False,,,,,,,,
338,,pytorch,"Hello,

I'm working on a causal variational autoencoder which works with class segmentation masks, class labels and causality(0 or 1) as the inputs.

I'm getting an error when working with batch sizes more than 1 due to the svi step. I'm using a Bernoulli distribution because I want it to learn the probability distribution for multiple classes in an image. I think that the Categorical distribution also fits the bill here, but I get the same error with it too.

When I tried narrowing down the code lines which create the problem, I think it's in the model function:

    one_vec2 = torch.ones([batch_size, self.lbl_shape[0]], **options) 
    class_labels = pyro.sample('class_labels', dist.Bernoulli(one_vec2*0.5), obs = lbls) 

The error:

    ValueError                                Traceback (most recent call last)
    &lt;ipython-input-19-8cbc046dd2c1&gt; in &lt;module&gt;()
          6 vae = Vae_Model1(lbl_sz, ch, img_sz).to(device)
          7 svi = SVI(vae.model, vae.guide, optimizer, loss = Trace_ELBO())
    ----&gt; 8 train(svi, train_loader, USE_CUDA)
    
    6 frames
    /usr/local/lib/python3.6/dist-packages/pyro/util.py in check_site_shape(site, max_plate_nesting)
        320                 '- enclose the batched tensor in a with plate(...): context',
        321                 '- .to_event(...) the distribution being sampled',
    --&gt; 322                 '- .permute() data dimensions']))
        323 
        324     # Check parallel dimensions on the left of max_plate_nesting.
    
    ValueError: at site ""class_labels"", invalid log_prob shape
      Expected [-1], actual [32, 21]
      Try one of the following fixes:
      - enclose the batched tensor in a with plate(...): context
      - .to_event(...) the distribution being sampled
      - .permute() data dimensions

Currently, the batch size is 32 and the lbl\_shape\[0\] is 21 (VOC Dataset (background and other labels))

Could someone help me with this? It'll be very much appreciated. Thank you",t2_4dcuwlo1,False,,0,False,Getting an error in svi step due to to a multiclass distribution in sample using pyro and pytorch,[],r/pytorch,False,6,,0,,,False,t3_itadd7,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,1600181990.0,,[],{},,,True,,1600210584.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on a causal variational autoencoder which works with class segmentation masks, class labels and causality(0 or 1) as the inputs.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m getting an error when working with batch sizes more than 1 due to the svi step. I&amp;#39;m using a Bernoulli distribution because I want it to learn the probability distribution for multiple classes in an image. I think that the Categorical distribution also fits the bill here, but I get the same error with it too.&lt;/p&gt;

&lt;p&gt;When I tried narrowing down the code lines which create the problem, I think it&amp;#39;s in the model function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;one_vec2 = torch.ones([batch_size, self.lbl_shape[0]], **options) 
class_labels = pyro.sample(&amp;#39;class_labels&amp;#39;, dist.Bernoulli(one_vec2*0.5), obs = lbls) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ValueError                                Traceback (most recent call last)
&amp;lt;ipython-input-19-8cbc046dd2c1&amp;gt; in &amp;lt;module&amp;gt;()
      6 vae = Vae_Model1(lbl_sz, ch, img_sz).to(device)
      7 svi = SVI(vae.model, vae.guide, optimizer, loss = Trace_ELBO())
----&amp;gt; 8 train(svi, train_loader, USE_CUDA)

6 frames
/usr/local/lib/python3.6/dist-packages/pyro/util.py in check_site_shape(site, max_plate_nesting)
    320                 &amp;#39;- enclose the batched tensor in a with plate(...): context&amp;#39;,
    321                 &amp;#39;- .to_event(...) the distribution being sampled&amp;#39;,
--&amp;gt; 322                 &amp;#39;- .permute() data dimensions&amp;#39;]))
    323 
    324     # Check parallel dimensions on the left of max_plate_nesting.

ValueError: at site &amp;quot;class_labels&amp;quot;, invalid log_prob shape
  Expected [-1], actual [32, 21]
  Try one of the following fixes:
  - enclose the batched tensor in a with plate(...): context
  - .to_event(...) the distribution being sampled
  - .permute() data dimensions
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Currently, the batch size is 32 and the lbl_shape[0] is 21 (VOC Dataset (background and other labels))&lt;/p&gt;

&lt;p&gt;Could someone help me with this? It&amp;#39;ll be very much appreciated. Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,itadd7,True,,wutheringsouls,,0,True,all_ads,False,[],False,,/r/pytorch/comments/itadd7/getting_an_error_in_svi_step_due_to_to_a/,all_ads,False,https://www.reddit.com/r/pytorch/comments/itadd7/getting_an_error_in_svi_step_due_to_to_a/,7135,1600181784.0,0,,False,,,,,,,,
339,,pytorch,"PyTorch’s default dataloader tends to get annoying, especially when we deal with custom datasets/conditional dataset loading. For a project we were working on we had to load a number of large datasets that weren’t structured the way the ImageFolder DataLoader expects, so we modified it to allow the user to specify whatever structure they want. It’s been a cool experiment so far, but we’re contemplating taking it on as a long-term open source project, and I was wondering if this problem is as common as we feel it would be? Would love to hear what people think, the code is at https://github.com/BinItAI/BetterLoader, if you want to check it out",t2_83ctn6ud,False,,0,False,My friend and I have been working on a tool to make loading custom image datasets less frustrating,[],r/pytorch,False,6,,0,,,False,t3_isucy7,False,dark,0.9,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},self,,True,,1600147006.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;PyTorch’s default dataloader tends to get annoying, especially when we deal with custom datasets/conditional dataset loading. For a project we were working on we had to load a number of large datasets that weren’t structured the way the ImageFolder DataLoader expects, so we modified it to allow the user to specify whatever structure they want. It’s been a cool experiment so far, but we’re contemplating taking it on as a long-term open source project, and I was wondering if this problem is as common as we feel it would be? Would love to hear what people think, the code is at &lt;a href=""https://github.com/BinItAI/BetterLoader""&gt;https://github.com/BinItAI/BetterLoader&lt;/a&gt;, if you want to check it out&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/2fMmYZcDsWWq4NwhreL6yzmcIs_L1oO2QznwEmu_aLY.jpg?auto=webp&amp;s=6db01d59d5d4e5c84dd986163434367dbab6a12a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/2fMmYZcDsWWq4NwhreL6yzmcIs_L1oO2QznwEmu_aLY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=434b959166754776649273f10c74b4162cfc909d', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/2fMmYZcDsWWq4NwhreL6yzmcIs_L1oO2QznwEmu_aLY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e8f3d760d5f0f535d9e8bab21a3b49e4316a254', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/2fMmYZcDsWWq4NwhreL6yzmcIs_L1oO2QznwEmu_aLY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f27bcf24610e8accd6e4fe11cd0e35a6cbfdbac', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'pFVl2PEKFqs3cpfesmFIg0OuZBYjZTML2BMj_0NtGRU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,isucy7,True,,JamesBollas,,1,True,all_ads,False,[],False,,/r/pytorch/comments/isucy7/my_friend_and_i_have_been_working_on_a_tool_to/,all_ads,False,https://www.reddit.com/r/pytorch/comments/isucy7/my_friend_and_i_have_been_working_on_a_tool_to/,7135,1600118206.0,0,,False,,,,,,,,
340,,pytorch,"My network just refuses to train. To make code reading less of a  hassle, I abbreviate some complicated logic. Would update more if  needed.

```python
model = DistMultNN()
optimizer = optim.SGD(model.parameters(), lr=0.0001)
for t in range(500):
    e1_neg = sampling_logic()
    e2_neg = sampling_logic()

    e1_pos = sampling_logic()
    r = sampling_logic()
    e2_pos = sampling_logic()
    optimizer.zero_grad()
    y_pred = model(tuple(zip(e1_pos, r, e2_pos)), e1_neg, e2_neg)
    loss = model.loss(y_pred)
    loss.backward()
    optimizer.step()
```
I define my network as follow

```python
class DistMultNN(nn.Module):

    def __init__(self):
        super().__init__()
        self.seed = 42
        self.entities_embedding = nn.init.xavier_uniform_(
            torch.zeros((self.NO_ENTITIES, self.ENCODING_DIM), requires_grad=True))
        self.relation_embedding = nn.init.xavier_uniform_(
            torch.zeros((self.NO_RELATIONSHIPS, self.ENCODING_DIM), requires_grad=True))
        self.W = torch.rand(self.ENCODING_DIM, self.ENCODING_DIM, requires_grad=True)  # W is symmetric, todo: requireGrad?
        self.W = (self.W + self.W.t()) / 2
        self.b = torch.rand(self.ENCODING_DIM, 1, requires_grad=True)
        self.lambda_ = 1.
        self.rnn = torch.nn.RNN(input_size=encoding_dim, hidden_size=1, num_layers=1, nonlinearity='relu')
        self.loss_func = torch.nn.LogSoftmax(dim=1)

    def loss(self, y_pred):
        softmax = -1 * self.loss_func(y_pred)
        result = torch.mean(softmax[:, 0])
        result.requires_grad = True
        return result

    def forward(self, samples, e1neg, e2neg):
        batch_size = len(samples)
        batch_result = np.zeros((batch_size, len(e1neg[0]) + 1))
        for datapoint_id in range(batch_size):
            entity_1 = entities_embed_lookup(datapoint_id[0])
            entity_2 = entities_embed_lookup(datapoint_id[2])
            r = relation_embed_lookup(datapoint_id[1])
            x = self.some_fourier_transform(entity_1, r, entity_2)
            batch_result[datapoint_id][0] = self.some_matmul(x)
            for negative_example_id in range(len(e1neg[0])):
                same_thing_with_negative_examples()
                batch_result[datapoint_id][negative_example_id + 1] = self.some_matmul(x)
        batch_result_tensor = torch.tensor(data=batch_result)
        return batch_result_tensor
```

I tried checking weights using e.g. `print(model.rnn.all_weights)` in the training loop but they do not change. What did I do wrong?",t2_qru4q0y,False,,0,False,Weights of network stays the same after optimizer step,[],r/pytorch,False,6,,0,,,False,t3_ist4h3,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1600143430.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My network just refuses to train. To make code reading less of a  hassle, I abbreviate some complicated logic. Would update more if  needed.&lt;/p&gt;

&lt;p&gt;```python
model = DistMultNN()
optimizer = optim.SGD(model.parameters(), lr=0.0001)
for t in range(500):
    e1_neg = sampling_logic()
    e2_neg = sampling_logic()&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;e1_pos = sampling_logic()
r = sampling_logic()
e2_pos = sampling_logic()
optimizer.zero_grad()
y_pred = model(tuple(zip(e1_pos, r, e2_pos)), e1_neg, e2_neg)
loss = model.loss(y_pred)
loss.backward()
optimizer.step()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```
I define my network as follow&lt;/p&gt;

&lt;p&gt;```python
class DistMultNN(nn.Module):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def __init__(self):
    super().__init__()
    self.seed = 42
    self.entities_embedding = nn.init.xavier_uniform_(
        torch.zeros((self.NO_ENTITIES, self.ENCODING_DIM), requires_grad=True))
    self.relation_embedding = nn.init.xavier_uniform_(
        torch.zeros((self.NO_RELATIONSHIPS, self.ENCODING_DIM), requires_grad=True))
    self.W = torch.rand(self.ENCODING_DIM, self.ENCODING_DIM, requires_grad=True)  # W is symmetric, todo: requireGrad?
    self.W = (self.W + self.W.t()) / 2
    self.b = torch.rand(self.ENCODING_DIM, 1, requires_grad=True)
    self.lambda_ = 1.
    self.rnn = torch.nn.RNN(input_size=encoding_dim, hidden_size=1, num_layers=1, nonlinearity=&amp;#39;relu&amp;#39;)
    self.loss_func = torch.nn.LogSoftmax(dim=1)

def loss(self, y_pred):
    softmax = -1 * self.loss_func(y_pred)
    result = torch.mean(softmax[:, 0])
    result.requires_grad = True
    return result

def forward(self, samples, e1neg, e2neg):
    batch_size = len(samples)
    batch_result = np.zeros((batch_size, len(e1neg[0]) + 1))
    for datapoint_id in range(batch_size):
        entity_1 = entities_embed_lookup(datapoint_id[0])
        entity_2 = entities_embed_lookup(datapoint_id[2])
        r = relation_embed_lookup(datapoint_id[1])
        x = self.some_fourier_transform(entity_1, r, entity_2)
        batch_result[datapoint_id][0] = self.some_matmul(x)
        for negative_example_id in range(len(e1neg[0])):
            same_thing_with_negative_examples()
            batch_result[datapoint_id][negative_example_id + 1] = self.some_matmul(x)
    batch_result_tensor = torch.tensor(data=batch_result)
    return batch_result_tensor
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;I tried checking weights using e.g. &lt;code&gt;print(model.rnn.all_weights)&lt;/code&gt; in the training loop but they do not change. What did I do wrong?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ist4h3,True,,magnusderrote,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ist4h3/weights_of_network_stays_the_same_after_optimizer/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ist4h3/weights_of_network_stays_the_same_after_optimizer/,7135,1600114630.0,0,,False,,,,,,,,
341,,pytorch,Simple question as above.,t2_ebu4m,False,,0,False,Does DataLoader(shuffle=True) shuffle the observations in the batches or shuffle the batches themselves?,[],r/pytorch,False,6,,0,,,False,t3_iq5xqg,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1599781790.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Simple question as above.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iq5xqg,True,,Pepipasta,,7,True,all_ads,False,[],False,,/r/pytorch/comments/iq5xqg/does_dataloadershuffletrue_shuffle_the/,all_ads,False,https://www.reddit.com/r/pytorch/comments/iq5xqg/does_dataloadershuffletrue_shuffle_the/,7135,1599752990.0,0,,False,,,,,,,,
342,,pytorch,,t2_ljhk2,False,,0,False,"[GitHub]: PyTorchLightning/pytorch-lightning: The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate.",[],r/pytorch,False,6,,0,33.0,,False,t3_ipw1sy,False,dark,0.63,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://a.thumbs.redditmedia.com/JDiVcwk293prywzLMftowyx547hEOgKvDrcpnYsw184.jpg,False,,[],{},link,,False,,1599739103.0,text,6,,,text,github.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/50wCDWVsYWxlrHoyILNhUa8T88R-U7w27Fx4pRglusw.jpg?auto=webp&amp;s=fd15c3146f6f5adee9406f42ec63f9ba35b5e23c', 'width': 847, 'height': 201}, 'resolutions': [{'url': 'https://external-preview.redd.it/50wCDWVsYWxlrHoyILNhUa8T88R-U7w27Fx4pRglusw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95a07f1deada1a901dbccbf4ba94f3e90150b73e', 'width': 108, 'height': 25}, {'url': 'https://external-preview.redd.it/50wCDWVsYWxlrHoyILNhUa8T88R-U7w27Fx4pRglusw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f2004c4ead2928dddd34f55ac601fb3d212482a', 'width': 216, 'height': 51}, {'url': 'https://external-preview.redd.it/50wCDWVsYWxlrHoyILNhUa8T88R-U7w27Fx4pRglusw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=882655357a4c1d8b52028d1d3609dd9fd7222216', 'width': 320, 'height': 75}, {'url': 'https://external-preview.redd.it/50wCDWVsYWxlrHoyILNhUa8T88R-U7w27Fx4pRglusw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ad9b92809a55433501de13a5bc21c05921a9f9d0', 'width': 640, 'height': 151}], 'variants': {}, 'id': 'bgqzOmA8QGH8yXIDAQoOp4TaC2mwfLsM_SLAw9EXcfs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ipw1sy,True,,Dawny33,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ipw1sy/github_pytorchlightningpytorchlightning_the/,all_ads,False,https://github.com/PyTorchLightning/pytorch-lightning,7135,1599710303.0,0,,False,https://github.com/PyTorchLightning/pytorch-lightning,,,,,,,
343,,pytorch,,t2_44mbtmjy,False,,0,False,State of the art in video completion from Facebook and Virginia Tech!,[],r/pytorch,False,6,,0,35.0,,False,t3_ipqq4k,False,dark,1.0,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://a.thumbs.redditmedia.com/yPH0s1s2-mADb4pBs3DNu24okYLWpdr8uRLuKux6am4.jpg,False,,[],{},link,,False,,1599719316.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/spj1_bsvInoM8pK4SJAOVAtMeUczzQiwIEep12FkEIY.jpg?auto=webp&amp;s=0db52c6c2207e885ec04615dbd17c0de8dbb720b', 'width': 956, 'height': 242}, 'resolutions': [{'url': 'https://external-preview.redd.it/spj1_bsvInoM8pK4SJAOVAtMeUczzQiwIEep12FkEIY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3021628085442118b46f3fa7b6a6ca0cbeda2997', 'width': 108, 'height': 27}, {'url': 'https://external-preview.redd.it/spj1_bsvInoM8pK4SJAOVAtMeUczzQiwIEep12FkEIY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb08f68aab27fa74029d589038a212c7d3b876e9', 'width': 216, 'height': 54}, {'url': 'https://external-preview.redd.it/spj1_bsvInoM8pK4SJAOVAtMeUczzQiwIEep12FkEIY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d126641d0ea9e59b52791f0f566e28d2306bdd80', 'width': 320, 'height': 81}, {'url': 'https://external-preview.redd.it/spj1_bsvInoM8pK4SJAOVAtMeUczzQiwIEep12FkEIY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3ac196cf36ff3f7a8f3df158239d49a1b1595dcc', 'width': 640, 'height': 162}], 'variants': {}, 'id': 'bcbQkvPXpKET1rOjq-_pZXNAgO21Lt_8r_EmtGzWtgg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ipqq4k,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ipqq4k/state_of_the_art_in_video_completion_from/,all_ads,False,/r/LatestInML/comments/ip8t7e/state_of_the_art_in_video_completion_from/,7135,1599690516.0,0,,False,/r/LatestInML/comments/ip8t7e/state_of_the_art_in_video_completion_from/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/expert/API requests: [click here](https://www.catalyzex.com/paper/arxiv:2009.01835?fbclid=IwAR2arCENh1S3vIGArGPSsFmQCjfFb4pelABPyvRj48FwlQdPtPi7g4rgahs)\n\nVideo completion is the task of filling a given space-time region with newly synthesized content.\n\n&amp;#x200B;\n\nhttps://reddit.com/link/ip8t7e/video/71mr1j4do1m51/player', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'State of the art in video completion from Facebook and Virginia Tech!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 35, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'71mr1j4do1m51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/ip8t7e/asset/71mr1j4do1m51/DASHPlaylist.mpd?a=1618044184%2CMWE0ZDA2NDY5NzEyZDZlMzc5ZWQ0MmMzYTI0YjlkZjkzNzQ3OTQ0Y2Q4ODBhMGRmZDc5ZmM1YjVlZDAxYTIyNQ%3D%3D&amp;v=1&amp;f=sd', 'x': 640, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/ip8t7e/asset/71mr1j4do1m51/HLSPlaylist.m3u8?a=1618044184%2CMTk4MGYwMmQ4NDM4YWFhMmNiOTRiODJjZGQ3ZTViOWQwNjJhZjJmZDcwYmRiNjg4YTc0NzJhNDg3MThjNjFmYw%3D%3D&amp;v=1&amp;f=sd', 'id': '71mr1j4do1m51', 'isGif': False}}, 'name': 't3_ip8t7e', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.87, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 16, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 16, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/yPH0s1s2-mADb4pBs3DNu24okYLWpdr8uRLuKux6am4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1599652286.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/expert/API requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2009.01835?fbclid=IwAR2arCENh1S3vIGArGPSsFmQCjfFb4pelABPyvRj48FwlQdPtPi7g4rgahs""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Video completion is the task of filling a given space-time region with newly synthesized content.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/ip8t7e/video/71mr1j4do1m51/player""&gt;https://reddit.com/link/ip8t7e/video/71mr1j4do1m51/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/spj1_bsvInoM8pK4SJAOVAtMeUczzQiwIEep12FkEIY.jpg?auto=webp&amp;s=0db52c6c2207e885ec04615dbd17c0de8dbb720b', 'width': 956, 'height': 242}, 'resolutions': [{'url': 'https://external-preview.redd.it/spj1_bsvInoM8pK4SJAOVAtMeUczzQiwIEep12FkEIY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3021628085442118b46f3fa7b6a6ca0cbeda2997', 'width': 108, 'height': 27}, {'url': 'https://external-preview.redd.it/spj1_bsvInoM8pK4SJAOVAtMeUczzQiwIEep12FkEIY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb08f68aab27fa74029d589038a212c7d3b876e9', 'width': 216, 'height': 54}, {'url': 'https://external-preview.redd.it/spj1_bsvInoM8pK4SJAOVAtMeUczzQiwIEep12FkEIY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d126641d0ea9e59b52791f0f566e28d2306bdd80', 'width': 320, 'height': 81}, {'url': 'https://external-preview.redd.it/spj1_bsvInoM8pK4SJAOVAtMeUczzQiwIEep12FkEIY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3ac196cf36ff3f7a8f3df158239d49a1b1595dcc', 'width': 640, 'height': 162}], 'variants': {}, 'id': 'bcbQkvPXpKET1rOjq-_pZXNAgO21Lt_8r_EmtGzWtgg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ip8t7e', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/ip8t7e/state_of_the_art_in_video_completion_from/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/ip8t7e/state_of_the_art_in_video_completion_from/', 'subreddit_subscribers': 6676, 'created_utc': 1599623486.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_ip8t7e,,,,,
344,,pytorch,"We are facing a very strange issue. We tested the exact same model into two different “execution” settings. In the first case, given a certain amount of epochs, we train using mini-batches for one epoch, and thereafter we test on the validation set following the same criteria. Then, we go for the next epoch. Clearly, before each training epoch, we use model.train(), and before validation we turn on model.eval().

Then we take the exact same model (same init, same dataset, same epochs, etc.) and we just train it without validation after each epoch.

Just looking at performance on training set, we observed that, even if we fixed all seeds, the two training procedures evolve differently and produce quite different metrics results (losses, accuracy, and so on). Specifically, the training-only procedure is less performing.

We also observe the following things:

* It is not a reproducibility issue, because multiple executions of the same procedure produce exactly the same results (and this is intended);
* Removing the dropout, it appears that the problem vanishes;
* Batchnorm1d layer, that still has different behaviours between training and evaluation, seems to work properly;
* The issue still happens if we move from training onto TPUs to CPUs.

We are working and tried Pythorch 1.6, Pythorch nightly, XLA 1.6.

We quite lost one full day in trying to tackle this issue (and no, we cannot avoid using dropout). Does anyone have any idea about how to solve this fact?

Thank you very much!

  
",t2_2ku5mj3m,False,,0,False,Dropout (?) causes different model convergence for training+validation and only training,[],r/pytorch,False,6,,0,,,False,t3_ipht11,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1599693339.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We are facing a very strange issue. We tested the exact same model into two different “execution” settings. In the first case, given a certain amount of epochs, we train using mini-batches for one epoch, and thereafter we test on the validation set following the same criteria. Then, we go for the next epoch. Clearly, before each training epoch, we use model.train(), and before validation we turn on model.eval().&lt;/p&gt;

&lt;p&gt;Then we take the exact same model (same init, same dataset, same epochs, etc.) and we just train it without validation after each epoch.&lt;/p&gt;

&lt;p&gt;Just looking at performance on training set, we observed that, even if we fixed all seeds, the two training procedures evolve differently and produce quite different metrics results (losses, accuracy, and so on). Specifically, the training-only procedure is less performing.&lt;/p&gt;

&lt;p&gt;We also observe the following things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It is not a reproducibility issue, because multiple executions of the same procedure produce exactly the same results (and this is intended);&lt;/li&gt;
&lt;li&gt;Removing the dropout, it appears that the problem vanishes;&lt;/li&gt;
&lt;li&gt;Batchnorm1d layer, that still has different behaviours between training and evaluation, seems to work properly;&lt;/li&gt;
&lt;li&gt;The issue still happens if we move from training onto TPUs to CPUs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are working and tried Pythorch 1.6, Pythorch nightly, XLA 1.6.&lt;/p&gt;

&lt;p&gt;We quite lost one full day in trying to tackle this issue (and no, we cannot avoid using dropout). Does anyone have any idea about how to solve this fact?&lt;/p&gt;

&lt;p&gt;Thank you very much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ipht11,True,,Brunix90,,6,True,all_ads,False,[],False,,/r/pytorch/comments/ipht11/dropout_causes_different_model_convergence_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ipht11/dropout_causes_different_model_convergence_for/,7135,1599664539.0,0,,False,,,,,,,,
345,,pytorch,"Hi all,

I have a network configured to train on a loss function F.  
Loss F is comprised of two terms: cross entropy and a custom loss C.   
My train is implemented as:  
`outputs = net(inputs)`  
`loss = cross_entropy(outputs, targets) + C`  
`loss.backward()`  
`optimizer.step()`

The cross entropy is a function of C, also, C can be calculated from the components of the cross entropy.

Does rewriting the cross entropy to produce the cross entropy summed with C will optimize the backward runtime performance?

I'm asking instead of trying because unfortunately i'm dealing with a very large architecture and it will take a lot of time to rewrite and test it.

I'm using Pytorch 1.6.1 and CUDA 11.",t2_6idx5nw6,False,,0,False,Backward optimization of a compound loss function,[],r/pytorch,False,6,,0,,,False,t3_ipcq3j,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1599672483.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I have a network configured to train on a loss function F.&lt;br/&gt;
Loss F is comprised of two terms: cross entropy and a custom loss C.&lt;br/&gt;
My train is implemented as:&lt;br/&gt;
&lt;code&gt;outputs = net(inputs)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;loss = cross_entropy(outputs, targets) + C&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;loss.backward()&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;optimizer.step()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The cross entropy is a function of C, also, C can be calculated from the components of the cross entropy.&lt;/p&gt;

&lt;p&gt;Does rewriting the cross entropy to produce the cross entropy summed with C will optimize the backward runtime performance?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m asking instead of trying because unfortunately i&amp;#39;m dealing with a very large architecture and it will take a lot of time to rewrite and test it.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m using Pytorch 1.6.1 and CUDA 11.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ipcq3j,True,,Mosh0110,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ipcq3j/backward_optimization_of_a_compound_loss_function/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ipcq3j/backward_optimization_of_a_compound_loss_function/,7135,1599643683.0,0,,False,,,,,,,,
346,,pytorch,"I’m getting started in PyTorch and have a few  years experience with Tensorflow v1. I’m a bit confused about how RNNs  work in PyTorch.

It seems to me that the provided RNNs in ‘nn’ are all C  implementations and I can’t seem to find an equivalent to Tensorflow’s  ‘scan’ or ‘dynamic\_rnn’ function. Furthermore, all custom  implementations of RNNs in PyTorch seem to work using Python for-loops.  Wouldn’t this result in multiple calls to the GPU, which slows everything down?

Second: I am used to dealing with minibatches containing variable  length sequences in Tensorflow by providing the length of each sequence  to the RNN function. This has the advantage (over using an explicit pad  token) that you do not need to create a useless entry in the vocabulary  just for a pad token. The padded data can be any existing token which  will be ignored because it lies outside of the declared length. But it seems that PyTorch assumes a pad token the same way that Keras does. Am I  understanding this right? Do I have to reserve an entry in my embedding matrix for pad tokens?

Thanks!",t2_467n2,False,,0,False,RNN implementation in PyTorch vs Tensorflow,[],r/pytorch,False,6,,0,,,False,t3_ipazen,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1599662622.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m getting started in PyTorch and have a few  years experience with Tensorflow v1. I’m a bit confused about how RNNs  work in PyTorch.&lt;/p&gt;

&lt;p&gt;It seems to me that the provided RNNs in ‘nn’ are all C  implementations and I can’t seem to find an equivalent to Tensorflow’s  ‘scan’ or ‘dynamic_rnn’ function. Furthermore, all custom  implementations of RNNs in PyTorch seem to work using Python for-loops.  Wouldn’t this result in multiple calls to the GPU, which slows everything down?&lt;/p&gt;

&lt;p&gt;Second: I am used to dealing with minibatches containing variable  length sequences in Tensorflow by providing the length of each sequence  to the RNN function. This has the advantage (over using an explicit pad  token) that you do not need to create a useless entry in the vocabulary  just for a pad token. The padded data can be any existing token which  will be ignored because it lies outside of the declared length. But it seems that PyTorch assumes a pad token the same way that Keras does. Am I  understanding this right? Do I have to reserve an entry in my embedding matrix for pad tokens?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ipazen,True,,mtanti,,3,True,all_ads,False,[],False,,/r/pytorch/comments/ipazen/rnn_implementation_in_pytorch_vs_tensorflow/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ipazen/rnn_implementation_in_pytorch_vs_tensorflow/,7135,1599633822.0,0,,False,,,,,,,,
347,,pytorch,"I recently started doing research on adversarial attacks and defenses of neural networks. I thought it would be interesting to share with you one of the earliest yet effective attacks (FGSM) and how to defend it through adversarial training, implemented in PyTorch:

[https://medium.com/@taying.cheng/adversarial-attack-and-defense-on-neural-networks-in-pytorch-82b5bcd9171](https://medium.com/@taying.cheng/adversarial-attack-and-defense-on-neural-networks-in-pytorch-82b5bcd9171)",t2_jn2eq6v,False,,0,False,Adversarial attacks and defenses on neural networks in PyTorch,[],r/pytorch,False,6,,0,,,False,t3_ioxdi6,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1599613200.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I recently started doing research on adversarial attacks and defenses of neural networks. I thought it would be interesting to share with you one of the earliest yet effective attacks (FGSM) and how to defend it through adversarial training, implemented in PyTorch:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://medium.com/@taying.cheng/adversarial-attack-and-defense-on-neural-networks-in-pytorch-82b5bcd9171""&gt;https://medium.com/@taying.cheng/adversarial-attack-and-defense-on-neural-networks-in-pytorch-82b5bcd9171&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/7YUtbNt1YrG6O2oWZOL6dQp5Wzexc3ZknEgG2c5-11c.jpg?auto=webp&amp;s=4beb551d64f2e008479b0b02f8783cb555be297a', 'width': 700, 'height': 280}, 'resolutions': [{'url': 'https://external-preview.redd.it/7YUtbNt1YrG6O2oWZOL6dQp5Wzexc3ZknEgG2c5-11c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=14de295fe92e18b3f61a91ed0d5192e5177bfa69', 'width': 108, 'height': 43}, {'url': 'https://external-preview.redd.it/7YUtbNt1YrG6O2oWZOL6dQp5Wzexc3ZknEgG2c5-11c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7bde9343d70a8e292da0e3138e0f9af5aaa94b11', 'width': 216, 'height': 86}, {'url': 'https://external-preview.redd.it/7YUtbNt1YrG6O2oWZOL6dQp5Wzexc3ZknEgG2c5-11c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f275449d154927e883abab6616027767ca0fccf', 'width': 320, 'height': 128}, {'url': 'https://external-preview.redd.it/7YUtbNt1YrG6O2oWZOL6dQp5Wzexc3ZknEgG2c5-11c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6367c2bd94000f7a17057a232071a84c82ac639', 'width': 640, 'height': 256}], 'variants': {}, 'id': 'd2JG0vF9DVA0kwASjA5tSLSSbgOVL2Uifc2WH0bN0iU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ioxdi6,True,,tt12343,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ioxdi6/adversarial_attacks_and_defenses_on_neural/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ioxdi6/adversarial_attacks_and_defenses_on_neural/,7135,1599584400.0,0,,False,,,,,,,,
348,,pytorch,,t2_44mbtmjy,False,,0,False,Latest in drones! Efficient trajectory generation for chasing a dynamic target,[],r/pytorch,False,6,,0,85.0,,False,t3_iom2fj,False,dark,0.81,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/tROLJ9N7eOTlBcyVJJ3x7uI-m7ZFodwh0LNGtRf7bvw.jpg,False,,[],{},link,,False,,1599563573.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iom2fj,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/iom2fj/latest_in_drones_efficient_trajectory_generation/,all_ads,False,/r/LatestInML/comments/iol6q3/latest_in_drones_efficient_trajectory_generation/,7135,1599534773.0,0,,False,/r/LatestInML/comments/iol6q3/latest_in_drones_efficient_trajectory_generation/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://catalyzex.com/paper/arxiv:2009.01565)\n\nhttps://preview.redd.it/4dau8cx12ul51.png?width=2008&amp;format=png&amp;auto=webp&amp;s=cc160a683876e4a8b582507c35634a595b9331f4', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Latest in drones! Efficient trajectory generation for chasing a dynamic target', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 85, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'4dau8cx12ul51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 66, 'x': 108, 'u': 'https://preview.redd.it/4dau8cx12ul51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=651029670a86f5067e548220d78c93805582100b'}, {'y': 132, 'x': 216, 'u': 'https://preview.redd.it/4dau8cx12ul51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d9a50dfb377ce035c018dbde7d936a96b44a93d8'}, {'y': 196, 'x': 320, 'u': 'https://preview.redd.it/4dau8cx12ul51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b2b0c7fa7bb34c8e02e0ea50dd74864509c27a03'}, {'y': 392, 'x': 640, 'u': 'https://preview.redd.it/4dau8cx12ul51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c92821d4373b43d98772f6d54df3529ae3e44133'}, {'y': 589, 'x': 960, 'u': 'https://preview.redd.it/4dau8cx12ul51.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=725a817be61f688645cdfb671c30f94c1c102a87'}, {'y': 662, 'x': 1080, 'u': 'https://preview.redd.it/4dau8cx12ul51.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cc1d65efee9239a35e553cae4b4084cc2587654d'}], 's': {'y': 1232, 'x': 2008, 'u': 'https://preview.redd.it/4dau8cx12ul51.png?width=2008&amp;format=png&amp;auto=webp&amp;s=cc160a683876e4a8b582507c35634a595b9331f4'}, 'id': '4dau8cx12ul51'}}, 'name': 't3_iol6q3', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 27, 'total_awards_received': 1, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 27, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/tROLJ9N7eOTlBcyVJJ3x7uI-m7ZFodwh0LNGtRf7bvw.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1599560034.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://catalyzex.com/paper/arxiv:2009.01565""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/4dau8cx12ul51.png?width=2008&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc160a683876e4a8b582507c35634a595b9331f4""&gt;https://preview.redd.it/4dau8cx12ul51.png?width=2008&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc160a683876e4a8b582507c35634a595b9331f4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}, 'all_awardings': [{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'iol6q3', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/iol6q3/latest_in_drones_efficient_trajectory_generation/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/iol6q3/latest_in_drones_efficient_trajectory_generation/', 'subreddit_subscribers': 6676, 'created_utc': 1599531234.0, 'num_crossposts': 10, 'media': None, 'is_video': False}]",t3_iol6q3,,,,,
349,,pytorch,I am having trouble understanding Packed Sequences and Packed Padded Sequences in PyTorch. Can someone please explain it briefly?,t2_7906ydve,False,,0,False,Packed Sequences,[],r/pytorch,False,6,,0,,,False,t3_iomc0b,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1599564686.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am having trouble understanding Packed Sequences and Packed Padded Sequences in PyTorch. Can someone please explain it briefly?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iomc0b,True,,pithree-wan_fournobi,,2,True,all_ads,False,[],False,,/r/pytorch/comments/iomc0b/packed_sequences/,all_ads,False,https://www.reddit.com/r/pytorch/comments/iomc0b/packed_sequences/,7135,1599535886.0,0,,False,,,,,,,,
350,,pytorch,"I'm trying to run two Pytorch Model in an application on Nvidia P5000 using Docker . One is YOLO V5 &amp; another one is a pose estimation model .

First Situation: I loaded two model in a single script, these are supposed to be runsequentially, YOLO V5 followed by Pose Estimator. In this case, GPU Memory
usage is around 1.5GB out of 16 GB and Volatile GPU Util is max up to 35%.

Second Situation: Since YOLO V5 is quite faster, I thought of separating it inanother container to parallelize the inference. Now, GPU Memory is around 1.5-2GB out of 16 GB for both the container but GPU Util around 70-80% with muchfluctuations.

In the first case, yolov5 was taking around 2030 ms and pose estimation wastaking upto 180ms/frame. While in the second case YOLO V5 optimised to19ms/frame and pose estimation started taking 300-350 ms/frame.

I believe, this could be because of YOLO V5 container started taking upto 50%Volatile GPU Util.

I'm new to this, could anyone suggest the possible way to reduce/customise thevolatile GPU Until so that inference of pose estimate could be faster.

Any suggestions are highly appreciated.
Thanks in advance.",t2_7dtd77wx,False,,0,False,[Discussion] How to limit Volatile GPU Util in Pytorch on NVIDIA P5000?,[],r/pytorch,False,6,,0,,,False,t3_io4qb2,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1599501398.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to run two Pytorch Model in an application on Nvidia P5000 using Docker . One is YOLO V5 &amp;amp; another one is a pose estimation model .&lt;/p&gt;

&lt;p&gt;First Situation: I loaded two model in a single script, these are supposed to be run
sequentially, YOLO V5 followed by Pose Estimator. In this case, GPU Memory
usage is around 1.5GB out of 16 GB and Volatile GPU Util is max up to 35%.&lt;/p&gt;

&lt;p&gt;Second Situation: Since YOLO V5 is quite faster, I thought of separating it in
another container to parallelize the inference. Now, GPU Memory is around 1.5-2
GB out of 16 GB for both the container but GPU Util around 70-80% with much
fluctuations.&lt;/p&gt;

&lt;p&gt;In the first case, yolov5 was taking around 2030 ms and pose estimation was
taking upto 180ms/frame. While in the second case YOLO V5 optimised to
19ms/frame and pose estimation started taking 300-350 ms/frame.&lt;/p&gt;

&lt;p&gt;I believe, this could be because of YOLO V5 container started taking upto 50%
Volatile GPU Util.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m new to this, could anyone suggest the possible way to reduce/customise the
volatile GPU Until so that inference of pose estimate could be faster.&lt;/p&gt;

&lt;p&gt;Any suggestions are highly appreciated.
Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,io4qb2,True,,yekitra,,0,True,all_ads,False,[],False,,/r/pytorch/comments/io4qb2/discussion_how_to_limit_volatile_gpu_util_in/,all_ads,False,https://www.reddit.com/r/pytorch/comments/io4qb2/discussion_how_to_limit_volatile_gpu_util_in/,7135,1599472598.0,0,,False,,,,,,,,
351,,pytorch,,t2_2wsvqwhg,False,,0,False,"Facebook AI open-sources Opacus, a new high-speed library for training PyTorch models with differential privacy (DP)",[],r/pytorch,False,6,,0,59.0,,False,t3_inu23y,False,dark,1.0,,public,21,0,{},140.0,,False,[],,False,False,,{},,False,21,,False,https://b.thumbs.redditmedia.com/lwd72s-MfHZuhWjDRucE9-H8BEooBVD-2uDu9VAcHLg.jpg,False,,[],{},link,,False,,1599455393.0,text,6,,,text,marktechpost.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jZPyiZ05TUTXTtHp5T5VvEWxZVppv_9MAYSKF3Hk6VE.jpg?auto=webp&amp;s=1c8d5273a63903ff72a39de2c3abecf4079ca8fe', 'width': 714, 'height': 302}, 'resolutions': [{'url': 'https://external-preview.redd.it/jZPyiZ05TUTXTtHp5T5VvEWxZVppv_9MAYSKF3Hk6VE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2d731133d2f8156946f9f51cd6b8353a5a188ed4', 'width': 108, 'height': 45}, {'url': 'https://external-preview.redd.it/jZPyiZ05TUTXTtHp5T5VvEWxZVppv_9MAYSKF3Hk6VE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=aea839992cf8fafbdd8b5b5e9f8c376269962c1b', 'width': 216, 'height': 91}, {'url': 'https://external-preview.redd.it/jZPyiZ05TUTXTtHp5T5VvEWxZVppv_9MAYSKF3Hk6VE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db831632f4a0888fa4c29d329011caf9b7e96350', 'width': 320, 'height': 135}, {'url': 'https://external-preview.redd.it/jZPyiZ05TUTXTtHp5T5VvEWxZVppv_9MAYSKF3Hk6VE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=364478420328c54ea2149bc920b052d9477e45de', 'width': 640, 'height': 270}], 'variants': {}, 'id': 'XmMWF1vVbtdsCl_-TL7rpjWQIZg3stx29tV6vO3fGRw'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,inu23y,True,,ai-lover,,0,True,all_ads,False,[],False,,/r/pytorch/comments/inu23y/facebook_ai_opensources_opacus_a_new_highspeed/,all_ads,False,https://www.marktechpost.com/2020/09/06/facebook-ai-open-sources-opacus-a-new-high-speed-library-for-training-pytorch-models-with-differential-privacy-dp/,7135,1599426593.0,0,,False,https://www.marktechpost.com/2020/09/06/facebook-ai-open-sources-opacus-a-new-high-speed-library-for-training-pytorch-models-with-differential-privacy-dp/,,,,,,,
352,,pytorch,"Hi, I'm coming from tensorflow to pytorch (very recently) and I have a question: in tensorflow a frozen(deployment ready) model is much smaller than the trainable model. (so .pb is much smaller than .ckpt, if you're familiar). 

I just traced a pytorch model with 
```python
torch.jit.trace(net, inputs\_test)
```
but it's the same size. I'm guessing I have to choose what variables I keep and what I discard, or something like it? Can you point me in the right direction ? Thanks",t2_j6y6qhw,False,,0,False,"Traced model same size, shouldn't it be smaller ?",[],r/pytorch,False,6,,0,,,False,t3_io3sw7,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1599496275.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m coming from tensorflow to pytorch (very recently) and I have a question: in tensorflow a frozen(deployment ready) model is much smaller than the trainable model. (so .pb is much smaller than .ckpt, if you&amp;#39;re familiar). &lt;/p&gt;

&lt;p&gt;I just traced a pytorch model with 
&lt;code&gt;python
torch.jit.trace(net, inputs\_test)
&lt;/code&gt;
but it&amp;#39;s the same size. I&amp;#39;m guessing I have to choose what variables I keep and what I discard, or something like it? Can you point me in the right direction ? Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,io3sw7,True,,ned334,,0,True,all_ads,False,[],False,,/r/pytorch/comments/io3sw7/traced_model_same_size_shouldnt_it_be_smaller/,all_ads,False,https://www.reddit.com/r/pytorch/comments/io3sw7/traced_model_same_size_shouldnt_it_be_smaller/,7135,1599467475.0,0,,False,,,,,,,,
353,,pytorch,"Hi,

I am experimenting with fine-tuning neural networks for NLP tasks. However, a lot of resources for fine-tuning are centered around the computer vision concepts.

Does anyone know of any resources with fine tuning for NLP?",t2_j3gqk,False,,0,False,Resources on Finetuning Generic Neural Networks,[],r/pytorch,False,6,,0,,,False,t3_inmg1i,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1599429552.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I am experimenting with fine-tuning neural networks for NLP tasks. However, a lot of resources for fine-tuning are centered around the computer vision concepts.&lt;/p&gt;

&lt;p&gt;Does anyone know of any resources with fine tuning for NLP?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,inmg1i,True,,exact-approximate,,6,True,all_ads,False,[],False,,/r/pytorch/comments/inmg1i/resources_on_finetuning_generic_neural_networks/,all_ads,False,https://www.reddit.com/r/pytorch/comments/inmg1i/resources_on_finetuning_generic_neural_networks/,7135,1599400752.0,0,,False,,,,,,,,
354,,pytorch,"Hello community , is it possible to extract weight of my hidden layer using pytorch ?

Something like :

w_hidden1 = MyModel.hidden1.weights

And the output should render the tensor with the values .is it possible ?

Thank you",t2_7l9ti89m,False,,0,False,PYTORCH: I want to extract weight of every layer of my model,[],r/pytorch,False,6,,0,,,False,t3_inj7s5,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1599413533.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello community , is it possible to extract weight of my hidden layer using pytorch ?&lt;/p&gt;

&lt;p&gt;Something like :&lt;/p&gt;

&lt;p&gt;w_hidden1 = MyModel.hidden1.weights&lt;/p&gt;

&lt;p&gt;And the output should render the tensor with the values .is it possible ?&lt;/p&gt;

&lt;p&gt;Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,inj7s5,True,,rayanaay,,2,True,all_ads,False,[],False,,/r/pytorch/comments/inj7s5/pytorch_i_want_to_extract_weight_of_every_layer/,all_ads,False,https://www.reddit.com/r/pytorch/comments/inj7s5/pytorch_i_want_to_extract_weight_of_every_layer/,7135,1599384733.0,0,,False,,,,,,,,
355,,pytorch,"Hello community , I saw that scientist/PhD prefered pytorch over TF/Keras as it is faster to implement and deploy.

Is that true ? How do you can explain this choice ?",t2_7l9ti89m,False,,0,False,Implementing an algorithm from a scientific paper ?,[],r/pytorch,False,6,,0,,,False,t3_imz33q,False,dark,0.78,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1599331416.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello community , I saw that scientist/PhD prefered pytorch over TF/Keras as it is faster to implement and deploy.&lt;/p&gt;

&lt;p&gt;Is that true ? How do you can explain this choice ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,imz33q,True,,rayanaay,,7,True,all_ads,False,[],False,,/r/pytorch/comments/imz33q/implementing_an_algorithm_from_a_scientific_paper/,all_ads,False,https://www.reddit.com/r/pytorch/comments/imz33q/implementing_an_algorithm_from_a_scientific_paper/,7135,1599302616.0,0,,False,,,,,,,,
356,,pytorch,"I'm getting into meta-learning, and as far as I'm aware, I need to calculate Hessian matrices in order to properly calculate the meta-gradients. I've written the following toy code as a test, just to see if I can get second derivatives/Hessians using torch.autograd.grad

    import torch
    net = torch.nn.Sequential(
        torch.nn.Linear(5, 5)
        )
    X = torch.randn(5, 5)
    loss = net(X).mean()
    first_derivative = torch.autograd.grad(loss, net[0].weight, retain_graph = True)
    second_derivative = torch.autograd.grad(loss, first_derivative)

I have been told that retaining the graph when calculating the first derivative should let me get a second derivative by just calling .grad again. But, first_derivative does not come out with requires_grad = True, so I can't get the second derivative. Any ideas on how to fix that? The documentation and forums aren't exactly chock-full of people clamoring to do this.

(And here ""first_derivative"" is actually a tuple of tensors, but that doesn't make a difference, first_derivative[0] still doesn't have requires_grad)",t2_qaixi,False,,0,False,Calculate second derivatives of a parameter with respect to loss,[],r/pytorch,False,6,,0,,,False,t3_imfcl6,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1599252656.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m getting into meta-learning, and as far as I&amp;#39;m aware, I need to calculate Hessian matrices in order to properly calculate the meta-gradients. I&amp;#39;ve written the following toy code as a test, just to see if I can get second derivatives/Hessians using torch.autograd.grad&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import torch
net = torch.nn.Sequential(
    torch.nn.Linear(5, 5)
    )
X = torch.randn(5, 5)
loss = net(X).mean()
first_derivative = torch.autograd.grad(loss, net[0].weight, retain_graph = True)
second_derivative = torch.autograd.grad(loss, first_derivative)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have been told that retaining the graph when calculating the first derivative should let me get a second derivative by just calling .grad again. But, first_derivative does not come out with requires_grad = True, so I can&amp;#39;t get the second derivative. Any ideas on how to fix that? The documentation and forums aren&amp;#39;t exactly chock-full of people clamoring to do this.&lt;/p&gt;

&lt;p&gt;(And here &amp;quot;first_derivative&amp;quot; is actually a tuple of tensors, but that doesn&amp;#39;t make a difference, first_derivative[0] still doesn&amp;#39;t have requires_grad)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,imfcl6,True,,MrAcurite,,10,True,all_ads,False,[],False,,/r/pytorch/comments/imfcl6/calculate_second_derivatives_of_a_parameter_with/,all_ads,False,https://www.reddit.com/r/pytorch/comments/imfcl6/calculate_second_derivatives_of_a_parameter_with/,7135,1599223856.0,0,,False,,,,,,,,
357,,pytorch,"Hello community , coming from TF 2.0 I want to use Pytorch for its flexibility and it’s proximity to python.

I’m working on autoencoder and I want to :
-calculate the loss from the output and the input 
-calculate another loss ( KL Divergence) from one of my hidden layer to a  arbitrary parameter .

In Keras I couldn’t do that , even with a custom function 

In resume ,I want something like that

loss = MSE( output,input) + KLDiv(output_of_hidden_layer , constant_tensor  )


Thank you",t2_7l9ti89m,False,,0,False,Combining two loss functions in Pytorch,[],r/pytorch,False,6,,0,,,False,t3_imgjw1,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1599257198.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello community , coming from TF 2.0 I want to use Pytorch for its flexibility and it’s proximity to python.&lt;/p&gt;

&lt;p&gt;I’m working on autoencoder and I want to :
-calculate the loss from the output and the input 
-calculate another loss ( KL Divergence) from one of my hidden layer to a  arbitrary parameter .&lt;/p&gt;

&lt;p&gt;In Keras I couldn’t do that , even with a custom function &lt;/p&gt;

&lt;p&gt;In resume ,I want something like that&lt;/p&gt;

&lt;p&gt;loss = MSE( output,input) + KLDiv(output_of_hidden_layer , constant_tensor  )&lt;/p&gt;

&lt;p&gt;Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,imgjw1,True,,rayanaay,,4,True,all_ads,False,[],False,,/r/pytorch/comments/imgjw1/combining_two_loss_functions_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/imgjw1/combining_two_loss_functions_in_pytorch/,7135,1599228398.0,0,,False,,,,,,,,
358,,pytorch,,t2_ljhk2,False,,0,False,The incredible Pytorch (A curated repo of Pytorch resources),[],r/pytorch,False,6,,0,140.0,,False,t3_ilmfsp,False,dark,0.93,,public,26,0,{},140.0,,False,[],,False,False,,{},,False,26,,False,https://b.thumbs.redditmedia.com/_zPfb1lNeEzoYFa-HbHrxeaqMgU4OGHqNZgmXGuklng.jpg,False,,[],{},link,,False,,1599134820.0,text,6,,,text,github.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Mce89ICHHT6Tj5XmbUxpAZSmJdjNEwJon1GpsDhl0gg.jpg?auto=webp&amp;s=69463a7630a4efd7a9a552739b687a6840b0f0ae', 'width': 367, 'height': 367}, 'resolutions': [{'url': 'https://external-preview.redd.it/Mce89ICHHT6Tj5XmbUxpAZSmJdjNEwJon1GpsDhl0gg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e2d76eb98cf6e25c166f51d18413a3775b7d8efe', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/Mce89ICHHT6Tj5XmbUxpAZSmJdjNEwJon1GpsDhl0gg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b280590eba3cc8dc9170528bb0579b3c4bb206b5', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/Mce89ICHHT6Tj5XmbUxpAZSmJdjNEwJon1GpsDhl0gg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b112334480da24bcaaf053a8bbac3bb1324d2ce2', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'OKnHG270v58GaLwTeG0I3wlSokmLhrhuYwPnS3SScH4'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ilmfsp,True,,Dawny33,,4,True,all_ads,False,[],False,,/r/pytorch/comments/ilmfsp/the_incredible_pytorch_a_curated_repo_of_pytorch/,all_ads,False,https://github.com/ritchieng/the-incredible-pytorch,7135,1599106020.0,0,,False,https://github.com/ritchieng/the-incredible-pytorch,,,,,,,
359,,pytorch,"[https://www.youtube.com/watch?v=ZWg1gKWwUhY](https://www.youtube.com/watch?v=ZWg1gKWwUhY)

You can find all links in the video description! Let me know your thoughts.

PS: This is a very alpha version work :) Would appreciate the feedback from this community!",t2_3sk7t1ft,False,,0,False,TorchExpo : My submission to PyTorch Summer Hackathon 2020,[],r/pytorch,False,6,,0,,,False,t3_ilq1sz,False,dark,1.0,,public,8,1,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},self,,True,,1599153117.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=ZWg1gKWwUhY""&gt;https://www.youtube.com/watch?v=ZWg1gKWwUhY&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can find all links in the video description! Let me know your thoughts.&lt;/p&gt;

&lt;p&gt;PS: This is a very alpha version work :) Would appreciate the feedback from this community!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/OSi1VW9vNQEYG6GJLh9vEdMxQOQ79t4ucERwWNBKCuQ.jpg?auto=webp&amp;s=090ff78608a87f8c26c558b3eb7ebd482c6fe3ec', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/OSi1VW9vNQEYG6GJLh9vEdMxQOQ79t4ucERwWNBKCuQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2911753a7371f316e01a94bcda8607d8eaadfa7', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/OSi1VW9vNQEYG6GJLh9vEdMxQOQ79t4ucERwWNBKCuQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fceb0f158f40a3dbd08701e214fdb7e94ec34528', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/OSi1VW9vNQEYG6GJLh9vEdMxQOQ79t4ucERwWNBKCuQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=575c6f98e09a04f6730e421dfdae6b7241a985f5', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'sw-y37PF87blekXYiHF5JQGsxq6IX0NR6WN-8n06ETw'}], 'enabled': False}","[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ilq1sz,True,,op_prabhuomkar,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ilq1sz/torchexpo_my_submission_to_pytorch_summer/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ilq1sz/torchexpo_my_submission_to_pytorch_summer/,7135,1599124317.0,0,,False,,,,,,,,
360,,pytorch,,t2_44mbtmjy,False,,0,False,Recovering multi-person 3D poses from a single RGB image!,[],r/pytorch,False,6,,0,98.0,,False,t3_ilijy4,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/zFxwXbd6YY8WegimANUDJbjrsBxkedVIlRQQxva15lU.jpg,False,,[],{},link,,False,,1599119590.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/P5dHnpgNoMYypzJAKOWBwvVO5ZEvMYbf__UAfaIKDFU.jpg?auto=webp&amp;s=6e6e992c74881ffc84a4b2ede231484edf71b3a7', 'width': 698, 'height': 490}, 'resolutions': [{'url': 'https://external-preview.redd.it/P5dHnpgNoMYypzJAKOWBwvVO5ZEvMYbf__UAfaIKDFU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=85a4f502dfaa793649e14f243bccbb0106fe5102', 'width': 108, 'height': 75}, {'url': 'https://external-preview.redd.it/P5dHnpgNoMYypzJAKOWBwvVO5ZEvMYbf__UAfaIKDFU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6c0a4af9a99e488f304b49ebdde5e5c10743d00', 'width': 216, 'height': 151}, {'url': 'https://external-preview.redd.it/P5dHnpgNoMYypzJAKOWBwvVO5ZEvMYbf__UAfaIKDFU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=63c06c2a49a148678340e352d46139fba4bad690', 'width': 320, 'height': 224}, {'url': 'https://external-preview.redd.it/P5dHnpgNoMYypzJAKOWBwvVO5ZEvMYbf__UAfaIKDFU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=73cd98997f4c01a39df12f1941950dfb7c4934c9', 'width': 640, 'height': 449}], 'variants': {}, 'id': '1n1slrsssSZeiTqXvfWZ0IA-TrY-Xy0GI9D6EEzt8Ck'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ilijy4,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ilijy4/recovering_multiperson_3d_poses_from_a_single_rgb/,all_ads,False,/r/LatestInML/comments/iliic2/recovering_multiperson_3d_poses_from_a_single_rgb/,7135,1599090790.0,0,,False,/r/LatestInML/comments/iliic2/recovering_multiperson_3d_poses_from_a_single_rgb/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2008.11469)\n\n&amp;#x200B;\n\nhttps://reddit.com/link/iliic2/video/3h3wr7pvntk51/player', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Recovering multi-person 3D poses from a single RGB image!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 98, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'3h3wr7pvntk51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/iliic2/asset/3h3wr7pvntk51/DASHPlaylist.mpd?a=1618044186%2CMzUyYzVhNTlhOTRhNDY0MGFkOTI2MGUwZDMxZjJiN2ZiZDU0ZWMyNjg2ZDU2ODY5MjQzOTE4YjIzMTM3NWYxMQ%3D%3D&amp;v=1&amp;f=sd', 'x': 640, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/iliic2/asset/3h3wr7pvntk51/HLSPlaylist.m3u8?a=1618044186%2CNTlhMTk0OGMzYmNkZjViOTkyNzU3YWE1ZDBjNzk2YzAwMzY0MDY0NmQxNGJlMTY2NDVlNWJkNTJjMWNmMjAyYQ%3D%3D&amp;v=1&amp;f=sd', 'id': '3h3wr7pvntk51', 'isGif': False}}, 'name': 't3_iliic2', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 73, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 73, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/zFxwXbd6YY8WegimANUDJbjrsBxkedVIlRQQxva15lU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1599119428.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2008.11469""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/iliic2/video/3h3wr7pvntk51/player""&gt;https://reddit.com/link/iliic2/video/3h3wr7pvntk51/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/P5dHnpgNoMYypzJAKOWBwvVO5ZEvMYbf__UAfaIKDFU.jpg?auto=webp&amp;s=6e6e992c74881ffc84a4b2ede231484edf71b3a7', 'width': 698, 'height': 490}, 'resolutions': [{'url': 'https://external-preview.redd.it/P5dHnpgNoMYypzJAKOWBwvVO5ZEvMYbf__UAfaIKDFU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=85a4f502dfaa793649e14f243bccbb0106fe5102', 'width': 108, 'height': 75}, {'url': 'https://external-preview.redd.it/P5dHnpgNoMYypzJAKOWBwvVO5ZEvMYbf__UAfaIKDFU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6c0a4af9a99e488f304b49ebdde5e5c10743d00', 'width': 216, 'height': 151}, {'url': 'https://external-preview.redd.it/P5dHnpgNoMYypzJAKOWBwvVO5ZEvMYbf__UAfaIKDFU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=63c06c2a49a148678340e352d46139fba4bad690', 'width': 320, 'height': 224}, {'url': 'https://external-preview.redd.it/P5dHnpgNoMYypzJAKOWBwvVO5ZEvMYbf__UAfaIKDFU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=73cd98997f4c01a39df12f1941950dfb7c4934c9', 'width': 640, 'height': 449}], 'variants': {}, 'id': '1n1slrsssSZeiTqXvfWZ0IA-TrY-Xy0GI9D6EEzt8Ck'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'iliic2', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/iliic2/recovering_multiperson_3d_poses_from_a_single_rgb/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/iliic2/recovering_multiperson_3d_poses_from_a_single_rgb/', 'subreddit_subscribers': 6676, 'created_utc': 1599090628.0, 'num_crossposts': 10, 'media': None, 'is_video': False}]",t3_iliic2,,,,,
361,,pytorch,"Podcast alert!

Recently, I was invited on a podcast run by [**Ritesh K**](https://www.linkedin.com/feed/#) of Augmented Startups. We discussed my journey as an entrepreneur and educator, why I started the blog - LearnOpenCV, OpenCV 20th anniversary celebrations, the AI courses and what we plan for the future.

You can find the full video at [https://youtu.be/SbnByehgxps](https://youtu.be/SbnByehgxps).

https://preview.redd.it/p6s31j5nork51.png?width=1280&amp;format=png&amp;auto=webp&amp;s=1968c208f8972ad2f0c5bb36b846e6e99ae4c800",t2_cvc9f,False,,0,False,"Podcast - LearnOpenCV, OpenCV 20th anniversary celebrations, AI courses",[],r/pytorch,False,6,,0,78.0,,False,t3_ilawxv,False,dark,1.0,,public,4,0,{},140.0,,False,[],,False,False,,{},,False,4,,False,https://b.thumbs.redditmedia.com/CJLzJPjrlmftSzxrpKJD074F9hMvGp8X9lgWOZk9DqY.jpg,False,,[],{},,,True,,1599095463.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Podcast alert!&lt;/p&gt;

&lt;p&gt;Recently, I was invited on a podcast run by &lt;a href=""https://www.linkedin.com/feed/#""&gt;&lt;strong&gt;Ritesh K&lt;/strong&gt;&lt;/a&gt; of Augmented Startups. We discussed my journey as an entrepreneur and educator, why I started the blog - LearnOpenCV, OpenCV 20th anniversary celebrations, the AI courses and what we plan for the future.&lt;/p&gt;

&lt;p&gt;You can find the full video at &lt;a href=""https://youtu.be/SbnByehgxps""&gt;https://youtu.be/SbnByehgxps&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/p6s31j5nork51.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1968c208f8972ad2f0c5bb36b846e6e99ae4c800""&gt;https://preview.redd.it/p6s31j5nork51.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1968c208f8972ad2f0c5bb36b846e6e99ae4c800&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ilawxv,True,,spmallick,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ilawxv/podcast_learnopencv_opencv_20th_anniversary/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ilawxv/podcast_learnopencv_opencv_20th_anniversary/,7135,1599066663.0,0,,False,,,,"{'p6s31j5nork51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/p6s31j5nork51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2d681e74b1a222dd3037cda3740cb149ef52f44'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/p6s31j5nork51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b842247fef21fd14197cd1397d8d2cd09d881c47'}, {'y': 180, 'x': 320, 'u': 'https://preview.redd.it/p6s31j5nork51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9dfb96613ba9c2e2d64390c18b2556c43288cbc1'}, {'y': 360, 'x': 640, 'u': 'https://preview.redd.it/p6s31j5nork51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2b4f2c6da499b43d717bab28a770ca7c7e2dc6ce'}, {'y': 540, 'x': 960, 'u': 'https://preview.redd.it/p6s31j5nork51.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=594fbf4aa504cf970e4803d215017a7764eda8b6'}, {'y': 607, 'x': 1080, 'u': 'https://preview.redd.it/p6s31j5nork51.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bed9ae8e63b276b2d27f6b1774f4e435251831b7'}], 's': {'y': 720, 'x': 1280, 'u': 'https://preview.redd.it/p6s31j5nork51.png?width=1280&amp;format=png&amp;auto=webp&amp;s=1968c208f8972ad2f0c5bb36b846e6e99ae4c800'}, 'id': 'p6s31j5nork51'}}",,,,
362,,pytorch,"Maybe this qualifies as customer experience or some shit, but this is why Nvidia has the ML community fucking locked down (besides the people that use Google cloud). AMD doesn't do shit for us, meanwhile Nvidia keeps pumping out upgraded tensor cores and library support.

So thank you Nvidia, and please consider me for a job on your ML R&amp;D team.",t2_qaixi,False,,0,False,"While Nvidia is a private corporation after nothing but their own bottom dollar, I still want to take a moment to appreciate them creating the Apex and AMP packages for Torch",[],r/pytorch,False,6,,0,,,False,t3_ikxq58,False,dark,0.78,,public,23,0,{},,,False,[],,False,False,,{},,False,23,,False,self,False,,[],{},,,True,,1599040004.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Maybe this qualifies as customer experience or some shit, but this is why Nvidia has the ML community fucking locked down (besides the people that use Google cloud). AMD doesn&amp;#39;t do shit for us, meanwhile Nvidia keeps pumping out upgraded tensor cores and library support.&lt;/p&gt;

&lt;p&gt;So thank you Nvidia, and please consider me for a job on your ML R&amp;amp;D team.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ikxq58,True,,MrAcurite,,5,True,all_ads,False,[],False,,/r/pytorch/comments/ikxq58/while_nvidia_is_a_private_corporation_after/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ikxq58/while_nvidia_is_a_private_corporation_after/,7135,1599011204.0,0,,False,,,,,,,,
363,,pytorch,"In today's blog post, we continue our discussion on how to get a 4 to 6 times inference speed-up using TensorRT.

In our previous post on [Using TensorRT for inference speed-up](https://el2.convertkit-mail.com/c/75ugmp32ogi8hdpmlkh9/kkhmh2u97w9z4gfl/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL2hvdy10by1jb252ZXJ0LWEtbW9kZWwtZnJvbS1weXRvcmNoLXRvLXRlbnNvcnJ0LWFuZC1zcGVlZC11cC1pbmZlcmVuY2Uv) ([https://www.learnopencv.com/how-to-convert-a-model-from-pytorch-to-tensorrt-and-speed-up-inference/?ck\_subscriber\_id=371373457](https://www.learnopencv.com/how-to-convert-a-model-from-pytorch-to-tensorrt-and-speed-up-inference/?ck_subscriber_id=371373457)), we discussed how to convert your PyTorch model to TensorRT FP16 (16-bit floating point) model using the Python API to achieve the speed-up.

In today's post, we learn how to do it using the C++ API. Python and C++ APIs have their own advantages and disadvantages. For example, Windows OS does not have support for the Python API, so if you are a Windows user, the C++ API is your only option. We are sharing step by step instructions and example code!

  
[https://www.learnopencv.com/how-to-run-inference-using-tensorrt-c-api/](https://el2.convertkit-mail.com/c/75ugmp32ogi8hdpmlkh9/58hvh8u9ex93lvt6/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL2hvdy10by1ydW4taW5mZXJlbmNlLXVzaW5nLXRlbnNvcnJ0LWMtYXBpLw==)

https://preview.redd.it/r4fyn28jehk51.png?width=1075&amp;format=png&amp;auto=webp&amp;s=2958ed4f8f4e834bfe28208043756ac652eba135",t2_cvc9f,False,,0,False,How To Run Inference Using TensorRT C++ API,[],r/pytorch,False,6,,0,73.0,,False,t3_ikfcpx,False,dark,1.0,,public,6,0,{},140.0,,False,[],,False,False,,{},,False,6,,False,https://b.thumbs.redditmedia.com/kfT7xE1RtWmbivuF1q7oNjhSBd47Y1ULbDN730crg2A.jpg,False,,[],{},,,True,,1598970994.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In today&amp;#39;s blog post, we continue our discussion on how to get a 4 to 6 times inference speed-up using TensorRT.&lt;/p&gt;

&lt;p&gt;In our previous post on &lt;a href=""https://el2.convertkit-mail.com/c/75ugmp32ogi8hdpmlkh9/kkhmh2u97w9z4gfl/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL2hvdy10by1jb252ZXJ0LWEtbW9kZWwtZnJvbS1weXRvcmNoLXRvLXRlbnNvcnJ0LWFuZC1zcGVlZC11cC1pbmZlcmVuY2Uv""&gt;Using TensorRT for inference speed-up&lt;/a&gt; (&lt;a href=""https://www.learnopencv.com/how-to-convert-a-model-from-pytorch-to-tensorrt-and-speed-up-inference/?ck_subscriber_id=371373457""&gt;https://www.learnopencv.com/how-to-convert-a-model-from-pytorch-to-tensorrt-and-speed-up-inference/?ck_subscriber_id=371373457&lt;/a&gt;), we discussed how to convert your PyTorch model to TensorRT FP16 (16-bit floating point) model using the Python API to achieve the speed-up.&lt;/p&gt;

&lt;p&gt;In today&amp;#39;s post, we learn how to do it using the C++ API. Python and C++ APIs have their own advantages and disadvantages. For example, Windows OS does not have support for the Python API, so if you are a Windows user, the C++ API is your only option. We are sharing step by step instructions and example code!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://el2.convertkit-mail.com/c/75ugmp32ogi8hdpmlkh9/58hvh8u9ex93lvt6/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL2hvdy10by1ydW4taW5mZXJlbmNlLXVzaW5nLXRlbnNvcnJ0LWMtYXBpLw==""&gt;https://www.learnopencv.com/how-to-run-inference-using-tensorrt-c-api/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/r4fyn28jehk51.png?width=1075&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2958ed4f8f4e834bfe28208043756ac652eba135""&gt;https://preview.redd.it/r4fyn28jehk51.png?width=1075&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2958ed4f8f4e834bfe28208043756ac652eba135&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ikfcpx,True,,spmallick,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ikfcpx/how_to_run_inference_using_tensorrt_c_api/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ikfcpx/how_to_run_inference_using_tensorrt_c_api/,7135,1598942194.0,0,,False,,,,"{'r4fyn28jehk51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 56, 'x': 108, 'u': 'https://preview.redd.it/r4fyn28jehk51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3aff0c19547481f190261101dfcad9466e2e5588'}, {'y': 112, 'x': 216, 'u': 'https://preview.redd.it/r4fyn28jehk51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=06e4cd0c4a4a1b9de045fbac982e777f9883bf7e'}, {'y': 166, 'x': 320, 'u': 'https://preview.redd.it/r4fyn28jehk51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dfb0aac4f51b25b8d8d2bf07eb53f8a354377714'}, {'y': 333, 'x': 640, 'u': 'https://preview.redd.it/r4fyn28jehk51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5c69a966d21dcb2e8f9665932163c66212ea8fca'}, {'y': 500, 'x': 960, 'u': 'https://preview.redd.it/r4fyn28jehk51.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5519e17a1779defa4de523ee0bb0850fb5e3f59'}], 's': {'y': 561, 'x': 1075, 'u': 'https://preview.redd.it/r4fyn28jehk51.png?width=1075&amp;format=png&amp;auto=webp&amp;s=2958ed4f8f4e834bfe28208043756ac652eba135'}, 'id': 'r4fyn28jehk51'}}",,,,
364,,pytorch,"So, I want to predict the average temperature with hourly times from 1980 to 2020. A linear function doesn´t work. How would I approach this? WHat should I do with the high date-numbers (f.e 2020310105) and which model do I use use? I tried also a RNN and LSTM, but in the end there is just a straight line with a loss of 65 (after training, it doesn´t get lower) My data has two columns, one for date and one for the average temperature. This is the best result I could get: 

https://preview.redd.it/nstwkc10hjk51.png?width=640&amp;format=png&amp;auto=webp&amp;s=104a1bb740f9cecd07c9c0895afbf958c2a26a41",t2_15p9us,False,,0,False,How do I do approach this?,[],r/pytorch,False,6,,0,105.0,,False,t3_ikkbss,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/RQP5TUOBwgnj7Ec3LpQccSphy6tXzKh0LOP8oiMcBKE.jpg,False,,[],{},,,True,,1598996020.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So, I want to predict the average temperature with hourly times from 1980 to 2020. A linear function doesn´t work. How would I approach this? WHat should I do with the high date-numbers (f.e 2020310105) and which model do I use use? I tried also a RNN and LSTM, but in the end there is just a straight line with a loss of 65 (after training, it doesn´t get lower) My data has two columns, one for date and one for the average temperature. This is the best result I could get: &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/nstwkc10hjk51.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=104a1bb740f9cecd07c9c0895afbf958c2a26a41""&gt;https://preview.redd.it/nstwkc10hjk51.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=104a1bb740f9cecd07c9c0895afbf958c2a26a41&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ikkbss,True,,Trexagamer,,1,True,all_ads,False,[],False,,/r/pytorch/comments/ikkbss/how_do_i_do_approach_this/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ikkbss/how_do_i_do_approach_this/,7135,1598967220.0,0,,False,,,,"{'nstwkc10hjk51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 81, 'x': 108, 'u': 'https://preview.redd.it/nstwkc10hjk51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a672010c1df8bd5843fd21d795b966559d80e37'}, {'y': 162, 'x': 216, 'u': 'https://preview.redd.it/nstwkc10hjk51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f21989dc046d2577fc5cc227e97f4d6aefe29a9e'}, {'y': 240, 'x': 320, 'u': 'https://preview.redd.it/nstwkc10hjk51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=89d53de6f3e6d86e2c92497aec2571e264623eca'}, {'y': 480, 'x': 640, 'u': 'https://preview.redd.it/nstwkc10hjk51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=af51c7dcc8b1f596223ff911386be665cfc86aec'}], 's': {'y': 480, 'x': 640, 'u': 'https://preview.redd.it/nstwkc10hjk51.png?width=640&amp;format=png&amp;auto=webp&amp;s=104a1bb740f9cecd07c9c0895afbf958c2a26a41'}, 'id': 'nstwkc10hjk51'}}",,,,
365,,pytorch,,t2_44mbtmjy,False,,0,False,Generating photo-realistic face images from hand-drawn sketches!,[],r/pytorch,False,6,,0,140.0,,False,t3_ikerbo,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/vI_WpCO3-3hbMwyAwVM0VnrQD1DnPKjhSFfCOE1t89M.jpg,False,,[],{},link,,False,,1598967965.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ikerbo,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ikerbo/generating_photorealistic_face_images_from/,all_ads,False,/r/LatestInML/comments/ikeh82/generating_photorealistic_face_images_from/,7135,1598939165.0,0,,False,/r/LatestInML/comments/ikeh82/generating_photorealistic_face_images_from/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2008.13343)\n\nhttps://preview.redd.it/4jkb6j580hk51.jpg?width=666&amp;format=pjpg&amp;auto=webp&amp;s=71fc07b1a247535c616db82d8430ebdc073b02f7', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Generating photo-realistic face images from hand-drawn sketches!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'4jkb6j580hk51': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 174, 'x': 108, 'u': 'https://preview.redd.it/4jkb6j580hk51.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=337702e56d3d991d3d01a85cf5fb2c927a83332b'}, {'y': 348, 'x': 216, 'u': 'https://preview.redd.it/4jkb6j580hk51.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0703a9c8da655fb51ed9a49377a3d28a4f690050'}, {'y': 516, 'x': 320, 'u': 'https://preview.redd.it/4jkb6j580hk51.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eefa17d3f07b92d10c706ffa21ee1c0efc1df15b'}, {'y': 1032, 'x': 640, 'u': 'https://preview.redd.it/4jkb6j580hk51.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=84d3c93dfc08fb5ca81f623a144bf6f97cdf1b56'}], 's': {'y': 1074, 'x': 666, 'u': 'https://preview.redd.it/4jkb6j580hk51.jpg?width=666&amp;format=pjpg&amp;auto=webp&amp;s=71fc07b1a247535c616db82d8430ebdc073b02f7'}, 'id': '4jkb6j580hk51'}}, 'name': 't3_ikeh82', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 30, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 30, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/vI_WpCO3-3hbMwyAwVM0VnrQD1DnPKjhSFfCOE1t89M.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1598966581.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2008.13343""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/4jkb6j580hk51.jpg?width=666&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=71fc07b1a247535c616db82d8430ebdc073b02f7""&gt;https://preview.redd.it/4jkb6j580hk51.jpg?width=666&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=71fc07b1a247535c616db82d8430ebdc073b02f7&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ikeh82', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/ikeh82/generating_photorealistic_face_images_from/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/ikeh82/generating_photorealistic_face_images_from/', 'subreddit_subscribers': 6676, 'created_utc': 1598937781.0, 'num_crossposts': 10, 'media': None, 'is_video': False}]",t3_ikeh82,,,,,
366,,pytorch,"Hi Guys, sharing my recently created PyPi package for monitoring feature maps of a neural network in real-time during training of the network. This is an open source projects so contributions and suggestions for new and existing features are most welcomed.

&gt;PyPi Link: [https://pypi.org/project/laymon/](https://pypi.org/project/laymon/)  
&gt;  
&gt;Github: [https://github.com/shubham3121/laymon/](https://github.com/shubham3121/laymon/)",t2_7ltba4ss,False,,0,False,PyTorch - Python package for monitoring and visualising layers of a neural network in real-time,[],r/pytorch,False,6,,0,,,False,t3_ijti91,False,dark,1.0,,public,25,0,{},,,False,[],,False,False,,{},,False,25,,False,self,False,,[],{},self,,True,,1598887056.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Guys, sharing my recently created PyPi package for monitoring feature maps of a neural network in real-time during training of the network. This is an open source projects so contributions and suggestions for new and existing features are most welcomed.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;PyPi Link: &lt;a href=""https://pypi.org/project/laymon/""&gt;https://pypi.org/project/laymon/&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/shubham3121/laymon/""&gt;https://github.com/shubham3121/laymon/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/p3HQXQzdkmyXyJ89enL6PkgmSdstFY5z1QkzOzRNUaU.jpg?auto=webp&amp;s=01b29ed2d2e90d072e1fc7295da2c1cb3797f686', 'width': 300, 'height': 300}, 'resolutions': [{'url': 'https://external-preview.redd.it/p3HQXQzdkmyXyJ89enL6PkgmSdstFY5z1QkzOzRNUaU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=54fdfef1cb192ed04e4ba25828970287fc1ecde9', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/p3HQXQzdkmyXyJ89enL6PkgmSdstFY5z1QkzOzRNUaU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=22ff535b36cf2b9412be48a21108ba34479e2ba5', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'POVR4AtJHvry29k6WQQwgYhMSdLrOeYwBodMqA6lPGk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ijti91,True,,shubham3121,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ijti91/pytorch_python_package_for_monitoring_and/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ijti91/pytorch_python_package_for_monitoring_and/,7135,1598858256.0,2,,False,,,,,,,,
367,,pytorch,,t2_ljhk2,False,,0,False,PyTorch Performance Tuning Guide,[],r/pytorch,False,6,,0,105.0,,False,t3_ijvts0,False,dark,1.0,,public,12,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/9mS1fIYj1So?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'PyTorch Performance Tuning Guide - Szymon Migacz, NVIDIA', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/9mS1fIYj1So?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Arun Mallya', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/9mS1fIYj1So/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC-UkNfaVzE2-du7bFmbJ9Ag'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/9mS1fIYj1So?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ijvts0', 'height': 338}",,False,12,,False,https://b.thumbs.redditmedia.com/ijtxM_ZzB_D1mOeffnYz_SptBr8e9hmJzxiEYfJbJJc.jpg,False,,[],{},rich:video,,False,,1598900064.0,text,6,,,text,youtube.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/A1Gosr6Vc30gcYkwqgWj1IvK1pcwGNbxxo7SAYNFSDs.jpg?auto=webp&amp;s=7292aa2e3763848b4a2c09fea4a207bc30d7b1be', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/A1Gosr6Vc30gcYkwqgWj1IvK1pcwGNbxxo7SAYNFSDs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f998fee1f6fe57097e779e8222a336b3799cad7', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/A1Gosr6Vc30gcYkwqgWj1IvK1pcwGNbxxo7SAYNFSDs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=40b25e932671d9683defb2538bc310c068ede37d', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/A1Gosr6Vc30gcYkwqgWj1IvK1pcwGNbxxo7SAYNFSDs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=216a1be429fddbb2de52fd5d6a44ef4f40b73f25', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'W0f5hZE5wLsf8hJE-KJEkol4mndu7REgRbGEwVUtIAk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ijvts0,True,,Dawny33,,1,True,all_ads,False,[],False,,/r/pytorch/comments/ijvts0/pytorch_performance_tuning_guide/,all_ads,False,https://www.youtube.com/watch?v=9mS1fIYj1So&amp;feature=youtu.be,7135,1598871264.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'PyTorch Performance Tuning Guide - Szymon Migacz, NVIDIA', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/9mS1fIYj1So?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Arun Mallya', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/9mS1fIYj1So/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC-UkNfaVzE2-du7bFmbJ9Ag'}, 'type': 'youtube.com'}",False,https://www.youtube.com/watch?v=9mS1fIYj1So&amp;feature=youtu.be,,,,,,,
368,,pytorch,"I am trying to run a simple LSTM with 2 hidden layer and 100 hidden dimension on MSR Action Dataset (which consist of 20 class label and 60 feature) and the accuracy on testset is very low (10-15%),I have run RNN on the same dataset and its giving pretty decent accuracy (45-50%).Can anybody please tell me what i am doing wrong and suggest a better way to improve my accuracy.

&amp;#x200B;

[LSTM code](https://github.com/mirsahib/Project-Andromeda/blob/master/Code/Pytorch/LSTM.ipynb)

[RNN code](https://github.com/mirsahib/Project-Andromeda/blob/master/Code/Pytorch/Recurrent_Neural_Network.ipynb)",t2_z7gxp,False,,0,False,LSTM accuracy lower than RNN,[],r/pytorch,False,6,,0,,,False,t3_ijsl74,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1598853573.0,,[],{},self,,True,,1598882078.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to run a simple LSTM with 2 hidden layer and 100 hidden dimension on MSR Action Dataset (which consist of 20 class label and 60 feature) and the accuracy on testset is very low (10-15%),I have run RNN on the same dataset and its giving pretty decent accuracy (45-50%).Can anybody please tell me what i am doing wrong and suggest a better way to improve my accuracy.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/mirsahib/Project-Andromeda/blob/master/Code/Pytorch/LSTM.ipynb""&gt;LSTM code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/mirsahib/Project-Andromeda/blob/master/Code/Pytorch/Recurrent_Neural_Network.ipynb""&gt;RNN code&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AiKJiUBlrNzIad5Djt5I5YZ5RVBzUmir7UFwkbULNGU.jpg?auto=webp&amp;s=4a31c2ddd3ddcf684884a09f4f1c489ea82a5d82', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/AiKJiUBlrNzIad5Djt5I5YZ5RVBzUmir7UFwkbULNGU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2eb72b92fd7e872ffaee330e66db01ea99ad8e0', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/AiKJiUBlrNzIad5Djt5I5YZ5RVBzUmir7UFwkbULNGU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd7b2fad9e451c4cccbed61c4365564dcd0fca48', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/AiKJiUBlrNzIad5Djt5I5YZ5RVBzUmir7UFwkbULNGU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=47235161d625f03ecd907a8517c6908bf3c12e44', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_y2sBQRncsBvC6Dpt9chF-Rz5BRO23HyVryWXZRNKb0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ijsl74,True,,mirsahib,,1,True,all_ads,False,[],False,,/r/pytorch/comments/ijsl74/lstm_accuracy_lower_than_rnn/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ijsl74/lstm_accuracy_lower_than_rnn/,7135,1598853278.0,0,,False,,,,,,,,
369,,pytorch," Hi guys,

Let me introduce a repository that you might find useful during deep learning training especially when you use large batch size in PyTorch. If you ever needed or wished to try out the training of a model with bigger batch size than you could solve with your own GPU memory or with Google Colab you would find our library a useful tool. There are some useful tools that help to manage CUDA memory. Remember, this is a first release, but we are working on the code to improve it.

[https://github.com/hyperrixel/infinitybatch](https://github.com/hyperrixel/infinitybatch)

Regards,

Richard",t2_5cm7w86k,False,,0,False,Repository to help training with larger batch size,[],r/pytorch,False,6,,0,,,False,t3_iizkh0,False,dark,0.91,,public,9,0,{},,,False,[],,False,False,,{},,False,9,,False,self,False,,[],{},self,,True,,1598761692.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys,&lt;/p&gt;

&lt;p&gt;Let me introduce a repository that you might find useful during deep learning training especially when you use large batch size in PyTorch. If you ever needed or wished to try out the training of a model with bigger batch size than you could solve with your own GPU memory or with Google Colab you would find our library a useful tool. There are some useful tools that help to manage CUDA memory. Remember, this is a first release, but we are working on the code to improve it.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/hyperrixel/infinitybatch""&gt;https://github.com/hyperrixel/infinitybatch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Regards,&lt;/p&gt;

&lt;p&gt;Richard&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/A5Lwx8Il3V4c-ILw_DPBY3skBiNFY8hxZ2Cg7k6y4cI.jpg?auto=webp&amp;s=065f3fe45ab193b35aaa797a4d267730b82b4e94', 'width': 247, 'height': 247}, 'resolutions': [{'url': 'https://external-preview.redd.it/A5Lwx8Il3V4c-ILw_DPBY3skBiNFY8hxZ2Cg7k6y4cI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=61e6b7669daff41f3599a6afef17826354d88e4f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/A5Lwx8Il3V4c-ILw_DPBY3skBiNFY8hxZ2Cg7k6y4cI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=489d7272bda64982c3bc584cddc2c05f17579bcc', 'width': 216, 'height': 216}], 'variants': {}, 'id': '1cLvCDuKHM7b5J3A76yn0N42LVXEZAMl717sIWiSCts'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iizkh0,True,,richardvecsey,,4,True,all_ads,False,[],False,,/r/pytorch/comments/iizkh0/repository_to_help_training_with_larger_batch_size/,all_ads,False,https://www.reddit.com/r/pytorch/comments/iizkh0/repository_to_help_training_with_larger_batch_size/,7135,1598732892.0,0,,False,,,,,,,,
370,,pytorch,,t2_44mbtmjy,False,,0,False,State of the art in lip-syncing a talking face video!,[],r/pytorch,False,6,,0,28.0,,False,t3_ij3e55,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/aLb3-5LwoSLdq-2Pd8FKOyE_ngSTDHuvHbxHMQKkcIg.jpg,False,,[],{},link,,False,,1598775818.0,text,6,,,text,self.LatestInML,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xtFs7g9kkWqXgZZ72wPdiJYKMUPMJ5AkAj4qm-gQUa0.jpg?auto=webp&amp;s=c6b4a4952b0d5958a1ee23175775b0888e3af850', 'width': 1256, 'height': 260}, 'resolutions': [{'url': 'https://external-preview.redd.it/xtFs7g9kkWqXgZZ72wPdiJYKMUPMJ5AkAj4qm-gQUa0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bc28199f96944b105964ffecc941ba55ce38f972', 'width': 108, 'height': 22}, {'url': 'https://external-preview.redd.it/xtFs7g9kkWqXgZZ72wPdiJYKMUPMJ5AkAj4qm-gQUa0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac387b5dbaf09f556d46fc4afbf15b0ce142cbee', 'width': 216, 'height': 44}, {'url': 'https://external-preview.redd.it/xtFs7g9kkWqXgZZ72wPdiJYKMUPMJ5AkAj4qm-gQUa0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d1cb92ece5baa3477f096cf0055a5253ff377a53', 'width': 320, 'height': 66}, {'url': 'https://external-preview.redd.it/xtFs7g9kkWqXgZZ72wPdiJYKMUPMJ5AkAj4qm-gQUa0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=658d52f410e5cc3f615977ffb7ddcfcbcfc49dfe', 'width': 640, 'height': 132}, {'url': 'https://external-preview.redd.it/xtFs7g9kkWqXgZZ72wPdiJYKMUPMJ5AkAj4qm-gQUa0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=029a760dfc6d27886ea0adf438eba7d16359f726', 'width': 960, 'height': 198}, {'url': 'https://external-preview.redd.it/xtFs7g9kkWqXgZZ72wPdiJYKMUPMJ5AkAj4qm-gQUa0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ae2ae5aa30793d03cf4a0d6c899aaa64a2b8c386', 'width': 1080, 'height': 223}], 'variants': {}, 'id': 'bgsfrs0KWgDkNbiz2DPur83aArfRHoebv1dokgiVsL4'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ij3e55,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ij3e55/state_of_the_art_in_lipsyncing_a_talking_face/,all_ads,False,/r/LatestInML/comments/ij326w/state_of_the_art_in_lipsyncing_a_talking_face/,7135,1598747018.0,0,,False,/r/LatestInML/comments/ij326w/state_of_the_art_in_lipsyncing_a_talking_face/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2008.10010)\n\n&amp;#x200B;\n\nhttps://reddit.com/link/ij326w/video/kyuixqdl51k51/player', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'State of the art in lip-syncing a talking face video!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 28, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'kyuixqdl51k51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/ij326w/asset/kyuixqdl51k51/DASHPlaylist.mpd?a=1618044186%2CYThkZWU0YzBlOWFiYWE4YTVlMjZjYjg0MjFiYWNkMDU5ZjFmNWYxYWZlOGMwYjQ3NzY0YzVhNDAwMjU5NDhkMg%3D%3D&amp;v=1&amp;f=sd', 'x': 421, 'y': 240, 'hlsUrl': 'https://v.redd.it/link/ij326w/asset/kyuixqdl51k51/HLSPlaylist.m3u8?a=1618044186%2CNmU2ZjIwZjk2OWU1NDFiNTZjMGU2ZjU2NDVmMzVkYzRjOWM2ZGNiZjlmZWJmZDhhMTQzZTNjMmE5Zjg5YjM3YQ%3D%3D&amp;v=1&amp;f=sd', 'id': 'kyuixqdl51k51', 'isGif': False}}, 'name': 't3_ij326w', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.82, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 7, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/aLb3-5LwoSLdq-2Pd8FKOyE_ngSTDHuvHbxHMQKkcIg.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1598774509.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2008.10010""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/ij326w/video/kyuixqdl51k51/player""&gt;https://reddit.com/link/ij326w/video/kyuixqdl51k51/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/xtFs7g9kkWqXgZZ72wPdiJYKMUPMJ5AkAj4qm-gQUa0.jpg?auto=webp&amp;s=c6b4a4952b0d5958a1ee23175775b0888e3af850', 'width': 1256, 'height': 260}, 'resolutions': [{'url': 'https://external-preview.redd.it/xtFs7g9kkWqXgZZ72wPdiJYKMUPMJ5AkAj4qm-gQUa0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bc28199f96944b105964ffecc941ba55ce38f972', 'width': 108, 'height': 22}, {'url': 'https://external-preview.redd.it/xtFs7g9kkWqXgZZ72wPdiJYKMUPMJ5AkAj4qm-gQUa0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac387b5dbaf09f556d46fc4afbf15b0ce142cbee', 'width': 216, 'height': 44}, {'url': 'https://external-preview.redd.it/xtFs7g9kkWqXgZZ72wPdiJYKMUPMJ5AkAj4qm-gQUa0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d1cb92ece5baa3477f096cf0055a5253ff377a53', 'width': 320, 'height': 66}, {'url': 'https://external-preview.redd.it/xtFs7g9kkWqXgZZ72wPdiJYKMUPMJ5AkAj4qm-gQUa0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=658d52f410e5cc3f615977ffb7ddcfcbcfc49dfe', 'width': 640, 'height': 132}, {'url': 'https://external-preview.redd.it/xtFs7g9kkWqXgZZ72wPdiJYKMUPMJ5AkAj4qm-gQUa0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=029a760dfc6d27886ea0adf438eba7d16359f726', 'width': 960, 'height': 198}, {'url': 'https://external-preview.redd.it/xtFs7g9kkWqXgZZ72wPdiJYKMUPMJ5AkAj4qm-gQUa0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ae2ae5aa30793d03cf4a0d6c899aaa64a2b8c386', 'width': 1080, 'height': 223}], 'variants': {}, 'id': 'bgsfrs0KWgDkNbiz2DPur83aArfRHoebv1dokgiVsL4'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ij326w', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/ij326w/state_of_the_art_in_lipsyncing_a_talking_face/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/ij326w/state_of_the_art_in_lipsyncing_a_talking_face/', 'subreddit_subscribers': 6676, 'created_utc': 1598745709.0, 'num_crossposts': 14, 'media': None, 'is_video': False}]",t3_ij326w,,,,,
371,,pytorch,"Hi,

I need to predict a set of coordinates based on a input set of coordinates. 

Basically I have a flightplan of a certain flight and want to predict the actual flightpath. 

I have all the data, my problem is that I can't get a network to work, does anyone has a hint where to look or a tutorial?

All Seq2Seq Networks I found are for language processing and I haven't managed to adapt them to my problem.

[This paper](https://arxiv.org/ftp/arxiv/papers/1812/1812.11670.pdf) roughly does what I want to do.",t2_la72z,False,,0,False,Coordinate prediction,[],r/pytorch,False,6,,0,,,False,t3_iitdl3,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1598740489.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I need to predict a set of coordinates based on a input set of coordinates. &lt;/p&gt;

&lt;p&gt;Basically I have a flightplan of a certain flight and want to predict the actual flightpath. &lt;/p&gt;

&lt;p&gt;I have all the data, my problem is that I can&amp;#39;t get a network to work, does anyone has a hint where to look or a tutorial?&lt;/p&gt;

&lt;p&gt;All Seq2Seq Networks I found are for language processing and I haven&amp;#39;t managed to adapt them to my problem.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://arxiv.org/ftp/arxiv/papers/1812/1812.11670.pdf""&gt;This paper&lt;/a&gt; roughly does what I want to do.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iitdl3,True,,tobimai,,2,True,all_ads,False,[],False,,/r/pytorch/comments/iitdl3/coordinate_prediction/,all_ads,False,https://www.reddit.com/r/pytorch/comments/iitdl3/coordinate_prediction/,7135,1598711689.0,0,,False,,,,,,,,
372,,pytorch,,t2_44mbtmjy,False,,0,False,Create 3d photos from old photos as well!,[],r/pytorch,False,6,,0,140.0,,False,t3_ihzm2d,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/wtJARMzeY72J7XbDPic0qhiLfqKmF7p_s5wEmpowBZc.jpg,False,,[],{},link,,False,,1598613649.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ihzm2d,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ihzm2d/create_3d_photos_from_old_photos_as_well/,all_ads,False,/r/LatestInML/comments/ihzd7p/create_3d_photos_from_old_photos_as_well/,7135,1598584849.0,0,,False,/r/LatestInML/comments/ihzd7p/create_3d_photos_from_old_photos_as_well/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2008.12298)\n\nhttps://reddit.com/link/ihzd7p/video/nzmopyhxsnj51/player', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Create 3d photos from old photos as well!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'nzmopyhxsnj51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/ihzd7p/asset/nzmopyhxsnj51/DASHPlaylist.mpd?a=1618044186%2CMmZiNzZiNjIwYjU0NjdmNWI0Yzk2YTAzMDRiMGNhZGQ5ZGY0ODJmOWY3ZTNjYzU3YTE5NjBmMDZjNjg2OGNjZg%3D%3D&amp;v=1&amp;f=sd', 'x': 426, 'y': 239, 'hlsUrl': 'https://v.redd.it/link/ihzd7p/asset/nzmopyhxsnj51/HLSPlaylist.m3u8?a=1618044186%2CM2I4ZTRiNTYzZmZmYmZkNWExMGMxODdiMTlhMmE1MmJiOGQ4N2Q4YTg3NWI2NDViZDBjYzc1YzFmMzEzYjI4ZA%3D%3D&amp;v=1&amp;f=sd', 'id': 'nzmopyhxsnj51', 'isGif': False}}, 'name': 't3_ihzd7p', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 20, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 20, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/wtJARMzeY72J7XbDPic0qhiLfqKmF7p_s5wEmpowBZc.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1598612631.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2008.12298""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/ihzd7p/video/nzmopyhxsnj51/player""&gt;https://reddit.com/link/ihzd7p/video/nzmopyhxsnj51/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ihzd7p', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/ihzd7p/create_3d_photos_from_old_photos_as_well/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/ihzd7p/create_3d_photos_from_old_photos_as_well/', 'subreddit_subscribers': 6676, 'created_utc': 1598583831.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_ihzd7p,,,,,
373,,pytorch,"    model.eval() #as our model has a dropout layer so we need to use model.eval
    with torch.no_grad():      #we don't need to calculate any of the gradients
    
        num_correct = 0   # stores values where predicted value == labels/actual value
        total = 0         #it will track the number of images we are testing
    
        for batch, (images,labels) in enumerate(testloader,1):
                                                           #we are getting images and labels from test folder
    
            logps = model(images)                                                                    # then we do forward pass through the model and predictions are stored in logps
            output = torch.exp(logps)                                         #values of prediction in logps are logps are ""log"" values , so if we need probabilities we need to take exponenet of these values
    
            pred = torch.argmax(output, 1)
            total += labels.size(0)
    
            num_correct +=  (pred == labels).sum().item()
    
            print(f'Batch({ batch } / {len(testloader)})')
    
        #if you wanna stop after 5 batches then uncomment the code below
    
        #if batch == 5:
           #break
    
        print(f'Accuracy of model on {total} test images:  { num_correct * 100 / total} % ')

This is the code I am using to find the accuracy initially when I trained this model I got the accuracy of 88% but now when I try to plot CM, I am not getting accurate results. I need help with plotting CM for my trained model.",t2_5vsto9dv,False,,0,False,Need help with finding Confusion Matrix or Classwise accuracy of a Trained VGG16,[],r/pytorch,False,6,,0,,,False,t3_ihsuy8,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1598588866.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;pre&gt;&lt;code&gt;model.eval() #as our model has a dropout layer so we need to use model.eval
with torch.no_grad():      #we don&amp;#39;t need to calculate any of the gradients

    num_correct = 0   # stores values where predicted value == labels/actual value
    total = 0         #it will track the number of images we are testing

    for batch, (images,labels) in enumerate(testloader,1):
                                                       #we are getting images and labels from test folder

        logps = model(images)                                                                    # then we do forward pass through the model and predictions are stored in logps
        output = torch.exp(logps)                                         #values of prediction in logps are logps are &amp;quot;log&amp;quot; values , so if we need probabilities we need to take exponenet of these values

        pred = torch.argmax(output, 1)
        total += labels.size(0)

        num_correct +=  (pred == labels).sum().item()

        print(f&amp;#39;Batch({ batch } / {len(testloader)})&amp;#39;)

    #if you wanna stop after 5 batches then uncomment the code below

    #if batch == 5:
       #break

    print(f&amp;#39;Accuracy of model on {total} test images:  { num_correct * 100 / total} % &amp;#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the code I am using to find the accuracy initially when I trained this model I got the accuracy of 88% but now when I try to plot CM, I am not getting accurate results. I need help with plotting CM for my trained model.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ihsuy8,True,,ItsSara99,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ihsuy8/need_help_with_finding_confusion_matrix_or/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ihsuy8/need_help_with_finding_confusion_matrix_or/,7135,1598560066.0,0,,False,,,,,,,,
374,,pytorch,"What's the optimum set up for PyTorch on MacBook Pro 2017 256GB or how should I install it? Please help a newbie set up their working environment so as to not mess up their final year thesis!

Is a virtual environment recommend? Pip vs Conda? etc etc. TIA!",t2_51tfh4zn,False,,0,False,PyTorch Set Up for MacBook Pro 2017 256GB,[],r/pytorch,False,6,,0,,,False,t3_igtymc,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1598452937.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What&amp;#39;s the optimum set up for PyTorch on MacBook Pro 2017 256GB or how should I install it? Please help a newbie set up their working environment so as to not mess up their final year thesis!&lt;/p&gt;

&lt;p&gt;Is a virtual environment recommend? Pip vs Conda? etc etc. TIA!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,igtymc,True,,yusraisonfire,,3,True,all_ads,False,[],False,,/r/pytorch/comments/igtymc/pytorch_set_up_for_macbook_pro_2017_256gb/,all_ads,False,https://www.reddit.com/r/pytorch/comments/igtymc/pytorch_set_up_for_macbook_pro_2017_256gb/,7135,1598424137.0,0,,False,,,,,,,,
375,,pytorch,"tl;dr: using FP16 in heavy but not precision sensitive parts of training - which is the vast majority - while still using FP32 in the precision sensitive ones can massively reduce memory usage - and run time if you have cards with tensor cores - by just adding few lines of code.

Good overview:

https://spell.ml/blog/mixed-precision-training-with-pytorch-Xuk7YBEAACAASJam

Pytorch AMP blog:

https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/

Official pytorch AMP doc:

https://pytorch.org/docs/stable/amp.html

Official pytorch AMP examples:

https://pytorch.org/docs/stable/notes/amp_examples.html",t2_j7t0w,False,,0,False,Tip: 1.6's Automatic Mixed Precision (AMP) Can Cut Memory Usage in Half Even on Older Cards,[],r/pytorch,False,6,,0,,,False,t3_ifn1cw,False,dark,0.95,,public,15,0,{},,,False,[],,False,False,,{},,False,15,,False,self,False,,[],{},self,,True,,1598296565.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;tl;dr: using FP16 in heavy but not precision sensitive parts of training - which is the vast majority - while still using FP32 in the precision sensitive ones can massively reduce memory usage - and run time if you have cards with tensor cores - by just adding few lines of code.&lt;/p&gt;

&lt;p&gt;Good overview:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://spell.ml/blog/mixed-precision-training-with-pytorch-Xuk7YBEAACAASJam""&gt;https://spell.ml/blog/mixed-precision-training-with-pytorch-Xuk7YBEAACAASJam&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Pytorch AMP blog:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/""&gt;https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Official pytorch AMP doc:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://pytorch.org/docs/stable/amp.html""&gt;https://pytorch.org/docs/stable/amp.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Official pytorch AMP examples:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://pytorch.org/docs/stable/notes/amp_examples.html""&gt;https://pytorch.org/docs/stable/notes/amp_examples.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/mMRn40YKP-bghZ_goH4vOurzvwowdzJFX9wTQu6UO5Q.jpg?auto=webp&amp;s=595dbee0f9e819a67f656a810ed9b9f522d2b9f9', 'width': 806, 'height': 683}, 'resolutions': [{'url': 'https://external-preview.redd.it/mMRn40YKP-bghZ_goH4vOurzvwowdzJFX9wTQu6UO5Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7eb04482dfd7947da29be8a07906419ffc69837d', 'width': 108, 'height': 91}, {'url': 'https://external-preview.redd.it/mMRn40YKP-bghZ_goH4vOurzvwowdzJFX9wTQu6UO5Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f3b89329988566d3df42595fe73454c7e32c2dd4', 'width': 216, 'height': 183}, {'url': 'https://external-preview.redd.it/mMRn40YKP-bghZ_goH4vOurzvwowdzJFX9wTQu6UO5Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=efc3527ce0eb24a1af07ae94941993b68662cca5', 'width': 320, 'height': 271}, {'url': 'https://external-preview.redd.it/mMRn40YKP-bghZ_goH4vOurzvwowdzJFX9wTQu6UO5Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=86382d74a8818c719bf8f3066e638a807c3eb492', 'width': 640, 'height': 542}], 'variants': {}, 'id': 'wCgRPGlFkPW-hvPw7dyxHI5xZPrqHw0gsPb9rmnT3_M'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ifn1cw,True,,KFUP,,3,True,all_ads,False,[],False,,/r/pytorch/comments/ifn1cw/tip_16s_automatic_mixed_precision_amp_can_cut/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ifn1cw/tip_16s_automatic_mixed_precision_amp_can_cut/,7135,1598267765.0,0,,False,,,,,,,,
376,,pytorch,"Hello PyTorchers!

I was wondering if some people here succesfully managed to import `libtorch` using ""casual"" library import in XCode? 

It works indeed as shown in the tutorial with CMake, but for some reasons I have to rather use XCode, so I tried to add the downloaded libtorch to search paths (lib in Library &amp; Framework Search Paths, and include in Header Search Paths recursively). XCode seems to find the headers (no errors in the editor) but fails at building, showing countless errors that seem to be due to some linking errors. 

Do you guys have any ideas? Thank you so much!",t2_16dsak,False,,0,False,Importing libtorch in XCode as library,[],r/pytorch,False,6,,0,,,False,t3_ifos16,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1598303808.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello PyTorchers!&lt;/p&gt;

&lt;p&gt;I was wondering if some people here succesfully managed to import &lt;code&gt;libtorch&lt;/code&gt; using &amp;quot;casual&amp;quot; library import in XCode? &lt;/p&gt;

&lt;p&gt;It works indeed as shown in the tutorial with CMake, but for some reasons I have to rather use XCode, so I tried to add the downloaded libtorch to search paths (lib in Library &amp;amp; Framework Search Paths, and include in Header Search Paths recursively). XCode seems to find the headers (no errors in the editor) but fails at building, showing countless errors that seem to be due to some linking errors. &lt;/p&gt;

&lt;p&gt;Do you guys have any ideas? Thank you so much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ifos16,True,,domkkirke,,1,True,all_ads,False,[],False,,/r/pytorch/comments/ifos16/importing_libtorch_in_xcode_as_library/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ifos16/importing_libtorch_in_xcode_as_library/,7135,1598275008.0,0,,False,,,,,,,,
377,,pytorch,"I am training a pretty simple LSTM and just wondering if there is a way I can set the global device for all tensors to be on the same device i.e. cpu or gpu.

Currently, I am just using an environment variable and using `tensor.to(os.environ.get('device'))` to set the device anytime I create a  new tensor. But wondering if there is a better way to do this?",t2_5l0i1cin,False,,0,False,Best way to have all operations occur on gpu,[],r/pytorch,False,6,,0,,,False,t3_ifs7ur,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1598315102.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am training a pretty simple LSTM and just wondering if there is a way I can set the global device for all tensors to be on the same device i.e. cpu or gpu.&lt;/p&gt;

&lt;p&gt;Currently, I am just using an environment variable and using &lt;code&gt;tensor.to(os.environ.get(&amp;#39;device&amp;#39;))&lt;/code&gt; to set the device anytime I create a  new tensor. But wondering if there is a better way to do this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ifs7ur,True,,rchinny,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ifs7ur/best_way_to_have_all_operations_occur_on_gpu/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ifs7ur/best_way_to_have_all_operations_occur_on_gpu/,7135,1598286302.0,0,,False,,,,,,,,
378,,pytorch,"I require to update grads of an intermediate tensor variable using the register\_hook method. Since the variable isn't a leaf-variable, I require to add the retain\_grad() method to it after which, I can use the register\_hook method to alter the grads.

    score.retain_grad() h = score.register_hook(lambda grad: grad * torch.FloatTensor(...)) 

This works perfectly fine during the training (model.train()) phase. However, it gives an error during the evaluation phase (model.eval()).

The error:

    File ""/home/envs/darthvader/lib/python3.6/site-packages/torch/tensor.py"", line 198, in register_hook     raise RuntimeError(""cannot register a hook on a tensor that "" RuntimeError: cannot register a hook on a tensor that doesn't require gradient 

How could the model automatically disable the register\_hook method when it in eval()  phase?

&amp;#x200B;

Original question [here](https://stackoverflow.com/questions/63564508/how-to-automatically-disable-register-hook-when-training-model-is-in-eval-phas).",t2_9ptho1m,False,,0,False,How to automatically disable register_hook when training model is in eval() phase in PyTorch?,[],r/pytorch,False,6,,0,,,False,t3_ifrpm8,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1598313516.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I require to update grads of an intermediate tensor variable using the register_hook method. Since the variable isn&amp;#39;t a leaf-variable, I require to add the retain_grad() method to it after which, I can use the register_hook method to alter the grads.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;score.retain_grad() h = score.register_hook(lambda grad: grad * torch.FloatTensor(...)) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works perfectly fine during the training (model.train()) phase. However, it gives an error during the evaluation phase (model.eval()).&lt;/p&gt;

&lt;p&gt;The error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;File &amp;quot;/home/envs/darthvader/lib/python3.6/site-packages/torch/tensor.py&amp;quot;, line 198, in register_hook     raise RuntimeError(&amp;quot;cannot register a hook on a tensor that &amp;quot; RuntimeError: cannot register a hook on a tensor that doesn&amp;#39;t require gradient 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How could the model automatically disable the register_hook method when it in eval()  phase?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Original question &lt;a href=""https://stackoverflow.com/questions/63564508/how-to-automatically-disable-register-hook-when-training-model-is-in-eval-phas""&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ifrpm8,True,,freaky_eater,,1,True,all_ads,False,[],False,,/r/pytorch/comments/ifrpm8/how_to_automatically_disable_register_hook_when/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ifrpm8/how_to_automatically_disable_register_hook_when/,7135,1598284716.0,0,,False,,,,,,,,
379,,pytorch,"Python Spark Certification Training using PySpark (CCA175)

Big Data Architects, Engineers and Developers - PySpark Certification Training will equip you to become a successful Spark Developer using Python and pass the Cloudera Hadoop and Spark Developer Certification Exam (CCA175). Gain in-depth knowledge of Apache Spark and the Spark Ecosystem including Spark RDD, Spark SQL, Spark MLlib and Spark Streaming and acquire comprehensive knowledge of Python Programming language, HDFS, Sqoop, Flume, Spark GraphX and Messaging System such as Kafka. Skill-based training modules cover: 1) Big Data Hadoop and Spark, 2) Python for Apache Spark, 3) Functions, OOPs, and Modules in Python, 4) Apache Spark Framework, 5) Spark RDDs, 6) DataFrames and Spark SQL, 7) Machine Learning using Spark MLlib, 8) Spark MLlib - Deep Dive, 9) Apache Kafka and Apache Flume, 10) Apache Spark Streaming - Processing Multiple Batches and Data Sources, 11) Implementing an End-to-End Project, and 12) Spark GraphX. 

Enroll today at: https://fxo.co/9YCB  

Much career success, Lawrence E. Wilson - Artificial Intelligence Academy (AIA)",t2_36ixj,False,,0,False,Python Spark Certification Training using PySpark (CCA175),[],r/pytorch,False,6,,0,,,False,t3_ifcxwr,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1598251072.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Python Spark Certification Training using PySpark (CCA175)&lt;/p&gt;

&lt;p&gt;Big Data Architects, Engineers and Developers - PySpark Certification Training will equip you to become a successful Spark Developer using Python and pass the Cloudera Hadoop and Spark Developer Certification Exam (CCA175). Gain in-depth knowledge of Apache Spark and the Spark Ecosystem including Spark RDD, Spark SQL, Spark MLlib and Spark Streaming and acquire comprehensive knowledge of Python Programming language, HDFS, Sqoop, Flume, Spark GraphX and Messaging System such as Kafka. Skill-based training modules cover: 1) Big Data Hadoop and Spark, 2) Python for Apache Spark, 3) Functions, OOPs, and Modules in Python, 4) Apache Spark Framework, 5) Spark RDDs, 6) DataFrames and Spark SQL, 7) Machine Learning using Spark MLlib, 8) Spark MLlib - Deep Dive, 9) Apache Kafka and Apache Flume, 10) Apache Spark Streaming - Processing Multiple Batches and Data Sources, 11) Implementing an End-to-End Project, and 12) Spark GraphX. &lt;/p&gt;

&lt;p&gt;Enroll today at: &lt;a href=""https://fxo.co/9YCB""&gt;https://fxo.co/9YCB&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;Much career success, Lawrence E. Wilson - Artificial Intelligence Academy (AIA)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/VMRFvkQHVdXfgNonNgQqyZ0OahUKPWNk2i8WCc1GvWQ.jpg?auto=webp&amp;s=cc797a16a4903a07ea59e911f5da734772198413', 'width': 133, 'height': 133}, 'resolutions': [{'url': 'https://external-preview.redd.it/VMRFvkQHVdXfgNonNgQqyZ0OahUKPWNk2i8WCc1GvWQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=75332b3d984e26528e25c2059a0dc95b0cb1af33', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'yPhkz8WPAAg_BC_Es2wblaXjyHMFhVUxxDxzL5mxSuA'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ifcxwr,True,,lwilson747,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ifcxwr/python_spark_certification_training_using_pyspark/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ifcxwr/python_spark_certification_training_using_pyspark/,7135,1598222272.0,0,,False,,,,,,,,
380,,pytorch,,t2_44mbtmjy,False,,0,False,"ICYMI from Nvidia and UWaterloo researchers: Latest in synthesizing scenes for graphics, gaming, and to create (labeled) synthetic datasets for ML",[],r/pytorch,False,6,,0,69.0,,False,t3_iechn7,False,dark,0.86,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/5DtYJx-AIReh9YeNq1R8jZ_MM9TtqbqidhKQsLL6wzk.jpg,False,,[],{},link,,False,,1598100752.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GhWa1cAotKt_r1qgvw72e_mebX_ze4ZPx4aq1lYv0ak.jpg?auto=webp&amp;s=8edab5d754125dfb2cc042b474ee0ab18f57d6ac', 'width': 802, 'height': 398}, 'resolutions': [{'url': 'https://external-preview.redd.it/GhWa1cAotKt_r1qgvw72e_mebX_ze4ZPx4aq1lYv0ak.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a12b2d6f9b24aa49525fe3ca2569b694a92eee69', 'width': 108, 'height': 53}, {'url': 'https://external-preview.redd.it/GhWa1cAotKt_r1qgvw72e_mebX_ze4ZPx4aq1lYv0ak.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8a13ea85f8dadcc4529c20ab82185fc47833b5c2', 'width': 216, 'height': 107}, {'url': 'https://external-preview.redd.it/GhWa1cAotKt_r1qgvw72e_mebX_ze4ZPx4aq1lYv0ak.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9bc4b405d21c6454e02c2226fbdbedeb20bc7b8f', 'width': 320, 'height': 158}, {'url': 'https://external-preview.redd.it/GhWa1cAotKt_r1qgvw72e_mebX_ze4ZPx4aq1lYv0ak.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bdab77b5cd9595487d02b522b559a3590d74147', 'width': 640, 'height': 317}], 'variants': {}, 'id': '4P5YWqGQ-o1lTx-L42b2LpcfoHDNSsSGt1zjGe7ws1o'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iechn7,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/iechn7/icymi_from_nvidia_and_uwaterloo_researchers/,all_ads,False,/r/LatestInML/comments/iecgez/icymi_from_nvidia_and_uwaterloo_researchers/,7135,1598071952.0,0,,False,/r/LatestInML/comments/iecgez/icymi_from_nvidia_and_uwaterloo_researchers/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/expert/API requests:\xa0[click here](https://www.catalyzex.com/paper/arxiv:2008.09092)\n\nhttps://reddit.com/link/iecgez/video/aprassleihi51/player', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'ICYMI from Nvidia and UWaterloo researchers: Latest in synthesizing scenes for graphics, gaming, and to create (labeled) synthetic datasets for ML', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 69, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'aprassleihi51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/iecgez/asset/aprassleihi51/DASHPlaylist.mpd?a=1618044191%2CMDQ2YzhjNjczOGE1ZDgxY2Y2MDBkMDI2MjQ3ZjIxNGFiMzY1OWIwODQ0NmE1ZTg3NWY4MDA1NDRhNzllNjMyYw%3D%3D&amp;v=1&amp;f=sd', 'x': 640, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/iecgez/asset/aprassleihi51/HLSPlaylist.m3u8?a=1618044191%2COWVhMTRmZmQ2NTI1NTNkNzcwNWU3NDJlYzdjM2EwOWM5YjViZDMzZWE4ZWMyZThmZTlkM2RkZWEzMWIwMDRiZA%3D%3D&amp;v=1&amp;f=sd', 'id': 'aprassleihi51', 'isGif': False}}, 'name': 't3_iecgez', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 17, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 17, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/5DtYJx-AIReh9YeNq1R8jZ_MM9TtqbqidhKQsLL6wzk.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1598100599.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/expert/API requests:\xa0&lt;a href=""https://www.catalyzex.com/paper/arxiv:2008.09092""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/iecgez/video/aprassleihi51/player""&gt;https://reddit.com/link/iecgez/video/aprassleihi51/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/GhWa1cAotKt_r1qgvw72e_mebX_ze4ZPx4aq1lYv0ak.jpg?auto=webp&amp;s=8edab5d754125dfb2cc042b474ee0ab18f57d6ac', 'width': 802, 'height': 398}, 'resolutions': [{'url': 'https://external-preview.redd.it/GhWa1cAotKt_r1qgvw72e_mebX_ze4ZPx4aq1lYv0ak.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a12b2d6f9b24aa49525fe3ca2569b694a92eee69', 'width': 108, 'height': 53}, {'url': 'https://external-preview.redd.it/GhWa1cAotKt_r1qgvw72e_mebX_ze4ZPx4aq1lYv0ak.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8a13ea85f8dadcc4529c20ab82185fc47833b5c2', 'width': 216, 'height': 107}, {'url': 'https://external-preview.redd.it/GhWa1cAotKt_r1qgvw72e_mebX_ze4ZPx4aq1lYv0ak.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9bc4b405d21c6454e02c2226fbdbedeb20bc7b8f', 'width': 320, 'height': 158}, {'url': 'https://external-preview.redd.it/GhWa1cAotKt_r1qgvw72e_mebX_ze4ZPx4aq1lYv0ak.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bdab77b5cd9595487d02b522b559a3590d74147', 'width': 640, 'height': 317}], 'variants': {}, 'id': '4P5YWqGQ-o1lTx-L42b2LpcfoHDNSsSGt1zjGe7ws1o'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'iecgez', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/iecgez/icymi_from_nvidia_and_uwaterloo_researchers/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/iecgez/icymi_from_nvidia_and_uwaterloo_researchers/', 'subreddit_subscribers': 6676, 'created_utc': 1598071799.0, 'num_crossposts': 13, 'media': None, 'is_video': False}]",t3_iecgez,,,,,
381,,pytorch,"Hello I need some help with lowering the parameters in a CNN model using pre-trained weights from the CNN architecture provided in the link. I can't figure out how to  decrease the parameters and still being able to use the weights from the pre-trained model. 

CNN architecture:  [https://stackoverflow.com/questions/63531538/pytorch-lower-the-parameters-in-u-net-model](https://stackoverflow.com/questions/63531538/pytorch-lower-the-parameters-in-u-net-model)",t2_128ob4,False,,0,False,Lower CNN U-net model parameters to prevent overfitting,[],r/pytorch,False,6,,0,,,False,t3_ie9fjm,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1598087597.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello I need some help with lowering the parameters in a CNN model using pre-trained weights from the CNN architecture provided in the link. I can&amp;#39;t figure out how to  decrease the parameters and still being able to use the weights from the pre-trained model. &lt;/p&gt;

&lt;p&gt;CNN architecture:  &lt;a href=""https://stackoverflow.com/questions/63531538/pytorch-lower-the-parameters-in-u-net-model""&gt;https://stackoverflow.com/questions/63531538/pytorch-lower-the-parameters-in-u-net-model&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ie9fjm,True,,darvidas,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ie9fjm/lower_cnn_unet_model_parameters_to_prevent/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ie9fjm/lower_cnn_unet_model_parameters_to_prevent/,7135,1598058797.0,0,,False,,,,,,,,
382,,pytorch,"Hi there,

I'm relatively new to PyTorch Geometric (I've coded up one GNN so far, though have some experience working with PyTorch), and for some research I'm doing, I want to implement the message-passing scheme described on page 4 of [this paper](https://arxiv.org/pdf/1905.10006.pdf). It appears to be more complex than the examples I've seen.

The data is in a tree format, and for each neighbour of a node, the message-passing algorithm operates differently (see step 2 in the referenced paper) depending on whether the neighbour is the child or parent of the node (it passes the neighbour's embedding through one of two MLPs). I'm not sure how I could code my MPNN to recognize when a neighbor is a parent or a child - perhaps it could be encoded as a feature in the edge between them?

The second issue I had was that the parent and child embeddings of a node are aggregated separately - the mean of each is taken, and concatenated with the original node's embedding (step 3 of the paper). As far as I can see, PyG only allows a simpler aggregation method where all of the neighbors are considered at once.

If anyone has any thoughts on how these might be approached, I'd be very interested to hear them :^)",t2_eetn0,False,,0,False,PyTorch Geometric: Replicating model from a paper which includes a tricky MPNN setup,[],r/pytorch,False,6,,0,,,False,t3_idm12l,False,dark,0.89,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1597997011.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m relatively new to PyTorch Geometric (I&amp;#39;ve coded up one GNN so far, though have some experience working with PyTorch), and for some research I&amp;#39;m doing, I want to implement the message-passing scheme described on page 4 of &lt;a href=""https://arxiv.org/pdf/1905.10006.pdf""&gt;this paper&lt;/a&gt;. It appears to be more complex than the examples I&amp;#39;ve seen.&lt;/p&gt;

&lt;p&gt;The data is in a tree format, and for each neighbour of a node, the message-passing algorithm operates differently (see step 2 in the referenced paper) depending on whether the neighbour is the child or parent of the node (it passes the neighbour&amp;#39;s embedding through one of two MLPs). I&amp;#39;m not sure how I could code my MPNN to recognize when a neighbor is a parent or a child - perhaps it could be encoded as a feature in the edge between them?&lt;/p&gt;

&lt;p&gt;The second issue I had was that the parent and child embeddings of a node are aggregated separately - the mean of each is taken, and concatenated with the original node&amp;#39;s embedding (step 3 of the paper). As far as I can see, PyG only allows a simpler aggregation method where all of the neighbors are considered at once.&lt;/p&gt;

&lt;p&gt;If anyone has any thoughts on how these might be approached, I&amp;#39;d be very interested to hear them :&lt;sup&gt;)&lt;/sup&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,idm12l,True,,Coprosmo,,0,True,all_ads,False,[],False,,/r/pytorch/comments/idm12l/pytorch_geometric_replicating_model_from_a_paper/,all_ads,False,https://www.reddit.com/r/pytorch/comments/idm12l/pytorch_geometric_replicating_model_from_a_paper/,7135,1597968211.0,0,,False,,,,,,,,
383,,pytorch,"I implemented a simple average pooling method and compared it with AvgPool2d, anyone knows why there are some tiny differences? Its some floating point precision thing. A related second question is: in equations people often do \`1/x \* y\` instead of \`y / x\` what is the reason for that?

Anyhow here is the code:

    import torch
    import torch.nn as nn
    
    def pool_avg(x):
        n, c, h, w = x.size()
        x = x.view((n, c, h * w))
        x = torch.sum(x, 2).div(h * w)
        return x
    
    for kw in range(3, 8):
        for kh in range(3, 8):
            x = torch.randn(10, 4, kw, kh).float()
            out1 = nn.AvgPool2d((kw, kh))(x).view((10, 4))
            out2 = pool_avg(x)
            if 0 != torch.sum(out1 - out2):
                print(kw, kh, 'diff: ', torch.sum(out2 - out1))

Prints:

    5 7 diff:  tensor(-1.2433e-07)
    6 6 diff:  tensor(3.4226e-08)
    6 7 diff:  tensor(1.0990e-07)
    7 5 diff:  tensor(2.3097e-07)
    7 6 diff:  tensor(-1.3970e-08)
    7 7 diff:  tensor(-1.2107e-07)

Did I do something wrong? Or is the difference so tiny I can ignore it in any case?",t2_1s9q9v,False,,0,False,Issue with my avg pooling function,[],r/pytorch,False,6,,0,,,False,t3_idm07x,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1597996923.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I implemented a simple average pooling method and compared it with AvgPool2d, anyone knows why there are some tiny differences? Its some floating point precision thing. A related second question is: in equations people often do `1/x * y` instead of `y / x` what is the reason for that?&lt;/p&gt;

&lt;p&gt;Anyhow here is the code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import torch
import torch.nn as nn

def pool_avg(x):
    n, c, h, w = x.size()
    x = x.view((n, c, h * w))
    x = torch.sum(x, 2).div(h * w)
    return x

for kw in range(3, 8):
    for kh in range(3, 8):
        x = torch.randn(10, 4, kw, kh).float()
        out1 = nn.AvgPool2d((kw, kh))(x).view((10, 4))
        out2 = pool_avg(x)
        if 0 != torch.sum(out1 - out2):
            print(kw, kh, &amp;#39;diff: &amp;#39;, torch.sum(out2 - out1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Prints:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;5 7 diff:  tensor(-1.2433e-07)
6 6 diff:  tensor(3.4226e-08)
6 7 diff:  tensor(1.0990e-07)
7 5 diff:  tensor(2.3097e-07)
7 6 diff:  tensor(-1.3970e-08)
7 7 diff:  tensor(-1.2107e-07)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Did I do something wrong? Or is the difference so tiny I can ignore it in any case?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,idm07x,True,,bpooqd,,0,True,all_ads,False,[],False,,/r/pytorch/comments/idm07x/issue_with_my_avg_pooling_function/,all_ads,False,https://www.reddit.com/r/pytorch/comments/idm07x/issue_with_my_avg_pooling_function/,7135,1597968123.0,0,,False,,,,,,,,
384,,pytorch,,t2_44mbtmjy,False,,0,False,Super-Human Performance in car racing using Deep Reinforcement Learning!,[],r/pytorch,False,6,,0,36.0,,False,t3_idozfa,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/nBQ1yh2TqKynnc7wdxIg5dnVQ1IfkIlp9BTb7vtk-Nk.jpg,False,,[],{},link,,False,,1598008395.0,text,6,,,text,self.LatestInML,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xdljGI-tvAQ34Buz42ayUNFpLqVL_-1TabF81LJGYhE.jpg?auto=webp&amp;s=f8893fade625b514ab676a103994656967edaff5', 'width': 1088, 'height': 284}, 'resolutions': [{'url': 'https://external-preview.redd.it/xdljGI-tvAQ34Buz42ayUNFpLqVL_-1TabF81LJGYhE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c6c3d6108b08b1485cc46c53852f4a808ade020', 'width': 108, 'height': 28}, {'url': 'https://external-preview.redd.it/xdljGI-tvAQ34Buz42ayUNFpLqVL_-1TabF81LJGYhE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=31a9bd4289ca51e7610ca1f45b13e63a7371690d', 'width': 216, 'height': 56}, {'url': 'https://external-preview.redd.it/xdljGI-tvAQ34Buz42ayUNFpLqVL_-1TabF81LJGYhE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=74b572b7a8c6727f8464de9bee4155d83d991e72', 'width': 320, 'height': 83}, {'url': 'https://external-preview.redd.it/xdljGI-tvAQ34Buz42ayUNFpLqVL_-1TabF81LJGYhE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7593e17300de41a41a74a2893df45311edeacb46', 'width': 640, 'height': 167}, {'url': 'https://external-preview.redd.it/xdljGI-tvAQ34Buz42ayUNFpLqVL_-1TabF81LJGYhE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c80a58684f876f319f3d3474da356d34fc960da5', 'width': 960, 'height': 250}, {'url': 'https://external-preview.redd.it/xdljGI-tvAQ34Buz42ayUNFpLqVL_-1TabF81LJGYhE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=607ba4bdec0d139ce82fd489dd30911375879c51', 'width': 1080, 'height': 281}], 'variants': {}, 'id': 'gyL6kVII-9A32pTpWUBTY-6W-_sLb6TaD1hEh0Ik5n0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,idozfa,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/idozfa/superhuman_performance_in_car_racing_using_deep/,all_ads,False,/r/LatestInML/comments/idoyox/superhuman_performance_in_car_racing_using_deep/,7135,1597979595.0,0,,False,/r/LatestInML/comments/idoyox/superhuman_performance_in_car_racing_using_deep/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/expert/API requests: [click here](https://www.catalyzex.com/paper/arxiv:2008.07971)\n\nhttps://reddit.com/link/idoyox/video/rh9lfglvt9i51/player\n\noutperform the fastest known times in a dataset of personal best lap times of over 50,000 human drivers.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Super-Human Performance in car racing using Deep Reinforcement Learning!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 36, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'rh9lfglvt9i51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/idoyox/asset/rh9lfglvt9i51/DASHPlaylist.mpd?a=1618044191%2CM2ZmNmU3NDczZTg5MjY3MDUxNDhkMTY4NGQ4NThlN2JlMTgxNjc5MWEyMDc0ZTI4NjEyMGE0MzI2YmFjYTY3MA%3D%3D&amp;v=1&amp;f=sd', 'x': 426, 'y': 239, 'hlsUrl': 'https://v.redd.it/link/idoyox/asset/rh9lfglvt9i51/HLSPlaylist.m3u8?a=1618044191%2CODNiZWRhOGFjM2QxNDVlMTRmNWExNjhkMTVhZGMxN2Q4MzQwNjFjYWE4ODllOTNjZTQyNzIyZTVjZTIzNTI5Nw%3D%3D&amp;v=1&amp;f=sd', 'id': 'rh9lfglvt9i51', 'isGif': False}}, 'name': 't3_idoyox', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 31, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 31, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/nBQ1yh2TqKynnc7wdxIg5dnVQ1IfkIlp9BTb7vtk-Nk.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1598008311.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/expert/API requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2008.07971""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/idoyox/video/rh9lfglvt9i51/player""&gt;https://reddit.com/link/idoyox/video/rh9lfglvt9i51/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;outperform the fastest known times in a dataset of personal best lap times of over 50,000 human drivers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/xdljGI-tvAQ34Buz42ayUNFpLqVL_-1TabF81LJGYhE.jpg?auto=webp&amp;s=f8893fade625b514ab676a103994656967edaff5', 'width': 1088, 'height': 284}, 'resolutions': [{'url': 'https://external-preview.redd.it/xdljGI-tvAQ34Buz42ayUNFpLqVL_-1TabF81LJGYhE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c6c3d6108b08b1485cc46c53852f4a808ade020', 'width': 108, 'height': 28}, {'url': 'https://external-preview.redd.it/xdljGI-tvAQ34Buz42ayUNFpLqVL_-1TabF81LJGYhE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=31a9bd4289ca51e7610ca1f45b13e63a7371690d', 'width': 216, 'height': 56}, {'url': 'https://external-preview.redd.it/xdljGI-tvAQ34Buz42ayUNFpLqVL_-1TabF81LJGYhE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=74b572b7a8c6727f8464de9bee4155d83d991e72', 'width': 320, 'height': 83}, {'url': 'https://external-preview.redd.it/xdljGI-tvAQ34Buz42ayUNFpLqVL_-1TabF81LJGYhE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7593e17300de41a41a74a2893df45311edeacb46', 'width': 640, 'height': 167}, {'url': 'https://external-preview.redd.it/xdljGI-tvAQ34Buz42ayUNFpLqVL_-1TabF81LJGYhE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c80a58684f876f319f3d3474da356d34fc960da5', 'width': 960, 'height': 250}, {'url': 'https://external-preview.redd.it/xdljGI-tvAQ34Buz42ayUNFpLqVL_-1TabF81LJGYhE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=607ba4bdec0d139ce82fd489dd30911375879c51', 'width': 1080, 'height': 281}], 'variants': {}, 'id': 'gyL6kVII-9A32pTpWUBTY-6W-_sLb6TaD1hEh0Ik5n0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'idoyox', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/idoyox/superhuman_performance_in_car_racing_using_deep/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/idoyox/superhuman_performance_in_car_racing_using_deep/', 'subreddit_subscribers': 6676, 'created_utc': 1597979511.0, 'num_crossposts': 13, 'media': None, 'is_video': False}]",t3_idoyox,,,,,
385,,pytorch,"Hi my fellow programmer,

I am programmer who work in the field of deep learning. Me and my colleagues usually make lectures and statistics about fields that we are interested in. That’s why we created a survey about the connection between deep learning and GPU utilization. If you have 3-4 minutes, please help us by answering our questions.

[https://forms.gle/2L6nUfxSN5KyVo786](https://forms.gle/2L6nUfxSN5KyVo786)

Thank you for your time and answers,

Richard",t2_5cm7w86k,False,,0,False,Help me with fill this survey for my research,[],r/pytorch,False,6,,0,,,False,t3_idh3hn,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1597980699.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi my fellow programmer,&lt;/p&gt;

&lt;p&gt;I am programmer who work in the field of deep learning. Me and my colleagues usually make lectures and statistics about fields that we are interested in. That’s why we created a survey about the connection between deep learning and GPU utilization. If you have 3-4 minutes, please help us by answering our questions.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://forms.gle/2L6nUfxSN5KyVo786""&gt;https://forms.gle/2L6nUfxSN5KyVo786&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you for your time and answers,&lt;/p&gt;

&lt;p&gt;Richard&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4i-kiSuZ95BJysiZlSBHd938bsx9vjbvL5jTsOR6ikw.jpg?auto=webp&amp;s=d58c4c3efd71db9fc66bea17ee21e54e6cc3e91c', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/4i-kiSuZ95BJysiZlSBHd938bsx9vjbvL5jTsOR6ikw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b0d9036b41e38235d3cd912b3cb2d97923d14b4', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/4i-kiSuZ95BJysiZlSBHd938bsx9vjbvL5jTsOR6ikw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ec9d1a1b49abf56236211463fe71c3c2b776b61', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/4i-kiSuZ95BJysiZlSBHd938bsx9vjbvL5jTsOR6ikw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2563a5b4c7a62368d6377cb1403dbdb42a719883', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/4i-kiSuZ95BJysiZlSBHd938bsx9vjbvL5jTsOR6ikw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=47c224427a93d976b268c6bbc417e5131654289f', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/4i-kiSuZ95BJysiZlSBHd938bsx9vjbvL5jTsOR6ikw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=204bfffc242d5b0d7ff1367f526a65ca7e1501d0', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/4i-kiSuZ95BJysiZlSBHd938bsx9vjbvL5jTsOR6ikw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e0248b8ebf24fb488fa27f6b1b9e1239c992bd4d', 'width': 1080, 'height': 567}], 'variants': {}, 'id': '0Zrv1MRL_7k7YKmZPDqu8DMqOmPqQm_9Io5ZSw8A5IU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,idh3hn,True,,richardvecsey,,0,True,all_ads,False,[],False,,/r/pytorch/comments/idh3hn/help_me_with_fill_this_survey_for_my_research/,all_ads,False,https://www.reddit.com/r/pytorch/comments/idh3hn/help_me_with_fill_this_survey_for_my_research/,7135,1597951899.0,0,,False,,,,,,,,
386,,pytorch,,t2_44mbtmjy,False,,0,False,From Facebook researchers: State of the art in 3D Hand and Body Motion Capture!,[],r/pytorch,False,6,,0,140.0,,False,t3_id6lj9,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://a.thumbs.redditmedia.com/Aqg3msPEdayJQ6hdboM8legjiZep3tCg8qSouPtrBL0.jpg,False,,[],{},link,,False,,1597941742.0,text,6,,,text,self.LatestInML,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,id6lj9,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/id6lj9/from_facebook_researchers_state_of_the_art_in_3d/,all_ads,False,/r/LatestInML/comments/id6ktu/from_facebook_researchers_state_of_the_art_in_3d/,7135,1597912942.0,0,,False,/r/LatestInML/comments/id6ktu/from_facebook_researchers_state_of_the_art_in_3d/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2008.08324)\n\nhttps://reddit.com/link/id6ktu/video/asfl1a3od4i51/player\n\nThey demonstrate the state-of-the-art performance of the hand motion capture system in public benchmarks, and show the high quality of our whole body motion capture result in various challenging real-world scenes, including a live demo scenario.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'From Facebook researchers: State of the art in 3D Hand and Body Motion Capture!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'asfl1a3od4i51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/id6ktu/asset/asfl1a3od4i51/DASHPlaylist.mpd?a=1618044191%2CMThlNjk3MzRjZWUxMmM5NGM1NTYyYmZhYjBhOGIzYWE2OGEzZDdiNDZjNDc2MjRlNTBmNTUzOTMwOTNjNTA0Mw%3D%3D&amp;v=1&amp;f=sd', 'x': 426, 'y': 239, 'hlsUrl': 'https://v.redd.it/link/id6ktu/asset/asfl1a3od4i51/HLSPlaylist.m3u8?a=1618044191%2CYzA1ZmYxNjhhYWEyODY5YjViNTQzYTYwNTVlNTMyZGFkNjc1NTYxZDNmNjRjMjg2NTU3OGMxODYzZGJmZWNmNQ%3D%3D&amp;v=1&amp;f=sd', 'id': 'asfl1a3od4i51', 'isGif': False}}, 'name': 't3_id6ktu', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.87, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 21, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 21, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/Aqg3msPEdayJQ6hdboM8legjiZep3tCg8qSouPtrBL0.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1597941624.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2008.08324""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/id6ktu/video/asfl1a3od4i51/player""&gt;https://reddit.com/link/id6ktu/video/asfl1a3od4i51/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They demonstrate the state-of-the-art performance of the hand motion capture system in public benchmarks, and show the high quality of our whole body motion capture result in various challenging real-world scenes, including a live demo scenario.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'id6ktu', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/id6ktu/from_facebook_researchers_state_of_the_art_in_3d/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/id6ktu/from_facebook_researchers_state_of_the_art_in_3d/', 'subreddit_subscribers': 6676, 'created_utc': 1597912824.0, 'num_crossposts': 10, 'media': None, 'is_video': False}]",t3_id6ktu,,,,,
387,,pytorch,"Hi community,

&amp;#x200B;

First, I want to mention, that this is our first project in a bigger scale and therefore we don't know everything but we learn fast.

&amp;#x200B;

We developed a code for image recognition. We tried it with a raspberry pi 4b but quickly faced that this is way to slow overall. Currently we are using a NVIDIA Jetson Nano. The first recognition was ok (around 30 sec.) and the second try was even better (around 6-7 sec.). The first took so long because the model will be loaded for the first time. Via an API the image recognition can be triggered and the meta data from the AI model will be the response. We use fast-API for this.

&amp;#x200B;

But there is a problem right now, where if I load my CNN as a global variable in the beginning of my classification file (loaded on import) and use it within a thread I need to use mp.set\_start\_method('spawn') because otherwise I will get the following error:

&amp;#x200B;

""RuntimeError: Cannot re-initialize CUDA in forked subprocess.

To use CUDA with multiprocessing, you must use the 'spawn' start method""

&amp;#x200B;

Now that is of course an easy fix. Just add the method above before starting my thread. Indeed this works but another challenge occurs at the same time. After setting the start method to 'spawn' the ERROR disappears but the Jetson starts to allocate way to much memory.

&amp;#x200B;

Because of the overhead and preloaded CNN model, the RAM is around 2.5Gig before the thread starts. After the start it doesn’t stop allocating RAM, it consumes all 4Gig of the RAM and also the whole 6Gig Swap. Right after this, the whole API process kill with this error: ""cannot allocate memory"" which is obvious.

&amp;#x200B;

I managed to fix that as well just by loading the CNN Model in the classification function. (Not preloading it on the GPU as in the two cases before). However, here I got problem as well. The process of loading the model to the GPU takes around 15s - 20s and this every time the recognition starts. This is not suitable for us and we are wondering why we cannot pre-load the model without killing the whole thing after two image-recognitions.  Our goal is to be under 5 sec with this.

&amp;#x200B;

    #classify.py
    import torchvision.transforms as transforms
    from skimage import io
    import time
    from torch.utils.data import Dataset
    from .loader import *
    from .ResNet import *
    
    #if this part is in the classify() function than no allocation problem occurs
    net = ResNet152(num_classes=25)
    net = net.to('cuda')
    save_file = torch.load(""./model.pt"", map_location=torch.device('cuda'))
    net.load_state_dict(save_file)
    def classify(imgp=""""):
    #do some classification with the net
    pass
    
    if __name__ == '__main__':
    mp.set_start_method('spawn') #if commented out the first error ocours
    manager = mp.Manager()
    return_dict = manager.dict()
    p = mp.Process(target=classify, args=('./bild.jpg', return_dict))
    p.start()
    p.join()
    print(return_dict.values())
    

Any help here will be much appreciated. Thank you.",t2_10o0xb58,False,,0,False,Python 3.8 RAM owerflow and loading issues,[],r/pytorch,False,6,,0,,,False,t3_id62eg,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1597938789.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi community,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;First, I want to mention, that this is our first project in a bigger scale and therefore we don&amp;#39;t know everything but we learn fast.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;We developed a code for image recognition. We tried it with a raspberry pi 4b but quickly faced that this is way to slow overall. Currently we are using a NVIDIA Jetson Nano. The first recognition was ok (around 30 sec.) and the second try was even better (around 6-7 sec.). The first took so long because the model will be loaded for the first time. Via an API the image recognition can be triggered and the meta data from the AI model will be the response. We use fast-API for this.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;But there is a problem right now, where if I load my CNN as a global variable in the beginning of my classification file (loaded on import) and use it within a thread I need to use mp.set_start_method(&amp;#39;spawn&amp;#39;) because otherwise I will get the following error:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;quot;RuntimeError: Cannot re-initialize CUDA in forked subprocess.&lt;/p&gt;

&lt;p&gt;To use CUDA with multiprocessing, you must use the &amp;#39;spawn&amp;#39; start method&amp;quot;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Now that is of course an easy fix. Just add the method above before starting my thread. Indeed this works but another challenge occurs at the same time. After setting the start method to &amp;#39;spawn&amp;#39; the ERROR disappears but the Jetson starts to allocate way to much memory.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Because of the overhead and preloaded CNN model, the RAM is around 2.5Gig before the thread starts. After the start it doesn’t stop allocating RAM, it consumes all 4Gig of the RAM and also the whole 6Gig Swap. Right after this, the whole API process kill with this error: &amp;quot;cannot allocate memory&amp;quot; which is obvious.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I managed to fix that as well just by loading the CNN Model in the classification function. (Not preloading it on the GPU as in the two cases before). However, here I got problem as well. The process of loading the model to the GPU takes around 15s - 20s and this every time the recognition starts. This is not suitable for us and we are wondering why we cannot pre-load the model without killing the whole thing after two image-recognitions.  Our goal is to be under 5 sec with this.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#classify.py
import torchvision.transforms as transforms
from skimage import io
import time
from torch.utils.data import Dataset
from .loader import *
from .ResNet import *

#if this part is in the classify() function than no allocation problem occurs
net = ResNet152(num_classes=25)
net = net.to(&amp;#39;cuda&amp;#39;)
save_file = torch.load(&amp;quot;./model.pt&amp;quot;, map_location=torch.device(&amp;#39;cuda&amp;#39;))
net.load_state_dict(save_file)
def classify(imgp=&amp;quot;&amp;quot;):
#do some classification with the net
pass

if __name__ == &amp;#39;__main__&amp;#39;:
mp.set_start_method(&amp;#39;spawn&amp;#39;) #if commented out the first error ocours
manager = mp.Manager()
return_dict = manager.dict()
p = mp.Process(target=classify, args=(&amp;#39;./bild.jpg&amp;#39;, return_dict))
p.start()
p.join()
print(return_dict.values())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Any help here will be much appreciated. Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,id62eg,True,,SheldonDE,,12,True,all_ads,False,[],False,,/r/pytorch/comments/id62eg/python_38_ram_owerflow_and_loading_issues/,all_ads,False,https://www.reddit.com/r/pytorch/comments/id62eg/python_38_ram_owerflow_and_loading_issues/,7135,1597909989.0,0,,False,,,,,,,,
388,,pytorch,"Probably more of a general ML question, but I'm doing this with PyTorch.

I somehow managed to train a very simple model with (basically):

    &lt;Input BPEmb Embeddings dim=100&gt;  # input embeddings from a pretrained tokenizer of vocab=10000
    
    Model:
    (PositionalEncoding)
    (Transformer Encoder)
        (Transformer Layer n_heads=5)
        (Transformer Layer n_heads=5)
        (Transformer Layer n_heads=5)
        (Transformer Layer n_heads=5)
    (Averaging)
    (LinearLayer: dim=512)  # &lt;------------------ ""sentence"" embedding?
    (LinearLayer: dim=N_CLASSES)

Now, I am wondering if I can somehow reverse the flow: from the highest predicted class, see which tokens most contributed for their prediction.

Another immediate question is: whilst I didn't explicit output a ""sentence"" embedding in the .forward() method, can I retroactively obtain that and the attention weights from the Transformer Encoder Layers from a given input?",t2_33ls9eif,False,,0,False,I wanted to highlight the words that most contributed for a text classification. Possible?,[],r/pytorch,False,6,,0,,,False,t3_ico0v4,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1597873921.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Probably more of a general ML question, but I&amp;#39;m doing this with PyTorch.&lt;/p&gt;

&lt;p&gt;I somehow managed to train a very simple model with (basically):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;Input BPEmb Embeddings dim=100&amp;gt;  # input embeddings from a pretrained tokenizer of vocab=10000

Model:
(PositionalEncoding)
(Transformer Encoder)
    (Transformer Layer n_heads=5)
    (Transformer Layer n_heads=5)
    (Transformer Layer n_heads=5)
    (Transformer Layer n_heads=5)
(Averaging)
(LinearLayer: dim=512)  # &amp;lt;------------------ &amp;quot;sentence&amp;quot; embedding?
(LinearLayer: dim=N_CLASSES)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, I am wondering if I can somehow reverse the flow: from the highest predicted class, see which tokens most contributed for their prediction.&lt;/p&gt;

&lt;p&gt;Another immediate question is: whilst I didn&amp;#39;t explicit output a &amp;quot;sentence&amp;quot; embedding in the .forward() method, can I retroactively obtain that and the attention weights from the Transformer Encoder Layers from a given input?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ico0v4,True,,SagaciousRaven,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ico0v4/i_wanted_to_highlight_the_words_that_most/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ico0v4/i_wanted_to_highlight_the_words_that_most/,7135,1597845121.0,0,,False,,,,,,,,
389,,pytorch,,t2_44mbtmjy,False,,0,False,Capture 3D human motion from internet videos!,[],r/pytorch,False,6,,0,140.0,,False,t3_icgdjj,False,dark,0.75,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/6maKmohjaFxAwSJnz_9hNh1_UPhtscpCG81o1P09ZCQ.jpg,False,,[],{},link,,False,,1597837590.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,icgdjj,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/icgdjj/capture_3d_human_motion_from_internet_videos/,all_ads,False,/r/LatestInML/comments/icg4y6/capture_3d_human_motion_from_internet_videos/,7135,1597808790.0,0,,False,/r/LatestInML/comments/icg4y6/capture_3d_human_motion_from_internet_videos/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2008.07931)\n\nhttps://reddit.com/link/icg4y6/video/rx30ltdqovh51/player\n\nThey propose to capture human motion by jointly analyzing these Internet videos instead of using single videos separately', 'author_fullname': 't2_7q1hpywu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Capture 3D human motion from internet videos!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'rx30ltdqovh51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/icg4y6/asset/rx30ltdqovh51/DASHPlaylist.mpd?a=1618044191%2CNmVjODdjNGE2MDQ0ZGQ5MjEzYzAwY2YzZTQyZWU5NzNmMTA1YzRiN2NkNTg5NmRlN2QxMTllZWY5MzYwNTBiYQ%3D%3D&amp;v=1&amp;f=sd', 'x': 426, 'y': 239, 'hlsUrl': 'https://v.redd.it/link/icg4y6/asset/rx30ltdqovh51/HLSPlaylist.m3u8?a=1618044191%2CY2NjM2EyZDNlODY5NzhhYTBjZDQzMDFmOTkzYmYzY2I2ODNlZjI0M2Q1MGE2NWRiODg1NDgwZmE3ODNlNjg3Mw%3D%3D&amp;v=1&amp;f=sd', 'id': 'rx30ltdqovh51', 'isGif': False}}, 'name': 't3_icg4y6', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 8, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/6maKmohjaFxAwSJnz_9hNh1_UPhtscpCG81o1P09ZCQ.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1597836620.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2008.07931""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/icg4y6/video/rx30ltdqovh51/player""&gt;https://reddit.com/link/icg4y6/video/rx30ltdqovh51/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They propose to capture human motion by jointly analyzing these Internet videos instead of using single videos separately&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'icg4y6', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'fullerhouse570', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/icg4y6/capture_3d_human_motion_from_internet_videos/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/icg4y6/capture_3d_human_motion_from_internet_videos/', 'subreddit_subscribers': 6676, 'created_utc': 1597807820.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_icg4y6,,,,,
390,,pytorch,"I decided to use pytorch lightning for a recent project, but I'm having issues in restoring the training session frok a checkpoint. 


Using model.load_from_chackpoint(checkpoint path)only seems to load the weights of the model (the state dict if you will) but not the optimizer states. I looked around online, and the recommended way is to do it through the logger ?? 
It really doesn't make any sense to me but according to the [this](https://girhub.com/PyTorchLightning/pytorch-lightning/issues/432) GitHub issue the correct way to restore a session is by setting the logger to the required version, which honestly doesn't seem like a good idea, not to mention that didn't work for me either. Any ideas on how to deal with this ?",t2_1b0ljhw6,False,,0,False,Restoring session in pytorch lightning,[],r/pytorch,False,6,,0,,,False,t3_icizwe,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1597850033.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I decided to use pytorch lightning for a recent project, but I&amp;#39;m having issues in restoring the training session frok a checkpoint. &lt;/p&gt;

&lt;p&gt;Using model.load_from_chackpoint(checkpoint path)only seems to load the weights of the model (the state dict if you will) but not the optimizer states. I looked around online, and the recommended way is to do it through the logger ?? 
It really doesn&amp;#39;t make any sense to me but according to the &lt;a href=""https://girhub.com/PyTorchLightning/pytorch-lightning/issues/432""&gt;this&lt;/a&gt; GitHub issue the correct way to restore a session is by setting the logger to the required version, which honestly doesn&amp;#39;t seem like a good idea, not to mention that didn&amp;#39;t work for me either. Any ideas on how to deal with this ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,icizwe,True,,Bowserwolf1,,0,True,all_ads,False,[],False,,/r/pytorch/comments/icizwe/restoring_session_in_pytorch_lightning/,all_ads,False,https://www.reddit.com/r/pytorch/comments/icizwe/restoring_session_in_pytorch_lightning/,7135,1597821233.0,0,,False,,,,,,,,
391,,pytorch,"Basically, the question is in the title. I see more and more DL accelerators being pumped out for both mobile and data center servers. Not sure about the software side of these chips. Anyone has any idea?",t2_i2qs3,False,,0,False,Does anyone know if pytorch can work with deep learning accelerator chips to do both training and inference?,[],r/pytorch,False,6,,0,,,False,t3_ic38n3,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1597793654.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Basically, the question is in the title. I see more and more DL accelerators being pumped out for both mobile and data center servers. Not sure about the software side of these chips. Anyone has any idea?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ic38n3,True,,reddit_tl,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ic38n3/does_anyone_know_if_pytorch_can_work_with_deep/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ic38n3/does_anyone_know_if_pytorch_can_work_with_deep/,7135,1597764854.0,0,,False,,,,,,,,
392,,pytorch,,t2_pkczw3v,False,,0,False,Using Reinforcement Learning to Design Resilient Spacecraft Trajectories,[],r/pytorch,False,6,,0,39.0,,False,t3_ibgmyj,False,dark,0.9,,public,7,0,{},140.0,,False,[],,False,False,,{},,False,7,,False,https://b.thumbs.redditmedia.com/Uw26fYnq5vUlKv9FwpBj7PHcC3YOYfuYWUEvK6Pbeag.jpg,False,,[],{},link,,False,,1597708167.0,text,6,,,text,gereshes.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/gNZNFzS9_cO0A955Nz0AD3cyUn4LY7LoGq8qmX8zlVQ.jpg?auto=webp&amp;s=49094e3ea82c9dc83ae4b4cdc9149e8a169cf5ff', 'width': 4961, 'height': 1417}, 'resolutions': [{'url': 'https://external-preview.redd.it/gNZNFzS9_cO0A955Nz0AD3cyUn4LY7LoGq8qmX8zlVQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49c53380020850c0a09de4b22c4e9337ac67e1b0', 'width': 108, 'height': 30}, {'url': 'https://external-preview.redd.it/gNZNFzS9_cO0A955Nz0AD3cyUn4LY7LoGq8qmX8zlVQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ecf6653a2ecf704dd8e3b7b322954f335080df85', 'width': 216, 'height': 61}, {'url': 'https://external-preview.redd.it/gNZNFzS9_cO0A955Nz0AD3cyUn4LY7LoGq8qmX8zlVQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=44bee960403b1a1bda2372aab439a7b6cff3e8c4', 'width': 320, 'height': 91}, {'url': 'https://external-preview.redd.it/gNZNFzS9_cO0A955Nz0AD3cyUn4LY7LoGq8qmX8zlVQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7963deec026440798f07f1f504426410ade987a', 'width': 640, 'height': 182}, {'url': 'https://external-preview.redd.it/gNZNFzS9_cO0A955Nz0AD3cyUn4LY7LoGq8qmX8zlVQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b70a70d46c352bdb0b736a7b5eb181a4ce8cdbff', 'width': 960, 'height': 274}, {'url': 'https://external-preview.redd.it/gNZNFzS9_cO0A955Nz0AD3cyUn4LY7LoGq8qmX8zlVQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=de14aa9bc971467e113784087a9e1787d67681c6', 'width': 1080, 'height': 308}], 'variants': {}, 'id': 'CZxFR0O8omzuLwWXSr_TS1v0cpQRT_Y1w3U-wg-QyV8'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ibgmyj,True,,Gereshes,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ibgmyj/using_reinforcement_learning_to_design_resilient/,all_ads,False,https://gereshes.com/2020/08/17/using-reinforcement-learning-to-design-missed-thrust-resilient-trajectories-asc-2020/,7135,1597679367.0,0,,False,https://gereshes.com/2020/08/17/using-reinforcement-learning-to-design-missed-thrust-resilient-trajectories-asc-2020/,,,,,,,
393,,pytorch,"While training a deep learning model, it is very important to visualize various aspects of the training process. This visualization is best achieved using [Tensorboard](https://el2.convertkit-mail.com/c/gkuog7wr5ot5hxq4nmf3/n2hohquodgkl02/aHR0cHM6Ly93d3cudGVuc29yZmxvdy5vcmcvdGVuc29yYm9hcmQ=) which we will cover in today's post.  


A few weeks back we had shared a post on [PyTorch Lightning for beginners](https://el2.convertkit-mail.com/c/gkuog7wr5ot5hxq4nmf3/48hvh7u2o9n78v/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL2dldHRpbmctc3RhcnRlZC13aXRoLXB5dG9yY2gtbGlnaHRuaW5nLw==) where we saw how using PyTorch Lightning simplifies the coding experience and removes a lot of grunt work.  
Today, we will show how to use Tensorboard with PyTorch Lightning.  


[https://www.learnopencv.com/tensorboard-with-pytorch-lightning/](https://el2.convertkit-mail.com/c/gkuog7wr5ot5hxq4nmf3/wnh2h6uoz43l67/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL3RlbnNvcmJvYXJkLXdpdGgtcHl0b3JjaC1saWdodG5pbmcv)  


and the code is at  
[https://github.com/spmallick/learnopencv/tree/master/TensorBoard-With-Pytorch-Lightning](https://el2.convertkit-mail.com/c/gkuog7wr5ot5hxq4nmf3/reh8h9u587eozz/aHR0cHM6Ly9naXRodWIuY29tL3NwbWFsbGljay9sZWFybm9wZW5jdi90cmVlL21hc3Rlci9UZW5zb3JCb2FyZC1XaXRoLVB5dG9yY2gtTGlnaHRuaW5n)

https://preview.redd.it/410cxcvahkh51.png?width=600&amp;format=png&amp;auto=webp&amp;s=c6128ab2018501f9f92ab6cb97f4e6a75e6d559e",t2_cvc9f,False,,0,False,TensorBoard with PyTorch Lightning,[],r/pytorch,False,6,,0,93.0,,False,t3_ibeep7,False,dark,0.75,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/Os4qMU882z5kUWIbvwupgk_A4PGbR2DPTl2GoiJzx4k.jpg,False,,[],{},,,True,,1597700709.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;While training a deep learning model, it is very important to visualize various aspects of the training process. This visualization is best achieved using &lt;a href=""https://el2.convertkit-mail.com/c/gkuog7wr5ot5hxq4nmf3/n2hohquodgkl02/aHR0cHM6Ly93d3cudGVuc29yZmxvdy5vcmcvdGVuc29yYm9hcmQ=""&gt;Tensorboard&lt;/a&gt; which we will cover in today&amp;#39;s post.  &lt;/p&gt;

&lt;p&gt;A few weeks back we had shared a post on &lt;a href=""https://el2.convertkit-mail.com/c/gkuog7wr5ot5hxq4nmf3/48hvh7u2o9n78v/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL2dldHRpbmctc3RhcnRlZC13aXRoLXB5dG9yY2gtbGlnaHRuaW5nLw==""&gt;PyTorch Lightning for beginners&lt;/a&gt; where we saw how using PyTorch Lightning simplifies the coding experience and removes a lot of grunt work.&lt;br/&gt;
Today, we will show how to use Tensorboard with PyTorch Lightning.  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://el2.convertkit-mail.com/c/gkuog7wr5ot5hxq4nmf3/wnh2h6uoz43l67/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL3RlbnNvcmJvYXJkLXdpdGgtcHl0b3JjaC1saWdodG5pbmcv""&gt;https://www.learnopencv.com/tensorboard-with-pytorch-lightning/&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;and the code is at&lt;br/&gt;
&lt;a href=""https://el2.convertkit-mail.com/c/gkuog7wr5ot5hxq4nmf3/reh8h9u587eozz/aHR0cHM6Ly9naXRodWIuY29tL3NwbWFsbGljay9sZWFybm9wZW5jdi90cmVlL21hc3Rlci9UZW5zb3JCb2FyZC1XaXRoLVB5dG9yY2gtTGlnaHRuaW5n""&gt;https://github.com/spmallick/learnopencv/tree/master/TensorBoard-With-Pytorch-Lightning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/410cxcvahkh51.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6128ab2018501f9f92ab6cb97f4e6a75e6d559e""&gt;https://preview.redd.it/410cxcvahkh51.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6128ab2018501f9f92ab6cb97f4e6a75e6d559e&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ibeep7,True,,spmallick,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ibeep7/tensorboard_with_pytorch_lightning/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ibeep7/tensorboard_with_pytorch_lightning/,7135,1597671909.0,0,,False,,,,"{'410cxcvahkh51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 72, 'x': 108, 'u': 'https://preview.redd.it/410cxcvahkh51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a39659e4fa481a01d6898a80a2e4b33bf4b1a6ee'}, {'y': 144, 'x': 216, 'u': 'https://preview.redd.it/410cxcvahkh51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=406bacc6e8beca5741d3f988b533e84f68b60259'}, {'y': 213, 'x': 320, 'u': 'https://preview.redd.it/410cxcvahkh51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=75f1511d30324adb47bfacc256a05bb0266b4797'}], 's': {'y': 400, 'x': 600, 'u': 'https://preview.redd.it/410cxcvahkh51.png?width=600&amp;format=png&amp;auto=webp&amp;s=c6128ab2018501f9f92ab6cb97f4e6a75e6d559e'}, 'id': '410cxcvahkh51'}}",,,,
394,,pytorch,"Hi!

I'm currently stuck on trying to extract data and turn into tensors for training some machine learning models.

I have a CSV with 3D joint data of human skeletons in columns (77 columns). Rows represent changing timesteps. In each column there is one number, and a group of three columns make up one joint coordinate (ie columns C, D and E represent x1, y1 and z1 respectively and so on). The first two columns A and B are not coordinates and hence redundant.

I can't seem to extract these columns into sets of 3D coordinates to turn into tensors, does anyone have any tips or guidance? How do I extract x1 from column C, y1 from D and z1 from E and combine them to turn into a tensor? Thank you for any help!",t2_i5nmk,False,,0,False,3D coordinate location data preprocessing for PyTorch,[],r/pytorch,False,6,,0,,,False,t3_ibbx9n,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1597690386.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently stuck on trying to extract data and turn into tensors for training some machine learning models.&lt;/p&gt;

&lt;p&gt;I have a CSV with 3D joint data of human skeletons in columns (77 columns). Rows represent changing timesteps. In each column there is one number, and a group of three columns make up one joint coordinate (ie columns C, D and E represent x1, y1 and z1 respectively and so on). The first two columns A and B are not coordinates and hence redundant.&lt;/p&gt;

&lt;p&gt;I can&amp;#39;t seem to extract these columns into sets of 3D coordinates to turn into tensors, does anyone have any tips or guidance? How do I extract x1 from column C, y1 from D and z1 from E and combine them to turn into a tensor? Thank you for any help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ibbx9n,True,,10jy,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ibbx9n/3d_coordinate_location_data_preprocessing_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ibbx9n/3d_coordinate_location_data_preprocessing_for/,7135,1597661586.0,0,,False,,,,,,,,
395,,pytorch,"&gt; [Kaggle Notebook](https://www.kaggle.com/carefree0910/titanic-neural-networks-made-easy-with-cflearn)

&gt; [GitHub](https://github.com/carefree0910/carefree-learn)
&gt; [Documents](https://carefree0910.me/carefree-learn-doc)
&gt; [Introduction video](https://youtu.be/hMzLmwmdQ_k)

&gt; cflearn - a minimal Automatic Machine Learning (AutoML) solution for tabular datasets based on PyTorch

Previous posts could be found at [1](https://www.reddit.com/r/pytorch/comments/hm6a4v/project_cflearn_a_minimal_automatic_machine/) [2](https://www.reddit.com/r/pytorch/comments/i1tf7b/project_cflearn_v011_update/).",t2_3itl62k7,False,,0,False,Titanic: Neural Networks made EASY with cflearn,[],r/pytorch,False,6,,0,,,False,t3_iaux1r,False,dark,0.4,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1597622776.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=""https://www.kaggle.com/carefree0910/titanic-neural-networks-made-easy-with-cflearn""&gt;Kaggle Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/carefree0910/carefree-learn""&gt;GitHub&lt;/a&gt;
&lt;a href=""https://carefree0910.me/carefree-learn-doc""&gt;Documents&lt;/a&gt;
&lt;a href=""https://youtu.be/hMzLmwmdQ_k""&gt;Introduction video&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;cflearn - a minimal Automatic Machine Learning (AutoML) solution for tabular datasets based on PyTorch&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Previous posts could be found at &lt;a href=""https://www.reddit.com/r/pytorch/comments/hm6a4v/project_cflearn_a_minimal_automatic_machine/""&gt;1&lt;/a&gt; &lt;a href=""https://www.reddit.com/r/pytorch/comments/i1tf7b/project_cflearn_v011_update/""&gt;2&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/hIZn74UCUzMss02AeqYwTjjr9Q4P0f4_c6SjRwG_mWM.jpg?auto=webp&amp;s=317a9be4dd095d5a4b95cfdd96ada08acea08513', 'width': 160, 'height': 160}, 'resolutions': [{'url': 'https://external-preview.redd.it/hIZn74UCUzMss02AeqYwTjjr9Q4P0f4_c6SjRwG_mWM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0ef6b067fd0b46d01dc9f262edb560782bc9f2c', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'p15coSqe7L8wApjnVlwASEYE50BcnmvRuPbSVpGUPaM'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iaux1r,True,,carefree0910,,0,True,all_ads,False,[],False,,/r/pytorch/comments/iaux1r/titanic_neural_networks_made_easy_with_cflearn/,all_ads,False,https://www.reddit.com/r/pytorch/comments/iaux1r/titanic_neural_networks_made_easy_with_cflearn/,7135,1597593976.0,0,,False,,,,,,,,
396,,pytorch,,t2_44mbtmjy,False,,0,False,"From Adobe, Stanford, &amp; UWashington: A new framework can predict a full head portrait for ages 0-70 from a single photo, modifying both texture and shape of the head",[],r/pytorch,False,6,,0,125.0,,False,t3_iadbw1,False,dark,0.92,,public,9,0,{},140.0,,False,[],,False,False,,{},,False,9,,False,https://a.thumbs.redditmedia.com/CvP4-fiyAWoySmSxQ6lmqOeuICswUvTl8JI1pIhyjK8.jpg,False,,[],{},link,,False,,1597547178.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xjzJUzvPs-tcrkIb3FA8ySormf2ZiwSwI7jD1u9dXow.jpg?auto=webp&amp;s=51bc2058bfbd280c70003faa1abcbbbfb01a96b2', 'width': 960, 'height': 862}, 'resolutions': [{'url': 'https://external-preview.redd.it/xjzJUzvPs-tcrkIb3FA8ySormf2ZiwSwI7jD1u9dXow.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b18969fbd5731f57d74b1db97ada219c852598', 'width': 108, 'height': 96}, {'url': 'https://external-preview.redd.it/xjzJUzvPs-tcrkIb3FA8ySormf2ZiwSwI7jD1u9dXow.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=29dd66ecff5cc93fc2fab7ba829b67d9df596359', 'width': 216, 'height': 193}, {'url': 'https://external-preview.redd.it/xjzJUzvPs-tcrkIb3FA8ySormf2ZiwSwI7jD1u9dXow.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7cc89c880d6bcde98e79587b68f197678d8c689d', 'width': 320, 'height': 287}, {'url': 'https://external-preview.redd.it/xjzJUzvPs-tcrkIb3FA8ySormf2ZiwSwI7jD1u9dXow.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=56e723d50d9384f47c9d80ecc66df608cb2cd734', 'width': 640, 'height': 574}, {'url': 'https://external-preview.redd.it/xjzJUzvPs-tcrkIb3FA8ySormf2ZiwSwI7jD1u9dXow.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9b2a8d0f877a04fd4a02196d3cb6905d28bdbdf2', 'width': 960, 'height': 862}], 'variants': {}, 'id': 'sslrp_1AOGVMDWUhNNvNYYkw5HWsWkq6FvzJbhQ21VQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,iadbw1,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/iadbw1/from_adobe_stanford_uwashington_a_new_framework/,all_ads,False,/r/LatestInML/comments/iad3mz/from_adobe_stanford_uwashington_a_new_framework/,7135,1597518378.0,0,,False,/r/LatestInML/comments/iad3mz/from_adobe_stanford_uwashington_a_new_framework/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2003.09764)\n\nhttps://reddit.com/link/iad3mz/video/wo6tgjtqp7h51/player', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'From Adobe, Stanford, &amp; UWashington: A new framework can predict a full head portrait for ages 0-70 from a single photo, modifying both texture and shape of the head', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 125, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'wo6tgjtqp7h51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/iad3mz/asset/wo6tgjtqp7h51/DASHPlaylist.mpd?a=1618044191%2CYWNlN2UzMGY4YWVlMmQ5ZTQ1YjAxMDg4NWNiMmY1MmY0ZTIwZDU5N2RjYWI0ZDYzZDZhMTk1Mzc5MzQyZjBjMA%3D%3D&amp;v=1&amp;f=sd', 'x': 240, 'y': 240, 'hlsUrl': 'https://v.redd.it/link/iad3mz/asset/wo6tgjtqp7h51/HLSPlaylist.m3u8?a=1618044191%2CZjhlMzAyZThlZGU1ZGIzMDVjZjM0MjRkMzc5ZWQ4YTQxM2VlNThiNDdiNDMwNWE0YjgyNGYyMmQ2MGJkODgxZg%3D%3D&amp;v=1&amp;f=sd', 'id': 'wo6tgjtqp7h51', 'isGif': False}}, 'name': 't3_iad3mz', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 21, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 21, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/CvP4-fiyAWoySmSxQ6lmqOeuICswUvTl8JI1pIhyjK8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1597546449.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2003.09764""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/iad3mz/video/wo6tgjtqp7h51/player""&gt;https://reddit.com/link/iad3mz/video/wo6tgjtqp7h51/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/xjzJUzvPs-tcrkIb3FA8ySormf2ZiwSwI7jD1u9dXow.jpg?auto=webp&amp;s=51bc2058bfbd280c70003faa1abcbbbfb01a96b2', 'width': 960, 'height': 862}, 'resolutions': [{'url': 'https://external-preview.redd.it/xjzJUzvPs-tcrkIb3FA8ySormf2ZiwSwI7jD1u9dXow.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b18969fbd5731f57d74b1db97ada219c852598', 'width': 108, 'height': 96}, {'url': 'https://external-preview.redd.it/xjzJUzvPs-tcrkIb3FA8ySormf2ZiwSwI7jD1u9dXow.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=29dd66ecff5cc93fc2fab7ba829b67d9df596359', 'width': 216, 'height': 193}, {'url': 'https://external-preview.redd.it/xjzJUzvPs-tcrkIb3FA8ySormf2ZiwSwI7jD1u9dXow.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7cc89c880d6bcde98e79587b68f197678d8c689d', 'width': 320, 'height': 287}, {'url': 'https://external-preview.redd.it/xjzJUzvPs-tcrkIb3FA8ySormf2ZiwSwI7jD1u9dXow.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=56e723d50d9384f47c9d80ecc66df608cb2cd734', 'width': 640, 'height': 574}, {'url': 'https://external-preview.redd.it/xjzJUzvPs-tcrkIb3FA8ySormf2ZiwSwI7jD1u9dXow.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9b2a8d0f877a04fd4a02196d3cb6905d28bdbdf2', 'width': 960, 'height': 862}], 'variants': {}, 'id': 'sslrp_1AOGVMDWUhNNvNYYkw5HWsWkq6FvzJbhQ21VQ'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'iad3mz', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/iad3mz/from_adobe_stanford_uwashington_a_new_framework/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/iad3mz/from_adobe_stanford_uwashington_a_new_framework/', 'subreddit_subscribers': 6676, 'created_utc': 1597517649.0, 'num_crossposts': 11, 'media': None, 'is_video': False}]",t3_iad3mz,,,,,
397,,pytorch,"For my project, I am attempting to write an autoencoder, where the input and output grayscale images are slightly different. At first, I thought my conv net was not working, so I tried to have the autoencoder recreate the original input, but no matter what, the autoencoder on returns an gray image.I’m currently using my custom dataset, which I wrote with [**@ptrblck**](https://discuss.pytorch.org/u/ptrblck) help, but when testing if my conv net has issues, I pass the x(input) as both x(input) and y(output).Here is my dataloading. My grayscale images are original 0 to 255, but I have read that Transform.toTensor(), automatically scales between 0 and 1, so I don’t need to change the input.

    transform = transforms.Compose(
                       [transforms.ToTensor()])
    
    # load the training and test datasets
    class MyDataset():
    	def __init__(self, csv_file,transform=None):
    		self.image_paths = pd.read_csv(csv_file, header = 0)
    		self.transform = transform
    
    	def __getitem__(self, index):
    		#print(self.image_paths[index])
    		#image_transformed = load_image(self.image_paths[index])
    		#print(index)
    		#print(self.image_paths.loc[[index]])
    		current = self.image_paths.iloc[index]
    		#print(current.shape)
    		#image_transformed = current.iloc[1]
    		image = Image.open(current.iloc[0])
    		image_transformed = Image.open(current.iloc[1])
    
    		#image, image_transformed = load_image(self.image_paths[index])
    
    		# transformations, e.g. Random Crop etc. 
    		# Make sure to perform the same transformations on image and target
    		# Here is a small example: https://discuss.pytorch.org/t/torchvision-transfors-how-to-perform-identical-transform-on-both-image-and-target/10606/7?u=ptrblck
    
    		#x, y = TF.to_tensor(image), TF.to_tensor(image_transformed)
    		x = torch.from_numpy(np.array(image))
    		y = torch.from_numpy(np.array(image_transformed))
    
    
    
    
    		return x, y
    
    	def __len__(self):
    		return len(self.image_paths)
    
    
    # In[ ]:
    
    
    # Create training and test dataloaders
    
    num_workers = 0
    # how many samples per batch to load
    batch_size = 10
    
    # prepare data loaders
    train_loader = torch.utils.data.DataLoader(MyDataset(""./train.csv""), batch_size=batch_size, num_workers=num_workers)
    test_loader = torch.utils.data.DataLoader(MyDataset(""./test.csv""), batch_size=batch_size, num_workers=num_workers)

For reading the shape, it seems like the grayscale image returns as a (width,height), then, I reshape the 2d image to a 4d input that I can feed into the neural network (1,1,width,height).Here is my convnet

    class ConvAutoencoder(nn.Module):
        def __init__(self):
            super(ConvAutoencoder, self).__init__()
            ## encoder layers ##
            # conv layer (depth from 1 --&gt; 16), 3x3 kernels
            self.conv1 = nn.Conv2d(1, 4, 64, stride=1, padding=1)  
            # conv layer (depth from 16 --&gt; 4), 3x3 kernels
            self.conv2 = nn.Conv2d(4, 4, 64, stride=1, padding=1)
    
            self.conv3 = nn.Conv2d(4, 4, 64, stride=1, padding=1)
    
            self.conv4 = nn.Conv2d(4, 3, 64, stride=1, padding=1)
    
            self.conv5 = nn.Conv2d(3, 2, 64, stride=1, padding=1)
    
            self.conv6 = nn.Conv2d(2, 2, 64, stride=1, padding=1)
    
    
            # pooling layer to reduce x-y dims by two; kernel and stride of 2
    
            ## decoder layers ##
            ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2
            self.t_conv6 = nn.ConvTranspose2d(2, 2, 64, stride=1, padding=1)
            
            self.t_conv5 = nn.ConvTranspose2d(2, 3, 64, stride=1, padding=1)
    
            self.t_conv4 = nn.ConvTranspose2d(3, 4, 64, stride=1, padding=1)
    
            self.t_conv3 = nn.ConvTranspose2d(4, 4, 64, stride=1, padding=1)
    
            self.t_conv2 = nn.ConvTranspose2d(4, 2, 64, stride=1, padding=1)
            self.t_conv1 = nn.ConvTranspose2d(2, 1, 64, stride=1, padding=1)
    
    
        def forward(self, x):
            ## encode ##
            # add hidden layers with relu activation function
            # and maxpooling after
            x = F.relu(self.conv1(x))
            
            # add second hidden layer
            x = F.relu(self.conv2(x))
            x = F.relu(self.conv3(x))
            x = F.relu(self.conv4(x))
            x = F.relu(self.conv5(x))
            x = F.relu(self.conv6(x))
    
            
            ## decode ##
            # add transpose conv layers, with relu activation function
            x = F.relu(self.t_conv6(x))
    
            x = F.relu(self.t_conv5(x))
    
            x = F.relu(self.t_conv4(x))
    
            x = F.relu(self.t_conv3(x))
    
            x = F.relu(self.t_conv2(x))
            # output layer (with sigmoid for scaling from 0 to 1)
            x = F.sigmoid(self.t_conv1(x))
                    
            return x                     return x 

At first, I thought that my convnet wasn’t deep enough, so I added more layers and increased the kernel size, but this didn’t have loss very much.Next, my training loop

    for epoch in range(10):
    	count_train = 0
    	count_test = 0
    	for step, (x, y) in enumerate(train_loader):
    		b_x = x  # batch x, shape (batch, 900*900)
    		b_y = x  # batch y, shape (batch, 900*900)
    		running_loss = 0.0
    		for i in range(len(x)): # breaking up into mini batches, where each iteration is input
    			xi = x[i, ...][None, ...] 
    			yi = y[i, ...][None, ...]
    			xi = xi[None, :, :, :]
    			yi = yi[None, :, :, :]
    			xi = xi.float().to(device)
    			yi = yi.float().to(device)
    			decoded = model(xi)
    			decoded = decoded.float()*255
    			#print(decoded)
    			loss = loss_func(decoded, yi)      # mean square error
    			optimizer.zero_grad()
    			loss.backward()                     # backpropagation, compute gradients
    			optimizer.step()
    			count_train = count_train + 1                    # apply gradients
    		running_loss += loss.item()
    	for step, (x, y) in enumerate(test_loader):
    		b_x = x.to(device)  # batch x, shape (batch, 900*900)
    		b_y = x.to(device)   # batch y, shape (batch, 900*900)
    		test_loss = 0.0
    		for i in range(len(x)): # breaking up into mini batches, where each iteration is input
    			xi = x[i, ...][None, ...] 
    			yi = y[i, ...][None, ...]
    			xi = xi[None, :, :, :]
    			yi = yi[None, :, :, :]
    			xi = xi.float().to(device)
    			yi = yi.float().to(device)
    	
    			decoded = model(xi)
    			decoded = decoded.float()*255
    			loss = loss_func(decoded, yi)      # mean square error
    			optimizer.zero_grad()
    			loss.backward()                     # backpropagation, compute gradients
    			optimizer.step()      
    			count_test = count_test + 1                    # apply gradients
                  # apply gradients
    		test_loss += loss.item()
    	if ((epoch+1)%5) == 0:
    		torch.save(model.state_dict(), ""./model/model-epoch-"" + str(epoch+1) + "".pth"")
    
    	print('Epoch: ', epoch + 1, '| train loss: ' + str(running_loss/count_train), ' | test loss ' + str(test_loss/count_test))
    	df.loc[epoch+1] = [epoch + 1,running_loss, test_loss]

For my training loop, first, I reshape the 2d input to a 4d input, from each image, individually. Then I update the gradients, then run the test lost, on the test dataloader. I then save the model every 5 epochs.Does anyone see than issue with my training code? I think that I may have had an issue with inference code.

    model = ConvAutoencoder().to(device)
    model.load_state_dict(torch.load(""./model/model-epoch-10.pth""))
    model.eval()
    
    
    image = Image.open(""./low_res/depth-map-low-0.png"")
    image = np.array(image)
    #image_transformed = Image.open(current.iloc[1])
    def scale(X, x_min, x_max):
        nom = (X-X.min(axis=0))*(x_max-x_min)
        denom = X.max(axis=0) - X.min(axis=0)
        denom[denom==0] = 1
        return x_min + nom/denom 
    x = scale(image,0,1)
    
    
    
    x = torch.from_numpy(x)
    
    print(x.max())
    xi = x[None ,None, :, :]
    
    xi = xi.float().to(device)

In my code, first, I load the Convolutional Autoencoder from a model file, then set the model as eval. Since the grayscale image is from 0 to 255, I first scale from 0 to 1 with min-max scaling, since during training, the toTensor Transform scales automatically to 0 to 1. I think convert from numpy to torch, reshape to a 4d, and pass through the network. I multiply the output by 255 to scale from 0 to 255, then squeeze to get rid of the batch size and grayscale dimension. I then move the tensor to cpu, convert to a numpy array, where it is shown as an image.However, despite this, my network still only outputs images of a single shade when inferencing. Is there an error in the code somewhere where I missed? If needed, you can reproduce my results by downloading the source code from drive below. Thank you for your time.Here is a link to my entire project: [https://drive.google.com/drive/folders/1mwep\_CMZfZI65BJXWqgjjSRU9FBGsSUg?usp=sharing](https://drive.google.com/drive/folders/1mwep_CMZfZI65BJXWqgjjSRU9FBGsSUg?usp=sharing)",t2_2jezkxap,False,,0,False,Autoencoder only returns gray images,[],r/pytorch,False,6,,0,,,False,t3_i9rmol,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,1597432823.0,,[],{},self,,True,,1597459206.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For my project, I am attempting to write an autoencoder, where the input and output grayscale images are slightly different. At first, I thought my conv net was not working, so I tried to have the autoencoder recreate the original input, but no matter what, the autoencoder on returns an gray image.I’m currently using my custom dataset, which I wrote with &lt;a href=""https://discuss.pytorch.org/u/ptrblck""&gt;&lt;strong&gt;@ptrblck&lt;/strong&gt;&lt;/a&gt; help, but when testing if my conv net has issues, I pass the x(input) as both x(input) and y(output).Here is my dataloading. My grayscale images are original 0 to 255, but I have read that Transform.toTensor(), automatically scales between 0 and 1, so I don’t need to change the input.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;transform = transforms.Compose(
                   [transforms.ToTensor()])

# load the training and test datasets
class MyDataset():
    def __init__(self, csv_file,transform=None):
        self.image_paths = pd.read_csv(csv_file, header = 0)
        self.transform = transform

    def __getitem__(self, index):
        #print(self.image_paths[index])
        #image_transformed = load_image(self.image_paths[index])
        #print(index)
        #print(self.image_paths.loc[[index]])
        current = self.image_paths.iloc[index]
        #print(current.shape)
        #image_transformed = current.iloc[1]
        image = Image.open(current.iloc[0])
        image_transformed = Image.open(current.iloc[1])

        #image, image_transformed = load_image(self.image_paths[index])

        # transformations, e.g. Random Crop etc. 
        # Make sure to perform the same transformations on image and target
        # Here is a small example: https://discuss.pytorch.org/t/torchvision-transfors-how-to-perform-identical-transform-on-both-image-and-target/10606/7?u=ptrblck

        #x, y = TF.to_tensor(image), TF.to_tensor(image_transformed)
        x = torch.from_numpy(np.array(image))
        y = torch.from_numpy(np.array(image_transformed))




        return x, y

    def __len__(self):
        return len(self.image_paths)


# In[ ]:


# Create training and test dataloaders

num_workers = 0
# how many samples per batch to load
batch_size = 10

# prepare data loaders
train_loader = torch.utils.data.DataLoader(MyDataset(&amp;quot;./train.csv&amp;quot;), batch_size=batch_size, num_workers=num_workers)
test_loader = torch.utils.data.DataLoader(MyDataset(&amp;quot;./test.csv&amp;quot;), batch_size=batch_size, num_workers=num_workers)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For reading the shape, it seems like the grayscale image returns as a (width,height), then, I reshape the 2d image to a 4d input that I can feed into the neural network (1,1,width,height).Here is my convnet&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class ConvAutoencoder(nn.Module):
    def __init__(self):
        super(ConvAutoencoder, self).__init__()
        ## encoder layers ##
        # conv layer (depth from 1 --&amp;gt; 16), 3x3 kernels
        self.conv1 = nn.Conv2d(1, 4, 64, stride=1, padding=1)  
        # conv layer (depth from 16 --&amp;gt; 4), 3x3 kernels
        self.conv2 = nn.Conv2d(4, 4, 64, stride=1, padding=1)

        self.conv3 = nn.Conv2d(4, 4, 64, stride=1, padding=1)

        self.conv4 = nn.Conv2d(4, 3, 64, stride=1, padding=1)

        self.conv5 = nn.Conv2d(3, 2, 64, stride=1, padding=1)

        self.conv6 = nn.Conv2d(2, 2, 64, stride=1, padding=1)


        # pooling layer to reduce x-y dims by two; kernel and stride of 2

        ## decoder layers ##
        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2
        self.t_conv6 = nn.ConvTranspose2d(2, 2, 64, stride=1, padding=1)

        self.t_conv5 = nn.ConvTranspose2d(2, 3, 64, stride=1, padding=1)

        self.t_conv4 = nn.ConvTranspose2d(3, 4, 64, stride=1, padding=1)

        self.t_conv3 = nn.ConvTranspose2d(4, 4, 64, stride=1, padding=1)

        self.t_conv2 = nn.ConvTranspose2d(4, 2, 64, stride=1, padding=1)
        self.t_conv1 = nn.ConvTranspose2d(2, 1, 64, stride=1, padding=1)


    def forward(self, x):
        ## encode ##
        # add hidden layers with relu activation function
        # and maxpooling after
        x = F.relu(self.conv1(x))

        # add second hidden layer
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        x = F.relu(self.conv5(x))
        x = F.relu(self.conv6(x))


        ## decode ##
        # add transpose conv layers, with relu activation function
        x = F.relu(self.t_conv6(x))

        x = F.relu(self.t_conv5(x))

        x = F.relu(self.t_conv4(x))

        x = F.relu(self.t_conv3(x))

        x = F.relu(self.t_conv2(x))
        # output layer (with sigmoid for scaling from 0 to 1)
        x = F.sigmoid(self.t_conv1(x))

        return x                     return x 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At first, I thought that my convnet wasn’t deep enough, so I added more layers and increased the kernel size, but this didn’t have loss very much.Next, my training loop&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for epoch in range(10):
    count_train = 0
    count_test = 0
    for step, (x, y) in enumerate(train_loader):
        b_x = x  # batch x, shape (batch, 900*900)
        b_y = x  # batch y, shape (batch, 900*900)
        running_loss = 0.0
        for i in range(len(x)): # breaking up into mini batches, where each iteration is input
            xi = x[i, ...][None, ...] 
            yi = y[i, ...][None, ...]
            xi = xi[None, :, :, :]
            yi = yi[None, :, :, :]
            xi = xi.float().to(device)
            yi = yi.float().to(device)
            decoded = model(xi)
            decoded = decoded.float()*255
            #print(decoded)
            loss = loss_func(decoded, yi)      # mean square error
            optimizer.zero_grad()
            loss.backward()                     # backpropagation, compute gradients
            optimizer.step()
            count_train = count_train + 1                    # apply gradients
        running_loss += loss.item()
    for step, (x, y) in enumerate(test_loader):
        b_x = x.to(device)  # batch x, shape (batch, 900*900)
        b_y = x.to(device)   # batch y, shape (batch, 900*900)
        test_loss = 0.0
        for i in range(len(x)): # breaking up into mini batches, where each iteration is input
            xi = x[i, ...][None, ...] 
            yi = y[i, ...][None, ...]
            xi = xi[None, :, :, :]
            yi = yi[None, :, :, :]
            xi = xi.float().to(device)
            yi = yi.float().to(device)

            decoded = model(xi)
            decoded = decoded.float()*255
            loss = loss_func(decoded, yi)      # mean square error
            optimizer.zero_grad()
            loss.backward()                     # backpropagation, compute gradients
            optimizer.step()      
            count_test = count_test + 1                    # apply gradients
              # apply gradients
        test_loss += loss.item()
    if ((epoch+1)%5) == 0:
        torch.save(model.state_dict(), &amp;quot;./model/model-epoch-&amp;quot; + str(epoch+1) + &amp;quot;.pth&amp;quot;)

    print(&amp;#39;Epoch: &amp;#39;, epoch + 1, &amp;#39;| train loss: &amp;#39; + str(running_loss/count_train), &amp;#39; | test loss &amp;#39; + str(test_loss/count_test))
    df.loc[epoch+1] = [epoch + 1,running_loss, test_loss]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For my training loop, first, I reshape the 2d input to a 4d input, from each image, individually. Then I update the gradients, then run the test lost, on the test dataloader. I then save the model every 5 epochs.Does anyone see than issue with my training code? I think that I may have had an issue with inference code.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = ConvAutoencoder().to(device)
model.load_state_dict(torch.load(&amp;quot;./model/model-epoch-10.pth&amp;quot;))
model.eval()


image = Image.open(&amp;quot;./low_res/depth-map-low-0.png&amp;quot;)
image = np.array(image)
#image_transformed = Image.open(current.iloc[1])
def scale(X, x_min, x_max):
    nom = (X-X.min(axis=0))*(x_max-x_min)
    denom = X.max(axis=0) - X.min(axis=0)
    denom[denom==0] = 1
    return x_min + nom/denom 
x = scale(image,0,1)



x = torch.from_numpy(x)

print(x.max())
xi = x[None ,None, :, :]

xi = xi.float().to(device)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In my code, first, I load the Convolutional Autoencoder from a model file, then set the model as eval. Since the grayscale image is from 0 to 255, I first scale from 0 to 1 with min-max scaling, since during training, the toTensor Transform scales automatically to 0 to 1. I think convert from numpy to torch, reshape to a 4d, and pass through the network. I multiply the output by 255 to scale from 0 to 255, then squeeze to get rid of the batch size and grayscale dimension. I then move the tensor to cpu, convert to a numpy array, where it is shown as an image.However, despite this, my network still only outputs images of a single shade when inferencing. Is there an error in the code somewhere where I missed? If needed, you can reproduce my results by downloading the source code from drive below. Thank you for your time.Here is a link to my entire project: &lt;a href=""https://drive.google.com/drive/folders/1mwep_CMZfZI65BJXWqgjjSRU9FBGsSUg?usp=sharing""&gt;https://drive.google.com/drive/folders/1mwep_CMZfZI65BJXWqgjjSRU9FBGsSUg?usp=sharing&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/9EsBA9Q7wDWjx5kXLzMnfM0AWbmEZoxxsEr_uIcW9j4.jpg?auto=webp&amp;s=21109dcbfd413ff5ac40639fe28756c90801122a', 'width': 45, 'height': 45}, 'resolutions': [], 'variants': {}, 'id': 'tAJpoX_vfeXmTfT7YAEtv0lPSrSW5AXgREkncFCwF2c'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i9rmol,True,,Stanley_C,,15,True,all_ads,False,[],False,,/r/pytorch/comments/i9rmol/autoencoder_only_returns_gray_images/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i9rmol/autoencoder_only_returns_gray_images/,7135,1597430406.0,0,,False,,,,,,,,
398,,pytorch,"Why does it appear that PyTorch tensors give preference to using default element datatype of float32 instead of float64?

For eg., the default element datatype for `torch.tensor()`is float32. This is the opposite with numpy arrays where the default element datatype for `numpy.array()`is float64. Why don’t PyTorch make it consistent with numpy arrays and make the default element datatype as float64? 

(Ps. I know I can change the element datatypes in the tensor but it would be more convenient if the default was float64)",t2_zmqho4m,False,,0,False,Why PyTorch tensors gives preference to float32 element datatype,[],r/pytorch,False,6,,0,,,False,t3_i8ze84,False,dark,0.87,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1597352102.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Why does it appear that PyTorch tensors give preference to using default element datatype of float32 instead of float64?&lt;/p&gt;

&lt;p&gt;For eg., the default element datatype for &lt;code&gt;torch.tensor()&lt;/code&gt;is float32. This is the opposite with numpy arrays where the default element datatype for &lt;code&gt;numpy.array()&lt;/code&gt;is float64. Why don’t PyTorch make it consistent with numpy arrays and make the default element datatype as float64? &lt;/p&gt;

&lt;p&gt;(Ps. I know I can change the element datatypes in the tensor but it would be more convenient if the default was float64)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i8ze84,True,,leockl,,10,True,all_ads,False,[],False,,/r/pytorch/comments/i8ze84/why_pytorch_tensors_gives_preference_to_float32/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i8ze84/why_pytorch_tensors_gives_preference_to_float32/,7135,1597323302.0,0,,False,,,,,,,,
399,,pytorch,,t2_44mbtmjy,False,,0,False,"From MIT, Google, UCSD researchers: Neural rendering--&gt;relight the scene photorealistically!",[],r/pytorch,False,6,,0,41.0,,False,t3_i8v9s2,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://a.thumbs.redditmedia.com/eBo2kEgtZpAkmgQWTqvTB-h_pwZBOuVZapksS-2d7l0.jpg,False,,[],{},link,,False,,1597331169.0,text,6,,,text,self.LatestInML,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/r5-Zdv--Vg0CQSQz74CXzZ4F-MbXvJmfYE09xCdFfZ4.jpg?auto=webp&amp;s=82697040da2cda91646c88419a222e2267ec7cf2', 'width': 1410, 'height': 418}, 'resolutions': [{'url': 'https://external-preview.redd.it/r5-Zdv--Vg0CQSQz74CXzZ4F-MbXvJmfYE09xCdFfZ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a73feb007fe68c9704d00101004311c67e80ff81', 'width': 108, 'height': 32}, {'url': 'https://external-preview.redd.it/r5-Zdv--Vg0CQSQz74CXzZ4F-MbXvJmfYE09xCdFfZ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d5bb9563565b4287a011b77fd4561e30dc1da05a', 'width': 216, 'height': 64}, {'url': 'https://external-preview.redd.it/r5-Zdv--Vg0CQSQz74CXzZ4F-MbXvJmfYE09xCdFfZ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=84018d630f2d58a8af19ec964425b70a611c5ab6', 'width': 320, 'height': 94}, {'url': 'https://external-preview.redd.it/r5-Zdv--Vg0CQSQz74CXzZ4F-MbXvJmfYE09xCdFfZ4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=51d21132218011f39f0314d028442212872d9b77', 'width': 640, 'height': 189}, {'url': 'https://external-preview.redd.it/r5-Zdv--Vg0CQSQz74CXzZ4F-MbXvJmfYE09xCdFfZ4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5030b3cbad83ff4982927513bb84af68a2907cbd', 'width': 960, 'height': 284}, {'url': 'https://external-preview.redd.it/r5-Zdv--Vg0CQSQz74CXzZ4F-MbXvJmfYE09xCdFfZ4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fe86777f9a0029e80d9141c0b6f6a0d920c49bb7', 'width': 1080, 'height': 320}], 'variants': {}, 'id': 'WJ5Yuf7wRuOMdA35Ei3dZ5B7Tt23zWMIHCk-uOISWOU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i8v9s2,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/i8v9s2/from_mit_google_ucsd_researchers_neural/,all_ads,False,/r/LatestInML/comments/i8v4ri/from_mit_google_ucsd_researchers_neural/,7135,1597302369.0,0,,False,/r/LatestInML/comments/i8v4ri/from_mit_google_ucsd_researchers_neural/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2008.03806)\n\nhttps://reddit.com/link/i8v4ri/video/olzl8vl8vpg51/player\n\nCheck out the video to see results!', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'From MIT, Google, UCSD researchers: Neural rendering--&gt;relight the scene photorealistically!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 41, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'olzl8vl8vpg51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/i8v4ri/asset/olzl8vl8vpg51/DASHPlaylist.mpd?a=1618044191%2CZmViNmU4NzVhYzlmYjU2MDA3MDM2OGRhZmVjMDNmZjJjNzFkYmJlMDU2ZDU4ZDBkMGU0NmVlMWZhMTczOTIyNA%3D%3D&amp;v=1&amp;f=sd', 'x': 640, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/i8v4ri/asset/olzl8vl8vpg51/HLSPlaylist.m3u8?a=1618044191%2CYzgxMjhiZDY5NGE4ZjljMDVmM2VmYmY0ZThjOThhOGVmMmNmOTY1Njc1YzE3MTFkMWQyMTA1MjcyYzI4YmY1Yw%3D%3D&amp;v=1&amp;f=sd', 'id': 'olzl8vl8vpg51', 'isGif': False}}, 'name': 't3_i8v4ri', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 51, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 51, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/eBo2kEgtZpAkmgQWTqvTB-h_pwZBOuVZapksS-2d7l0.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1597330439.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2008.03806""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/i8v4ri/video/olzl8vl8vpg51/player""&gt;https://reddit.com/link/i8v4ri/video/olzl8vl8vpg51/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Check out the video to see results!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/r5-Zdv--Vg0CQSQz74CXzZ4F-MbXvJmfYE09xCdFfZ4.jpg?auto=webp&amp;s=82697040da2cda91646c88419a222e2267ec7cf2', 'width': 1410, 'height': 418}, 'resolutions': [{'url': 'https://external-preview.redd.it/r5-Zdv--Vg0CQSQz74CXzZ4F-MbXvJmfYE09xCdFfZ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a73feb007fe68c9704d00101004311c67e80ff81', 'width': 108, 'height': 32}, {'url': 'https://external-preview.redd.it/r5-Zdv--Vg0CQSQz74CXzZ4F-MbXvJmfYE09xCdFfZ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d5bb9563565b4287a011b77fd4561e30dc1da05a', 'width': 216, 'height': 64}, {'url': 'https://external-preview.redd.it/r5-Zdv--Vg0CQSQz74CXzZ4F-MbXvJmfYE09xCdFfZ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=84018d630f2d58a8af19ec964425b70a611c5ab6', 'width': 320, 'height': 94}, {'url': 'https://external-preview.redd.it/r5-Zdv--Vg0CQSQz74CXzZ4F-MbXvJmfYE09xCdFfZ4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=51d21132218011f39f0314d028442212872d9b77', 'width': 640, 'height': 189}, {'url': 'https://external-preview.redd.it/r5-Zdv--Vg0CQSQz74CXzZ4F-MbXvJmfYE09xCdFfZ4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5030b3cbad83ff4982927513bb84af68a2907cbd', 'width': 960, 'height': 284}, {'url': 'https://external-preview.redd.it/r5-Zdv--Vg0CQSQz74CXzZ4F-MbXvJmfYE09xCdFfZ4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fe86777f9a0029e80d9141c0b6f6a0d920c49bb7', 'width': 1080, 'height': 320}], 'variants': {}, 'id': 'WJ5Yuf7wRuOMdA35Ei3dZ5B7Tt23zWMIHCk-uOISWOU'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'i8v4ri', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/i8v4ri/from_mit_google_ucsd_researchers_neural/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/i8v4ri/from_mit_google_ucsd_researchers_neural/', 'subreddit_subscribers': 6676, 'created_utc': 1597301639.0, 'num_crossposts': 14, 'media': None, 'is_video': False}]",t3_i8v4ri,,,,,
400,,pytorch,Does anyone have any tips on how to debug the problem of non-converging losses during training?,t2_ebu4m,False,,0,False,Loss Troubleshooting,[],r/pytorch,False,6,,0,,,False,t3_i8l26n,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1597291498.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone have any tips on how to debug the problem of non-converging losses during training?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i8l26n,True,,Pepipasta,,3,True,all_ads,False,[],False,,/r/pytorch/comments/i8l26n/loss_troubleshooting/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i8l26n/loss_troubleshooting/,7135,1597262698.0,0,,False,,,,,,,,
401,,pytorch,"Hello! I am trying to make a gan with pytorch. After I made everything and start training the loss got stuck after 1 epoch and a few batches. Suddenly I have no clue why is it doing it. Has someone got the same issue ever? And how did you solve it? Thank you :D

Github: [https://github.com/BoykoDenis/GAN](https://github.com/BoykoDenis/GAN)

https://preview.redd.it/qezd9hecfkg51.png?width=725&amp;format=png&amp;auto=webp&amp;s=30817f0cd78f409503859e958a3a4ad4f9b5a8ad",t2_3p351vi2,False,,0,False,Stuck loss while training GAN,[],r/pytorch,False,6,,0,140.0,,False,t3_i8cv81,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://a.thumbs.redditmedia.com/nO8_W_8D2JjKbjgN0sLBNUbn9F_IF-ec2BkK-gs89v8.jpg,1597240178.0,,[],{},self,,True,,1597264214.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! I am trying to make a gan with pytorch. After I made everything and start training the loss got stuck after 1 epoch and a few batches. Suddenly I have no clue why is it doing it. Has someone got the same issue ever? And how did you solve it? Thank you :D&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/BoykoDenis/GAN""&gt;https://github.com/BoykoDenis/GAN&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/qezd9hecfkg51.png?width=725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30817f0cd78f409503859e958a3a4ad4f9b5a8ad""&gt;https://preview.redd.it/qezd9hecfkg51.png?width=725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30817f0cd78f409503859e958a3a4ad4f9b5a8ad&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/k58D6phMdVF7bdB8rLCDuUumAnkCCCAaLwsH03VzF68.jpg?auto=webp&amp;s=ca17368d6422b9c1f6ea7f030478fa5b72a620f1', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/k58D6phMdVF7bdB8rLCDuUumAnkCCCAaLwsH03VzF68.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d11f53899db9e6f89eb26e7c0d5022b702c65d5f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/k58D6phMdVF7bdB8rLCDuUumAnkCCCAaLwsH03VzF68.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b29a9aa27b692a2b5c52d1f2313a3dd7574bad73', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/k58D6phMdVF7bdB8rLCDuUumAnkCCCAaLwsH03VzF68.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=489eb74c3f553f46984ef697fb8e0b2662610be1', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'ESRn18eqMhrdR78WhjNi4qMeGtI6PglYtq_JPHiQqNw'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i8cv81,True,,r0CT0r,,2,True,all_ads,False,[],False,,/r/pytorch/comments/i8cv81/stuck_loss_while_training_gan/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i8cv81/stuck_loss_while_training_gan/,7135,1597235414.0,0,,False,,,,"{'qezd9hecfkg51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 84, 'x': 108, 'u': 'https://preview.redd.it/qezd9hecfkg51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ea95b2f799caa8af99aa7b3cbe98ae3bc55f57a6'}, {'y': 168, 'x': 216, 'u': 'https://preview.redd.it/qezd9hecfkg51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b3c35d7801047363add4ef8bc8d4a73222a82af7'}, {'y': 249, 'x': 320, 'u': 'https://preview.redd.it/qezd9hecfkg51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e84b613c844d9f77a1035469a29f68c9abbeb839'}, {'y': 499, 'x': 640, 'u': 'https://preview.redd.it/qezd9hecfkg51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=934f128e4e8b3ffb2fe64ba2566f26d1ee9d869a'}], 's': {'y': 566, 'x': 725, 'u': 'https://preview.redd.it/qezd9hecfkg51.png?width=725&amp;format=png&amp;auto=webp&amp;s=30817f0cd78f409503859e958a3a4ad4f9b5a8ad'}, 'id': 'qezd9hecfkg51'}}",,,,
402,,pytorch,"https://pytorch.org/tutorials/beginner/transformer_tutorial.html

Was looking at the tutorial for seq2seq with transformer but the transformer model itself is not what the paper describes, or am I reading the code wrong?",,False,,0,False,Transformer tutorial seem incorrect,[],r/pytorch,False,6,,0,,,False,t3_i7zbqw,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,,self,False,,,{},self,,True,,1597206190.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html""&gt;https://pytorch.org/tutorials/beginner/transformer_tutorial.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Was looking at the tutorial for seq2seq with transformer but the transformer model itself is not what the paper describes, or am I reading the code wrong?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/t8NMh0Yg5C6VrYcyemtsGbIxbIlYELwkOwwdTnA1P1Q.jpg?auto=webp&amp;s=b79f32ca86527768ad498b9849486558612c0620', 'width': 672, 'height': 916}, 'resolutions': [{'url': 'https://external-preview.redd.it/t8NMh0Yg5C6VrYcyemtsGbIxbIlYELwkOwwdTnA1P1Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=807db369b7298f9b5ad25f990b6989d6da22f526', 'width': 108, 'height': 147}, {'url': 'https://external-preview.redd.it/t8NMh0Yg5C6VrYcyemtsGbIxbIlYELwkOwwdTnA1P1Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80fc50f6c79c7c3eb20fd3d38a97ce95f40d3d15', 'width': 216, 'height': 294}, {'url': 'https://external-preview.redd.it/t8NMh0Yg5C6VrYcyemtsGbIxbIlYELwkOwwdTnA1P1Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=52af6c90263eca480e86b7f7eac792835614031a', 'width': 320, 'height': 436}, {'url': 'https://external-preview.redd.it/t8NMh0Yg5C6VrYcyemtsGbIxbIlYELwkOwwdTnA1P1Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=547cbd55e4618544e78e7a374878491027c67803', 'width': 640, 'height': 872}], 'variants': {}, 'id': 'SBBuyhlMHkjquFJKlWN8-8G_pA7rdRrer4N9tH8zdwI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i7zbqw,True,,[deleted],,4,True,all_ads,False,[],,dark,/r/pytorch/comments/i7zbqw/transformer_tutorial_seem_incorrect/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i7zbqw/transformer_tutorial_seem_incorrect/,7135,1597177390.0,0,,False,,,,,,,,
403,,pytorch,"https://preview.redd.it/coyyuk8vjcg51.png?width=960&amp;format=png&amp;auto=webp&amp;s=4a4f9cd02434de96e0e130ad8d7b7b3bc48b2e7a

We are releasing a new version for Kornia that includes different functionalities to work with **3D augmentations** and **volumetric data**, **local features matching**, **homographies** and **epipolar geometry**.

In short, we list the following new features:

* Support to PyTorch v1.6.0.
* Local descriptors matching, homography and epipolar geometry API.
* 3D augmentations and low level API to work with volumetric data.

## Local features matching

We include an [kornia.feature.matching](https://kornia.readthedocs.io/en/latest/feature.html#matching) API to perform local descriptors matching such classical and derived version of the nearest neighbor (NN).

https://preview.redd.it/ma7ghuawjcg51.png?width=594&amp;format=png&amp;auto=webp&amp;s=a1a8cd7c473fbf6883c1104781819ccbffe72acf

https://preview.redd.it/mjf5wswwjcg51.png?width=658&amp;format=png&amp;auto=webp&amp;s=966b8827b5844c964e8553fca15abc64eced2331

## Homography and epipolar geometry

We also introduce [kornia.geometry.homography](https://kornia.readthedocs.io/en/latest/geometry.homography.html) including different functionalities to work with homographies and differentiable estimators based on the DLT formulation and the iteratively-reweighted least squares (IRWLS).

https://preview.redd.it/18or79xzjcg51.png?width=1200&amp;format=png&amp;auto=webp&amp;s=47024a11c1b6258cd22f3fb384615ee5c63d0d37

https://preview.redd.it/jvicn10zjcg51.png?width=657&amp;format=png&amp;auto=webp&amp;s=941d865cab78ce12e12f3f1f7afca9ad5b0bd961

In addition, we have ported some of the existing algorithms from [opencv.sfm](https://docs.opencv.org/master/d8/d8c/group__sfm.html) to PyTorch under [kornia.geometry.epipolar](https://kornia.readthedocs.io/en/latest/geometry.epipolar.html) that includes different functionalities to work with **Fundamental**, **Essential** or **Projection** matrices, and **Triangulation** methods useful for Structure from Motion problems.

## 3D augmentations and volumetric

We expand the [kornia.augmentaion](https://kornia.readthedocs.io/en/latest/augmentation.html) with a series of operators to perform 3D augmentations for volumetric data.

https://i.redd.it/1pe7y6u0kcg51.gif

In this release, we include the following first set of geometric 3D augmentations methods:

* RandomDepthicalFlip3D (along depth axis)
* RandomVerticalFlip3D (along height axis)
* RandomHorizontalFlip3D (along width axis)
* RandomRotation3D
* RandomAffine3D

Finally, we introduce also a low level API to perform 4D features transformations [kornia.warp\_projective](https://kornia.readthedocs.io/en/latest/geometry.transform.html#kornia.geometry.transform.warp_projective) and extending the filtering operators to support 3D kernels [kornia.filter3D](https://kornia.readthedocs.io/en/latest/filters.html#kornia.filters.filter3D).

&amp;#x200B;

Happy coding day - **The Kornia team.**",t2_11dkcj,False,,0,False,"Kornia v0.4.0: 3D augmentations, local features matching, homographies and epipolar geometry",[],r/pytorch,False,6,,0,78.0,,False,t3_i7p2dv,False,dark,1.0,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/UNeMbRJlzMKZuqoHYWu1XNZVerSsXs81Wnp1FJhdKsE.jpg,False,,[],{},,,True,,1597169001.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://preview.redd.it/coyyuk8vjcg51.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a4f9cd02434de96e0e130ad8d7b7b3bc48b2e7a""&gt;https://preview.redd.it/coyyuk8vjcg51.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a4f9cd02434de96e0e130ad8d7b7b3bc48b2e7a&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We are releasing a new version for Kornia that includes different functionalities to work with &lt;strong&gt;3D augmentations&lt;/strong&gt; and &lt;strong&gt;volumetric data&lt;/strong&gt;, &lt;strong&gt;local features matching&lt;/strong&gt;, &lt;strong&gt;homographies&lt;/strong&gt; and &lt;strong&gt;epipolar geometry&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In short, we list the following new features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Support to PyTorch v1.6.0.&lt;/li&gt;
&lt;li&gt;Local descriptors matching, homography and epipolar geometry API.&lt;/li&gt;
&lt;li&gt;3D augmentations and low level API to work with volumetric data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Local features matching&lt;/h2&gt;

&lt;p&gt;We include an &lt;a href=""https://kornia.readthedocs.io/en/latest/feature.html#matching""&gt;kornia.feature.matching&lt;/a&gt; API to perform local descriptors matching such classical and derived version of the nearest neighbor (NN).&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/ma7ghuawjcg51.png?width=594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1a8cd7c473fbf6883c1104781819ccbffe72acf""&gt;https://preview.redd.it/ma7ghuawjcg51.png?width=594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1a8cd7c473fbf6883c1104781819ccbffe72acf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/mjf5wswwjcg51.png?width=658&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=966b8827b5844c964e8553fca15abc64eced2331""&gt;https://preview.redd.it/mjf5wswwjcg51.png?width=658&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=966b8827b5844c964e8553fca15abc64eced2331&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Homography and epipolar geometry&lt;/h2&gt;

&lt;p&gt;We also introduce &lt;a href=""https://kornia.readthedocs.io/en/latest/geometry.homography.html""&gt;kornia.geometry.homography&lt;/a&gt; including different functionalities to work with homographies and differentiable estimators based on the DLT formulation and the iteratively-reweighted least squares (IRWLS).&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/18or79xzjcg51.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47024a11c1b6258cd22f3fb384615ee5c63d0d37""&gt;https://preview.redd.it/18or79xzjcg51.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47024a11c1b6258cd22f3fb384615ee5c63d0d37&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/jvicn10zjcg51.png?width=657&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=941d865cab78ce12e12f3f1f7afca9ad5b0bd961""&gt;https://preview.redd.it/jvicn10zjcg51.png?width=657&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=941d865cab78ce12e12f3f1f7afca9ad5b0bd961&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In addition, we have ported some of the existing algorithms from &lt;a href=""https://docs.opencv.org/master/d8/d8c/group__sfm.html""&gt;opencv.sfm&lt;/a&gt; to PyTorch under &lt;a href=""https://kornia.readthedocs.io/en/latest/geometry.epipolar.html""&gt;kornia.geometry.epipolar&lt;/a&gt; that includes different functionalities to work with &lt;strong&gt;Fundamental&lt;/strong&gt;, &lt;strong&gt;Essential&lt;/strong&gt; or &lt;strong&gt;Projection&lt;/strong&gt; matrices, and &lt;strong&gt;Triangulation&lt;/strong&gt; methods useful for Structure from Motion problems.&lt;/p&gt;

&lt;h2&gt;3D augmentations and volumetric&lt;/h2&gt;

&lt;p&gt;We expand the &lt;a href=""https://kornia.readthedocs.io/en/latest/augmentation.html""&gt;kornia.augmentaion&lt;/a&gt; with a series of operators to perform 3D augmentations for volumetric data.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/1pe7y6u0kcg51.gif""&gt;https://i.redd.it/1pe7y6u0kcg51.gif&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this release, we include the following first set of geometric 3D augmentations methods:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RandomDepthicalFlip3D (along depth axis)&lt;/li&gt;
&lt;li&gt;RandomVerticalFlip3D (along height axis)&lt;/li&gt;
&lt;li&gt;RandomHorizontalFlip3D (along width axis)&lt;/li&gt;
&lt;li&gt;RandomRotation3D&lt;/li&gt;
&lt;li&gt;RandomAffine3D&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, we introduce also a low level API to perform 4D features transformations &lt;a href=""https://kornia.readthedocs.io/en/latest/geometry.transform.html#kornia.geometry.transform.warp_projective""&gt;kornia.warp_projective&lt;/a&gt; and extending the filtering operators to support 3D kernels &lt;a href=""https://kornia.readthedocs.io/en/latest/filters.html#kornia.filters.filter3D""&gt;kornia.filter3D&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Happy coding day - &lt;strong&gt;The Kornia team.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i7p2dv,True,,edgarriba,,2,True,all_ads,False,[],False,,/r/pytorch/comments/i7p2dv/kornia_v040_3d_augmentations_local_features/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i7p2dv/kornia_v040_3d_augmentations_local_features/,7135,1597140201.0,0,,False,,,,"{'mjf5wswwjcg51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 26, 'x': 108, 'u': 'https://preview.redd.it/mjf5wswwjcg51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=77f21720629e9c173724fb22493d66825946b6b2'}, {'y': 52, 'x': 216, 'u': 'https://preview.redd.it/mjf5wswwjcg51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=109e9402a322801929ef24fb9f61fe7b41222cfc'}, {'y': 78, 'x': 320, 'u': 'https://preview.redd.it/mjf5wswwjcg51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=656fd32142da2809964fa87a04e8f99b2f012538'}, {'y': 156, 'x': 640, 'u': 'https://preview.redd.it/mjf5wswwjcg51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=57f64c020217258943ee640fbe3fe21461dc5a6e'}], 's': {'y': 161, 'x': 658, 'u': 'https://preview.redd.it/mjf5wswwjcg51.png?width=658&amp;format=png&amp;auto=webp&amp;s=966b8827b5844c964e8553fca15abc64eced2331'}, 'id': 'mjf5wswwjcg51'}, 'coyyuk8vjcg51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/coyyuk8vjcg51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c79a9415583b3d312db5fea76c9af41ce8dcc0f7'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/coyyuk8vjcg51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c16e0e3d901c0be2c1d97175a5a2fcc8fb0fbe7a'}, {'y': 180, 'x': 320, 'u': 'https://preview.redd.it/coyyuk8vjcg51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=81ed040d36a7edf58ebe024727a6d4fa27a4d795'}, {'y': 360, 'x': 640, 'u': 'https://preview.redd.it/coyyuk8vjcg51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e17810aeee03ac979e29195f6ecde1a88cafa41'}, {'y': 540, 'x': 960, 'u': 'https://preview.redd.it/coyyuk8vjcg51.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7b7516489c031d3944354ff74f3f2acd28044c1'}], 's': {'y': 540, 'x': 960, 'u': 'https://preview.redd.it/coyyuk8vjcg51.png?width=960&amp;format=png&amp;auto=webp&amp;s=4a4f9cd02434de96e0e130ad8d7b7b3bc48b2e7a'}, 'id': 'coyyuk8vjcg51'}, '18or79xzjcg51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 72, 'x': 108, 'u': 'https://preview.redd.it/18or79xzjcg51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33824b2d684182f7c294a2cdecaa89f32efdb06e'}, {'y': 144, 'x': 216, 'u': 'https://preview.redd.it/18or79xzjcg51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a04f9f00366afe5d66d9c86c278dff55e6cb70e6'}, {'y': 213, 'x': 320, 'u': 'https://preview.redd.it/18or79xzjcg51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9608f068bd84369f6b7a0827dc91c6a6e46b92a1'}, {'y': 426, 'x': 640, 'u': 'https://preview.redd.it/18or79xzjcg51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9fe14263253c6ab2cccfde4e4d9aad8f8ec38ec'}, {'y': 640, 'x': 960, 'u': 'https://preview.redd.it/18or79xzjcg51.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fa82b0099eccc6f2f5c1b916d260ab9be434ee0e'}, {'y': 720, 'x': 1080, 'u': 'https://preview.redd.it/18or79xzjcg51.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=edfa7d9a24259a0d5c58ca32dee5a743f2f3b134'}], 's': {'y': 800, 'x': 1200, 'u': 'https://preview.redd.it/18or79xzjcg51.png?width=1200&amp;format=png&amp;auto=webp&amp;s=47024a11c1b6258cd22f3fb384615ee5c63d0d37'}, 'id': '18or79xzjcg51'}, 'jvicn10zjcg51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 22, 'x': 108, 'u': 'https://preview.redd.it/jvicn10zjcg51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=11a8480dd35d0e7f3938a9d4b99f1a1e6244e17a'}, {'y': 45, 'x': 216, 'u': 'https://preview.redd.it/jvicn10zjcg51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ba290a888fa8fa31ca970ed2b0e152c852875fbc'}, {'y': 67, 'x': 320, 'u': 'https://preview.redd.it/jvicn10zjcg51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4ca1034754c94076f0d068bbeaa2a7b39874dde6'}, {'y': 135, 'x': 640, 'u': 'https://preview.redd.it/jvicn10zjcg51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=60adf5145b30b43b5dbd474f0855fe1504ee0e6b'}], 's': {'y': 139, 'x': 657, 'u': 'https://preview.redd.it/jvicn10zjcg51.png?width=657&amp;format=png&amp;auto=webp&amp;s=941d865cab78ce12e12f3f1f7afca9ad5b0bd961'}, 'id': 'jvicn10zjcg51'}, '1pe7y6u0kcg51': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 63, 'x': 108, 'u': 'https://preview.redd.it/1pe7y6u0kcg51.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=f7eb7f596ec38de41a0fa4c569339e877a68e81f'}, {'y': 126, 'x': 216, 'u': 'https://preview.redd.it/1pe7y6u0kcg51.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=45776d0ec44af1c66e22b93e7ad3e017b8877956'}, {'y': 188, 'x': 320, 'u': 'https://preview.redd.it/1pe7y6u0kcg51.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=75fec2f30de36d920c4c15d6d8610913d1ce4dea'}, {'y': 376, 'x': 640, 'u': 'https://preview.redd.it/1pe7y6u0kcg51.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=2310622a9b7ad68a55d8907b60c47457b3c9d3ea'}], 's': {'y': 376, 'gif': 'https://i.redd.it/1pe7y6u0kcg51.gif', 'mp4': 'https://preview.redd.it/1pe7y6u0kcg51.gif?format=mp4&amp;s=e040774f6cc0d837941938a3536985c724ce5b99', 'x': 640}, 'id': '1pe7y6u0kcg51'}, 'ma7ghuawjcg51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 40, 'x': 108, 'u': 'https://preview.redd.it/ma7ghuawjcg51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=12a679402fa7d103286c0bd48cee12d8be62aa31'}, {'y': 81, 'x': 216, 'u': 'https://preview.redd.it/ma7ghuawjcg51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=93a553a20eff4247cb97a2ea2c04831e657af690'}, {'y': 120, 'x': 320, 'u': 'https://preview.redd.it/ma7ghuawjcg51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf904e8425e34ab8b62cca7e90180702c445cff9'}], 's': {'y': 224, 'x': 594, 'u': 'https://preview.redd.it/ma7ghuawjcg51.png?width=594&amp;format=png&amp;auto=webp&amp;s=a1a8cd7c473fbf6883c1104781819ccbffe72acf'}, 'id': 'ma7ghuawjcg51'}}",,,,
404,,pytorch,,t2_44mbtmjy,False,,0,False,Denoise old music recordings with neural networks,[],r/pytorch,False,6,,0,105.0,,False,t3_i7k247,False,dark,0.86,,public,10,0,{},140.0,,False,[],,False,False,,{},,False,10,,False,https://b.thumbs.redditmedia.com/bPcFK0_JxhQPpBv_I_THg8paNqlKx2TQBZUJ-pZEH8I.jpg,False,,[],{},link,,False,,1597144572.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/X2NUczC88esF-TbEHjaph9mn2JLKyWZn6K8MrR20C8U.jpg?auto=webp&amp;s=dafd5477a12e2890d47e9477a8b2774ed51182ce', 'width': 588, 'height': 444}, 'resolutions': [{'url': 'https://external-preview.redd.it/X2NUczC88esF-TbEHjaph9mn2JLKyWZn6K8MrR20C8U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f9c5d8e79f41eb6d38d34098c61b06050638bfc2', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/X2NUczC88esF-TbEHjaph9mn2JLKyWZn6K8MrR20C8U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06af3a7ae8772be4c40dc173fad2e0afd83c58e1', 'width': 216, 'height': 163}, {'url': 'https://external-preview.redd.it/X2NUczC88esF-TbEHjaph9mn2JLKyWZn6K8MrR20C8U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1ff7570184850d05888c6f5700cc2cd6d880129', 'width': 320, 'height': 241}], 'variants': {}, 'id': 'XdzkaVqvFP5aQ0GFo2ZjqWivRlKst5gdotOwVHGJ-do'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i7k247,True,,MLtinkerer,,1,True,all_ads,False,[],False,,/r/pytorch/comments/i7k247/denoise_old_music_recordings_with_neural_networks/,all_ads,False,/r/LatestInML/comments/i7jn8r/denoise_old_music_recordings_with_neural_networks/,7135,1597115772.0,0,,False,/r/LatestInML/comments/i7jn8r/denoise_old_music_recordings_with_neural_networks/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2008.02027)\n\nhttps://reddit.com/link/i7jn8r/video/8w6eq0zreag51/player\n\nResults show that the proposed method is effective in removing noise while preserving the quality and details of the original music.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Denoise old music recordings with neural networks', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 105, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'8w6eq0zreag51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/i7jn8r/asset/8w6eq0zreag51/DASHPlaylist.mpd?a=1618044201%2CYWRjYjkxYjkyZTY4MDNiMWFlMzkxYmNkODQxNGYwN2NlYzk4YmIzM2YyZjc5NThiMThkNjQzNDkyMTU2YTgwYQ%3D%3D&amp;v=1&amp;f=sd', 'x': 640, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/i7jn8r/asset/8w6eq0zreag51/HLSPlaylist.m3u8?a=1618044201%2CMGZjNjI5ZWFhOGQyNTA1YzVmYTY0MGIyMzVhNmViNGIwNzVhZWNjYTVmNTVjZTIxZWQ3NjQ4Nzg2MGEzN2RhOA%3D%3D&amp;v=1&amp;f=sd', 'id': '8w6eq0zreag51', 'isGif': False}}, 'name': 't3_i7jn8r', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 27, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 27, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/bPcFK0_JxhQPpBv_I_THg8paNqlKx2TQBZUJ-pZEH8I.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1597142951.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2008.02027""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/i7jn8r/video/8w6eq0zreag51/player""&gt;https://reddit.com/link/i7jn8r/video/8w6eq0zreag51/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Results show that the proposed method is effective in removing noise while preserving the quality and details of the original music.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/X2NUczC88esF-TbEHjaph9mn2JLKyWZn6K8MrR20C8U.jpg?auto=webp&amp;s=dafd5477a12e2890d47e9477a8b2774ed51182ce', 'width': 588, 'height': 444}, 'resolutions': [{'url': 'https://external-preview.redd.it/X2NUczC88esF-TbEHjaph9mn2JLKyWZn6K8MrR20C8U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f9c5d8e79f41eb6d38d34098c61b06050638bfc2', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/X2NUczC88esF-TbEHjaph9mn2JLKyWZn6K8MrR20C8U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06af3a7ae8772be4c40dc173fad2e0afd83c58e1', 'width': 216, 'height': 163}, {'url': 'https://external-preview.redd.it/X2NUczC88esF-TbEHjaph9mn2JLKyWZn6K8MrR20C8U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1ff7570184850d05888c6f5700cc2cd6d880129', 'width': 320, 'height': 241}], 'variants': {}, 'id': 'XdzkaVqvFP5aQ0GFo2ZjqWivRlKst5gdotOwVHGJ-do'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'i7jn8r', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/i7jn8r/denoise_old_music_recordings_with_neural_networks/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/i7jn8r/denoise_old_music_recordings_with_neural_networks/', 'subreddit_subscribers': 6676, 'created_utc': 1597114151.0, 'num_crossposts': 17, 'media': None, 'is_video': False}]",t3_i7jn8r,,,,,
405,,pytorch,"So I have been using LSTM/GRU cells for lang model and NLP for a month now, and I have a new task to predict a sequence (the output) from a data with 6-8 features (these all are taken at some specific time) . Can I get any guidence for this topic because I haven't used these structures for Series prediction(have to predict a sequence of output for next 5-10 timesteps by seeing M training samples). I was thinking of using a sliding window... 

**Or should I use some different approach?** 

TLDR; guide for using LSTM sequence prediction (predict next 5-10 outputs from given data).

Thank you for your patience and guidance.",t2_4yj3qq2w,False,,0,False,Sequence prediction using LSTM/GRU,[],r/pytorch,False,6,,0,,,False,t3_i7oj5j,False,dark,0.99,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1597166119.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I have been using LSTM/GRU cells for lang model and NLP for a month now, and I have a new task to predict a sequence (the output) from a data with 6-8 features (these all are taken at some specific time) . Can I get any guidence for this topic because I haven&amp;#39;t used these structures for Series prediction(have to predict a sequence of output for next 5-10 timesteps by seeing M training samples). I was thinking of using a sliding window... &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Or should I use some different approach?&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;TLDR; guide for using LSTM sequence prediction (predict next 5-10 outputs from given data).&lt;/p&gt;

&lt;p&gt;Thank you for your patience and guidance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i7oj5j,True,,crazy_sax_guy,,3,True,all_ads,False,[],False,,/r/pytorch/comments/i7oj5j/sequence_prediction_using_lstmgru/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i7oj5j/sequence_prediction_using_lstmgru/,7135,1597137319.0,0,,False,,,,,,,,
406,,pytorch,"I’m currently trying to write an autoencoder, but the input and output are both different images. I wrote a custom dataloader that takes a csv file, where one column is the paths to the input images, and in the same row, yet the other column, the output file path is located.  
Here is my dataloader:

    class MyDataset(): def __init__(self, csv_file,transform=None): 		self.image_paths = pd.read_csv(csv_file, header = 0) 		self.transform = transform  	def __getitem__(self, index): #print(self.image_paths[index]) #image_transformed = load_image(self.image_paths[index]) #print(index) #print(self.image_paths.loc[[index]]) 		current = self.image_paths.iloc[index] 		#print(current.shape) #image_transformed = current.iloc[1] 		image = Image.open(current.iloc[0]) 		image_transformed = Image.open(current.iloc[1])  		#image, image_transformed = load_image(self.image_paths[index]) # transformations, e.g. Random Crop etc.  # Make sure to perform the same transformations on image and target # Here is a small example: https://discuss.pytorch.org/t/torchvision-transfors-how-to-perform-identical-transform-on-both-image-and-target/10606/7?u=ptrblck #x, y = TF.to_tensor(image), TF.to_tensor(image_transformed) 		x = torch.from_numpy(np.array(image)) 		y = torch.from_numpy(np.array(image_transformed))  		return x, y  	def __len__(self): return len(self.image_paths) 

I tried the dataloader in a loop, and it seems to be loading the batchs in correctly as a 3d array, with the first dimension representing the number of batchs, and the second and third dimensions representing the image width and height.  
Here is my cnn

    class ConvAutoencoder(nn.Module): def __init__(self):         super(ConvAutoencoder, self).__init__()         ## encoder layers ## # conv layer (depth from 1 --&gt; 16), 3x3 kernels         self.conv1 = nn.Conv2d(1, 16, 3, padding=1)           # conv layer (depth from 16 --&gt; 4), 3x3 kernels         self.conv2 = nn.Conv2d(16, 4, 3, padding=1)         # pooling layer to reduce x-y dims by two; kernel and stride of 2         self.pool = nn.MaxPool2d(2, 2)                  ## decoder layers ## ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2         self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)         self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)       def forward(self, x): ## encode ## # add hidden layers with relu activation function # and maxpooling after         x = F.relu(self.conv1(x))         x = self.pool(x)         # add second hidden layer         x = F.relu(self.conv2(x))         x = self.pool(x)  # compressed representation ## decode ## # add transpose conv layers, with relu activation function         x = F.relu(self.t_conv1(x))         # output layer (with sigmoid for scaling from 0 to 1)         x = F.sigmoid(self.t_conv2(x))                          return x 

And my train loop:

    for epoch in range(1): 	for step, (x, y) in enumerate(train_loader): 		b_x = x.to(device)   # batch x, shape (batch, 750*750) print(b_x.shape) 		b_y = y.to(device)   # batch y, shape (batch, 900*800)  		encoded, decoded = model(b_x) 		loss = loss_func(decoded, b_y)      # mean square error 		optimizer.zero_grad()               # clear gradients for this training step 		loss.backward()                     # backpropagation, compute gradients 		optimizer.step()                    # apply gradients 		running_loss += loss.item() 		print(running_loss) 

I get this error: RuntimeError: Expected 4-dimensional input for 4-dimensional weight \[16, 1, 3, 3\], but got 3-dimensional input of size \[4, 900, 900\].   
How do I change the dimensions of the input from the dataloader from a 3d to a 4d input, so I can train my images with my cnn autoencoder?",t2_2jezkxap,False,,0,False,"Issue with CNN autoencoder input with expecting 4-dimensional, but got 3-dimensional input",[],r/pytorch,False,6,,0,,,False,t3_i7m3rk,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1597153360.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m currently trying to write an autoencoder, but the input and output are both different images. I wrote a custom dataloader that takes a csv file, where one column is the paths to the input images, and in the same row, yet the other column, the output file path is located.&lt;br/&gt;
Here is my dataloader:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class MyDataset(): def __init__(self, csv_file,transform=None):         self.image_paths = pd.read_csv(csv_file, header = 0)        self.transform = transform      def __getitem__(self, index): #print(self.image_paths[index]) #image_transformed = load_image(self.image_paths[index]) #print(index) #print(self.image_paths.loc[[index]])      current = self.image_paths.iloc[index]      #print(current.shape) #image_transformed = current.iloc[1]      image = Image.open(current.iloc[0])         image_transformed = Image.open(current.iloc[1])         #image, image_transformed = load_image(self.image_paths[index]) # transformations, e.g. Random Crop etc.  # Make sure to perform the same transformations on image and target # Here is a small example: https://discuss.pytorch.org/t/torchvision-transfors-how-to-perform-identical-transform-on-both-image-and-target/10606/7?u=ptrblck #x, y = TF.to_tensor(image), TF.to_tensor(image_transformed)         x = torch.from_numpy(np.array(image))       y = torch.from_numpy(np.array(image_transformed))       return x, y     def __len__(self): return len(self.image_paths) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I tried the dataloader in a loop, and it seems to be loading the batchs in correctly as a 3d array, with the first dimension representing the number of batchs, and the second and third dimensions representing the image width and height.&lt;br/&gt;
Here is my cnn&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class ConvAutoencoder(nn.Module): def __init__(self):         super(ConvAutoencoder, self).__init__()         ## encoder layers ## # conv layer (depth from 1 --&amp;gt; 16), 3x3 kernels         self.conv1 = nn.Conv2d(1, 16, 3, padding=1)           # conv layer (depth from 16 --&amp;gt; 4), 3x3 kernels         self.conv2 = nn.Conv2d(16, 4, 3, padding=1)         # pooling layer to reduce x-y dims by two; kernel and stride of 2         self.pool = nn.MaxPool2d(2, 2)                  ## decoder layers ## ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2         self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)         self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)       def forward(self, x): ## encode ## # add hidden layers with relu activation function # and maxpooling after         x = F.relu(self.conv1(x))         x = self.pool(x)         # add second hidden layer         x = F.relu(self.conv2(x))         x = self.pool(x)  # compressed representation ## decode ## # add transpose conv layers, with relu activation function         x = F.relu(self.t_conv1(x))         # output layer (with sigmoid for scaling from 0 to 1)         x = F.sigmoid(self.t_conv2(x))                          return x 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And my train loop:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for epoch in range(1):  for step, (x, y) in enumerate(train_loader):        b_x = x.to(device)   # batch x, shape (batch, 750*750) print(b_x.shape)         b_y = y.to(device)   # batch y, shape (batch, 900*800)          encoded, decoded = model(b_x)       loss = loss_func(decoded, b_y)      # mean square error         optimizer.zero_grad()               # clear gradients for this training step        loss.backward()                     # backpropagation, compute gradients        optimizer.step()                    # apply gradients       running_loss += loss.item()         print(running_loss) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I get this error: RuntimeError: Expected 4-dimensional input for 4-dimensional weight [16, 1, 3, 3], but got 3-dimensional input of size [4, 900, 900].&lt;br/&gt;
How do I change the dimensions of the input from the dataloader from a 3d to a 4d input, so I can train my images with my cnn autoencoder?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i7m3rk,True,,Stanley_C,,5,True,all_ads,False,[],False,,/r/pytorch/comments/i7m3rk/issue_with_cnn_autoencoder_input_with_expecting/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i7m3rk/issue_with_cnn_autoencoder_input_with_expecting/,7135,1597124560.0,0,,False,,,,,,,,
407,,pytorch," I am using a pre-trained mobilenetv2 model running experiments for 30 epochs with a cosine annealing learning rate scheduler and these hyperparameters:

learning rate = 0.001  
weight decay = 4e-5  
momentum = 0.9  
batch size = 64  
optimizer = SGD  
dropout = 0.2

My data augmentation techniques include:

    train_transform = transforms.Compose([
                transforms.RandomResizedCrop(224,scale = (0.2,1.0)),
                #transforms.RandomRotation(15),      # rotate +/- 10 degrees
                transforms.RandomHorizontalFlip(),  # reverse 50% of images        
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406],
                                     [0.229, 0.224, 0.225])
            ])
    
        test_transform = transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406],
                                     [0.229, 0.224, 0.225])
            ])

I am using class\_weights combined with a data sampler( see here: [https://github.com/ufoym/imbalanced-dataset-sampler](https://github.com/ufoym/imbalanced-dataset-sampler) ) since my dataset is imbalanced and the formula for determining each weight is minority class size/class size. I am getting good results as my confusion report is showing that each class is generating accuracies in the 80s and 90s. How can I push my model's performance to be in the high 90s?.

**What I have tried so far:**

Adjusting the class weights

Changing the initial learning rate",t2_48egj5xw,False,,0,False,How can I improve my model's performance,[],r/pytorch,False,6,,0,,,False,t3_i77vaq,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1597105031.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am using a pre-trained mobilenetv2 model running experiments for 30 epochs with a cosine annealing learning rate scheduler and these hyperparameters:&lt;/p&gt;

&lt;p&gt;learning rate = 0.001&lt;br/&gt;
weight decay = 4e-5&lt;br/&gt;
momentum = 0.9&lt;br/&gt;
batch size = 64&lt;br/&gt;
optimizer = SGD&lt;br/&gt;
dropout = 0.2&lt;/p&gt;

&lt;p&gt;My data augmentation techniques include:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_transform = transforms.Compose([
            transforms.RandomResizedCrop(224,scale = (0.2,1.0)),
            #transforms.RandomRotation(15),      # rotate +/- 10 degrees
            transforms.RandomHorizontalFlip(),  # reverse 50% of images        
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                                 [0.229, 0.224, 0.225])
        ])

    test_transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                                 [0.229, 0.224, 0.225])
        ])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I am using class_weights combined with a data sampler( see here: &lt;a href=""https://github.com/ufoym/imbalanced-dataset-sampler""&gt;https://github.com/ufoym/imbalanced-dataset-sampler&lt;/a&gt; ) since my dataset is imbalanced and the formula for determining each weight is minority class size/class size. I am getting good results as my confusion report is showing that each class is generating accuracies in the 80s and 90s. How can I push my model&amp;#39;s performance to be in the high 90s?.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What I have tried so far:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Adjusting the class weights&lt;/p&gt;

&lt;p&gt;Changing the initial learning rate&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/sLWAWlMxvMGZIAw7S8r_jLWmalJ49RjdXRDSda29zYg.jpg?auto=webp&amp;s=f6d3f196ced3b3026a76078867bb0f97816194ae', 'width': 256, 'height': 256}, 'resolutions': [{'url': 'https://external-preview.redd.it/sLWAWlMxvMGZIAw7S8r_jLWmalJ49RjdXRDSda29zYg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=130a657947ba083ec2498104d9f8a07b921bec69', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/sLWAWlMxvMGZIAw7S8r_jLWmalJ49RjdXRDSda29zYg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e842e529197640b70059f7f3b153a14cff0163e', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'tPyERDb-2mtEWia5BcAjPdcoFHS3IHV9HW6_FaY644k'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i77vaq,True,,stunbomb1,,9,True,all_ads,False,[],False,,/r/pytorch/comments/i77vaq/how_can_i_improve_my_models_performance/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i77vaq/how_can_i_improve_my_models_performance/,7135,1597076231.0,0,,False,,,,,,,,
408,,pytorch,,t2_75peu,False,,0,False,DeepOrb AI Machine Learning Art and Music Ep 1,[],r/pytorch,False,6,,0,105.0,,False,t3_i6u5qg,False,dark,1.0,,public,12,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/RdpNZ36K9hM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'DeepOrb AI Machine Learning Art and Music Ep 1', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/RdpNZ36K9hM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'DeepOrb AI Coding', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/RdpNZ36K9hM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCxf5jeaNJRfBS90tYZMdpcQ'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/RdpNZ36K9hM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/i6u5qg', 'height': 338}",,False,12,,False,https://b.thumbs.redditmedia.com/ciKGMZ68pQ_awXd_0nEYgkwfXQ96fdJYDqfaSHWoi5o.jpg,False,,[],{},rich:video,,False,,1597046515.0,text,6,,,text,youtube.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QJfiH4X-kC02Y0Vz5smSXjEjxpqq4GMxf0fHHx3ftQY.jpg?auto=webp&amp;s=898ae1c535580094458dee1a976755ff736d4325', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/QJfiH4X-kC02Y0Vz5smSXjEjxpqq4GMxf0fHHx3ftQY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8479f409dd80f49fe86c9cd0c4577a2c0384c845', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/QJfiH4X-kC02Y0Vz5smSXjEjxpqq4GMxf0fHHx3ftQY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f3f7d2dddef162a7f2df725208b67d33f48104d9', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/QJfiH4X-kC02Y0Vz5smSXjEjxpqq4GMxf0fHHx3ftQY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=681e5ed8024d14fac3c6a2ec5739dcc2b5c5e877', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'V81d8tQ49v_OVQqzEFdW3AyaUCwi4hPiWOQ5AAk35Tw'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i6u5qg,True,,claytantor,,1,True,all_ads,False,[],False,,/r/pytorch/comments/i6u5qg/deeporb_ai_machine_learning_art_and_music_ep_1/,all_ads,False,https://www.youtube.com/watch?v=RdpNZ36K9hM&amp;feature=share,7135,1597017715.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'DeepOrb AI Machine Learning Art and Music Ep 1', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/RdpNZ36K9hM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'DeepOrb AI Coding', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/RdpNZ36K9hM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCxf5jeaNJRfBS90tYZMdpcQ'}}",False,https://www.youtube.com/watch?v=RdpNZ36K9hM&amp;feature=share,,,,,,,
409,,pytorch,,t2_44mbtmjy,False,,0,False,"Make easy edits to high-quality, diverse, and photorealistic images on real images and those generated by GANs!",[],r/pytorch,False,6,,0,75.0,,False,t3_i6c7mb,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/nLRtGBMWWCU8eDZbh_ica_P3R9nISeY4fYwNoQpBfRk.jpg,False,,[],{},link,,False,,1596970841.0,text,6,,,text,self.LatestInML,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/pxts_51FlMtddiL4mn5LGhGcqx0qXoUvu_sYXUp8a_g.jpg?auto=webp&amp;s=646ebc62ecd7e723362cd4ffb2eea2fd2a65d4a0', 'width': 1408, 'height': 764}, 'resolutions': [{'url': 'https://external-preview.redd.it/pxts_51FlMtddiL4mn5LGhGcqx0qXoUvu_sYXUp8a_g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fadcbc4d53536a0269b75e856854f1661d36a682', 'width': 108, 'height': 58}, {'url': 'https://external-preview.redd.it/pxts_51FlMtddiL4mn5LGhGcqx0qXoUvu_sYXUp8a_g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ceac5092a3c979b2a639a4c961b61610c791f9bc', 'width': 216, 'height': 117}, {'url': 'https://external-preview.redd.it/pxts_51FlMtddiL4mn5LGhGcqx0qXoUvu_sYXUp8a_g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b10a649fb4a5eff62dc61edb621275fb8fc55fc4', 'width': 320, 'height': 173}, {'url': 'https://external-preview.redd.it/pxts_51FlMtddiL4mn5LGhGcqx0qXoUvu_sYXUp8a_g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a716d67960377bc025bd229da547d0a9c944540b', 'width': 640, 'height': 347}, {'url': 'https://external-preview.redd.it/pxts_51FlMtddiL4mn5LGhGcqx0qXoUvu_sYXUp8a_g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4b121c8c3be46dd60a4ae80808db7e9e08a6c645', 'width': 960, 'height': 520}, {'url': 'https://external-preview.redd.it/pxts_51FlMtddiL4mn5LGhGcqx0qXoUvu_sYXUp8a_g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=65529146445feceb4c85fed349caae128043fd91', 'width': 1080, 'height': 586}], 'variants': {}, 'id': 'ca0F55OMGFvcO4MVpPdo28Yoy6hp6ZzCCMR94jdiEtg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i6c7mb,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/i6c7mb/make_easy_edits_to_highquality_diverse_and/,all_ads,False,/r/LatestInML/comments/i6bz07/make_easy_edits_to_highquality_diverse_and/,7135,1596942041.0,0,,False,/r/LatestInML/comments/i6bz07/make_easy_edits_to_highquality_diverse_and/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2008.02401)\n\nhttps://reddit.com/link/i6bz07/video/lkmgqc7p3wf51/player', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Make easy edits to high-quality, diverse, and photorealistic images on real images and those generated by GANs!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 75, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'lkmgqc7p3wf51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/i6bz07/asset/lkmgqc7p3wf51/DASHPlaylist.mpd?a=1618044201%2CNjYwZWE3ZDQzNjI4M2UxMTdkNGQ3MzFkNzU5MzdmMGQ0OGQ0MjIyMGFiZTVlOGQyOWFiOWUwMzA5OWJiMzIwMA%3D%3D&amp;v=1&amp;f=sd', 'x': 640, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/i6bz07/asset/lkmgqc7p3wf51/HLSPlaylist.m3u8?a=1618044201%2CZjFmOTY2ZTc5MjZkMmUyNmQ3YmRiMGE0ZDYxODQzNDQ1MzZiMjcwN2UwNGQ3OGM3N2E2YWRlZTZhNmRmZjE0Zg%3D%3D&amp;v=1&amp;f=sd', 'id': 'lkmgqc7p3wf51', 'isGif': False}}, 'name': 't3_i6bz07', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 28, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 28, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/nLRtGBMWWCU8eDZbh_ica_P3R9nISeY4fYwNoQpBfRk.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1596969759.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2008.02401""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/i6bz07/video/lkmgqc7p3wf51/player""&gt;https://reddit.com/link/i6bz07/video/lkmgqc7p3wf51/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/pxts_51FlMtddiL4mn5LGhGcqx0qXoUvu_sYXUp8a_g.jpg?auto=webp&amp;s=646ebc62ecd7e723362cd4ffb2eea2fd2a65d4a0', 'width': 1408, 'height': 764}, 'resolutions': [{'url': 'https://external-preview.redd.it/pxts_51FlMtddiL4mn5LGhGcqx0qXoUvu_sYXUp8a_g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fadcbc4d53536a0269b75e856854f1661d36a682', 'width': 108, 'height': 58}, {'url': 'https://external-preview.redd.it/pxts_51FlMtddiL4mn5LGhGcqx0qXoUvu_sYXUp8a_g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ceac5092a3c979b2a639a4c961b61610c791f9bc', 'width': 216, 'height': 117}, {'url': 'https://external-preview.redd.it/pxts_51FlMtddiL4mn5LGhGcqx0qXoUvu_sYXUp8a_g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b10a649fb4a5eff62dc61edb621275fb8fc55fc4', 'width': 320, 'height': 173}, {'url': 'https://external-preview.redd.it/pxts_51FlMtddiL4mn5LGhGcqx0qXoUvu_sYXUp8a_g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a716d67960377bc025bd229da547d0a9c944540b', 'width': 640, 'height': 347}, {'url': 'https://external-preview.redd.it/pxts_51FlMtddiL4mn5LGhGcqx0qXoUvu_sYXUp8a_g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4b121c8c3be46dd60a4ae80808db7e9e08a6c645', 'width': 960, 'height': 520}, {'url': 'https://external-preview.redd.it/pxts_51FlMtddiL4mn5LGhGcqx0qXoUvu_sYXUp8a_g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=65529146445feceb4c85fed349caae128043fd91', 'width': 1080, 'height': 586}], 'variants': {}, 'id': 'ca0F55OMGFvcO4MVpPdo28Yoy6hp6ZzCCMR94jdiEtg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'i6bz07', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/i6bz07/make_easy_edits_to_highquality_diverse_and/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/i6bz07/make_easy_edits_to_highquality_diverse_and/', 'subreddit_subscribers': 6676, 'created_utc': 1596940959.0, 'num_crossposts': 13, 'media': None, 'is_video': False}]",t3_i6bz07,,,,,
410,,pytorch,"In general, does anyone know which one would be faster? Numpy array CPU vectorization or PyTorch tensor GPU vectorization?",t2_zmqho4m,False,,0,False,Numpy array CPU vectorization vs. PyTorch tensor GPU vectorization,[],r/pytorch,False,6,,0,,,False,t3_i5yrvn,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1596920638.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In general, does anyone know which one would be faster? Numpy array CPU vectorization or PyTorch tensor GPU vectorization?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i5yrvn,True,,leockl,,9,True,all_ads,False,[],False,,/r/pytorch/comments/i5yrvn/numpy_array_cpu_vectorization_vs_pytorch_tensor/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i5yrvn/numpy_array_cpu_vectorization_vs_pytorch_tensor/,7135,1596891838.0,0,,False,,,,,,,,
411,,pytorch,"Hello, I know that this is a question probably asked quite often on this forum, but after going through the forum posts, I still am perplexed and could use some clarification. I am running Pytorch 1.6.0 on Ubuntu 20.04 with Cuda Toolkit 11.0 installed (hopefully, that’s all I needed to get everything running smoothly), and I am running some models on my local GPU.

Some models are as small as a few dense layers while I have also done transfer learning with the densenet121 model. I can tell that my GPU is being used via Nvidia X Server and by printing my Tensors to see that they have been loaded onto my GPU. However, I can also see that my CPU usage has increased significantly (to a much more significant degree on the densenet121 model of course) even when the GPU is being used. Is this normal?",t2_22sw0d85,False,,0,False,Running Pytorch on GPU,[],r/pytorch,False,6,,0,,,False,t3_i67066,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1596950209.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I know that this is a question probably asked quite often on this forum, but after going through the forum posts, I still am perplexed and could use some clarification. I am running Pytorch 1.6.0 on Ubuntu 20.04 with Cuda Toolkit 11.0 installed (hopefully, that’s all I needed to get everything running smoothly), and I am running some models on my local GPU.&lt;/p&gt;

&lt;p&gt;Some models are as small as a few dense layers while I have also done transfer learning with the densenet121 model. I can tell that my GPU is being used via Nvidia X Server and by printing my Tensors to see that they have been loaded onto my GPU. However, I can also see that my CPU usage has increased significantly (to a much more significant degree on the densenet121 model of course) even when the GPU is being used. Is this normal?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i67066,True,,jaynight43,,2,True,all_ads,False,[],False,,/r/pytorch/comments/i67066/running_pytorch_on_gpu/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i67066/running_pytorch_on_gpu/,7135,1596921409.0,0,,False,,,,,,,,
412,,pytorch,,t2_44mbtmjy,False,,0,False,"Manipulate novel images in realistic ways, such as changing lighting effects and scene geometry!",[],r/pytorch,False,6,,0,32.0,,False,t3_i5t9n0,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/Cp4Y-YwJXj7IHaNOdQYEsilLDCyP6fOzuZX4lXrv7cg.jpg,False,,[],{},link,,False,,1596891119.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BMEfMNLmuusYLSYRqbKJlrXO3ZwKcZgghLUWQtfsIGQ.jpg?auto=webp&amp;s=2a3007084fc3f66fb30edd0c386a3979c4785c3e', 'width': 1002, 'height': 408}, 'resolutions': [{'url': 'https://external-preview.redd.it/BMEfMNLmuusYLSYRqbKJlrXO3ZwKcZgghLUWQtfsIGQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed2874bf7265fdb72695be0c1809c1f441bbfeb5', 'width': 108, 'height': 43}, {'url': 'https://external-preview.redd.it/BMEfMNLmuusYLSYRqbKJlrXO3ZwKcZgghLUWQtfsIGQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=19581d27c00b466a9bfe7d8c7825c216b92643d8', 'width': 216, 'height': 87}, {'url': 'https://external-preview.redd.it/BMEfMNLmuusYLSYRqbKJlrXO3ZwKcZgghLUWQtfsIGQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c9d249eb93601736b92de74ea5055c09dae7e51', 'width': 320, 'height': 130}, {'url': 'https://external-preview.redd.it/BMEfMNLmuusYLSYRqbKJlrXO3ZwKcZgghLUWQtfsIGQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=adae7b4de09f0ea65babae574f7dcbfb43fdd1fd', 'width': 640, 'height': 260}, {'url': 'https://external-preview.redd.it/BMEfMNLmuusYLSYRqbKJlrXO3ZwKcZgghLUWQtfsIGQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a074cc628f4eeb25025c8970f72fcad710277bbc', 'width': 960, 'height': 390}], 'variants': {}, 'id': '-kEuTKfN5LgZObiWfN-L9X4oYGysRJrWq1nLzu-Brb8'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i5t9n0,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/i5t9n0/manipulate_novel_images_in_realistic_ways_such_as/,all_ads,False,/r/LatestInML/comments/i5t303/manipulate_novel_images_in_realistic_ways_such_as/,7135,1596862319.0,0,,False,/r/LatestInML/comments/i5t303/manipulate_novel_images_in_realistic_ways_such_as/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2008.02796)\n\nhttps://i.redd.it/n0o27f7lipf51.gif', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Manipulate novel images in realistic ways, such as changing lighting effects and scene geometry!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 32, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'n0o27f7lipf51': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 24, 'x': 108, 'u': 'https://preview.redd.it/n0o27f7lipf51.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=28048bfdba95215c792d2f49f1d81e0d06d6d505'}, {'y': 49, 'x': 216, 'u': 'https://preview.redd.it/n0o27f7lipf51.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=e46c9c450f10fe38b29b9f60fc30ac372c94e7d5'}, {'y': 73, 'x': 320, 'u': 'https://preview.redd.it/n0o27f7lipf51.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=ebd5d2274aada0851aa02d07c9d7ce15ebc0d1cb'}, {'y': 146, 'x': 640, 'u': 'https://preview.redd.it/n0o27f7lipf51.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=9c5a6f9ad74a70dc6225f432f32f0dfaa6b8fe47'}, {'y': 220, 'x': 960, 'u': 'https://preview.redd.it/n0o27f7lipf51.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=36e331b5611c26f5d4957de8f4053d1c4e32d0ea'}, {'y': 247, 'x': 1080, 'u': 'https://preview.redd.it/n0o27f7lipf51.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=f2f1860a5994ef9684c4b154f21aad2c0da0f288'}], 's': {'y': 335, 'gif': 'https://i.redd.it/n0o27f7lipf51.gif', 'mp4': 'https://preview.redd.it/n0o27f7lipf51.gif?format=mp4&amp;s=f96230cd5bf65eedf5259077aef1ead7cacec06d', 'x': 1460}, 'id': 'n0o27f7lipf51'}}, 'name': 't3_i5t303', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 48, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 48, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/Cp4Y-YwJXj7IHaNOdQYEsilLDCyP6fOzuZX4lXrv7cg.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1596890240.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': True, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2008.02796""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://i.redd.it/n0o27f7lipf51.gif""&gt;https://i.redd.it/n0o27f7lipf51.gif&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/BMEfMNLmuusYLSYRqbKJlrXO3ZwKcZgghLUWQtfsIGQ.jpg?auto=webp&amp;s=2a3007084fc3f66fb30edd0c386a3979c4785c3e', 'width': 1002, 'height': 408}, 'resolutions': [{'url': 'https://external-preview.redd.it/BMEfMNLmuusYLSYRqbKJlrXO3ZwKcZgghLUWQtfsIGQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed2874bf7265fdb72695be0c1809c1f441bbfeb5', 'width': 108, 'height': 43}, {'url': 'https://external-preview.redd.it/BMEfMNLmuusYLSYRqbKJlrXO3ZwKcZgghLUWQtfsIGQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=19581d27c00b466a9bfe7d8c7825c216b92643d8', 'width': 216, 'height': 87}, {'url': 'https://external-preview.redd.it/BMEfMNLmuusYLSYRqbKJlrXO3ZwKcZgghLUWQtfsIGQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c9d249eb93601736b92de74ea5055c09dae7e51', 'width': 320, 'height': 130}, {'url': 'https://external-preview.redd.it/BMEfMNLmuusYLSYRqbKJlrXO3ZwKcZgghLUWQtfsIGQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=adae7b4de09f0ea65babae574f7dcbfb43fdd1fd', 'width': 640, 'height': 260}, {'url': 'https://external-preview.redd.it/BMEfMNLmuusYLSYRqbKJlrXO3ZwKcZgghLUWQtfsIGQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a074cc628f4eeb25025c8970f72fcad710277bbc', 'width': 960, 'height': 390}], 'variants': {}, 'id': '-kEuTKfN5LgZObiWfN-L9X4oYGysRJrWq1nLzu-Brb8'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'i5t303', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/i5t303/manipulate_novel_images_in_realistic_ways_such_as/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/i5t303/manipulate_novel_images_in_realistic_ways_such_as/', 'subreddit_subscribers': 6676, 'created_utc': 1596861440.0, 'num_crossposts': 19, 'media': None, 'is_video': False}]",t3_i5t303,,,,,
413,,pytorch,[https://analyticsindiamag.com/fastai-with-tpu-in-pytorch-for-multiclass-image-classification/](https://analyticsindiamag.com/fastai-with-tpu-in-pytorch-for-multiclass-image-classification/),t2_40d0zt4s,False,,0,False,FastAI With TPU In PyTorch For Multiclass Image Classification,[],r/pytorch,False,6,,0,,,False,t3_i5dpcj,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1596835350.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://analyticsindiamag.com/fastai-with-tpu-in-pytorch-for-multiclass-image-classification/""&gt;https://analyticsindiamag.com/fastai-with-tpu-in-pytorch-for-multiclass-image-classification/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/t2Rc6qtEojKyXchFbk5e31PzTaipXKoFPkUXk0z5Jv0.jpg?auto=webp&amp;s=83408e9dc5aa59512ac6c63d35189cd41b418832', 'width': 868, 'height': 488}, 'resolutions': [{'url': 'https://external-preview.redd.it/t2Rc6qtEojKyXchFbk5e31PzTaipXKoFPkUXk0z5Jv0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=77f4dc57df3e616dc67a3705862368067381a98d', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/t2Rc6qtEojKyXchFbk5e31PzTaipXKoFPkUXk0z5Jv0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e0512d67d27e6fb944e10bd3720280746ec20a36', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/t2Rc6qtEojKyXchFbk5e31PzTaipXKoFPkUXk0z5Jv0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c718aca82399673fc417e2f5cdf4d576869dfce', 'width': 320, 'height': 179}, {'url': 'https://external-preview.redd.it/t2Rc6qtEojKyXchFbk5e31PzTaipXKoFPkUXk0z5Jv0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=83e47a81113916a6c49a74717a4f09b20f78d174', 'width': 640, 'height': 359}], 'variants': {}, 'id': 'oUSj5FJIqC_Z83vNxCuClqAs3-1gwJZ00wMZD3Ne80k'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i5dpcj,True,,analyticsindiam,,0,True,all_ads,False,[],False,,/r/pytorch/comments/i5dpcj/fastai_with_tpu_in_pytorch_for_multiclass_image/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i5dpcj/fastai_with_tpu_in_pytorch_for_multiclass_image/,7135,1596806550.0,0,,False,,,,,,,,
414,,pytorch,,t2_44mbtmjy,False,,0,False,ICYMI: State of the art in image-to-image translation!,[],r/pytorch,False,6,,0,89.0,,False,t3_i56rto,False,dark,0.81,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/C9OlYVmKXRMprjw7SN2Wa_vARQBqxboPwSU6TwSo80A.jpg,False,,[],{},link,,False,,1596800985.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jwsGt9B4CwjgVh7UuwXbHyCGYTWhKFXDIJIXuv9-Ggs.jpg?auto=webp&amp;s=7375330299d3de815fd4757ff0430a4f7540da6f', 'width': 628, 'height': 278}, 'resolutions': [{'url': 'https://external-preview.redd.it/jwsGt9B4CwjgVh7UuwXbHyCGYTWhKFXDIJIXuv9-Ggs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3ce9181abcb4e7c86fafbd701b0ad8a4b66a815c', 'width': 108, 'height': 47}, {'url': 'https://external-preview.redd.it/jwsGt9B4CwjgVh7UuwXbHyCGYTWhKFXDIJIXuv9-Ggs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2092bca6b9ad2b4e8d8665dfc49cffddc7d25d29', 'width': 216, 'height': 95}, {'url': 'https://external-preview.redd.it/jwsGt9B4CwjgVh7UuwXbHyCGYTWhKFXDIJIXuv9-Ggs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f3b6568998376f9e2a86e55db9c6280f9055e266', 'width': 320, 'height': 141}], 'variants': {}, 'id': 'L3l7xms3COf60Ql9Xir7_rctQ8BN2amOU5e8MSFUFTE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i56rto,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/i56rto/icymi_state_of_the_art_in_imagetoimage_translation/,all_ads,False,/r/LatestInML/comments/i56a3h/icymi_state_of_the_art_in_imagetoimage_translation/,7135,1596772185.0,0,,False,/r/LatestInML/comments/i56a3h/icymi_state_of_the_art_in_imagetoimage_translation/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2008.00951)\n\nhttps://preview.redd.it/cauwmv8xzhf51.png?width=1920&amp;format=png&amp;auto=webp&amp;s=766bb1e6f1af1f1be53c8a200a6db42f59793466\n\nTheir pSp framework can be used for a wide variety of image-to-image problems: Face Generation from Segmentation, Super Resolution, Face Interpolation for Real Images, Inpainting, Frontalization, StyleGAN Inversion', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'ICYMI: State of the art in image-to-image translation!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 89, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'cauwmv8xzhf51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 68, 'x': 108, 'u': 'https://preview.redd.it/cauwmv8xzhf51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b06121cfc015ff77abcc3832c9d576637b313d9'}, {'y': 137, 'x': 216, 'u': 'https://preview.redd.it/cauwmv8xzhf51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec7ee60c2d761ae893abaf748915f343e8b55401'}, {'y': 203, 'x': 320, 'u': 'https://preview.redd.it/cauwmv8xzhf51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b0ceb1029e31173e8898e7dfe5ce1c18ba4de871'}, {'y': 407, 'x': 640, 'u': 'https://preview.redd.it/cauwmv8xzhf51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0eba8fdce1e9015cfef69a31b9a3c77dd2e57af'}, {'y': 611, 'x': 960, 'u': 'https://preview.redd.it/cauwmv8xzhf51.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d1780be70ca5ee6619b6b4cc8291b4e70da5e822'}, {'y': 687, 'x': 1080, 'u': 'https://preview.redd.it/cauwmv8xzhf51.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aa5d6d1b7488e0a373527c035a32fef623d45d39'}], 's': {'y': 1222, 'x': 1920, 'u': 'https://preview.redd.it/cauwmv8xzhf51.png?width=1920&amp;format=png&amp;auto=webp&amp;s=766bb1e6f1af1f1be53c8a200a6db42f59793466'}, 'id': 'cauwmv8xzhf51'}}, 'name': 't3_i56a3h', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 20, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 20, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/C9OlYVmKXRMprjw7SN2Wa_vARQBqxboPwSU6TwSo80A.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1596798953.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2008.00951""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/cauwmv8xzhf51.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=766bb1e6f1af1f1be53c8a200a6db42f59793466""&gt;https://preview.redd.it/cauwmv8xzhf51.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=766bb1e6f1af1f1be53c8a200a6db42f59793466&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Their pSp framework can be used for a wide variety of image-to-image problems: Face Generation from Segmentation, Super Resolution, Face Interpolation for Real Images, Inpainting, Frontalization, StyleGAN Inversion&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/jwsGt9B4CwjgVh7UuwXbHyCGYTWhKFXDIJIXuv9-Ggs.jpg?auto=webp&amp;s=7375330299d3de815fd4757ff0430a4f7540da6f', 'width': 628, 'height': 278}, 'resolutions': [{'url': 'https://external-preview.redd.it/jwsGt9B4CwjgVh7UuwXbHyCGYTWhKFXDIJIXuv9-Ggs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3ce9181abcb4e7c86fafbd701b0ad8a4b66a815c', 'width': 108, 'height': 47}, {'url': 'https://external-preview.redd.it/jwsGt9B4CwjgVh7UuwXbHyCGYTWhKFXDIJIXuv9-Ggs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2092bca6b9ad2b4e8d8665dfc49cffddc7d25d29', 'width': 216, 'height': 95}, {'url': 'https://external-preview.redd.it/jwsGt9B4CwjgVh7UuwXbHyCGYTWhKFXDIJIXuv9-Ggs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f3b6568998376f9e2a86e55db9c6280f9055e266', 'width': 320, 'height': 141}], 'variants': {}, 'id': 'L3l7xms3COf60Ql9Xir7_rctQ8BN2amOU5e8MSFUFTE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'i56a3h', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/i56a3h/icymi_state_of_the_art_in_imagetoimage_translation/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/i56a3h/icymi_state_of_the_art_in_imagetoimage_translation/', 'subreddit_subscribers': 6676, 'created_utc': 1596770153.0, 'num_crossposts': 11, 'media': None, 'is_video': False}]",t3_i56a3h,,,,,
415,,pytorch,"I can’t seem to find the source code for `torch.chunk` in PyTorch’s Github page or in the documentation. Anyone knows where this is in PyTorch’s Github page?

Many thanks in advance.",t2_zmqho4m,False,,0,False,PyTorch: `torch.chunk` source code Github location,[],r/pytorch,False,6,,0,,,False,t3_i570it,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1596802014.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I can’t seem to find the source code for &lt;code&gt;torch.chunk&lt;/code&gt; in PyTorch’s Github page or in the documentation. Anyone knows where this is in PyTorch’s Github page?&lt;/p&gt;

&lt;p&gt;Many thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i570it,True,,leockl,,2,True,all_ads,False,[],False,,/r/pytorch/comments/i570it/pytorch_torchchunk_source_code_github_location/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i570it/pytorch_torchchunk_source_code_github_location/,7135,1596773214.0,0,,False,,,,,,,,
416,,pytorch,"We have been receiving requests to write posts on creating mobile applications using the Deep Learning models that we train in PyTorch or Tensorflow. So, we decided to start a series on deploying deep learning models to mobile devices!  


In today's post, we provide step by step instructions for converting a model trained in PyTorch to CoreML - a format identified by Apple's devices.  
The good news is this post isn't strictly for Apple users because in the first part of the post you will learn how to convert a PyTorch model to ONNX format and perform the required checks to ensure that the conversion was correct!  
[https://www.learnopencv.com/pytorch-to-coreml-model-conversion/](https://www.learnopencv.com/pytorch-to-coreml-model-conversion/) 

 and the code is at

https://preview.redd.it/vlty4zwjoff51.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=3323da36522bb5ebe7cdfb2f3e0880700432f4b9

[https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-CoreML-model-conversion](https://el2.convertkit-mail.com/c/n4ukn4827ktvhe9qk4t0/8ghqh3u589rd6l/aHR0cHM6Ly9naXRodWIuY29tL3NwbWFsbGljay9sZWFybm9wZW5jdi90cmVlL21hc3Rlci9QeVRvcmNoLXRvLUNvcmVNTC1tb2RlbC1jb252ZXJzaW9u)",t2_cvc9f,False,,0,False,PyTorch to CoreML model conversion,[],r/pytorch,False,6,,0,93.0,,False,t3_i4yezm,False,dark,0.86,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://a.thumbs.redditmedia.com/TZ3aNVwO91Gy3K1upXi4193h2NG90-sYW5GzOS3doz4.jpg,False,,[],{},,,True,,1596770930.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We have been receiving requests to write posts on creating mobile applications using the Deep Learning models that we train in PyTorch or Tensorflow. So, we decided to start a series on deploying deep learning models to mobile devices!  &lt;/p&gt;

&lt;p&gt;In today&amp;#39;s post, we provide step by step instructions for converting a model trained in PyTorch to CoreML - a format identified by Apple&amp;#39;s devices.&lt;br/&gt;
The good news is this post isn&amp;#39;t strictly for Apple users because in the first part of the post you will learn how to convert a PyTorch model to ONNX format and perform the required checks to ensure that the conversion was correct!&lt;br/&gt;
&lt;a href=""https://www.learnopencv.com/pytorch-to-coreml-model-conversion/""&gt;https://www.learnopencv.com/pytorch-to-coreml-model-conversion/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;and the code is at&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/vlty4zwjoff51.jpg?width=600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3323da36522bb5ebe7cdfb2f3e0880700432f4b9""&gt;https://preview.redd.it/vlty4zwjoff51.jpg?width=600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3323da36522bb5ebe7cdfb2f3e0880700432f4b9&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://el2.convertkit-mail.com/c/n4ukn4827ktvhe9qk4t0/8ghqh3u589rd6l/aHR0cHM6Ly9naXRodWIuY29tL3NwbWFsbGljay9sZWFybm9wZW5jdi90cmVlL21hc3Rlci9QeVRvcmNoLXRvLUNvcmVNTC1tb2RlbC1jb252ZXJzaW9u""&gt;https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-CoreML-model-conversion&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i4yezm,True,,spmallick,,0,True,all_ads,False,[],False,,/r/pytorch/comments/i4yezm/pytorch_to_coreml_model_conversion/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i4yezm/pytorch_to_coreml_model_conversion/,7135,1596742130.0,0,,False,,,,"{'vlty4zwjoff51': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 72, 'x': 108, 'u': 'https://preview.redd.it/vlty4zwjoff51.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3c62a618dd648d0dd50c008425ecf8bbd0d0956e'}, {'y': 144, 'x': 216, 'u': 'https://preview.redd.it/vlty4zwjoff51.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ce050cff869c440e9e975389fd7fb3ba08f9bdf6'}, {'y': 213, 'x': 320, 'u': 'https://preview.redd.it/vlty4zwjoff51.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e66e8611c5c375397c827eb3cb0f6a342b384442'}], 's': {'y': 400, 'x': 600, 'u': 'https://preview.redd.it/vlty4zwjoff51.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=3323da36522bb5ebe7cdfb2f3e0880700432f4b9'}, 'id': 'vlty4zwjoff51'}}",,,,
417,,pytorch,"I'm working on a project using a Multivariate LSTM, and upon training I get the Runtime error:  ""one of the variables needed for gradient computation has been modified by an inplace operation"".

I have looked it up and it has been suggested to use: `autograd.set_detect_anomaly(True)` to find the inplace operation that is causing the error.

However, when I use that command I don't see any difference in the traceback, it still points to the backprop line: `loss.backward(retain_graph=True)`.

Does anyone have any advice on where exactly to put the `autograd.set_detect_anomaly(True)` command in order to find the problematic inplace operation? At the moment I have it in the training loop, but I speculate the problem may be somewhere else. Is there a generic way to best place the command, or does it depend on where certain functions are being called? 

Any help would be greatly appreciated.

Thanks in advance!",t2_ebu4m,False,,0,False,Usage for autograd.set_detect_anomaly(True),[],r/pytorch,False,6,,0,,,False,t3_i504vd,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1596776414.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a project using a Multivariate LSTM, and upon training I get the Runtime error:  &amp;quot;one of the variables needed for gradient computation has been modified by an inplace operation&amp;quot;.&lt;/p&gt;

&lt;p&gt;I have looked it up and it has been suggested to use: &lt;code&gt;autograd.set_detect_anomaly(True)&lt;/code&gt; to find the inplace operation that is causing the error.&lt;/p&gt;

&lt;p&gt;However, when I use that command I don&amp;#39;t see any difference in the traceback, it still points to the backprop line: &lt;code&gt;loss.backward(retain_graph=True)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Does anyone have any advice on where exactly to put the &lt;code&gt;autograd.set_detect_anomaly(True)&lt;/code&gt; command in order to find the problematic inplace operation? At the moment I have it in the training loop, but I speculate the problem may be somewhere else. Is there a generic way to best place the command, or does it depend on where certain functions are being called? &lt;/p&gt;

&lt;p&gt;Any help would be greatly appreciated.&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i504vd,True,,Pepipasta,,0,True,all_ads,False,[],False,,/r/pytorch/comments/i504vd/usage_for_autogradset_detect_anomalytrue/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i504vd/usage_for_autogradset_detect_anomalytrue/,7135,1596747614.0,0,,False,,,,,,,,
418,,pytorch,"Hello all, I'm using PyTorch every day since v0.4  
Don't understand why do we need jit trace/script.  
Could someone explain me please or give a link, why do we need them both, for which purposes and why we can't just use `nn.Module`, convert to `ONNX` and use the model everywhere?  

Thanks!",t2_j0uptpk,False,,0,False,[Q] torch jit trace vs script vs nn.Module,[],r/pytorch,False,6,,0,,,False,t3_i43xox,False,dark,1.0,,public,8,0,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},,,True,,1596657302.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all, I&amp;#39;m using PyTorch every day since v0.4&lt;br/&gt;
Don&amp;#39;t understand why do we need jit trace/script.&lt;br/&gt;
Could someone explain me please or give a link, why do we need them both, for which purposes and why we can&amp;#39;t just use &lt;code&gt;nn.Module&lt;/code&gt;, convert to &lt;code&gt;ONNX&lt;/code&gt; and use the model everywhere?  &lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i43xox,True,,RedEyed__,,3,True,all_ads,False,[],False,,/r/pytorch/comments/i43xox/q_torch_jit_trace_vs_script_vs_nnmodule/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i43xox/q_torch_jit_trace_vs_script_vs_nnmodule/,7135,1596628502.0,0,,False,,,,,,,,
419,,pytorch,"[blendtorch](https://github.com/cheind/pytorch-blender) is a Python framework to seamlessly integrate Blender into PyTorch for deep learning from artificial visual data. We utilize Eevee, a new physically based real-time renderer, to synthesize images and annotations in real-time and thus avoid stalling model training in many cases.

v0.2 now features

* *Data Streaming*: Stream distributed Blender renderings directly into PyTorch data pipelines in real-time for supervised learning and domain randomization applications. Supports arbitrary pickle-able objects to be send alongside images/videos. Built-in recording capability to replay data without Blender.
* *OpenAI Gym Support*: Create and run remotely controlled Blender gyms to train reinforcement agents. Blender serves as simulation, visualization, and interactive live manipulation environment. An optional real-time mode will keep the environment running while the agent thinks about its next step.

[An agent controlling a cartpole environment in Blender through OpenAI interface. Blender is used for modelling, simulation \(physics and animation\) and interactive manipulation.](https://i.redd.it/goj7862lyxe51.gif)",t2_2cm37mcd,False,,0,False,blendtorch v0.2: OpenAI gym support added - train PyTorch agents in Blender environments through OpenAI.,[],r/pytorch,False,6,,0,82.0,,False,t3_i3f8xp,False,dark,0.97,,public,26,0,{},140.0,,False,[],,False,False,,{},,False,26,,False,https://a.thumbs.redditmedia.com/_FhjBmeOP2Q8y68BlVGUwro59oGoRHc7v-AKr2I7uG8.jpg,False,,[],{},self,,True,,1596556726.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://github.com/cheind/pytorch-blender""&gt;blendtorch&lt;/a&gt; is a Python framework to seamlessly integrate Blender into PyTorch for deep learning from artificial visual data. We utilize Eevee, a new physically based real-time renderer, to synthesize images and annotations in real-time and thus avoid stalling model training in many cases.&lt;/p&gt;

&lt;p&gt;v0.2 now features&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Data Streaming&lt;/em&gt;: Stream distributed Blender renderings directly into PyTorch data pipelines in real-time for supervised learning and domain randomization applications. Supports arbitrary pickle-able objects to be send alongside images/videos. Built-in recording capability to replay data without Blender.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;OpenAI Gym Support&lt;/em&gt;: Create and run remotely controlled Blender gyms to train reinforcement agents. Blender serves as simulation, visualization, and interactive live manipulation environment. An optional real-time mode will keep the environment running while the agent thinks about its next step.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/goj7862lyxe51.gif""&gt;An agent controlling a cartpole environment in Blender through OpenAI interface. Blender is used for modelling, simulation (physics and animation) and interactive manipulation.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/bhsaEyy-Nl3_4eSsPWRwFDUATrQ1fDcsYC8CGuJji10.jpg?auto=webp&amp;s=ec51a9ad53a549d1d1c16e2d140a814e10df4da1', 'width': 358, 'height': 358}, 'resolutions': [{'url': 'https://external-preview.redd.it/bhsaEyy-Nl3_4eSsPWRwFDUATrQ1fDcsYC8CGuJji10.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f64017958a16f3e88bd5ceab43e696b63b8cd04', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/bhsaEyy-Nl3_4eSsPWRwFDUATrQ1fDcsYC8CGuJji10.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44f708723c2b0224c29430dbcce41b083908846c', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/bhsaEyy-Nl3_4eSsPWRwFDUATrQ1fDcsYC8CGuJji10.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0efc91b878dfeabece042d6a2e9c8a24ec9d3d44', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'YB9bJGYXKwYzklRmc0A942TXn48WyZIo2CAoqvTrsPQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i3f8xp,True,,chrisheind,,2,True,all_ads,False,[],False,,/r/pytorch/comments/i3f8xp/blendtorch_v02_openai_gym_support_added_train/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i3f8xp/blendtorch_v02_openai_gym_support_added_train/,7135,1596527926.0,7,,False,,,,"{'goj7862lyxe51': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 63, 'x': 108, 'u': 'https://preview.redd.it/goj7862lyxe51.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=f9033da9121fb00f2c868de2d9b2f90dd941b687'}, {'y': 127, 'x': 216, 'u': 'https://preview.redd.it/goj7862lyxe51.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=19c40864383b271dee31e2da739ffadaa13a5a6e'}, {'y': 188, 'x': 320, 'u': 'https://preview.redd.it/goj7862lyxe51.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=385164ace77a23438c2a428bc00baae3401c2c63'}, {'y': 377, 'x': 640, 'u': 'https://preview.redd.it/goj7862lyxe51.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=dda20102347609a20cb99fe74ceed00377bb3e96'}], 's': {'y': 429, 'gif': 'https://i.redd.it/goj7862lyxe51.gif', 'mp4': 'https://preview.redd.it/goj7862lyxe51.gif?format=mp4&amp;s=393f2d6b40bd49f51eada8a3df73027d4f357fa8', 'x': 727}, 'id': 'goj7862lyxe51'}}",,,,
420,,pytorch,"Was working on a C++ inference for a PyTorch model. Can somebody help me with the equivalent C++ syntax of the python code below:

\----------------------  
transformations = transforms.Compose(\[  
transforms.ToPILImage(),  
transforms.Resize(256),  
transforms.CenterCrop(224),  
transforms.ToTensor(),  
transforms.Normalize(\[0.485, 0.456, 0.406\], \[0.229, 0.224, 0.225\])  
\])  
\---------------------",t2_5ugstn08,False,,0,False,[New to TorchScript]C++ Inference Syntax,[],r/pytorch,False,6,,0,,,False,t3_i3g1zh,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1596561265.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Was working on a C++ inference for a PyTorch model. Can somebody help me with the equivalent C++ syntax of the python code below:&lt;/p&gt;

&lt;p&gt;----------------------&lt;br/&gt;
transformations = transforms.Compose([&lt;br/&gt;
transforms.ToPILImage(),&lt;br/&gt;
transforms.Resize(256),&lt;br/&gt;
transforms.CenterCrop(224),&lt;br/&gt;
transforms.ToTensor(),&lt;br/&gt;
transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])&lt;br/&gt;
])&lt;br/&gt;
---------------------&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i3g1zh,True,,wittywarren,,0,True,all_ads,False,[],False,,/r/pytorch/comments/i3g1zh/new_to_torchscriptc_inference_syntax/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i3g1zh/new_to_torchscriptc_inference_syntax/,7135,1596532465.0,0,,False,,,,,,,,
421,,pytorch,,t2_44mbtmjy,False,,0,False,"Infer spatial arrangements and shapes of humans and objects from a 2-D image: Latest from Facebook, Berkeley, Carnegie Mellon, and Argo researchers:",[],r/pytorch,False,6,,0,58.0,,False,t3_i3c2bz,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/RiTBR8CZ9mItIWMKAHqWtdoqC_3MzdOsq13JCQWLVuA.jpg,False,,[],{},link,,False,,1596541182.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/HO4GHGK3RGDt2XiKMAN8xWAiV-FLZdOSIFoAdXld33s.jpg?auto=webp&amp;s=239500c121b8e9fb5fd77b938929f36d7c9acae8', 'width': 958, 'height': 402}, 'resolutions': [{'url': 'https://external-preview.redd.it/HO4GHGK3RGDt2XiKMAN8xWAiV-FLZdOSIFoAdXld33s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=01d6fb2f246bb0d34032714a18915b80466fc1d7', 'width': 108, 'height': 45}, {'url': 'https://external-preview.redd.it/HO4GHGK3RGDt2XiKMAN8xWAiV-FLZdOSIFoAdXld33s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=40c009b9efaa63e6500d305cdafb4c36270007d6', 'width': 216, 'height': 90}, {'url': 'https://external-preview.redd.it/HO4GHGK3RGDt2XiKMAN8xWAiV-FLZdOSIFoAdXld33s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=52487d5c7cb661f63cb4ed13ab85d657fc145524', 'width': 320, 'height': 134}, {'url': 'https://external-preview.redd.it/HO4GHGK3RGDt2XiKMAN8xWAiV-FLZdOSIFoAdXld33s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4afab0c11c2a7b58be7969c34bd7e888a8ee5898', 'width': 640, 'height': 268}], 'variants': {}, 'id': 'DEnDN0SNaBrSYQhE4ggE3BPzFavK1uuftXYNb_zDD5o'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i3c2bz,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/i3c2bz/infer_spatial_arrangements_and_shapes_of_humans/,all_ads,False,/r/LatestInML/comments/i3bn09/infer_spatial_arrangements_and_shapes_of_humans/,7135,1596512382.0,0,,False,/r/LatestInML/comments/i3bn09/infer_spatial_arrangements_and_shapes_of_humans/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2007.15649)\n\nhttps://reddit.com/link/i3bn09/video/26l2024qjwe51/player', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Infer spatial arrangements and shapes of humans and objects from a 2-D image: Latest from Facebook, Berkeley, Carnegie Mellon, and Argo researchers:', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 58, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'26l2024qjwe51': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/i3bn09/asset/26l2024qjwe51/DASHPlaylist.mpd?a=1618044201%2CNmRkMDJkNDE3M2ZjNTQzY2U4MDM3NTc3MTlkZWFlYWIxNGRlOWIwYzQ4N2Y3NDhlODk4NzI0Y2YxMTg2ZGQwYQ%3D%3D&amp;v=1&amp;f=sd', 'x': 524, 'y': 270, 'hlsUrl': 'https://v.redd.it/link/i3bn09/asset/26l2024qjwe51/HLSPlaylist.m3u8?a=1618044201%2CMmQ1YTJmY2I5YTdkM2RjYzMyN2M1MTkwOGVjNjI2MzBhOWQ2ZjU4MjQ0NjVkMTNmMzhjMTMxNmM2YzFkNmIwYQ%3D%3D&amp;v=1&amp;f=sd', 'id': '26l2024qjwe51', 'isGif': False}}, 'name': 't3_i3bn09', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.82, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 7, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/RiTBR8CZ9mItIWMKAHqWtdoqC_3MzdOsq13JCQWLVuA.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1596539415.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2007.15649""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/i3bn09/video/26l2024qjwe51/player""&gt;https://reddit.com/link/i3bn09/video/26l2024qjwe51/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/HO4GHGK3RGDt2XiKMAN8xWAiV-FLZdOSIFoAdXld33s.jpg?auto=webp&amp;s=239500c121b8e9fb5fd77b938929f36d7c9acae8', 'width': 958, 'height': 402}, 'resolutions': [{'url': 'https://external-preview.redd.it/HO4GHGK3RGDt2XiKMAN8xWAiV-FLZdOSIFoAdXld33s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=01d6fb2f246bb0d34032714a18915b80466fc1d7', 'width': 108, 'height': 45}, {'url': 'https://external-preview.redd.it/HO4GHGK3RGDt2XiKMAN8xWAiV-FLZdOSIFoAdXld33s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=40c009b9efaa63e6500d305cdafb4c36270007d6', 'width': 216, 'height': 90}, {'url': 'https://external-preview.redd.it/HO4GHGK3RGDt2XiKMAN8xWAiV-FLZdOSIFoAdXld33s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=52487d5c7cb661f63cb4ed13ab85d657fc145524', 'width': 320, 'height': 134}, {'url': 'https://external-preview.redd.it/HO4GHGK3RGDt2XiKMAN8xWAiV-FLZdOSIFoAdXld33s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4afab0c11c2a7b58be7969c34bd7e888a8ee5898', 'width': 640, 'height': 268}], 'variants': {}, 'id': 'DEnDN0SNaBrSYQhE4ggE3BPzFavK1uuftXYNb_zDD5o'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'i3bn09', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/i3bn09/infer_spatial_arrangements_and_shapes_of_humans/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/i3bn09/infer_spatial_arrangements_and_shapes_of_humans/', 'subreddit_subscribers': 6676, 'created_utc': 1596510615.0, 'num_crossposts': 15, 'media': None, 'is_video': False}]",t3_i3bn09,,,,,
422,,pytorch,,t2_44mbtmjy,False,,0,False,Un-selfie your pictures: From Adobe and Berkeley researchers!,[],r/pytorch,False,6,,0,50.0,,False,t3_i32g4d,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/vIjYVhsMmJcIFKADGleB43V8kQiTam8oegpGDIjKDRw.jpg,False,,[],{},link,,False,,1596507877.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/JakuZTwnPEWjUNeU0SQYrPXa1CnHs2DeBSdv8hfZ7fg.jpg?auto=webp&amp;s=ebbafc4b66b9e417efc4b3bb1b8c5e33cc7ec4cd', 'width': 884, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/JakuZTwnPEWjUNeU0SQYrPXa1CnHs2DeBSdv8hfZ7fg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ca20dcaaf32e8509555882d65e2ea2e8ce1eeaf', 'width': 108, 'height': 38}, {'url': 'https://external-preview.redd.it/JakuZTwnPEWjUNeU0SQYrPXa1CnHs2DeBSdv8hfZ7fg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=242d0d8ead89cce2235a2f5f6b0dd40880f8aa4a', 'width': 216, 'height': 77}, {'url': 'https://external-preview.redd.it/JakuZTwnPEWjUNeU0SQYrPXa1CnHs2DeBSdv8hfZ7fg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc75654271126240cc96d0a6c355efb40ae86ee8', 'width': 320, 'height': 114}, {'url': 'https://external-preview.redd.it/JakuZTwnPEWjUNeU0SQYrPXa1CnHs2DeBSdv8hfZ7fg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da76c7c38efe72962704559987c66a3b6fd46b7c', 'width': 640, 'height': 228}], 'variants': {}, 'id': 'suFrqskFindUbMiTX5DkZNsZ2RKFp5busQvFy3LnMy8'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i32g4d,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/i32g4d/unselfie_your_pictures_from_adobe_and_berkeley/,all_ads,False,/r/LatestInML/comments/i31bdw/unselfie_your_pictures_from_adobe_and_berkeley/,7135,1596479077.0,0,,False,/r/LatestInML/comments/i31bdw/unselfie_your_pictures_from_adobe_and_berkeley/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2007.15068)\n\nhttps://preview.redd.it/rfxapignmte51.png?width=1772&amp;format=png&amp;auto=webp&amp;s=103854cf3ab532cfcbdae517206a67d0ee804f9d\n\nTo achieve this, they first collect an unpaired dataset and introduce a way to synthesize paired training data for self-supervised learning. Then, to unselfie a photo, they propose a new three-stage pipeline, where they first find a target neutral pose, inpaint the body texture, and finally refine and composite the person on the background.\n\n&amp;#x200B;\n\nGet the free ML code finder browser extension:\n\nChrome [https://bit.ly/code\\_finder\\_chrome](https://bit.ly/code_finder_chrome)  \nFirefox [https://bit.ly/code\\_finder\\_firefox](https://bit.ly/code_finder_firefox).', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Un-selfie your pictures: From Adobe and Berkeley researchers!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 50, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'rfxapignmte51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 47, 'x': 108, 'u': 'https://preview.redd.it/rfxapignmte51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b5437a2bb225351321f5dfc388ec9cfe07ddb8e2'}, {'y': 94, 'x': 216, 'u': 'https://preview.redd.it/rfxapignmte51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=abcac7297e8e9fab32e87fe2b33a0576dae44007'}, {'y': 140, 'x': 320, 'u': 'https://preview.redd.it/rfxapignmte51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=56c97c4e42c94524c0c47800c9f6efe1e27163ef'}, {'y': 280, 'x': 640, 'u': 'https://preview.redd.it/rfxapignmte51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=05c34575bf3c89a4eba393b6ff8fe5aec597bb2f'}, {'y': 421, 'x': 960, 'u': 'https://preview.redd.it/rfxapignmte51.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9b6df88dd7b5ff1ade9242227a5de380ac1ba5d3'}, {'y': 474, 'x': 1080, 'u': 'https://preview.redd.it/rfxapignmte51.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b9ac361ce5e7df6546ebeed28a876f6e693c34a1'}], 's': {'y': 778, 'x': 1772, 'u': 'https://preview.redd.it/rfxapignmte51.png?width=1772&amp;format=png&amp;auto=webp&amp;s=103854cf3ab532cfcbdae517206a67d0ee804f9d'}, 'id': 'rfxapignmte51'}}, 'name': 't3_i31bdw', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 120, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 120, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/vIjYVhsMmJcIFKADGleB43V8kQiTam8oegpGDIjKDRw.jpg', 'edited': 1596478731.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1596504452.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2007.15068""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/rfxapignmte51.png?width=1772&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=103854cf3ab532cfcbdae517206a67d0ee804f9d""&gt;https://preview.redd.it/rfxapignmte51.png?width=1772&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=103854cf3ab532cfcbdae517206a67d0ee804f9d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To achieve this, they first collect an unpaired dataset and introduce a way to synthesize paired training data for self-supervised learning. Then, to unselfie a photo, they propose a new three-stage pipeline, where they first find a target neutral pose, inpaint the body texture, and finally refine and composite the person on the background.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Get the free ML code finder browser extension:&lt;/p&gt;\n\n&lt;p&gt;Chrome &lt;a href=""https://bit.ly/code_finder_chrome""&gt;https://bit.ly/code_finder_chrome&lt;/a&gt;&lt;br/&gt;\nFirefox &lt;a href=""https://bit.ly/code_finder_firefox""&gt;https://bit.ly/code_finder_firefox&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/JakuZTwnPEWjUNeU0SQYrPXa1CnHs2DeBSdv8hfZ7fg.jpg?auto=webp&amp;s=ebbafc4b66b9e417efc4b3bb1b8c5e33cc7ec4cd', 'width': 884, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/JakuZTwnPEWjUNeU0SQYrPXa1CnHs2DeBSdv8hfZ7fg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ca20dcaaf32e8509555882d65e2ea2e8ce1eeaf', 'width': 108, 'height': 38}, {'url': 'https://external-preview.redd.it/JakuZTwnPEWjUNeU0SQYrPXa1CnHs2DeBSdv8hfZ7fg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=242d0d8ead89cce2235a2f5f6b0dd40880f8aa4a', 'width': 216, 'height': 77}, {'url': 'https://external-preview.redd.it/JakuZTwnPEWjUNeU0SQYrPXa1CnHs2DeBSdv8hfZ7fg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc75654271126240cc96d0a6c355efb40ae86ee8', 'width': 320, 'height': 114}, {'url': 'https://external-preview.redd.it/JakuZTwnPEWjUNeU0SQYrPXa1CnHs2DeBSdv8hfZ7fg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da76c7c38efe72962704559987c66a3b6fd46b7c', 'width': 640, 'height': 228}], 'variants': {}, 'id': 'suFrqskFindUbMiTX5DkZNsZ2RKFp5busQvFy3LnMy8'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'i31bdw', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/i31bdw/unselfie_your_pictures_from_adobe_and_berkeley/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/i31bdw/unselfie_your_pictures_from_adobe_and_berkeley/', 'subreddit_subscribers': 6676, 'created_utc': 1596475652.0, 'num_crossposts': 16, 'media': None, 'is_video': False}]",t3_i31bdw,,,,,
423,,pytorch,"I am still new to PyTorch and I am going off this link:  [https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html) I don't see many examples of being applied online so this is how I thought it should look.

     Q = math.floor(len(train_data)/batch)
    
     lrs = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = Q)

Then in my training loop, I have it set up like so:

     # Update parameters
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            lrs.step()",t2_48egj5xw,False,,0,False,Am I using Cosine annealing correctly?,[],r/pytorch,False,6,,0,,,False,t3_i2xkw5,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1596487497.0,,[],{},,,True,,1596492208.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am still new to PyTorch and I am going off this link:  &lt;a href=""https://pytorch.org/docs/stable/optim.html""&gt;https://pytorch.org/docs/stable/optim.html&lt;/a&gt; I don&amp;#39;t see many examples of being applied online so this is how I thought it should look.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; Q = math.floor(len(train_data)/batch)

 lrs = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = Q)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then in my training loop, I have it set up like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; # Update parameters
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        lrs.step()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i2xkw5,True,,stunbomb1,,1,True,all_ads,False,[],False,,/r/pytorch/comments/i2xkw5/am_i_using_cosine_annealing_correctly/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i2xkw5/am_i_using_cosine_annealing_correctly/,7135,1596463408.0,0,,False,,,,,,,,
424,,pytorch,,t2_57s4p771,False,,0,False,Brand new paper on solar irradiance forecasting with deep neural networks. The article also proposes to adopt the so-called domain adaptation in the field of solar irradiance. Code written in pytorch.,[],r/pytorch,False,6,,0,112.0,,False,t3_i2tcjo,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/kuJV3l-k9ZmNCayZfZS6iekA9OvrkwD09zjS-0yqaLc.jpg,False,,[],{},link,,False,,1596471974.0,text,6,,,text,mdpi.com,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QoCpFcfh9VUj2RzvjLScVUisxa0jp1Cun6cLvuQonic.jpg?auto=webp&amp;s=275c47ba212ff58d69d553c14cd8963840b41ee9', 'width': 550, 'height': 442}, 'resolutions': [{'url': 'https://external-preview.redd.it/QoCpFcfh9VUj2RzvjLScVUisxa0jp1Cun6cLvuQonic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c3f7afc0336abc455f51cc6cc99f8cb4dbdc8078', 'width': 108, 'height': 86}, {'url': 'https://external-preview.redd.it/QoCpFcfh9VUj2RzvjLScVUisxa0jp1Cun6cLvuQonic.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ace702aad22ced829f09d000a5edd03b92dcb89', 'width': 216, 'height': 173}, {'url': 'https://external-preview.redd.it/QoCpFcfh9VUj2RzvjLScVUisxa0jp1Cun6cLvuQonic.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=30938b0d3ed698755adbfd00101c71f6b7c71f93', 'width': 320, 'height': 257}], 'variants': {}, 'id': 'Vj4WHlJBsYdrVsCWon6xN7nM3-n83dSIxuE09yijY7I'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i2tcjo,True,,_Mat_San_,,0,True,all_ads,False,[],False,,/r/pytorch/comments/i2tcjo/brand_new_paper_on_solar_irradiance_forecasting/,all_ads,False,https://www.mdpi.com/1996-1073/13/15/3987,7135,1596443174.0,0,,False,https://www.mdpi.com/1996-1073/13/15/3987,,,,,,,
425,,pytorch,"Hi, I am new to pytorch so apologies if this is a stupid question. I have been following sentdex's tutorial on pytorch CNN : 

https://pythonprogramming.net/convnet-model-deep-learning-neural-network-pytorch/

My particular use case (apart from having different/more classifications) is to sort images that fit a category from all unrelated images (not in any classification). I've added my categories trained and got good results from my test image set.

However, if I test an unrelated image, I get (what I am interpreting as) a strong classification in one of my trained categories. I was expected a 'low score' across all trained categories.

I found this article, that appears to address this type of issue :

https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd

But I dont know enough (yet) to really understand it.

So I know I am on the right track can someone just give me some advice on the following questions :

1) Is it 'normal' for a trained network to be wrong on images that don't actually belong to any trained category ?

2) Is this simply a problem with my training data (not extensive enough etc.) ?

3) Is there a simple change I can make to get a probability score (so I can ignore low probabilities), or is the Bayesian approach (I dont understand the 'guide' function) in the above article the only solution?

I want to research and understand this problem, but I am hoping someone can give me a steer on the above so I am investing my time in the right area.

Many thanks",t2_16rmrd,False,,0,False,pytorch CNN poor performance with unrelated image (theory question),[],r/pytorch,False,6,,0,,,False,t3_i2vc6e,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1596482841.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am new to pytorch so apologies if this is a stupid question. I have been following sentdex&amp;#39;s tutorial on pytorch CNN : &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://pythonprogramming.net/convnet-model-deep-learning-neural-network-pytorch/""&gt;https://pythonprogramming.net/convnet-model-deep-learning-neural-network-pytorch/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;My particular use case (apart from having different/more classifications) is to sort images that fit a category from all unrelated images (not in any classification). I&amp;#39;ve added my categories trained and got good results from my test image set.&lt;/p&gt;

&lt;p&gt;However, if I test an unrelated image, I get (what I am interpreting as) a strong classification in one of my trained categories. I was expected a &amp;#39;low score&amp;#39; across all trained categories.&lt;/p&gt;

&lt;p&gt;I found this article, that appears to address this type of issue :&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd""&gt;https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;But I dont know enough (yet) to really understand it.&lt;/p&gt;

&lt;p&gt;So I know I am on the right track can someone just give me some advice on the following questions :&lt;/p&gt;

&lt;p&gt;1) Is it &amp;#39;normal&amp;#39; for a trained network to be wrong on images that don&amp;#39;t actually belong to any trained category ?&lt;/p&gt;

&lt;p&gt;2) Is this simply a problem with my training data (not extensive enough etc.) ?&lt;/p&gt;

&lt;p&gt;3) Is there a simple change I can make to get a probability score (so I can ignore low probabilities), or is the Bayesian approach (I dont understand the &amp;#39;guide&amp;#39; function) in the above article the only solution?&lt;/p&gt;

&lt;p&gt;I want to research and understand this problem, but I am hoping someone can give me a steer on the above so I am investing my time in the right area.&lt;/p&gt;

&lt;p&gt;Many thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/iW7UvqjpXc_NcUO7KbOIVJ0rT8rY0_GnrQrGbuxoma4.jpg?auto=webp&amp;s=a8c325fd91f01504bf04781a753482cdae15584f', 'width': 320, 'height': 320}, 'resolutions': [{'url': 'https://external-preview.redd.it/iW7UvqjpXc_NcUO7KbOIVJ0rT8rY0_GnrQrGbuxoma4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3bbfbd4372d0ca8994be6e778b0e370067761ef6', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/iW7UvqjpXc_NcUO7KbOIVJ0rT8rY0_GnrQrGbuxoma4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b438bd9d64db77f2d9a4cd5c53c373f0995a377a', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/iW7UvqjpXc_NcUO7KbOIVJ0rT8rY0_GnrQrGbuxoma4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=42c7d7d39c525cdcc52619d352d317ae86d88b36', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'WULOx1TQIJMRrrgTgmDWChCjF1AXeJlIoQ-xv5atO7o'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i2vc6e,True,,junker44,,6,True,all_ads,False,[],False,,/r/pytorch/comments/i2vc6e/pytorch_cnn_poor_performance_with_unrelated_image/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i2vc6e/pytorch_cnn_poor_performance_with_unrelated_image/,7135,1596454041.0,0,,False,,,,,,,,
426,,pytorch,"I am using a pre-trained mobilenetv2 model running experiments for 30 epochs with the learning rate being adjusted by 0.1 every 10 epochs and these hyperparameters:

learning rate = 0.001  
weight decay = 4e-5  
momentum = 0.9  
batch size = 64  
optimizer = SGD  
dropout = 0.2

I am using class\_weights since my dataset is imbalanced and the formula for determining each weight is minority class size/class size.  I am getting good results as my confusion report is showing that each class is generating accuracies in the 80s and 90s. The issue I am facing is that after a while my model will start to fluctuate, for example in my latest experiment after 11 epochs the accuracy keep bouncing between 87% and 91%. 

**What I have tried so far:**

doubling the weight of the minority class, so the model can learn it as well as the others.

changing the learning rate: either 0.01 or 0.0001

changing how the learning rate is adjusted",t2_48egj5xw,False,,0,False,How to handle validation fluctuation?,[],r/pytorch,False,6,,0,,,False,t3_i24lbt,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1596363857.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am using a pre-trained mobilenetv2 model running experiments for 30 epochs with the learning rate being adjusted by 0.1 every 10 epochs and these hyperparameters:&lt;/p&gt;

&lt;p&gt;learning rate = 0.001&lt;br/&gt;
weight decay = 4e-5&lt;br/&gt;
momentum = 0.9&lt;br/&gt;
batch size = 64&lt;br/&gt;
optimizer = SGD&lt;br/&gt;
dropout = 0.2&lt;/p&gt;

&lt;p&gt;I am using class_weights since my dataset is imbalanced and the formula for determining each weight is minority class size/class size.  I am getting good results as my confusion report is showing that each class is generating accuracies in the 80s and 90s. The issue I am facing is that after a while my model will start to fluctuate, for example in my latest experiment after 11 epochs the accuracy keep bouncing between 87% and 91%. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What I have tried so far:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;doubling the weight of the minority class, so the model can learn it as well as the others.&lt;/p&gt;

&lt;p&gt;changing the learning rate: either 0.01 or 0.0001&lt;/p&gt;

&lt;p&gt;changing how the learning rate is adjusted&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i24lbt,True,,stunbomb1,,4,True,all_ads,False,[],False,,/r/pytorch/comments/i24lbt/how_to_handle_validation_fluctuation/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i24lbt/how_to_handle_validation_fluctuation/,7135,1596335057.0,0,,False,,,,,,,,
427,,pytorch,"&gt; [GitHub](https://github.com/carefree0910/carefree-learn)
&gt; [Documents](https://carefree0910.me/carefree-learn-doc)
&gt; [Introduction video](https://youtu.be/hMzLmwmdQ_k)

&gt; cflearn - a minimal Automatic Machine Learning (AutoML) solution for tabular datasets based on PyTorch


## Updates

### `Experiments`

In v0.1.1, `Experiments` is much more powerful and much easier to use:

```python
import cflearn
import numpy as np

from cfdata.tabular import *

def main():
    x, y = TabularDataset.iris().xy
    experiments = cflearn.Experiments()
    experiments.add_task(x, y, model=""fcnn"")
    experiments.add_task(x, y, model=""fcnn"")
    experiments.add_task(x, y, model=""tree_dnn"")
    experiments.add_task(x, y, model=""tree_dnn"")
    results = experiments.run_tasks(num_jobs=2)
    # {'fcnn': [Task(fcnn_0), Task(fcnn_1)], 'tree_dnn': [Task(tree_dnn_0), Task(tree_dnn_1)]}
    print(results)
    ms = {k: list(map(cflearn.load_task, v)) for k, v in results.items()}
    # {'fcnn': [FCNN(), FCNN()], 'tree_dnn': [TreeDNN(), TreeDNN()]}
    print(ms)
    # experiments could be saved &amp; loaded easily
    saving_folder = ""__temp__""
    experiments.save(saving_folder)
    loaded = cflearn.Experiments.load(saving_folder)
    ms_loaded = {k: list(map(cflearn.load_task, v)) for k, v in loaded.tasks.items()}
    # {'fcnn': [FCNN(), FCNN()], 'tree_dnn': [TreeDNN(), TreeDNN()]}
    print(ms_loaded)
    assert np.allclose(ms[""fcnn""][1].predict(x), ms_loaded[""fcnn""][1].predict(x))

if __name__ == '__main__':
    main()
```

We can see that `experiments.run_tasks` returns a bunch of `Task`s, which can be easily transfered to models through `cflearn.load_task`.

&gt; It is important to wrap the codes with `main()` on some platforms (e.g. Windows), because running codes in parallel will cause some issues if we don't do so. [Here](https://stackoverflow.com/questions/20222534/python-multiprocessing-on-windows-if-name-main)'s an explaination.

### `Benchmark`

In v0.1.1, `Benchmark` class is implemented for easier benchmark testing:

```python
import cflearn
import numpy as np

from cfdata.tabular import *

def main():
    x, y = TabularDataset.iris().xy
    benchmark = cflearn.Benchmark(
        ""foo"",
        TaskTypes.CLASSIFICATION,
        models=[""fcnn"", ""tree_dnn""]
    )
    benchmarks = {
        ""fcnn"": {""default"": {}, ""sgd"": {""optimizer"": ""sgd""}},
        ""tree_dnn"": {""default"": {}, ""adamw"": {""optimizer"": ""adamw""}}
    }
    msg1 = benchmark.k_fold(3, x, y, num_jobs=2, benchmarks=benchmarks).comparer.log_statistics()
    """"""
    ~~~  [ info ] Results
    ===============================================================================================================================
    |        metrics         |                       acc                        |                       auc                        |
    --------------------------------------------------------------------------------------------------------------------------------
    |                        |      mean      |      std       |     score      |      mean      |      std       |     score      |
    --------------------------------------------------------------------------------------------------------------------------------
    |    fcnn_foo_default    |    0.780000    | -- 0.032660 -- |    0.747340    |    0.914408    |    0.040008    |    0.874400    |
    --------------------------------------------------------------------------------------------------------------------------------
    |      fcnn_foo_sgd      |    0.113333    |    0.080554    |    0.032780    |    0.460903    |    0.061548    |    0.399355    |
    --------------------------------------------------------------------------------------------------------------------------------
    |   tree_dnn_foo_adamw   | -- 0.833333 -- |    0.077172    | -- 0.756161 -- | -- 0.944698 -- | -- 0.034248 -- | -- 0.910451 -- |
    --------------------------------------------------------------------------------------------------------------------------------
    |  tree_dnn_foo_default  |    0.706667    |    0.253684    |    0.452983    |    0.924830    |    0.060007    |    0.864824    |
    ================================================================================================================================
    """"""
    # save &amp; load
    saving_folder = ""__temp__""
    benchmark.save(saving_folder)
    loaded_benchmark, loaded_results = cflearn.Benchmark.load(saving_folder)
    msg2 = loaded_results.comparer.log_statistics()
    assert msg1 == msg2

if __name__ == '__main__':
    main()
```

### Misc

+ Integrated `trains`.
+ Integrated `Tracker` from `carefree-toolkit`.
+ Integrated native `amp` from PyTorch.
+ Implemented `FocalLoss`.
+ Implemented `cflearn.zoo`.

---

+ Introduced CI.
+ Fixed some bugs.
+ Simplified some APIs.
+ Optimized some default settings.

## What's next

I've already done some experiments on some benchmark datasets with `Benchmark` and already achieved satisfying performance. However, large scale benchmark testing is not done yet, limited by my lack of GPU cards🤣

So the next step is to do large scale benchmark testing and optimize `carefree-learn`'s performance, in a much more general way.

In the mean time, I'll do some research and implement some SOTA methods on tabular datasets (e.g. Deep Sparse Network, β-LASSO MLP, ...)

And, as always, bug fixing XD",t2_3itl62k7,False,,0,False,[Project] cflearn - v0.1.1 update!,[],r/pytorch,False,6,,0,,,False,t3_i1tf7b,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1596322323.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=""https://github.com/carefree0910/carefree-learn""&gt;GitHub&lt;/a&gt;
&lt;a href=""https://carefree0910.me/carefree-learn-doc""&gt;Documents&lt;/a&gt;
&lt;a href=""https://youtu.be/hMzLmwmdQ_k""&gt;Introduction video&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;cflearn - a minimal Automatic Machine Learning (AutoML) solution for tabular datasets based on PyTorch&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;Updates&lt;/h2&gt;

&lt;h3&gt;&lt;code&gt;Experiments&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;In v0.1.1, &lt;code&gt;Experiments&lt;/code&gt; is much more powerful and much easier to use:&lt;/p&gt;

&lt;p&gt;```python
import cflearn
import numpy as np&lt;/p&gt;

&lt;p&gt;from cfdata.tabular import *&lt;/p&gt;

&lt;p&gt;def main():
    x, y = TabularDataset.iris().xy
    experiments = cflearn.Experiments()
    experiments.add&lt;em&gt;task(x, y, model=&amp;quot;fcnn&amp;quot;)
    experiments.add_task(x, y, model=&amp;quot;fcnn&amp;quot;)
    experiments.add_task(x, y, model=&amp;quot;tree_dnn&amp;quot;)
    experiments.add_task(x, y, model=&amp;quot;tree_dnn&amp;quot;)
    results = experiments.run_tasks(num_jobs=2)
    # {&amp;#39;fcnn&amp;#39;: [Task(fcnn_0), Task(fcnn_1)], &amp;#39;tree_dnn&amp;#39;: [Task(tree_dnn_0), Task(tree_dnn_1)]}
    print(results)
    ms = {k: list(map(cflearn.load_task, v)) for k, v in results.items()}
    # {&amp;#39;fcnn&amp;#39;: [FCNN(), FCNN()], &amp;#39;tree_dnn&amp;#39;: [TreeDNN(), TreeDNN()]}
    print(ms)
    # experiments could be saved &amp;amp; loaded easily
    saving_folder = &amp;quot;&lt;/em&gt;&lt;em&gt;temp&lt;/em&gt;_&amp;quot;
    experiments.save(saving_folder)
    loaded = cflearn.Experiments.load(saving_folder)
    ms_loaded = {k: list(map(cflearn.load_task, v)) for k, v in loaded.tasks.items()}
    # {&amp;#39;fcnn&amp;#39;: [FCNN(), FCNN()], &amp;#39;tree_dnn&amp;#39;: [TreeDNN(), TreeDNN()]}
    print(ms_loaded)
    assert np.allclose(ms[&amp;quot;fcnn&amp;quot;][1].predict(x), ms_loaded[&amp;quot;fcnn&amp;quot;][1].predict(x))&lt;/p&gt;

&lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;#39;&lt;strong&gt;main&lt;/strong&gt;&amp;#39;:
    main()
```&lt;/p&gt;

&lt;p&gt;We can see that &lt;code&gt;experiments.run_tasks&lt;/code&gt; returns a bunch of &lt;code&gt;Task&lt;/code&gt;s, which can be easily transfered to models through &lt;code&gt;cflearn.load_task&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It is important to wrap the codes with &lt;code&gt;main()&lt;/code&gt; on some platforms (e.g. Windows), because running codes in parallel will cause some issues if we don&amp;#39;t do so. &lt;a href=""https://stackoverflow.com/questions/20222534/python-multiprocessing-on-windows-if-name-main""&gt;Here&lt;/a&gt;&amp;#39;s an explaination.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3&gt;&lt;code&gt;Benchmark&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;In v0.1.1, &lt;code&gt;Benchmark&lt;/code&gt; class is implemented for easier benchmark testing:&lt;/p&gt;

&lt;p&gt;```python
import cflearn
import numpy as np&lt;/p&gt;

&lt;p&gt;from cfdata.tabular import *&lt;/p&gt;

&lt;p&gt;def main():
    x, y = TabularDataset.iris().xy
    benchmark = cflearn.Benchmark(
        &amp;quot;foo&amp;quot;,
        TaskTypes.CLASSIFICATION,
        models=[&amp;quot;fcnn&amp;quot;, &amp;quot;tree&lt;em&gt;dnn&amp;quot;]
    )
    benchmarks = {
        &amp;quot;fcnn&amp;quot;: {&amp;quot;default&amp;quot;: {}, &amp;quot;sgd&amp;quot;: {&amp;quot;optimizer&amp;quot;: &amp;quot;sgd&amp;quot;}},
        &amp;quot;tree_dnn&amp;quot;: {&amp;quot;default&amp;quot;: {}, &amp;quot;adamw&amp;quot;: {&amp;quot;optimizer&amp;quot;: &amp;quot;adamw&amp;quot;}}
    }
    msg1 = benchmark.k_fold(3, x, y, num_jobs=2, benchmarks=benchmarks).comparer.log_statistics()
    &amp;quot;&amp;quot;&amp;quot;
    ~~~  [ info ] Results
    ===============================================================================================================================
    |        metrics         |                       acc                        |                       auc                        |
    --------------------------------------------------------------------------------------------------------------------------------
    |                        |      mean      |      std       |     score      |      mean      |      std       |     score      |
    --------------------------------------------------------------------------------------------------------------------------------
    |    fcnn_foo_default    |    0.780000    | -- 0.032660 -- |    0.747340    |    0.914408    |    0.040008    |    0.874400    |
    --------------------------------------------------------------------------------------------------------------------------------
    |      fcnn_foo_sgd      |    0.113333    |    0.080554    |    0.032780    |    0.460903    |    0.061548    |    0.399355    |
    --------------------------------------------------------------------------------------------------------------------------------
    |   tree_dnn_foo_adamw   | -- 0.833333 -- |    0.077172    | -- 0.756161 -- | -- 0.944698 -- | -- 0.034248 -- | -- 0.910451 -- |
    --------------------------------------------------------------------------------------------------------------------------------
    |  tree_dnn_foo_default  |    0.706667    |    0.253684    |    0.452983    |    0.924830    |    0.060007    |    0.864824    |
    ================================================================================================================================
    &amp;quot;&amp;quot;&amp;quot;
    # save &amp;amp; load
    saving_folder = &amp;quot;&lt;/em&gt;&lt;em&gt;temp&lt;/em&gt;_&amp;quot;
    benchmark.save(saving_folder)
    loaded_benchmark, loaded_results = cflearn.Benchmark.load(saving_folder)
    msg2 = loaded_results.comparer.log_statistics()
    assert msg1 == msg2&lt;/p&gt;

&lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;#39;&lt;strong&gt;main&lt;/strong&gt;&amp;#39;:
    main()
```&lt;/p&gt;

&lt;h3&gt;Misc&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Integrated &lt;code&gt;trains&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Integrated &lt;code&gt;Tracker&lt;/code&gt; from &lt;code&gt;carefree-toolkit&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Integrated native &lt;code&gt;amp&lt;/code&gt; from PyTorch.&lt;/li&gt;
&lt;li&gt;Implemented &lt;code&gt;FocalLoss&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Implemented &lt;code&gt;cflearn.zoo&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr/&gt;

&lt;ul&gt;
&lt;li&gt;Introduced CI.&lt;/li&gt;
&lt;li&gt;Fixed some bugs.&lt;/li&gt;
&lt;li&gt;Simplified some APIs.&lt;/li&gt;
&lt;li&gt;Optimized some default settings.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;What&amp;#39;s next&lt;/h2&gt;

&lt;p&gt;I&amp;#39;ve already done some experiments on some benchmark datasets with &lt;code&gt;Benchmark&lt;/code&gt; and already achieved satisfying performance. However, large scale benchmark testing is not done yet, limited by my lack of GPU cards🤣&lt;/p&gt;

&lt;p&gt;So the next step is to do large scale benchmark testing and optimize &lt;code&gt;carefree-learn&lt;/code&gt;&amp;#39;s performance, in a much more general way.&lt;/p&gt;

&lt;p&gt;In the mean time, I&amp;#39;ll do some research and implement some SOTA methods on tabular datasets (e.g. Deep Sparse Network, β-LASSO MLP, ...)&lt;/p&gt;

&lt;p&gt;And, as always, bug fixing XD&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vHdaaEHbWw5XE0iQAhf-CsRFGsNqPfUW7FI26TfLKaU.jpg?auto=webp&amp;s=d3049e77d02b53ba40d585d23e6f8428a01818b3', 'width': 226, 'height': 226}, 'resolutions': [{'url': 'https://external-preview.redd.it/vHdaaEHbWw5XE0iQAhf-CsRFGsNqPfUW7FI26TfLKaU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d362eef75f2ea87b58c27dfcf4a3820abb9bdb7a', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/vHdaaEHbWw5XE0iQAhf-CsRFGsNqPfUW7FI26TfLKaU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e7466f00203466b18dac176d8b8a280896a4ca2', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'EqN0XJr07Jx8fczb5MjHgzw_SrjHP_xdfksH_u8ebEA'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i1tf7b,True,,carefree0910,,1,True,all_ads,False,[],False,,/r/pytorch/comments/i1tf7b/project_cflearn_v011_update/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i1tf7b/project_cflearn_v011_update/,7135,1596293523.0,0,,False,,,,,,,,
428,,pytorch," I am trying to bulid MNIST Digit classifier using simple ANN .

But my CrossEntropyLoss is remaining constant at Log(10) i.e 2.30

code=

`class NET(nn.Module):`

`def __init__(self):`

`super().__init__()`

`self.model=nn.Sequential(`

`nn.Linear(784, 128),`

`nn.ReLU(),`

`nn.Linear(128, 256),`

`nn.ReLU(),`

`nn.Linear(256, 512),`

`nn.ReLU(),`

`nn.Linear(512, 10) )`

`def forward(self,x):`

`return self.model(x)`

`loss_fn=nn.CrossEntropyLoss()`

`opt=optim.Adam(net.parameters() , lr=0.2)`

`loss_epoch=[]`

`epochs=5`

`for i in range(epochs):`

`opt.zero_grad()`

`output=net(x_train)`

`loss=loss_fn(output, y_train)`

`loss.backward()`

`opt.step()`

`loss_epoch.append(loss.item())`

`print(""Epochs: {}/{} , Loss:{}"".format(i,epochs,loss))`

Output=

`Epochs: 0/5, Loss:2.301159143447876`

`Epochs: 1/5, Loss:2.301161289215088`

`Epochs: 2/5, Loss:2.3011562824249268`

`Epochs: 3/5, Loss:2.3011701107025146`

`Epochs: 4/5, Loss:2.3011698722839355`

 Rather i increase or decrease epochs and Learning Rate it remains constant at 2.30",t2_757mta07,False,,0,False,Pytorch Constant Loss D,[],r/pytorch,False,6,,0,,,False,t3_i189h2,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1596233798.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to bulid MNIST Digit classifier using simple ANN .&lt;/p&gt;

&lt;p&gt;But my CrossEntropyLoss is remaining constant at Log(10) i.e 2.30&lt;/p&gt;

&lt;p&gt;code=&lt;/p&gt;

&lt;p&gt;&lt;code&gt;class NET(nn.Module):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def __init__(self):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;super().__init__()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.model=nn.Sequential(&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nn.Linear(784, 128),&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nn.ReLU(),&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nn.Linear(128, 256),&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nn.ReLU(),&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nn.Linear(256, 512),&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nn.ReLU(),&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nn.Linear(512, 10) )&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def forward(self,x):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;return self.model(x)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;loss_fn=nn.CrossEntropyLoss()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;opt=optim.Adam(net.parameters() , lr=0.2)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;loss_epoch=[]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;epochs=5&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for i in range(epochs):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;opt.zero_grad()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;output=net(x_train)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;loss=loss_fn(output, y_train)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;loss.backward()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;opt.step()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;loss_epoch.append(loss.item())&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(&amp;quot;Epochs: {}/{} , Loss:{}&amp;quot;.format(i,epochs,loss))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Output=&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Epochs: 0/5, Loss:2.301159143447876&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Epochs: 1/5, Loss:2.301161289215088&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Epochs: 2/5, Loss:2.3011562824249268&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Epochs: 3/5, Loss:2.3011701107025146&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Epochs: 4/5, Loss:2.3011698722839355&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Rather i increase or decrease epochs and Learning Rate it remains constant at 2.30&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i189h2,True,,Heisenberg_082001,,10,True,all_ads,False,[],False,,/r/pytorch/comments/i189h2/pytorch_constant_loss_d/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i189h2/pytorch_constant_loss_d/,7135,1596204998.0,0,,False,,,,,,,,
429,,pytorch,[https://analyticsindiamag.com/pytorch-1-6-released-microsoft-to-take-care-of-the-windows-version-of-pytorch/](https://analyticsindiamag.com/pytorch-1-6-released-microsoft-to-take-care-of-the-windows-version-of-pytorch/),t2_40d0zt4s,False,,0,False,"PyTorch 1.6 Released, Microsoft To Take Care Of The Windows Version of PyTorch",[],r/pytorch,False,6,,0,,,False,t3_i0sb95,False,dark,0.93,,public,13,0,{},,,False,[],,False,False,,{},,False,13,,False,self,False,,[],{},self,,True,,1596165666.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://analyticsindiamag.com/pytorch-1-6-released-microsoft-to-take-care-of-the-windows-version-of-pytorch/""&gt;https://analyticsindiamag.com/pytorch-1-6-released-microsoft-to-take-care-of-the-windows-version-of-pytorch/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/gO3N2Fh9gxIKMGlX_CFtqDw16RvmO_lugNU6f2FSqnU.jpg?auto=webp&amp;s=747bc0a7d343cf7df2d90eb88f368849b2921fc4', 'width': 1366, 'height': 768}, 'resolutions': [{'url': 'https://external-preview.redd.it/gO3N2Fh9gxIKMGlX_CFtqDw16RvmO_lugNU6f2FSqnU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2338e64a3ccb1df5bf29a6fed5fcf1a65ae0237', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/gO3N2Fh9gxIKMGlX_CFtqDw16RvmO_lugNU6f2FSqnU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e6ddcd1ffab5fde22f52842231b3d138ea7ffaf', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/gO3N2Fh9gxIKMGlX_CFtqDw16RvmO_lugNU6f2FSqnU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b45052df24927f04cca82aa45f1df3645592fac9', 'width': 320, 'height': 179}, {'url': 'https://external-preview.redd.it/gO3N2Fh9gxIKMGlX_CFtqDw16RvmO_lugNU6f2FSqnU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e21bc030e09c08e39e1b92506b37b34efcb430da', 'width': 640, 'height': 359}, {'url': 'https://external-preview.redd.it/gO3N2Fh9gxIKMGlX_CFtqDw16RvmO_lugNU6f2FSqnU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6265ac498d2cb063bdf3bf93ef5966192e3a295d', 'width': 960, 'height': 539}, {'url': 'https://external-preview.redd.it/gO3N2Fh9gxIKMGlX_CFtqDw16RvmO_lugNU6f2FSqnU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=711f0295372c759a57009a4d1ae22039e6a63142', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'TEH95YD-LuwMOm8PFWXEM2jQHSKlxjzO_3s-cZYf31Q'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i0sb95,True,,analyticsindiam,,3,True,all_ads,False,[],False,,/r/pytorch/comments/i0sb95/pytorch_16_released_microsoft_to_take_care_of_the/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i0sb95/pytorch_16_released_microsoft_to_take_care_of_the/,7135,1596136866.0,0,,False,,,,,,,,
430,,pytorch,,t2_44mbtmjy,False,,0,False,State of the art in instance segmentation: (Instance segmentation aims to classify each pixel in an image into an object category),[],r/pytorch,False,6,,0,55.0,,False,t3_i0uw1m,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://a.thumbs.redditmedia.com/jc81B0IW3ve9UhvTye21qAenzFroAG_05-iJJP9Btg8.jpg,False,,[],{},link,,False,,1596174183.0,text,6,,,text,self.LatestInML,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/aFLHKJP0L2IBVDbbyeFgr5jSRoOso2gcR0e1YJjyUqo.jpg?auto=webp&amp;s=c0a94a916b2b54e45e2154bc162e356914ef0526', 'width': 962, 'height': 378}, 'resolutions': [{'url': 'https://external-preview.redd.it/aFLHKJP0L2IBVDbbyeFgr5jSRoOso2gcR0e1YJjyUqo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a3187adeadf20ac289d83cb89ae8d4c8e7b04355', 'width': 108, 'height': 42}, {'url': 'https://external-preview.redd.it/aFLHKJP0L2IBVDbbyeFgr5jSRoOso2gcR0e1YJjyUqo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2a6a9598e086b48f72805d18ea4fab1c3f690285', 'width': 216, 'height': 84}, {'url': 'https://external-preview.redd.it/aFLHKJP0L2IBVDbbyeFgr5jSRoOso2gcR0e1YJjyUqo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9831226ff17a4fdf09f643dd44a28967a6f4ab1d', 'width': 320, 'height': 125}, {'url': 'https://external-preview.redd.it/aFLHKJP0L2IBVDbbyeFgr5jSRoOso2gcR0e1YJjyUqo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bae028cfc2a538c0257fbb90e47c8238cd5e4e34', 'width': 640, 'height': 251}, {'url': 'https://external-preview.redd.it/aFLHKJP0L2IBVDbbyeFgr5jSRoOso2gcR0e1YJjyUqo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5c5b3286d1480951c8a9b967d10f599b6ed41ba7', 'width': 960, 'height': 377}], 'variants': {}, 'id': 'HyC8EcO81pIffkEh_bQNmxN4GiNxC0E_59Qg8ATrGUc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i0uw1m,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/i0uw1m/state_of_the_art_in_instance_segmentation/,all_ads,False,/r/LatestInML/comments/i0utf4/state_of_the_art_in_instance_segmentation/,7135,1596145383.0,0,,False,/r/LatestInML/comments/i0utf4/state_of_the_art_in_instance_segmentation/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and API/expert/code requests: [click here](https://www.catalyzex.com/paper/arxiv:2007.14772)\n\nhttps://preview.redd.it/y7mba0wcd2e51.png?width=1952&amp;format=png&amp;auto=webp&amp;s=1ce4376048a16f678958e916de6a8be6dde50f02\n\nResearchers propose a fast single-stage instance segmentation method, called SipMask, that preserves instance-specific spatial information by separating mask prediction of an instance to different sub-regions of a detected bounding-box.\n\nGet the free ML code finder browser extension  \nChrome [https://bit.ly/code\\_finder\\_chrome](https://bit.ly/code_finder_chrome)  \nFirefox [https://bit.ly/code\\_finder\\_firefox](https://bit.ly/code_finder_firefox)', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'State of the art in instance segmentation: (Instance segmentation aims to classify each pixel in an image into an object category)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 55, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'y7mba0wcd2e51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 42, 'x': 108, 'u': 'https://preview.redd.it/y7mba0wcd2e51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=300d112183ecfc9eec46d537a1a6f51e72899f49'}, {'y': 85, 'x': 216, 'u': 'https://preview.redd.it/y7mba0wcd2e51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fee47d9ff02d8a75710e580cbffec68b216dea1'}, {'y': 126, 'x': 320, 'u': 'https://preview.redd.it/y7mba0wcd2e51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=48c11fef148d0777fe936b339ad0424812ae989b'}, {'y': 253, 'x': 640, 'u': 'https://preview.redd.it/y7mba0wcd2e51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc46a59952848c2f978f98b3de966977c4399430'}, {'y': 380, 'x': 960, 'u': 'https://preview.redd.it/y7mba0wcd2e51.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=498d1f592f31ece7122c092fa68909483d20008a'}, {'y': 428, 'x': 1080, 'u': 'https://preview.redd.it/y7mba0wcd2e51.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1d3ceff91322ab9211318904743a7ed9500e2672'}], 's': {'y': 774, 'x': 1952, 'u': 'https://preview.redd.it/y7mba0wcd2e51.png?width=1952&amp;format=png&amp;auto=webp&amp;s=1ce4376048a16f678958e916de6a8be6dde50f02'}, 'id': 'y7mba0wcd2e51'}}, 'name': 't3_i0utf4', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 24, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 24, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/jc81B0IW3ve9UhvTye21qAenzFroAG_05-iJJP9Btg8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1596173938.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and API/expert/code requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2007.14772""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/y7mba0wcd2e51.png?width=1952&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ce4376048a16f678958e916de6a8be6dde50f02""&gt;https://preview.redd.it/y7mba0wcd2e51.png?width=1952&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ce4376048a16f678958e916de6a8be6dde50f02&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Researchers propose a fast single-stage instance segmentation method, called SipMask, that preserves instance-specific spatial information by separating mask prediction of an instance to different sub-regions of a detected bounding-box.&lt;/p&gt;\n\n&lt;p&gt;Get the free ML code finder browser extension&lt;br/&gt;\nChrome &lt;a href=""https://bit.ly/code_finder_chrome""&gt;https://bit.ly/code_finder_chrome&lt;/a&gt;&lt;br/&gt;\nFirefox &lt;a href=""https://bit.ly/code_finder_firefox""&gt;https://bit.ly/code_finder_firefox&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/aFLHKJP0L2IBVDbbyeFgr5jSRoOso2gcR0e1YJjyUqo.jpg?auto=webp&amp;s=c0a94a916b2b54e45e2154bc162e356914ef0526', 'width': 962, 'height': 378}, 'resolutions': [{'url': 'https://external-preview.redd.it/aFLHKJP0L2IBVDbbyeFgr5jSRoOso2gcR0e1YJjyUqo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a3187adeadf20ac289d83cb89ae8d4c8e7b04355', 'width': 108, 'height': 42}, {'url': 'https://external-preview.redd.it/aFLHKJP0L2IBVDbbyeFgr5jSRoOso2gcR0e1YJjyUqo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2a6a9598e086b48f72805d18ea4fab1c3f690285', 'width': 216, 'height': 84}, {'url': 'https://external-preview.redd.it/aFLHKJP0L2IBVDbbyeFgr5jSRoOso2gcR0e1YJjyUqo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9831226ff17a4fdf09f643dd44a28967a6f4ab1d', 'width': 320, 'height': 125}, {'url': 'https://external-preview.redd.it/aFLHKJP0L2IBVDbbyeFgr5jSRoOso2gcR0e1YJjyUqo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bae028cfc2a538c0257fbb90e47c8238cd5e4e34', 'width': 640, 'height': 251}, {'url': 'https://external-preview.redd.it/aFLHKJP0L2IBVDbbyeFgr5jSRoOso2gcR0e1YJjyUqo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5c5b3286d1480951c8a9b967d10f599b6ed41ba7', 'width': 960, 'height': 377}], 'variants': {}, 'id': 'HyC8EcO81pIffkEh_bQNmxN4GiNxC0E_59Qg8ATrGUc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'i0utf4', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/i0utf4/state_of_the_art_in_instance_segmentation/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/i0utf4/state_of_the_art_in_instance_segmentation/', 'subreddit_subscribers': 6676, 'created_utc': 1596145138.0, 'num_crossposts': 11, 'media': None, 'is_video': False}]",t3_i0utf4,,,,,
431,,pytorch,"I am trying to implement [detnet backbone](https://arxiv.org/pdf/1804.06215.pdf#page=7) using code from a book, but I am bit confused with how the convolution blocks are arranged for each bottleneck…

The random number generator output tensor (input) is of size (1, 1024), not sure what the parameters value 14 are for.

I am also confused with how the input is used to interface with bottleneck\_b

&amp;#x200B;

https://preview.redd.it/vntnvuvsk0e51.png?width=977&amp;format=png&amp;auto=webp&amp;s=7674d31782fbaa9e1f834db9d9a3c228f6b75279

**detnet.py**

    import torch
    from detnet_bottleneck import DetBottleneck
    
    # 完成一个Stage 5,即B-A-A的结构,Stage 4输出通道数为1024
    bottleneck_b = DetBottleneck(1024, 256, 1, True).cuda()
    
    bottleneck_a1 = DetBottleneck(256, 256).cuda()
    bottleneck_a2 = DetBottleneck(256, 256).cuda()
    
    input = torch.randn(1, 1024, 14, 14).cuda()
    
    # 将input作为某一层的特征图,依次传入Bottleneck B、A1与A2三个模块
    output1 = bottleneck_b(input)
    output2 = bottleneck_a1(output1)
    output3 = bottleneck_a2(output2)
    
    print(output1.shape)
    print(output2.shape)
    print(output3.shape)

**detnet\_bottleneck.py**

    from torch import nn
    class DetBottleneck(nn.Module):
        # 初始化时extra为False时为Bottleneck A,为True时则为Bottleneck B
        def __init__(self, inplanes, planes, stride=1, extra=False):
            super(DetBottleneck, self).__init__()
            # 构建连续3个卷积层的Bottleneck
            self.bottleneck = nn.Sequential
            (
                nn.Conv2d(inplanes, planes, 1, bias=False),
                nn.BatchNorm2d(planes),
                nn.ReLU(inplace=True),
                nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=2, dilation=2, bias=False),
                nn.BatchNorm2d(planes),
                nn.ReLU(inplace=True),
                nn.Conv2d(planes, planes, 1, bias=False),
                nn.BatchNorm2d(planes),
            )
    
            self.relu = nn.ReLU(inplace=True)
            self.extra = extra
            # Bottleneck B的1×1卷积
            if self.extra:
                self.extra_conv = nn.Sequential(
                    nn.Conv2d(inplanes, planes, 1, bias=False),
                    nn.BatchNorm2d(planes)
                )
    
        def forward(self, x):
            # 对于Bottleneck B来讲,需要对恒等映射增加卷积处理,与ResNet类似
            if self.extra:
                identity = self.extra_conv(x)
            else:
                identity = x
            out = self.bottleneck(x)
            out += identity
            out = self.relu(out)
            return out",t2_bpftl,False,,0,False,Detnet implementation issues,[],r/pytorch,False,6,,0,127.0,,False,t3_i0o6hv,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/_qDLfg50-OiAbZeXDRrPOfvHG8-fb3jMubXlSJgTLRA.jpg,False,,[],{},,,True,,1596152200.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to implement &lt;a href=""https://arxiv.org/pdf/1804.06215.pdf#page=7""&gt;detnet backbone&lt;/a&gt; using code from a book, but I am bit confused with how the convolution blocks are arranged for each bottleneck…&lt;/p&gt;

&lt;p&gt;The random number generator output tensor (input) is of size (1, 1024), not sure what the parameters value 14 are for.&lt;/p&gt;

&lt;p&gt;I am also confused with how the input is used to interface with bottleneck_b&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/vntnvuvsk0e51.png?width=977&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7674d31782fbaa9e1f834db9d9a3c228f6b75279""&gt;https://preview.redd.it/vntnvuvsk0e51.png?width=977&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7674d31782fbaa9e1f834db9d9a3c228f6b75279&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;detnet.py&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import torch
from detnet_bottleneck import DetBottleneck

# 完成一个Stage 5,即B-A-A的结构,Stage 4输出通道数为1024
bottleneck_b = DetBottleneck(1024, 256, 1, True).cuda()

bottleneck_a1 = DetBottleneck(256, 256).cuda()
bottleneck_a2 = DetBottleneck(256, 256).cuda()

input = torch.randn(1, 1024, 14, 14).cuda()

# 将input作为某一层的特征图,依次传入Bottleneck B、A1与A2三个模块
output1 = bottleneck_b(input)
output2 = bottleneck_a1(output1)
output3 = bottleneck_a2(output2)

print(output1.shape)
print(output2.shape)
print(output3.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;detnet_bottleneck.py&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from torch import nn
class DetBottleneck(nn.Module):
    # 初始化时extra为False时为Bottleneck A,为True时则为Bottleneck B
    def __init__(self, inplanes, planes, stride=1, extra=False):
        super(DetBottleneck, self).__init__()
        # 构建连续3个卷积层的Bottleneck
        self.bottleneck = nn.Sequential
        (
            nn.Conv2d(inplanes, planes, 1, bias=False),
            nn.BatchNorm2d(planes),
            nn.ReLU(inplace=True),
            nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=2, dilation=2, bias=False),
            nn.BatchNorm2d(planes),
            nn.ReLU(inplace=True),
            nn.Conv2d(planes, planes, 1, bias=False),
            nn.BatchNorm2d(planes),
        )

        self.relu = nn.ReLU(inplace=True)
        self.extra = extra
        # Bottleneck B的1×1卷积
        if self.extra:
            self.extra_conv = nn.Sequential(
                nn.Conv2d(inplanes, planes, 1, bias=False),
                nn.BatchNorm2d(planes)
            )

    def forward(self, x):
        # 对于Bottleneck B来讲,需要对恒等映射增加卷积处理,与ResNet类似
        if self.extra:
            identity = self.extra_conv(x)
        else:
            identity = x
        out = self.bottleneck(x)
        out += identity
        out = self.relu(out)
        return out
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,i0o6hv,True,,promach,,1,True,all_ads,False,[],False,,/r/pytorch/comments/i0o6hv/detnet_implementation_issues/,all_ads,False,https://www.reddit.com/r/pytorch/comments/i0o6hv/detnet_implementation_issues/,7135,1596123400.0,0,,False,,,,"{'vntnvuvsk0e51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 98, 'x': 108, 'u': 'https://preview.redd.it/vntnvuvsk0e51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad2fd387f7bf5f33cfd8d9eef2339f3909b3a1be'}, {'y': 196, 'x': 216, 'u': 'https://preview.redd.it/vntnvuvsk0e51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0c06009791467887f89c443bba36d61fde51c24'}, {'y': 290, 'x': 320, 'u': 'https://preview.redd.it/vntnvuvsk0e51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b1244c7a59b48b03959cb15ff31c2a11ddeabf0a'}, {'y': 581, 'x': 640, 'u': 'https://preview.redd.it/vntnvuvsk0e51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=631697047685eaf0b9e5f6aa06f26ebcf6a62731'}, {'y': 872, 'x': 960, 'u': 'https://preview.redd.it/vntnvuvsk0e51.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=379ea1a4dc0df4d9f59993dee92d49ae3882d42b'}], 's': {'y': 888, 'x': 977, 'u': 'https://preview.redd.it/vntnvuvsk0e51.png?width=977&amp;format=png&amp;auto=webp&amp;s=7674d31782fbaa9e1f834db9d9a3c228f6b75279'}, 'id': 'vntnvuvsk0e51'}}",,,,
432,,pytorch,,t2_44mbtmjy,False,,0,False,Latest from Carnegie Mellon and Facebook Researchers: 3D Human Shape and Pose from a Single Low-Resolution Image with Self-Supervised Learning,[],r/pytorch,False,6,,0,84.0,,False,t3_hzu39j,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://a.thumbs.redditmedia.com/pH0DkaLNE52pWIBpxVso6PaxejeZ3g6DwoT4wVRAGd0.jpg,False,,[],{},link,,False,,1596025483.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/A3KJj-gIep7ptEdIcl45V6vX5o88_eWC1nb9iBNa4oY.jpg?auto=webp&amp;s=dad3387c0eab86fc0cf5aebda2bfb152631fd044', 'width': 940, 'height': 346}, 'resolutions': [{'url': 'https://external-preview.redd.it/A3KJj-gIep7ptEdIcl45V6vX5o88_eWC1nb9iBNa4oY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=657baceb60f6a5666501bb089bd0cbd46f2261ba', 'width': 108, 'height': 39}, {'url': 'https://external-preview.redd.it/A3KJj-gIep7ptEdIcl45V6vX5o88_eWC1nb9iBNa4oY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b841578116938a5d30e05ee4b5e6da3001f09746', 'width': 216, 'height': 79}, {'url': 'https://external-preview.redd.it/A3KJj-gIep7ptEdIcl45V6vX5o88_eWC1nb9iBNa4oY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27c668c54d3efe67aa85f28feed598df10de9a09', 'width': 320, 'height': 117}, {'url': 'https://external-preview.redd.it/A3KJj-gIep7ptEdIcl45V6vX5o88_eWC1nb9iBNa4oY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=afef64971c5082f3de28f70adbd8c4193eff1c72', 'width': 640, 'height': 235}], 'variants': {}, 'id': 'dbDz1zwiueA0eJpQAsdlhjReNTJrIKnZiJG-FZkC-OQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hzu39j,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hzu39j/latest_from_carnegie_mellon_and_facebook/,all_ads,False,/r/LatestInML/comments/hztvin/latest_from_carnegie_mellon_and_facebook/,7135,1595996683.0,0,,False,/r/LatestInML/comments/hztvin/latest_from_carnegie_mellon_and_facebook/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/expert/API requests: [click here](https://www.catalyzex.com/paper/arxiv:2007.13666)\n\nhttps://preview.redd.it/gxgv7qyq0qd51.png?width=1280&amp;format=png&amp;auto=webp&amp;s=919d1ab42f3dabbde41a010cfc9d833a76504ced\n\nThe proposed network is able to learn the 3D body shape and pose across different resolutions with a single model. The self-supervision loss encourages scale-consistency of the output, and the contrastive learning scheme enforces scale-consistency of the deep features.\n\nGet the free ML code finder browser extension:  \nChrome [https://bit.ly/code\\_finder\\_chrome](https://bit.ly/code_finder_chrome)  \nFirefox [https://bit.ly/code\\_finder\\_firefox](https://bit.ly/code_finder_firefox)', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Latest from Carnegie Mellon and Facebook Researchers: 3D Human Shape and Pose from a Single Low-Resolution Image with Self-Supervised Learning', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 84, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'gxgv7qyq0qd51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 65, 'x': 108, 'u': 'https://preview.redd.it/gxgv7qyq0qd51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a9816dca4a9441afe48572491da7d84c3955a19'}, {'y': 130, 'x': 216, 'u': 'https://preview.redd.it/gxgv7qyq0qd51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9e165900d480d5a1f4863122b919f1ef01518983'}, {'y': 194, 'x': 320, 'u': 'https://preview.redd.it/gxgv7qyq0qd51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cccd3bd072d4e4d4cf70ba0e4924c7e60b15e9c7'}, {'y': 388, 'x': 640, 'u': 'https://preview.redd.it/gxgv7qyq0qd51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=89436d065a83549c7c33fc1819702a5790df71fb'}, {'y': 582, 'x': 960, 'u': 'https://preview.redd.it/gxgv7qyq0qd51.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8fb2aab88044788a6d6df3b9cc0233fcfd8b51ed'}, {'y': 654, 'x': 1080, 'u': 'https://preview.redd.it/gxgv7qyq0qd51.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc1392abcb5429fdc861c42f1e4b548dd92fa95a'}], 's': {'y': 776, 'x': 1280, 'u': 'https://preview.redd.it/gxgv7qyq0qd51.png?width=1280&amp;format=png&amp;auto=webp&amp;s=919d1ab42f3dabbde41a010cfc9d833a76504ced'}, 'id': 'gxgv7qyq0qd51'}}, 'name': 't3_hztvin', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 18, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 18, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/pH0DkaLNE52pWIBpxVso6PaxejeZ3g6DwoT4wVRAGd0.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1596024564.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/expert/API requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2007.13666""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/gxgv7qyq0qd51.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=919d1ab42f3dabbde41a010cfc9d833a76504ced""&gt;https://preview.redd.it/gxgv7qyq0qd51.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=919d1ab42f3dabbde41a010cfc9d833a76504ced&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The proposed network is able to learn the 3D body shape and pose across different resolutions with a single model. The self-supervision loss encourages scale-consistency of the output, and the contrastive learning scheme enforces scale-consistency of the deep features.&lt;/p&gt;\n\n&lt;p&gt;Get the free ML code finder browser extension:&lt;br/&gt;\nChrome &lt;a href=""https://bit.ly/code_finder_chrome""&gt;https://bit.ly/code_finder_chrome&lt;/a&gt;&lt;br/&gt;\nFirefox &lt;a href=""https://bit.ly/code_finder_firefox""&gt;https://bit.ly/code_finder_firefox&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/A3KJj-gIep7ptEdIcl45V6vX5o88_eWC1nb9iBNa4oY.jpg?auto=webp&amp;s=dad3387c0eab86fc0cf5aebda2bfb152631fd044', 'width': 940, 'height': 346}, 'resolutions': [{'url': 'https://external-preview.redd.it/A3KJj-gIep7ptEdIcl45V6vX5o88_eWC1nb9iBNa4oY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=657baceb60f6a5666501bb089bd0cbd46f2261ba', 'width': 108, 'height': 39}, {'url': 'https://external-preview.redd.it/A3KJj-gIep7ptEdIcl45V6vX5o88_eWC1nb9iBNa4oY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b841578116938a5d30e05ee4b5e6da3001f09746', 'width': 216, 'height': 79}, {'url': 'https://external-preview.redd.it/A3KJj-gIep7ptEdIcl45V6vX5o88_eWC1nb9iBNa4oY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27c668c54d3efe67aa85f28feed598df10de9a09', 'width': 320, 'height': 117}, {'url': 'https://external-preview.redd.it/A3KJj-gIep7ptEdIcl45V6vX5o88_eWC1nb9iBNa4oY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=afef64971c5082f3de28f70adbd8c4193eff1c72', 'width': 640, 'height': 235}], 'variants': {}, 'id': 'dbDz1zwiueA0eJpQAsdlhjReNTJrIKnZiJG-FZkC-OQ'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'hztvin', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/hztvin/latest_from_carnegie_mellon_and_facebook/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/hztvin/latest_from_carnegie_mellon_and_facebook/', 'subreddit_subscribers': 6676, 'created_utc': 1595995764.0, 'num_crossposts': 13, 'media': None, 'is_video': False}]",t3_hztvin,,,,,
433,,pytorch," I am using a pre-trained MobileNetV2 model on a custom dataset. I configured the outputs from 1000 to 3 since that is the number of classes I am working with. Class A is 8000 images Class B is 5000 images and class C is 500 images. I took 100 of each out and use those as the validation set and the rest as the training set. I ran the experiment for 90 epochs with the learning rate being adjusted by 0.1 every 30 epochs and these hyperparameters:

learning rate = 0.001  
weight decay = 4e-5  
momentum = 0.9  
batch size = 64  
optimizer = SGD  
dropout = 0.2

after completing my experiment I am noticing that class A and B have accuracies between 80% and 97% in the last few runs and Class C is in the 70 percentile range.

I understand this because of how small the data sample is for class C compared to A and B, and I am looking for ways to run around that since even if I add more data it still would be less than half the size of the other two classes. This is my data augmentation technique:

     train_transform = transforms.Compose([
                transforms.RandomResizedCrop(224,scale = (0.2,1.0)),
                #transforms.RandomRotation(15),      # rotate +/- 10 degrees
                transforms.RandomHorizontalFlip(),  # reverse 50% of images        
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406],
                                     [0.229, 0.224, 0.225])
            ])
    
        test_transform = transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406],
                                     [0.229, 0.224, 0.225])
            ])

 Cross-validation is the only technique I found while searching this problem that I am not using and using cross-validation in Pytroch is proving rather tricky for me. One technique I read about in a paper is where the author claimed the augmented their class data to be 6,000 images each, but they didn’t say how they did this. Are there anyways I can increase the detection accuracy of class C with limited samples available and by association increase the overall accuracy of my model from 84% to 95+%?",t2_48egj5xw,False,,0,False,How to improve model performance with limited samples?,[],r/pytorch,False,6,,0,,,False,t3_hzrxtn,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1596016732.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am using a pre-trained MobileNetV2 model on a custom dataset. I configured the outputs from 1000 to 3 since that is the number of classes I am working with. Class A is 8000 images Class B is 5000 images and class C is 500 images. I took 100 of each out and use those as the validation set and the rest as the training set. I ran the experiment for 90 epochs with the learning rate being adjusted by 0.1 every 30 epochs and these hyperparameters:&lt;/p&gt;

&lt;p&gt;learning rate = 0.001&lt;br/&gt;
weight decay = 4e-5&lt;br/&gt;
momentum = 0.9&lt;br/&gt;
batch size = 64&lt;br/&gt;
optimizer = SGD&lt;br/&gt;
dropout = 0.2&lt;/p&gt;

&lt;p&gt;after completing my experiment I am noticing that class A and B have accuracies between 80% and 97% in the last few runs and Class C is in the 70 percentile range.&lt;/p&gt;

&lt;p&gt;I understand this because of how small the data sample is for class C compared to A and B, and I am looking for ways to run around that since even if I add more data it still would be less than half the size of the other two classes. This is my data augmentation technique:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; train_transform = transforms.Compose([
            transforms.RandomResizedCrop(224,scale = (0.2,1.0)),
            #transforms.RandomRotation(15),      # rotate +/- 10 degrees
            transforms.RandomHorizontalFlip(),  # reverse 50% of images        
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                                 [0.229, 0.224, 0.225])
        ])

    test_transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                                 [0.229, 0.224, 0.225])
        ])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cross-validation is the only technique I found while searching this problem that I am not using and using cross-validation in Pytroch is proving rather tricky for me. One technique I read about in a paper is where the author claimed the augmented their class data to be 6,000 images each, but they didn’t say how they did this. Are there anyways I can increase the detection accuracy of class C with limited samples available and by association increase the overall accuracy of my model from 84% to 95+%?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hzrxtn,True,,stunbomb1,,5,True,all_ads,False,[],False,,/r/pytorch/comments/hzrxtn/how_to_improve_model_performance_with_limited/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hzrxtn/how_to_improve_model_performance_with_limited/,7135,1595987932.0,0,,False,,,,,,,,
434,,pytorch,I can understand the way attention works but for images I'm not able to get it implemented if you guys could recommend me to some resources could be of lot of help,t2_12axvohb,False,,0,False,Need help with implementing cnn with attention,[],r/pytorch,False,6,,0,,,False,t3_hysya7,False,dark,0.99,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1595887819.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I can understand the way attention works but for images I&amp;#39;m not able to get it implemented if you guys could recommend me to some resources could be of lot of help&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hysya7,True,,narainp1,,1,True,all_ads,False,[],False,,/r/pytorch/comments/hysya7/need_help_with_implementing_cnn_with_attention/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hysya7/need_help_with_implementing_cnn_with_attention/,7135,1595859019.0,0,,False,,,,,,,,
435,,pytorch,"I have a  neural network model using PyTorch , and I get ""Expected device  cuda:0 but got device cpu "" error and I can't figure out why. I  assign the device to be cuda and the print line returns cuda. I've  tried assign the device as device = cuda:0 as well just in case but that  had no effect. Code in pastebin because it's quite long [https://pastebin.com/WWJ1Y5Pw](https://pastebin.com/WWJ1Y5Pw)

 

I have never run into this kind of issue before so any help would be great!",t2_5mvjitga,False,,0,False,Pytorch assigning device to cuda issue,[],r/pytorch,False,6,,0,,,False,t3_hyot4o,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1595869479.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a  neural network model using PyTorch , and I get &amp;quot;Expected device  cuda:0 but got device cpu &amp;quot; error and I can&amp;#39;t figure out why. I  assign the device to be cuda and the print line returns cuda. I&amp;#39;ve  tried assign the device as device = cuda:0 as well just in case but that  had no effect. Code in pastebin because it&amp;#39;s quite long &lt;a href=""https://pastebin.com/WWJ1Y5Pw""&gt;https://pastebin.com/WWJ1Y5Pw&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have never run into this kind of issue before so any help would be great!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?auto=webp&amp;s=07c121a0180003f7373863af66192b6ff6a937da', 'width': 150, 'height': 150}, 'resolutions': [{'url': 'https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df9c6a296446d05d873c629a30253398c4d29c1b', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'OgFzGCIRw1ZxjMOSkfV1OiH-_nQiZl8rzSonmOAuhGs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hyot4o,True,,BackgroundDisk4,,4,True,all_ads,False,[],False,,/r/pytorch/comments/hyot4o/pytorch_assigning_device_to_cuda_issue/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hyot4o/pytorch_assigning_device_to_cuda_issue/,7135,1595840679.0,0,,False,,,,,,,,
436,,pytorch,,t2_1ffz9tjt,False,,0,False,[AI application] Python implementation of Proximal Policy Optimization (PPO) algorithm for Super Mario Bros. 29/32 levels have been conquered,[],r/pytorch,False,6,,0,131.0,,False,t3_hy6535,False,dark,0.85,,public,9,0,{},140.0,,False,[],"{'reddit_video': {'fallback_url': 'https://v.redd.it/mxjrjtq247d51/DASH_240.mp4?source=fallback', 'height': 240, 'width': 256, 'scrubber_media_url': 'https://v.redd.it/mxjrjtq247d51/DASH_96.mp4', 'dash_url': 'https://v.redd.it/mxjrjtq247d51/DASHPlaylist.mpd?a=1618044212%2CYWUyY2Y1ZGI3OTA2MDM3YzMxNjU0YTU5MzI2ZGE5MTM1NmFmZGNhMWYyYzkwZTlkZTA4ZGVjN2QyNjg5NDQyYw%3D%3D&amp;v=1&amp;f=sd', 'duration': 607, 'hls_url': 'https://v.redd.it/mxjrjtq247d51/HLSPlaylist.m3u8?a=1618044212%2CMDQ0ZDQzNzc4ZjllNTJkM2I2Y2JhZGE5MjIyZWEyOWRmMTliYTFlMjU0NjdhMmVhZTU2NzI3MjhlZTI5NzQwOA%3D%3D&amp;v=1&amp;f=sd', 'is_gif': False, 'transcoding_status': 'completed'}}",True,False,,{},,False,9,,False,https://b.thumbs.redditmedia.com/OxzGoD139Dp44y1WvCPzkv4XJB4YrKNvJB27s15T7Po.jpg,False,,[],{},hosted:video,,False,,1595795483.0,text,6,,,text,v.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jjPjqnZgMJhMUECflBYH7ielWdISVTeH7Na8Kic0ldQ.png?format=pjpg&amp;auto=webp&amp;s=f076080d783fd5fd76403a999f8c013db93df01c', 'width': 256, 'height': 240}, 'resolutions': [{'url': 'https://external-preview.redd.it/jjPjqnZgMJhMUECflBYH7ielWdISVTeH7Na8Kic0ldQ.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b8735a1aa4c531d3d9ba2701ac43cc11f7a12645', 'width': 108, 'height': 101}, {'url': 'https://external-preview.redd.it/jjPjqnZgMJhMUECflBYH7ielWdISVTeH7Na8Kic0ldQ.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c020eac87cbf7cff8663e86bc323c15c0eb7f461', 'width': 216, 'height': 202}], 'variants': {}, 'id': 'gseUPDYVTxW1K2Bo_DAiyOXXySQcboYmoGrifs_5Q8w'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hy6535,True,,1991viet,,4,True,all_ads,False,[],False,,/r/pytorch/comments/hy6535/ai_application_python_implementation_of_proximal/,all_ads,False,https://v.redd.it/mxjrjtq247d51,7135,1595766683.0,0,"{'reddit_video': {'fallback_url': 'https://v.redd.it/mxjrjtq247d51/DASH_240.mp4?source=fallback', 'height': 240, 'width': 256, 'scrubber_media_url': 'https://v.redd.it/mxjrjtq247d51/DASH_96.mp4', 'dash_url': 'https://v.redd.it/mxjrjtq247d51/DASHPlaylist.mpd?a=1618044212%2CYWUyY2Y1ZGI3OTA2MDM3YzMxNjU0YTU5MzI2ZGE5MTM1NmFmZGNhMWYyYzkwZTlkZTA4ZGVjN2QyNjg5NDQyYw%3D%3D&amp;v=1&amp;f=sd', 'duration': 607, 'hls_url': 'https://v.redd.it/mxjrjtq247d51/HLSPlaylist.m3u8?a=1618044212%2CMDQ0ZDQzNzc4ZjllNTJkM2I2Y2JhZGE5MjIyZWEyOWRmMTliYTFlMjU0NjdhMmVhZTU2NzI3MjhlZTI5NzQwOA%3D%3D&amp;v=1&amp;f=sd', 'is_gif': False, 'transcoding_status': 'completed'}}",True,https://v.redd.it/mxjrjtq247d51,,,,,,,
437,,pytorch,"[https://github.com/Yura52/zero/issues/21](https://github.com/Yura52/zero/issues/21)

Hi! I introduce Zero - a new general-purpose library for PyTorch users. Zero:  
\- simplifies training loop, models evaluation, models application and other typical Deep Learning tasks  
\- provides a collection of tools and leaves code organization to you  
\- can be used on its own or together with PyTorch frameworks such as Ignite, Lightning, Catalyst and others

Follow the link above to read the full release announcement. See also the website:

[https://yura52.github.io/zero](https://yura52.github.io/zero/)",t2_i999rgs,False,,0,False,Zero: a new general-purpose library for PyTorch users,[],r/pytorch,False,6,,0,,,False,t3_hxqc7u,False,dark,0.96,,public,17,0,{},,,False,[],,False,False,,{},,False,17,,False,self,False,,[],{},self,,True,,1595726226.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://github.com/Yura52/zero/issues/21""&gt;https://github.com/Yura52/zero/issues/21&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hi! I introduce Zero - a new general-purpose library for PyTorch users. Zero:&lt;br/&gt;
- simplifies training loop, models evaluation, models application and other typical Deep Learning tasks&lt;br/&gt;
- provides a collection of tools and leaves code organization to you&lt;br/&gt;
- can be used on its own or together with PyTorch frameworks such as Ignite, Lightning, Catalyst and others&lt;/p&gt;

&lt;p&gt;Follow the link above to read the full release announcement. See also the website:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://yura52.github.io/zero/""&gt;https://yura52.github.io/zero&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/wQAeg__VVBxYcjZx7nxz9_YBQIP8QfHlw6DmIAjLAG0.jpg?auto=webp&amp;s=af002d7512ac21cc17b47a1622bfb333d383582e', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/wQAeg__VVBxYcjZx7nxz9_YBQIP8QfHlw6DmIAjLAG0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cfa8e3e49da772b12083039a90f333ceee319ca4', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/wQAeg__VVBxYcjZx7nxz9_YBQIP8QfHlw6DmIAjLAG0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd9bbb4c40eefd4b2fdebca0e48bd938c0e3f4d9', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/wQAeg__VVBxYcjZx7nxz9_YBQIP8QfHlw6DmIAjLAG0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6872b62d71b035a6d61aae44a6e2ba6273cad324', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'uv5A2PAF0HB8rbfjoDpjOHfgsyym74rPUpKzxt0PMfM'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hxqc7u,True,,StrausMG,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hxqc7u/zero_a_new_generalpurpose_library_for_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hxqc7u/zero_a_new_generalpurpose_library_for_pytorch/,7135,1595697426.0,0,,False,,,,,,,,
438,,pytorch,"I am trying to parallelize a piece of code over multiple GPU using `torch.multiprocessing.pool`. 

The code below hangs or keeps running forever without any errors when using `set_start_method('spawn', force=True)` in `torch.multiprocessing.pool`.

Code:
```
import numpy as np
import torch
from torch.multiprocessing import Pool, set_start_method

X = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]])
X = torch.DoubleTensor(X)

def X_power_func(j):
    X_power = X.cuda()**j
    return X_power

if __name__ == '__main__':
    set_start_method('spawn', force=True)
    with Pool(processes = 2) as p:   # Parallelizing over 2 GPUs
    results = p.map(X_power_func, range(4))

results
```

When I removed `set_start_method('spawn', force=True)`, the code ran properly and gave me the results, but this only works for when I run the code once. When I ran the code again in subsequent runs, I get the error `RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method`.

Any help would really be appreciated!",t2_zmqho4m,False,,0,False,PyTorch: How to parallelize over multiple GPU using torch.multiprocessing.pool,[],r/pytorch,False,6,,0,,,False,t3_hxlwh9,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1595696362.0,,[],{},,,True,,1595708792.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to parallelize a piece of code over multiple GPU using &lt;code&gt;torch.multiprocessing.pool&lt;/code&gt;. &lt;/p&gt;

&lt;p&gt;The code below hangs or keeps running forever without any errors when using &lt;code&gt;set_start_method(&amp;#39;spawn&amp;#39;, force=True)&lt;/code&gt; in &lt;code&gt;torch.multiprocessing.pool&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Code:
```
import numpy as np
import torch
from torch.multiprocessing import Pool, set_start_method&lt;/p&gt;

&lt;p&gt;X = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]])
X = torch.DoubleTensor(X)&lt;/p&gt;

&lt;p&gt;def X_power_func(j):
    X_power = X.cuda()**j
    return X_power&lt;/p&gt;

&lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;#39;&lt;strong&gt;main&lt;/strong&gt;&amp;#39;:
    set_start_method(&amp;#39;spawn&amp;#39;, force=True)
    with Pool(processes = 2) as p:   # Parallelizing over 2 GPUs
    results = p.map(X_power_func, range(4))&lt;/p&gt;

&lt;p&gt;results
```&lt;/p&gt;

&lt;p&gt;When I removed &lt;code&gt;set_start_method(&amp;#39;spawn&amp;#39;, force=True)&lt;/code&gt;, the code ran properly and gave me the results, but this only works for when I run the code once. When I ran the code again in subsequent runs, I get the error &lt;code&gt;RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the &amp;#39;spawn&amp;#39; start method&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Any help would really be appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hxlwh9,True,,leockl,,8,True,all_ads,False,[],False,,/r/pytorch/comments/hxlwh9/pytorch_how_to_parallelize_over_multiple_gpu/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hxlwh9/pytorch_how_to_parallelize_over_multiple_gpu/,7135,1595679992.0,0,,False,,,,,,,,
439,,pytorch,[https://analyticsindiamag.com/building-your-own-object-recognition-in-pytorch-a-guide-to-implement-hardnet-in-pytorch/](https://analyticsindiamag.com/building-your-own-object-recognition-in-pytorch-a-guide-to-implement-hardnet-in-pytorch/),t2_40d0zt4s,False,,0,False,Building your own Object Recognition in Pytorch - A Guide to Implement HarDNet in PyTorch,[],r/pytorch,False,6,,0,,,False,t3_hwwzrc,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1595601902.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://analyticsindiamag.com/building-your-own-object-recognition-in-pytorch-a-guide-to-implement-hardnet-in-pytorch/""&gt;https://analyticsindiamag.com/building-your-own-object-recognition-in-pytorch-a-guide-to-implement-hardnet-in-pytorch/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/cWamYrhtUfz5mx9_WlYT76GWGzwTfLMqnzmE4pclEJU.jpg?auto=webp&amp;s=e277cf8b1dae3ee1b1021f03ea796eb45d369a6d', 'width': 640, 'height': 480}, 'resolutions': [{'url': 'https://external-preview.redd.it/cWamYrhtUfz5mx9_WlYT76GWGzwTfLMqnzmE4pclEJU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3dc005c002e0835607a78c8345ffa0e594b884e', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/cWamYrhtUfz5mx9_WlYT76GWGzwTfLMqnzmE4pclEJU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=45272ece2c0ad85879214390979af16bab3a62f7', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/cWamYrhtUfz5mx9_WlYT76GWGzwTfLMqnzmE4pclEJU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6085325525174e1eaa87c6ef8020ce6a2db91d9f', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/cWamYrhtUfz5mx9_WlYT76GWGzwTfLMqnzmE4pclEJU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=02915212f7bdfbfc61f8018a11caac3397f3be7e', 'width': 640, 'height': 480}], 'variants': {}, 'id': 'hnwb0JTO5sRzMEN_Cve7lVwVxLnLfgyiYX2jekFWLNI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hwwzrc,True,,analyticsindiam,,1,True,all_ads,False,[],False,,/r/pytorch/comments/hwwzrc/building_your_own_object_recognition_in_pytorch_a/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hwwzrc/building_your_own_object_recognition_in_pytorch_a/,7135,1595573102.0,0,,False,,,,,,,,
440,,pytorch,[https://analyticsindiamag.com/the-battle-between-google-facebook-that-nobody-is-talking-about/](https://analyticsindiamag.com/the-battle-between-google-facebook-that-nobody-is-talking-about/),t2_40d0zt4s,False,,0,False,The Battle Between Google &amp; Facebook That Nobody Is Talking About,[],r/pytorch,False,6,,0,,,False,t3_hwy86q,False,dark,0.43,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1595608674.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://analyticsindiamag.com/the-battle-between-google-facebook-that-nobody-is-talking-about/""&gt;https://analyticsindiamag.com/the-battle-between-google-facebook-that-nobody-is-talking-about/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/9U0AYad8fSCUjLomrTa4RmNxh1l4sgZFX6jZWNe2DW0.jpg?auto=webp&amp;s=58263061c32b350c587b14391f15614bbc5ad100', 'width': 1142, 'height': 583}, 'resolutions': [{'url': 'https://external-preview.redd.it/9U0AYad8fSCUjLomrTa4RmNxh1l4sgZFX6jZWNe2DW0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec58cd48d9cd4b966d9204c6c50c8930ef81c4ac', 'width': 108, 'height': 55}, {'url': 'https://external-preview.redd.it/9U0AYad8fSCUjLomrTa4RmNxh1l4sgZFX6jZWNe2DW0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=33a87568c093f570d5ae78f59a8227df842ee281', 'width': 216, 'height': 110}, {'url': 'https://external-preview.redd.it/9U0AYad8fSCUjLomrTa4RmNxh1l4sgZFX6jZWNe2DW0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=916304306d5884af8fc3cc9fa1345709951618a9', 'width': 320, 'height': 163}, {'url': 'https://external-preview.redd.it/9U0AYad8fSCUjLomrTa4RmNxh1l4sgZFX6jZWNe2DW0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3d59a9d06f112bf6b14d049675768c59ab4aabd0', 'width': 640, 'height': 326}, {'url': 'https://external-preview.redd.it/9U0AYad8fSCUjLomrTa4RmNxh1l4sgZFX6jZWNe2DW0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7ce9bb90261e2af96818c3d8eab83b115f41655c', 'width': 960, 'height': 490}, {'url': 'https://external-preview.redd.it/9U0AYad8fSCUjLomrTa4RmNxh1l4sgZFX6jZWNe2DW0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18b381589c9082d4730e67ba96b3f717e4eda9e2', 'width': 1080, 'height': 551}], 'variants': {}, 'id': 'YebiwkZ90mCmDEwWpWKcPO_20R1xSNagbZA8qUdyYks'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hwy86q,True,,analyticsindiam,,1,True,all_ads,False,[],False,,/r/pytorch/comments/hwy86q/the_battle_between_google_facebook_that_nobody_is/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hwy86q/the_battle_between_google_facebook_that_nobody_is/,7135,1595579874.0,0,,False,,,,,,,,
441,,pytorch," I made a classic matrix factorization model for movie recommendation system using keras using batch size 128, Stochastic Gradient Descent and mse loss on the 20M movielens dataset. It gave me the minimum loss of 0.59 in 5 epochs. Then I recreated the exact same model ( the architecture was same and I also checked the total params in both to be same just for confirmation ) in Pytorch. The loss that I get for every batch in Pytorch is around 0.007 (in case of keras it came down from 1 to 0.5). When I multiply it with 128, the loss is in the range of 0.9 so I thought of comparing the pytorch loss after multiplying with 128. Now the issue is that the Pytorch model takes forever to train as I have had more than 40 epochs but the loss won't go below 0.78 (after multiplying with 128) and the results are poor in comparison to keras model. Can anyone please explain me what I did wrong. My guess was that I did something wrong with the training loop. It would also help if someone shows how a training loop is supposed to be with Stochastic Grad descent in Pytorch with batch size 128 without the default pytorch DataLoader because I guess it does not support csv as of now. Thanks in advance 

The Link for keras model code is:  [https://gist.github.com/Yash-567/344ad748be4c4d3df1344eb506e38d58](https://gist.github.com/Yash-567/344ad748be4c4d3df1344eb506e38d58)

The link for Pytorch implementation is: [https://gist.github.com/Yash-567/3da2cc10ffc261f565d5cfa0b040f544](https://gist.github.com/Yash-567/3da2cc10ffc261f565d5cfa0b040f544)",t2_4lkm36m5,False,,0,False,Pytorch Implementation of Keras model,[],r/pytorch,False,6,,0,,,False,t3_hvq7lh,False,dark,0.71,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1595436170.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I made a classic matrix factorization model for movie recommendation system using keras using batch size 128, Stochastic Gradient Descent and mse loss on the 20M movielens dataset. It gave me the minimum loss of 0.59 in 5 epochs. Then I recreated the exact same model ( the architecture was same and I also checked the total params in both to be same just for confirmation ) in Pytorch. The loss that I get for every batch in Pytorch is around 0.007 (in case of keras it came down from 1 to 0.5). When I multiply it with 128, the loss is in the range of 0.9 so I thought of comparing the pytorch loss after multiplying with 128. Now the issue is that the Pytorch model takes forever to train as I have had more than 40 epochs but the loss won&amp;#39;t go below 0.78 (after multiplying with 128) and the results are poor in comparison to keras model. Can anyone please explain me what I did wrong. My guess was that I did something wrong with the training loop. It would also help if someone shows how a training loop is supposed to be with Stochastic Grad descent in Pytorch with batch size 128 without the default pytorch DataLoader because I guess it does not support csv as of now. Thanks in advance &lt;/p&gt;

&lt;p&gt;The Link for keras model code is:  &lt;a href=""https://gist.github.com/Yash-567/344ad748be4c4d3df1344eb506e38d58""&gt;https://gist.github.com/Yash-567/344ad748be4c4d3df1344eb506e38d58&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The link for Pytorch implementation is: &lt;a href=""https://gist.github.com/Yash-567/3da2cc10ffc261f565d5cfa0b040f544""&gt;https://gist.github.com/Yash-567/3da2cc10ffc261f565d5cfa0b040f544&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?auto=webp&amp;s=079a7260ec149880c73263d64811698adb22760a', 'width': 1280, 'height': 640}, 'resolutions': [{'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5811c5bda5fece1040636a6af8702ba790f0fd4', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eee576fd4da7535eb53ceb88dd8b52f073048441', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=72872d880460efa723918c000adca0ed259cf775', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3545b9335d763c9da9c16bf7bf9a3f907dbd6f6', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d241ace0f1c07088fac3f8469dbad3b05d2d419', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9055f11bdc00beb0b3589e1cae5817d6070d83bc', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hvq7lh,True,,Viper213567,,1,True,all_ads,False,[],False,,/r/pytorch/comments/hvq7lh/pytorch_implementation_of_keras_model/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hvq7lh/pytorch_implementation_of_keras_model/,7135,1595407370.0,0,,False,,,,,,,,
442,,pytorch,,t2_44mbtmjy,False,,0,False,Latest from Microsoft researchers: High-quality video inpainting!,[],r/pytorch,False,6,,0,77.0,,False,t3_hvkl84,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/oRLzHp3wpcE2_ewREbu9wWV73Ms8J9Sza0QGxSUiuPo.jpg,False,,[],{},link,,False,,1595410744.0,text,6,,,text,self.LatestInML,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5GsLXuDaHWVug-zI24zPoGd7VpFs843JX_K3vYDHBns.jpg?auto=webp&amp;s=abb24cbabfd9911a54217e3b8725bc98d0156ae5', 'width': 958, 'height': 260}, 'resolutions': [{'url': 'https://external-preview.redd.it/5GsLXuDaHWVug-zI24zPoGd7VpFs843JX_K3vYDHBns.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff17a6aa9c1d7de9d362fb0b1510d7485a856999', 'width': 108, 'height': 29}, {'url': 'https://external-preview.redd.it/5GsLXuDaHWVug-zI24zPoGd7VpFs843JX_K3vYDHBns.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dac8b5697e60740235c3bb872f4e94ef3d553954', 'width': 216, 'height': 58}, {'url': 'https://external-preview.redd.it/5GsLXuDaHWVug-zI24zPoGd7VpFs843JX_K3vYDHBns.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eab17063df8b161a893c6c55c22398cbe7781ce6', 'width': 320, 'height': 86}, {'url': 'https://external-preview.redd.it/5GsLXuDaHWVug-zI24zPoGd7VpFs843JX_K3vYDHBns.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb81882f2babf600b9fd98fc3540269b94749759', 'width': 640, 'height': 173}], 'variants': {}, 'id': 'n4A3BFBMPGZZPYPbVGEid-igOkMvL8-N_LmhDLUfbPg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hvkl84,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hvkl84/latest_from_microsoft_researchers_highquality/,all_ads,False,/r/LatestInML/comments/hvk0rh/latest_from_microsoft_researchers_highquality/,7135,1595381944.0,0,,False,/r/LatestInML/comments/hvk0rh/latest_from_microsoft_researchers_highquality/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': '&amp;#x200B;\n\nhttps://i.redd.it/8if3dpdm5bc51.gif\n\nFor project and code/expert/API requests: [click here](https://www.catalyzex.com/paper/arxiv:2007.10247)\n\nThey propose to learn a joint Spatial-Temporal Transformer Network (STTN) for video inpainting. Specifically, they simultaneously fill missing regions in all input frames by self-attention, and propose to optimize STTN by a spatial-temporal adversarial loss', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Latest from Microsoft researchers: High-quality video inpainting!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 77, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'8if3dpdm5bc51': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/8if3dpdm5bc51.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=e1e5b36e3b79944e54526fc3fd725426755101d2'}, {'y': 120, 'x': 216, 'u': 'https://preview.redd.it/8if3dpdm5bc51.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=13586e6df49909f1f445afed6fa01291c2bf078a'}, {'y': 178, 'x': 320, 'u': 'https://preview.redd.it/8if3dpdm5bc51.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=65cb31d65adba2831786ae4c5623d19e7459c1c1'}], 's': {'y': 334, 'gif': 'https://i.redd.it/8if3dpdm5bc51.gif', 'mp4': 'https://preview.redd.it/8if3dpdm5bc51.gif?format=mp4&amp;s=21a4b7b4cbcf7ee5119c6c96405180f702ede34c', 'x': 600}, 'id': '8if3dpdm5bc51'}}, 'name': 't3_hvk0rh', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 29, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 29, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/oRLzHp3wpcE2_ewREbu9wWV73Ms8J9Sza0QGxSUiuPo.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1595408601.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://i.redd.it/8if3dpdm5bc51.gif""&gt;https://i.redd.it/8if3dpdm5bc51.gif&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For project and code/expert/API requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2007.10247""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They propose to learn a joint Spatial-Temporal Transformer Network (STTN) for video inpainting. Specifically, they simultaneously fill missing regions in all input frames by self-attention, and propose to optimize STTN by a spatial-temporal adversarial loss&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/5GsLXuDaHWVug-zI24zPoGd7VpFs843JX_K3vYDHBns.jpg?auto=webp&amp;s=abb24cbabfd9911a54217e3b8725bc98d0156ae5', 'width': 958, 'height': 260}, 'resolutions': [{'url': 'https://external-preview.redd.it/5GsLXuDaHWVug-zI24zPoGd7VpFs843JX_K3vYDHBns.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff17a6aa9c1d7de9d362fb0b1510d7485a856999', 'width': 108, 'height': 29}, {'url': 'https://external-preview.redd.it/5GsLXuDaHWVug-zI24zPoGd7VpFs843JX_K3vYDHBns.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dac8b5697e60740235c3bb872f4e94ef3d553954', 'width': 216, 'height': 58}, {'url': 'https://external-preview.redd.it/5GsLXuDaHWVug-zI24zPoGd7VpFs843JX_K3vYDHBns.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eab17063df8b161a893c6c55c22398cbe7781ce6', 'width': 320, 'height': 86}, {'url': 'https://external-preview.redd.it/5GsLXuDaHWVug-zI24zPoGd7VpFs843JX_K3vYDHBns.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb81882f2babf600b9fd98fc3540269b94749759', 'width': 640, 'height': 173}], 'variants': {}, 'id': 'n4A3BFBMPGZZPYPbVGEid-igOkMvL8-N_LmhDLUfbPg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'hvk0rh', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/hvk0rh/latest_from_microsoft_researchers_highquality/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/hvk0rh/latest_from_microsoft_researchers_highquality/', 'subreddit_subscribers': 6676, 'created_utc': 1595379801.0, 'num_crossposts': 10, 'media': None, 'is_video': False}]",t3_hvk0rh,,,,,
443,,pytorch,"I am modifying a script I download from [https://github.com/pytorch/examples/blob/master/imagenet/main.py](https://github.com/pytorch/examples/blob/master/imagenet/main.py).

I am adding code design to create a confusion matrix, but I keep getting the error message **ValueError: Found input variables with inconsistent numbers of samples: \[225, 1\]**

    class_names = ['cat', '  dog', ' pig']
    def validate(val_loader, model, criterion, args):
    batch_time = AverageMeter('Time', ':6.3f')
    losses = AverageMeter('Loss', ':.4e')
    top1 = AverageMeter('Acc@1', ':6.2f')
    top3 = AverageMeter('Acc@3', ':6.2f')
    progress = ProgressMeter(
    len(val_loader),
    [batch_time, losses, top1, top3],
    prefix='Test: ')
    # switch to evaluate mode
    model.eval()
    with torch.no_grad():
    end = time.time()
    for i, (images, target) in enumerate(val_loader):
    if args.gpu is not None:
    images = images.cuda(args.gpu, non_blocking=True)
    if torch.cuda.is_available():
    target = target.cuda(args.gpu, non_blocking=True)
    # compute output
    output = model(images)
    loss = criterion(output, target)
    # measure accuracy and record loss
    acc1, acc3 = accuracy(output, target, topk=(1, 5))
    losses.update(loss.item(), images.size(0))
    top1.update(acc1[0], images.size(0))
    top3.update(acc3[0], images.size(0))
    # measure elapsed time
    batch_time.update(time.time() - end)
    end = time.time()
    if i % args.print_freq == 0:
    progress.display(i)
    # TODO: this should also be done with the ProgressMeter
    print(' * Acc@1 {top1.avg:.3f} Acc@3 {top3.avg:.3f}'
    .format(top1=top1, top3=top3))
    target = target.cpu()
    acc3 = acc3.cpu()
    confusion_matrix(target.view(-1), acc3.view(-1))
    return top1.avg

I code I wrote from scratch I did something like this:

    with torch.no_grad():
    for b, (pics, names) in enumerate(test_loader):
    if torch.cuda.is_available():
    pics = pics.cuda()
    names = names.cuda()
    b+=1
    # Apply the model
    y_val = MobileNet(pics)
    val_loss = criterion(y_val, names)
    # Tally the number of correct predictions
    test_predicted = torch.max(y_val.data, 1)[1]
    tst_corr += (test_predicted == names).sum()
    test_correct.append(tst_corr)
    Val_accuracy = tst_corr.item()*100/(test_batch)
    names = names.cpu()
    test_predicted = test_predicted.cpu()
    arr = confusion_matrix(names.view(-1), test_predicted.view(-1))
    df_cm = pd.DataFrame(arr, class_names, class_names)
    plt.figure(figsize = (9,6))
    sn.heatmap(df_cm, annot=True, fmt=""d"", cmap='BuGn')
    plt.xlabel(""prediction"")
    plt.ylabel(""True label"")
    plt.show();

&amp;#x200B;",t2_48egj5xw,False,,0,False,Help with confusion matrix,[],r/pytorch,False,6,,0,,,False,t3_hv6qju,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1595340208.0,,[],{},self,,True,,1595364414.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am modifying a script I download from &lt;a href=""https://github.com/pytorch/examples/blob/master/imagenet/main.py""&gt;https://github.com/pytorch/examples/blob/master/imagenet/main.py&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am adding code design to create a confusion matrix, but I keep getting the error message &lt;strong&gt;ValueError: Found input variables with inconsistent numbers of samples: [225, 1]&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class_names = [&amp;#39;cat&amp;#39;, &amp;#39;  dog&amp;#39;, &amp;#39; pig&amp;#39;]
def validate(val_loader, model, criterion, args):
batch_time = AverageMeter(&amp;#39;Time&amp;#39;, &amp;#39;:6.3f&amp;#39;)
losses = AverageMeter(&amp;#39;Loss&amp;#39;, &amp;#39;:.4e&amp;#39;)
top1 = AverageMeter(&amp;#39;Acc@1&amp;#39;, &amp;#39;:6.2f&amp;#39;)
top3 = AverageMeter(&amp;#39;Acc@3&amp;#39;, &amp;#39;:6.2f&amp;#39;)
progress = ProgressMeter(
len(val_loader),
[batch_time, losses, top1, top3],
prefix=&amp;#39;Test: &amp;#39;)
# switch to evaluate mode
model.eval()
with torch.no_grad():
end = time.time()
for i, (images, target) in enumerate(val_loader):
if args.gpu is not None:
images = images.cuda(args.gpu, non_blocking=True)
if torch.cuda.is_available():
target = target.cuda(args.gpu, non_blocking=True)
# compute output
output = model(images)
loss = criterion(output, target)
# measure accuracy and record loss
acc1, acc3 = accuracy(output, target, topk=(1, 5))
losses.update(loss.item(), images.size(0))
top1.update(acc1[0], images.size(0))
top3.update(acc3[0], images.size(0))
# measure elapsed time
batch_time.update(time.time() - end)
end = time.time()
if i % args.print_freq == 0:
progress.display(i)
# TODO: this should also be done with the ProgressMeter
print(&amp;#39; * Acc@1 {top1.avg:.3f} Acc@3 {top3.avg:.3f}&amp;#39;
.format(top1=top1, top3=top3))
target = target.cpu()
acc3 = acc3.cpu()
confusion_matrix(target.view(-1), acc3.view(-1))
return top1.avg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I code I wrote from scratch I did something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;with torch.no_grad():
for b, (pics, names) in enumerate(test_loader):
if torch.cuda.is_available():
pics = pics.cuda()
names = names.cuda()
b+=1
# Apply the model
y_val = MobileNet(pics)
val_loss = criterion(y_val, names)
# Tally the number of correct predictions
test_predicted = torch.max(y_val.data, 1)[1]
tst_corr += (test_predicted == names).sum()
test_correct.append(tst_corr)
Val_accuracy = tst_corr.item()*100/(test_batch)
names = names.cpu()
test_predicted = test_predicted.cpu()
arr = confusion_matrix(names.view(-1), test_predicted.view(-1))
df_cm = pd.DataFrame(arr, class_names, class_names)
plt.figure(figsize = (9,6))
sn.heatmap(df_cm, annot=True, fmt=&amp;quot;d&amp;quot;, cmap=&amp;#39;BuGn&amp;#39;)
plt.xlabel(&amp;quot;prediction&amp;quot;)
plt.ylabel(&amp;quot;True label&amp;quot;)
plt.show();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?auto=webp&amp;s=08c43430ff8793a25b767aa172468acb35be611a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4adad794ed3c0e2f76e2da992ae9eef8c074cae4', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5b3cb400fea60e0dfe87ebc5314f7db8016f8ab', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f149723b636ef3087022ddc1efa6ab53e3d4bc69', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_YPUxJ8AyYwgpO9r-tD7JUoQ-Nx8IlATLoEfmjW48iQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hv6qju,True,,stunbomb1,,2,True,all_ads,False,[],False,,/r/pytorch/comments/hv6qju/help_with_confusion_matrix/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hv6qju/help_with_confusion_matrix/,7135,1595335614.0,0,,False,,,,,,,,
444,,pytorch,"I am running a deep learning script that has me using the command prompt, but it keeps telling me I do not have enough free space. Normally I just reset my Sypder or Jupiter kernel but since I am using the command prompt I don't know how to do it, so how would I clear it out in the windows command prompt?",t2_48egj5xw,False,,0,False,How can I free up Cuda memory from the command line?,[],r/pytorch,False,6,,0,,,False,t3_hus8tr,False,dark,0.66,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1595303180.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am running a deep learning script that has me using the command prompt, but it keeps telling me I do not have enough free space. Normally I just reset my Sypder or Jupiter kernel but since I am using the command prompt I don&amp;#39;t know how to do it, so how would I clear it out in the windows command prompt?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hus8tr,True,,stunbomb1,,6,True,all_ads,False,[],False,,/r/pytorch/comments/hus8tr/how_can_i_free_up_cuda_memory_from_the_command/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hus8tr/how_can_i_free_up_cuda_memory_from_the_command/,7135,1595274380.0,0,,False,,,,,,,,
445,,pytorch,"Hi guys! 

I am looking for a way to implement Reshape layer just like in Tensorflow. 

The reason for this is that I intend to reshape 1D data into 3D input into Conv1D layer.

I've found a way to do so on github, but I can't figure our how to implement it successfully. 

Much appreciate your feedback and advice!",t2_4ecq42y4,False,,0,False,Using Reshape/View module,[],r/pytorch,False,6,,0,,,False,t3_huny53,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1595290037.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys! &lt;/p&gt;

&lt;p&gt;I am looking for a way to implement Reshape layer just like in Tensorflow. &lt;/p&gt;

&lt;p&gt;The reason for this is that I intend to reshape 1D data into 3D input into Conv1D layer.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve found a way to do so on github, but I can&amp;#39;t figure our how to implement it successfully. &lt;/p&gt;

&lt;p&gt;Much appreciate your feedback and advice!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,huny53,True,,ncuxomun,,3,True,all_ads,False,[],False,,/r/pytorch/comments/huny53/using_reshapeview_module/,all_ads,False,https://www.reddit.com/r/pytorch/comments/huny53/using_reshapeview_module/,7135,1595261237.0,0,,False,,,,,,,,
446,,pytorch,"I have an encoder-decoder architecture where the encoder is a pyramidal LSTM where each timestep of the consecutive layer will have two timesteps from the previous layer combined. In order to do that I have two separate LSTMs. Here is part of the code:


    self.lstm = nn.LSTM(self.input_tensor,
                    self.hidden_size,
                    num_layers=1,
                    bidirectional=True
                    )

    self.lstm_pyramid = nn.LSTM(self.hidden_size,
                     self.hidden_size,
                     num_layers=1,
                     bidirectional=True
                    )



    def forward(self, input_tensor, input_feature_lengths):
        #pad the input timesteps to be divisible by 2
        if input_tensor.size(0) % 2 != 0:
           padding = torch.zeros(1, input_tensor.size(1), input_tensor.size(2), device=self.device)
           input_tensor = torch.cat((input_tensor, padding), dim=0)


        input_tensor = pack_padded_sequence(input_tensor, input_feature_lengths)
        output, hidden = self.lstm(input_tensor)
        output = pad_packed_sequence(output)[0]
        output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]
    
        # pyramidal structure
        for i in range(1, self.num_layers):
            idx_odd = [j for j in range(output.size(0)) if j % 2 != 0]
            idx_even = [j for j in range(output.size(0)) if j % 2 == 0]
    
            output_odd = output[idx_odd, :, :]
            output_even = output[idx_even, :, :]
            if output_even.size(0) &gt; output_odd.size(0):
               output_odd = torch.cat((output_odd, torch.zeros(1, output_odd.size(1), output_odd.size(2), device=self.device)))
            output = torch.mean(torch.stack([output_odd, output_even]), 0)
        
            input_feature_lengths = np.floor_divide(np.array(input_feature_lengths), 2)
            output = pack_padded_sequence(output, input_feature_lengths)
            output, hidden = self.lstm_pyramid(output, hidden)
            output = pad_packed_sequence(output)[0]
            output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]


Now, what I have noticed is that when I try to overfit with just couple of examples, the network remembers only one sentence and outputs that one for all the examples, even though the loss gets around 0.

In order to debug the problem I tried to simplify the architecture and instead of using pyramidal LSTM, to use two separate LSTMs as follows:


    input_tensor = pack_padded_sequence(input_tensor, input_feature_lengths)
    output, hidden = self.lstm(input_tensor)
    output = pad_packed_sequence(output)[0]
    output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]
    
    output = pack_padded_sequence(output, input_feature_lengths)
    output, hidden = self.lstm_pyramid(output, hidden)
    output = pad_packed_sequence(output)[0]
    output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]

From my understanding, having a stacked two layer LSTM and two separate LSTMs should give the same results, which was not the case. This way the network was still remembering only one sentence and repeating it for all the examples. When I use only one LSTM with 2 layers, then the network remembers all the sentences and I get 0 word error rate, which is expected.

I assume that I am doing something wrong when using `pack_padded_sequence` and `pad_packed_sequence`. I also tried not using `pack_padded_sequence` and `pad_packed_sequence` but it was still remembering only one sentence.

Can anyone tell me how to properly use them when I have a pyramidal LSTM?",t2_b7l557y,False,,0,False,Padded sequences with Pyramidal LSTM,[],r/pytorch,False,6,,0,,,False,t3_huhcnk,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1596193037.0,,[],{},,,True,,1595260449.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have an encoder-decoder architecture where the encoder is a pyramidal LSTM where each timestep of the consecutive layer will have two timesteps from the previous layer combined. In order to do that I have two separate LSTMs. Here is part of the code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;self.lstm = nn.LSTM(self.input_tensor,
                self.hidden_size,
                num_layers=1,
                bidirectional=True
                )

self.lstm_pyramid = nn.LSTM(self.hidden_size,
                 self.hidden_size,
                 num_layers=1,
                 bidirectional=True
                )



def forward(self, input_tensor, input_feature_lengths):
    #pad the input timesteps to be divisible by 2
    if input_tensor.size(0) % 2 != 0:
       padding = torch.zeros(1, input_tensor.size(1), input_tensor.size(2), device=self.device)
       input_tensor = torch.cat((input_tensor, padding), dim=0)


    input_tensor = pack_padded_sequence(input_tensor, input_feature_lengths)
    output, hidden = self.lstm(input_tensor)
    output = pad_packed_sequence(output)[0]
    output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]

    # pyramidal structure
    for i in range(1, self.num_layers):
        idx_odd = [j for j in range(output.size(0)) if j % 2 != 0]
        idx_even = [j for j in range(output.size(0)) if j % 2 == 0]

        output_odd = output[idx_odd, :, :]
        output_even = output[idx_even, :, :]
        if output_even.size(0) &amp;gt; output_odd.size(0):
           output_odd = torch.cat((output_odd, torch.zeros(1, output_odd.size(1), output_odd.size(2), device=self.device)))
        output = torch.mean(torch.stack([output_odd, output_even]), 0)

        input_feature_lengths = np.floor_divide(np.array(input_feature_lengths), 2)
        output = pack_padded_sequence(output, input_feature_lengths)
        output, hidden = self.lstm_pyramid(output, hidden)
        output = pad_packed_sequence(output)[0]
        output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, what I have noticed is that when I try to overfit with just couple of examples, the network remembers only one sentence and outputs that one for all the examples, even though the loss gets around 0.&lt;/p&gt;

&lt;p&gt;In order to debug the problem I tried to simplify the architecture and instead of using pyramidal LSTM, to use two separate LSTMs as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input_tensor = pack_padded_sequence(input_tensor, input_feature_lengths)
output, hidden = self.lstm(input_tensor)
output = pad_packed_sequence(output)[0]
output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]

output = pack_padded_sequence(output, input_feature_lengths)
output, hidden = self.lstm_pyramid(output, hidden)
output = pad_packed_sequence(output)[0]
output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From my understanding, having a stacked two layer LSTM and two separate LSTMs should give the same results, which was not the case. This way the network was still remembering only one sentence and repeating it for all the examples. When I use only one LSTM with 2 layers, then the network remembers all the sentences and I get 0 word error rate, which is expected.&lt;/p&gt;

&lt;p&gt;I assume that I am doing something wrong when using &lt;code&gt;pack_padded_sequence&lt;/code&gt; and &lt;code&gt;pad_packed_sequence&lt;/code&gt;. I also tried not using &lt;code&gt;pack_padded_sequence&lt;/code&gt; and &lt;code&gt;pad_packed_sequence&lt;/code&gt; but it was still remembering only one sentence.&lt;/p&gt;

&lt;p&gt;Can anyone tell me how to properly use them when I have a pyramidal LSTM?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,huhcnk,True,,tetrix994,,0,True,all_ads,False,[],False,,/r/pytorch/comments/huhcnk/padded_sequences_with_pyramidal_lstm/,all_ads,False,https://www.reddit.com/r/pytorch/comments/huhcnk/padded_sequences_with_pyramidal_lstm/,7135,1595231649.0,0,,False,,,,,,,,
447,,pytorch,,t2_44mbtmjy,False,,0,False,ECCV 2020: From Microsoft and UWashington researchers: Personalized Face Modeing!,[],r/pytorch,False,6,,0,78.0,,False,t3_huh0ly,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/HjVwujLhneYwzF9KA0ayjhZk6b0bN3FO5QeoXHE11lc.jpg,False,,[],{},link,,False,,1595258555.0,text,6,,,text,self.LatestInML,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/SY2t6ouPBd8KJQi7gxT6O67kk83r9AKwkzszAlKbI1g.jpg?auto=webp&amp;s=37386509f933164dbc40ac85de4becedbb8eb74c', 'width': 958, 'height': 340}, 'resolutions': [{'url': 'https://external-preview.redd.it/SY2t6ouPBd8KJQi7gxT6O67kk83r9AKwkzszAlKbI1g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d331264007a935fbaf7203502cd6870ad4904d86', 'width': 108, 'height': 38}, {'url': 'https://external-preview.redd.it/SY2t6ouPBd8KJQi7gxT6O67kk83r9AKwkzszAlKbI1g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=020fb6fcefc92fa1709ad7c09f3b4229bd3fb4af', 'width': 216, 'height': 76}, {'url': 'https://external-preview.redd.it/SY2t6ouPBd8KJQi7gxT6O67kk83r9AKwkzszAlKbI1g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6ee9492dee2ddcda2d7f8e220710f41551979c0', 'width': 320, 'height': 113}, {'url': 'https://external-preview.redd.it/SY2t6ouPBd8KJQi7gxT6O67kk83r9AKwkzszAlKbI1g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=98b858d70b01a48f9c319693c35e594329403be3', 'width': 640, 'height': 227}], 'variants': {}, 'id': 'X_lUFJRvjIrQiKjRuCFstgf7AOuLlcypy7fPaI2md6U'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,huh0ly,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/huh0ly/eccv_2020_from_microsoft_and_uwashington/,all_ads,False,/r/LatestInML/comments/hugmzi/eccv_2020_from_microsoft_and_uwashington/,7135,1595229755.0,0,,False,/r/LatestInML/comments/hugmzi/eccv_2020_from_microsoft_and_uwashington/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/expert/API requests: [click here](https://www.catalyzex.com/paper/arxiv:2007.06759)\n\nhttps://i.redd.it/h5ftfl9glyb51.gif\n\nTheir framework takes frames from in-the-wild video(s) of a user as input and generates per-frame tracking parameters via the TrackNet and personalized face model of the user via the ModelNet', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'ECCV 2020: From Microsoft and UWashington researchers: Personalized Face Modeing!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 78, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'h5ftfl9glyb51': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/h5ftfl9glyb51.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=1433248e9a3759ef98f9d584cb550f5f1c1bc55a'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/h5ftfl9glyb51.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=6caaf3e1274ebd2b2d834c3d5647e94f4b3a1229'}, {'y': 180, 'x': 320, 'u': 'https://preview.redd.it/h5ftfl9glyb51.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=b7dbcbbd02b805860784ad529bf0bd91b594a308'}], 's': {'y': 338, 'gif': 'https://i.redd.it/h5ftfl9glyb51.gif', 'mp4': 'https://preview.redd.it/h5ftfl9glyb51.gif?format=mp4&amp;s=4423b96518b8fddb883fefa553df552c3e9283cb', 'x': 600}, 'id': 'h5ftfl9glyb51'}}, 'name': 't3_hugmzi', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 5, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/HjVwujLhneYwzF9KA0ayjhZk6b0bN3FO5QeoXHE11lc.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1595256508.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/expert/API requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2007.06759""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://i.redd.it/h5ftfl9glyb51.gif""&gt;https://i.redd.it/h5ftfl9glyb51.gif&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Their framework takes frames from in-the-wild video(s) of a user as input and generates per-frame tracking parameters via the TrackNet and personalized face model of the user via the ModelNet&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/SY2t6ouPBd8KJQi7gxT6O67kk83r9AKwkzszAlKbI1g.jpg?auto=webp&amp;s=37386509f933164dbc40ac85de4becedbb8eb74c', 'width': 958, 'height': 340}, 'resolutions': [{'url': 'https://external-preview.redd.it/SY2t6ouPBd8KJQi7gxT6O67kk83r9AKwkzszAlKbI1g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d331264007a935fbaf7203502cd6870ad4904d86', 'width': 108, 'height': 38}, {'url': 'https://external-preview.redd.it/SY2t6ouPBd8KJQi7gxT6O67kk83r9AKwkzszAlKbI1g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=020fb6fcefc92fa1709ad7c09f3b4229bd3fb4af', 'width': 216, 'height': 76}, {'url': 'https://external-preview.redd.it/SY2t6ouPBd8KJQi7gxT6O67kk83r9AKwkzszAlKbI1g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6ee9492dee2ddcda2d7f8e220710f41551979c0', 'width': 320, 'height': 113}, {'url': 'https://external-preview.redd.it/SY2t6ouPBd8KJQi7gxT6O67kk83r9AKwkzszAlKbI1g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=98b858d70b01a48f9c319693c35e594329403be3', 'width': 640, 'height': 227}], 'variants': {}, 'id': 'X_lUFJRvjIrQiKjRuCFstgf7AOuLlcypy7fPaI2md6U'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'hugmzi', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/hugmzi/eccv_2020_from_microsoft_and_uwashington/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/hugmzi/eccv_2020_from_microsoft_and_uwashington/', 'subreddit_subscribers': 6676, 'created_utc': 1595227708.0, 'num_crossposts': 11, 'media': None, 'is_video': False}]",t3_hugmzi,,,,,
448,,pytorch,,t2_153bdglc,False,,0,False,fastai MultiLabel Classification using Kfold Cross Validation,[],r/pytorch,False,6,,0,90.0,,False,t3_htb5ya,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/KHLm9UucgvVu39tFkviLEArEpmtVyRfTw7I9CSJKc8Y.jpg,False,,[],{},link,,False,,1595077815.0,text,6,,,text,kirankamath.netlify.app,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/N_b_cefAxUqVqrV1Z53JMslRLWcbgHxGkkX4eCobX2Q.jpg?auto=webp&amp;s=0ec2d9f85b8a8e0c2de344e37f09dfd955f99380', 'width': 381, 'height': 245}, 'resolutions': [{'url': 'https://external-preview.redd.it/N_b_cefAxUqVqrV1Z53JMslRLWcbgHxGkkX4eCobX2Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=40dc72b0f0183c8cc1e79fdd47b06dac954f9a7b', 'width': 108, 'height': 69}, {'url': 'https://external-preview.redd.it/N_b_cefAxUqVqrV1Z53JMslRLWcbgHxGkkX4eCobX2Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2ff546c04be66e8fa494276e66d6cb1cee64a18e', 'width': 216, 'height': 138}, {'url': 'https://external-preview.redd.it/N_b_cefAxUqVqrV1Z53JMslRLWcbgHxGkkX4eCobX2Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5604118164ee7eb5313c26c34f2a0e8eefeb7582', 'width': 320, 'height': 205}], 'variants': {}, 'id': 'gOD9RV-5NELmpFR3GgkcarkXDS9Xzv5RJZxNWlCl_8s'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,htb5ya,True,,Kirankamat,,0,True,all_ads,False,[],False,,/r/pytorch/comments/htb5ya/fastai_multilabel_classification_using_kfold/,all_ads,False,https://kirankamath.netlify.app/blog/fastai-multilabel-classification-using-kfold-cross-validation/,7135,1595049015.0,0,,False,https://kirankamath.netlify.app/blog/fastai-multilabel-classification-using-kfold-cross-validation/,,,,,,,
449,,pytorch,[https://analyticsindiamag.com/coding-tools-federated-learning/](https://analyticsindiamag.com/coding-tools-federated-learning/),t2_40d0zt4s,False,,0,False,Top 10 Coding Tools For Federated Learning,[],r/pytorch,False,6,,0,,,False,t3_hsunvv,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1595017092.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://analyticsindiamag.com/coding-tools-federated-learning/""&gt;https://analyticsindiamag.com/coding-tools-federated-learning/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BmJlQhg-lRKJY18q5VACJbj_qQqhrHLmqCkLwbZClDE.jpg?auto=webp&amp;s=983ddb93561c566862d01fda07a36fcaae58cc52', 'width': 750, 'height': 501}, 'resolutions': [{'url': 'https://external-preview.redd.it/BmJlQhg-lRKJY18q5VACJbj_qQqhrHLmqCkLwbZClDE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c61d73fd2e29bbc3c7a793e8c8d59c80d262fedb', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/BmJlQhg-lRKJY18q5VACJbj_qQqhrHLmqCkLwbZClDE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=169d484662d23609163ca0de9fa6778ec0d06b95', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/BmJlQhg-lRKJY18q5VACJbj_qQqhrHLmqCkLwbZClDE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e4a66b9cd632ed4ab077c92f1b7cb91b90ae93c6', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/BmJlQhg-lRKJY18q5VACJbj_qQqhrHLmqCkLwbZClDE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b606c29111b72cdcc21d466a84c1befbae4b97e', 'width': 640, 'height': 427}], 'variants': {}, 'id': 'Odn0RG61McA4UQJAvJE9jLfximEQsBSKu0r0vWyhHcU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hsunvv,True,,analyticsindiam,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hsunvv/top_10_coding_tools_for_federated_learning/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hsunvv/top_10_coding_tools_for_federated_learning/,7135,1594988292.0,0,,False,,,,,,,,
450,,pytorch,"Generate Chest X ray Images From tensors of Random Values! Used Vanilla Variational Autoencoder with KL Divergence Loss and Binary Cross Entropy Loss and Code built in PyTorch!

The model is trained for 50 epochs with learning rate of 0.003 and Optimizer used is Adam!

The autoencoder uses normal encoder-decoder networks where the decoder is trained with reparameterized outputs from the encoder.

Cheers! 

[https://github.com/cskarthik7/Vanilla-VAE-PyTorch](https://github.com/cskarthik7/Vanilla-VAE-PyTorch)",t2_5appcd9u,False,,0,False,Variational Autoencoder to generate chest XRAY Images!,[],r/pytorch,False,6,,0,,,False,t3_hsrqq0,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},self,,True,,1595001508.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Generate Chest X ray Images From tensors of Random Values! Used Vanilla Variational Autoencoder with KL Divergence Loss and Binary Cross Entropy Loss and Code built in PyTorch!&lt;/p&gt;

&lt;p&gt;The model is trained for 50 epochs with learning rate of 0.003 and Optimizer used is Adam!&lt;/p&gt;

&lt;p&gt;The autoencoder uses normal encoder-decoder networks where the decoder is trained with reparameterized outputs from the encoder.&lt;/p&gt;

&lt;p&gt;Cheers! &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/cskarthik7/Vanilla-VAE-PyTorch""&gt;https://github.com/cskarthik7/Vanilla-VAE-PyTorch&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/wDwx6rRGtBQRjLKPOUXFuJnyhT0SKUYO6BWMFdXhLLs.jpg?auto=webp&amp;s=1e719de661e65b66139627610f34182ef16c2eef', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/wDwx6rRGtBQRjLKPOUXFuJnyhT0SKUYO6BWMFdXhLLs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=80f5ca99cf355e99310988a6444b066886d7c362', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/wDwx6rRGtBQRjLKPOUXFuJnyhT0SKUYO6BWMFdXhLLs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f36d1f513f76b55bd64ed6bfb28bcaa6087b1c59', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/wDwx6rRGtBQRjLKPOUXFuJnyhT0SKUYO6BWMFdXhLLs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27ba5ebe55b611e2003dc9793841975f90689b0b', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'Gpypu1x71Ba5lCatMKhQ57tTgjmrsQrf5iodeMZcIkE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hsrqq0,True,,lazermajor69,,2,True,all_ads,False,[],False,,/r/pytorch/comments/hsrqq0/variational_autoencoder_to_generate_chest_xray/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hsrqq0/variational_autoencoder_to_generate_chest_xray/,7135,1594972708.0,0,,False,,,,,,,,
451,,pytorch,"I have a notebook that creates audio and outputs it using IPython display. But IPython display audio doesnt work with vscode, and so i want to just write the audio file to my drive. How can i achieve this, saving a tensor as an audio file, using pytorch ? i cant use torchaudio since im on WIndows",t2_13v6migd,False,,0,False,How to save audio,[],r/pytorch,False,6,,0,,,False,t3_hsbii8,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1594941679.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a notebook that creates audio and outputs it using IPython display. But IPython display audio doesnt work with vscode, and so i want to just write the audio file to my drive. How can i achieve this, saving a tensor as an audio file, using pytorch ? i cant use torchaudio since im on WIndows&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hsbii8,True,,samurzele,,4,True,all_ads,False,[],False,,/r/pytorch/comments/hsbii8/how_to_save_audio/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hsbii8/how_to_save_audio/,7135,1594912879.0,0,,False,,,,,,,,
452,,pytorch,,t2_44mbtmjy,False,,0,False,Epidemic Exposure Notification with Smartwatch using Proximity Based Privacy Preserving Approach.,[],r/pytorch,False,6,,0,115.0,,False,t3_hrb06a,False,dark,0.81,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://a.thumbs.redditmedia.com/i1HDeJnQQuHTu4dbpfTVX5rP58TQaX28kLHZheUIgA8.jpg,False,,[],{},link,,False,,1594792800.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/LgGGlccUhLemGbqHuRN8GWoBO4lcfzgM8stodCC6wXQ.jpg?auto=webp&amp;s=744dca04ae66d2fd0e3d2ad6d50dcbc6026bafca', 'width': 656, 'height': 540}, 'resolutions': [{'url': 'https://external-preview.redd.it/LgGGlccUhLemGbqHuRN8GWoBO4lcfzgM8stodCC6wXQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c439520e30bca20c2acff698c76fcf2771881071', 'width': 108, 'height': 88}, {'url': 'https://external-preview.redd.it/LgGGlccUhLemGbqHuRN8GWoBO4lcfzgM8stodCC6wXQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7e592a0a6abf802cec39d04dc17ad3c1f125b74c', 'width': 216, 'height': 177}, {'url': 'https://external-preview.redd.it/LgGGlccUhLemGbqHuRN8GWoBO4lcfzgM8stodCC6wXQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cdc57cb5c798757505123ea09ba514bf684bacb3', 'width': 320, 'height': 263}, {'url': 'https://external-preview.redd.it/LgGGlccUhLemGbqHuRN8GWoBO4lcfzgM8stodCC6wXQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=06542191e1fbdd087c794cf21cfce099c92783a4', 'width': 640, 'height': 526}], 'variants': {}, 'id': 'FgAotEI6_kq-q5RYb-41ZYWvuAxdHFmQtDRT71vuKKs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hrb06a,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hrb06a/epidemic_exposure_notification_with_smartwatch/,all_ads,False,/r/LatestInML/comments/hraxf9/epidemic_exposure_notification_with_smartwatch/,7135,1594764000.0,0,,False,/r/LatestInML/comments/hraxf9/epidemic_exposure_notification_with_smartwatch/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2007.04399)\n\nhttps://preview.redd.it/ehlpq58u9wa51.png?width=974&amp;format=png&amp;auto=webp&amp;s=2fec5aafcadcc960cbf67e19ef5e637726c4641b\n\nProximity sensing exploits the received signal strength to detect the users interaction and thus classifying them into low or high risk with respect to a patient diagnosed with an infectious disease. More precisely, a user is notified of their exposure based on their interactions, in terms of distance and time, with a patient.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Epidemic Exposure Notification with Smartwatch using Proximity Based Privacy Preserving Approach.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 115, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'ehlpq58u9wa51': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 88, 'x': 108, 'u': 'https://preview.redd.it/ehlpq58u9wa51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4325a7e33827ef415c952a2f17a25b5016b6e429'}, {'y': 177, 'x': 216, 'u': 'https://preview.redd.it/ehlpq58u9wa51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b1a685cb885830f7cd8538d5bc704eef3dcaecd'}, {'y': 263, 'x': 320, 'u': 'https://preview.redd.it/ehlpq58u9wa51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=be6e5bcd9f81f59c929e6f81a3cbfd33270dfbe6'}, {'y': 526, 'x': 640, 'u': 'https://preview.redd.it/ehlpq58u9wa51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b484bcafacb0a933524442d89ee6e37110232139'}, {'y': 790, 'x': 960, 'u': 'https://preview.redd.it/ehlpq58u9wa51.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9006a4d8e66640e5f52b0df4d0d86d34e876a3f3'}], 's': {'y': 802, 'x': 974, 'u': 'https://preview.redd.it/ehlpq58u9wa51.png?width=974&amp;format=png&amp;auto=webp&amp;s=2fec5aafcadcc960cbf67e19ef5e637726c4641b'}, 'id': 'ehlpq58u9wa51'}}, 'name': 't3_hraxf9', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 13, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/i1HDeJnQQuHTu4dbpfTVX5rP58TQaX28kLHZheUIgA8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1594792538.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2007.04399""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/ehlpq58u9wa51.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2fec5aafcadcc960cbf67e19ef5e637726c4641b""&gt;https://preview.redd.it/ehlpq58u9wa51.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2fec5aafcadcc960cbf67e19ef5e637726c4641b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Proximity sensing exploits the received signal strength to detect the users interaction and thus classifying them into low or high risk with respect to a patient diagnosed with an infectious disease. More precisely, a user is notified of their exposure based on their interactions, in terms of distance and time, with a patient.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/LgGGlccUhLemGbqHuRN8GWoBO4lcfzgM8stodCC6wXQ.jpg?auto=webp&amp;s=744dca04ae66d2fd0e3d2ad6d50dcbc6026bafca', 'width': 656, 'height': 540}, 'resolutions': [{'url': 'https://external-preview.redd.it/LgGGlccUhLemGbqHuRN8GWoBO4lcfzgM8stodCC6wXQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c439520e30bca20c2acff698c76fcf2771881071', 'width': 108, 'height': 88}, {'url': 'https://external-preview.redd.it/LgGGlccUhLemGbqHuRN8GWoBO4lcfzgM8stodCC6wXQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7e592a0a6abf802cec39d04dc17ad3c1f125b74c', 'width': 216, 'height': 177}, {'url': 'https://external-preview.redd.it/LgGGlccUhLemGbqHuRN8GWoBO4lcfzgM8stodCC6wXQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cdc57cb5c798757505123ea09ba514bf684bacb3', 'width': 320, 'height': 263}, {'url': 'https://external-preview.redd.it/LgGGlccUhLemGbqHuRN8GWoBO4lcfzgM8stodCC6wXQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=06542191e1fbdd087c794cf21cfce099c92783a4', 'width': 640, 'height': 526}], 'variants': {}, 'id': 'FgAotEI6_kq-q5RYb-41ZYWvuAxdHFmQtDRT71vuKKs'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'hraxf9', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/hraxf9/epidemic_exposure_notification_with_smartwatch/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/hraxf9/epidemic_exposure_notification_with_smartwatch/', 'subreddit_subscribers': 6676, 'created_utc': 1594763738.0, 'num_crossposts': 17, 'media': None, 'is_video': False}]",t3_hraxf9,,,,,
453,,pytorch,"The Kickstarter Campaign for OpenCV AI Kit (OAK) goes live on July 14, 9 AM Eastern Time.  


https://preview.redd.it/1bw67dl1pta51.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=cacefd6bfcdfbe7bfc809a520acb69052c5ee476

[https://www.kickstarter.com/projects/opencv/opencv-ai-kit](https://www.kickstarter.com/projects/opencv/opencv-ai-kit)  


What is OAK?  


OpenCV AI Kit (OAK) is a smart camera based on Intel® Myriad X™. There are two variants of OAK.  


OAK-1 is a single camera solution that can do neural inference (image classification, object detection, segmentation and a lot more) on the device.  


OAK-D is our Spatial AI solution. It comes with a stereo camera in addition to the standard RGB camera.  


We have come up with super attractive pricing. The early bird prices are limited to 200 smart cameras of each kind.   


OAK-1 : $79 \[Early Bird Price\] and $99 \[Kickstarter Price\]  
OAK-D : $129 \[Early Bird Price\] and $149 \[Kickstarter Price\]  


For the price of a webcam, you can buy a smart camera that can not only do neural inference on the device, it can also do depth estimation in real time.  


It is not only a good solution for companies wanting to build an industrial smart camera, it is also an excellent platform for students, programmers, engineers and hobbyists to get a taste of Spatial AI and Edge AI.  


The two cameras will come with excellent software support.",t2_cvc9f,False,,0,False,Kickstarter Campaign for OpenCV AI Kit (OAK),[],r/pytorch,False,6,,0,78.0,,False,t3_hr1awh,False,dark,0.78,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/dgEz6WdIyic356CKWaFR0vFo-nU-sNHIgcUq_y1xbxs.jpg,False,,[],{},,,True,,1594761331.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The Kickstarter Campaign for OpenCV AI Kit (OAK) goes live on July 14, 9 AM Eastern Time.  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/1bw67dl1pta51.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cacefd6bfcdfbe7bfc809a520acb69052c5ee476""&gt;https://preview.redd.it/1bw67dl1pta51.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cacefd6bfcdfbe7bfc809a520acb69052c5ee476&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.kickstarter.com/projects/opencv/opencv-ai-kit""&gt;https://www.kickstarter.com/projects/opencv/opencv-ai-kit&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;What is OAK?  &lt;/p&gt;

&lt;p&gt;OpenCV AI Kit (OAK) is a smart camera based on Intel® Myriad X™. There are two variants of OAK.  &lt;/p&gt;

&lt;p&gt;OAK-1 is a single camera solution that can do neural inference (image classification, object detection, segmentation and a lot more) on the device.  &lt;/p&gt;

&lt;p&gt;OAK-D is our Spatial AI solution. It comes with a stereo camera in addition to the standard RGB camera.  &lt;/p&gt;

&lt;p&gt;We have come up with super attractive pricing. The early bird prices are limited to 200 smart cameras of each kind.   &lt;/p&gt;

&lt;p&gt;OAK-1 : $79 [Early Bird Price] and $99 [Kickstarter Price]&lt;br/&gt;
OAK-D : $129 [Early Bird Price] and $149 [Kickstarter Price]  &lt;/p&gt;

&lt;p&gt;For the price of a webcam, you can buy a smart camera that can not only do neural inference on the device, it can also do depth estimation in real time.  &lt;/p&gt;

&lt;p&gt;It is not only a good solution for companies wanting to build an industrial smart camera, it is also an excellent platform for students, programmers, engineers and hobbyists to get a taste of Spatial AI and Edge AI.  &lt;/p&gt;

&lt;p&gt;The two cameras will come with excellent software support.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hr1awh,True,,spmallick,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hr1awh/kickstarter_campaign_for_opencv_ai_kit_oak/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hr1awh/kickstarter_campaign_for_opencv_ai_kit_oak/,7135,1594732531.0,0,,False,,,,"{'1bw67dl1pta51': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/1bw67dl1pta51.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=186bb98d4a71dee693a12561d522c7cb592f70f8'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/1bw67dl1pta51.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3774b9e2e42256b6cfd16f8e88a94183c23f9d48'}, {'y': 180, 'x': 320, 'u': 'https://preview.redd.it/1bw67dl1pta51.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc2aaaaf8e3a31696f0ecf0908502d118223d0f9'}, {'y': 360, 'x': 640, 'u': 'https://preview.redd.it/1bw67dl1pta51.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f9b92156655a77dc3356946dd5e2a09451bba82'}, {'y': 540, 'x': 960, 'u': 'https://preview.redd.it/1bw67dl1pta51.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f751f69a45c00ae4212613efbe40ec64332114eb'}], 's': {'y': 576, 'x': 1024, 'u': 'https://preview.redd.it/1bw67dl1pta51.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=cacefd6bfcdfbe7bfc809a520acb69052c5ee476'}, 'id': '1bw67dl1pta51'}}",,,,
454,,pytorch,"I'm working on a cnn project and im trying to improve it.. i was told me how I augment my data can greatly influence my results. I was told to look into inception style data augmentation, but I can't find anything on. Does anyone know what that is and where I can find out about it, aswell as in any other augmentation techniques/styles?",t2_48egj5xw,False,,0,False,Data augmentationm,[],r/pytorch,False,6,,0,,,False,t3_hqgctg,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1594679767.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a cnn project and im trying to improve it.. i was told me how I augment my data can greatly influence my results. I was told to look into inception style data augmentation, but I can&amp;#39;t find anything on. Does anyone know what that is and where I can find out about it, aswell as in any other augmentation techniques/styles?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hqgctg,True,,stunbomb1,,1,True,all_ads,False,[],False,,/r/pytorch/comments/hqgctg/data_augmentationm/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hqgctg/data_augmentationm/,7135,1594650967.0,0,,False,,,,,,,,
455,,pytorch,"&amp;#x200B;

https://preview.redd.it/9551rl4kkma51.jpg?width=900&amp;format=pjpg&amp;auto=webp&amp;s=f604784bcd8964efd6825727f8f917f93f21df52",t2_45ysx8lx,False,,0,False,[UNet] PyTorch implementation from Abhishek thakur's video,[],r/pytorch,False,6,,0,105.0,,False,t3_hqf3c5,False,dark,0.6,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/xFE1j8N53uMEECVFUR3ctFO0VNeL37J0Wkl8ZzGJ33U.jpg,False,,[],{},,,True,,1594675072.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/9551rl4kkma51.jpg?width=900&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f604784bcd8964efd6825727f8f917f93f21df52""&gt;https://preview.redd.it/9551rl4kkma51.jpg?width=900&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f604784bcd8964efd6825727f8f917f93f21df52&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hqf3c5,True,,spctr77,,1,True,all_ads,False,[],False,,/r/pytorch/comments/hqf3c5/unet_pytorch_implementation_from_abhishek_thakurs/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hqf3c5/unet_pytorch_implementation_from_abhishek_thakurs/,7135,1594646272.0,0,,False,,,,"{'9551rl4kkma51': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 81, 'x': 108, 'u': 'https://preview.redd.it/9551rl4kkma51.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4e44e15959c49915c40f38d4d05b4ba2042d27c'}, {'y': 162, 'x': 216, 'u': 'https://preview.redd.it/9551rl4kkma51.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ac7aafe9c3be9515063851f2543fb46808777e1'}, {'y': 240, 'x': 320, 'u': 'https://preview.redd.it/9551rl4kkma51.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d0d37848b14ebab985e0d14f4876a1c916fb7bc0'}, {'y': 480, 'x': 640, 'u': 'https://preview.redd.it/9551rl4kkma51.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=20b59a44dd6428cf4665e9a9f00d394836c26a62'}], 's': {'y': 675, 'x': 900, 'u': 'https://preview.redd.it/9551rl4kkma51.jpg?width=900&amp;format=pjpg&amp;auto=webp&amp;s=f604784bcd8964efd6825727f8f917f93f21df52'}, 'id': '9551rl4kkma51'}}",,,,
456,,pytorch," For my project , i’m trying to predict the ratings that a user will give to an unseen movie, based on the ratings he gave to other movies. I’m using the **movielens dataset** .The Main folder, which is ml-100k contains informations about **100,000 movies** .

Before processing the data, the main data (ratings data) contains **user ID, movie ID, user rating** from 0 to 5 and **timestamps** (not considered for this project).I then split the data into **Training set(80%) and test data(20%) using sklearn Library.**

To create the recommendation systems, the model ‘ **Stacked-Autoencoder** ’ is being used. I’m using **PyTorch** and the **code is implemented on Google Colab** . The project is based on this [https://towardsdatascience.com/stacked-auto-encoder-as-a-recommendation-system-for-movie-rating-prediction-33842386338](https://towardsdatascience.com/stacked-auto-encoder-as-a-recommendation-system-for-movie-rating-prediction-33842386338)

I’m new to deep Learning and I want to compare this model(Stacked\_Autoencoder) to another Deep Learning model. For Instance,I want to use **Multilayer Perception(MLP)** . This is for research purposes. This is the code below for creating Stacked-Autoencoder model and training the model.

    ### Part 1 : Archirecture of the AutoEncoder 
    
    #nn.Module is a parent class 
    # SAE is a child class of the parent class nn.Module
    class SAE(nn.Module): 
    # self is the object of the SAE class 
    
    # Archirecture 
        def __init__(self, ): 
        # self can use alll the methods of the class nn.Module
            super(SAE,self).__init__()
        # Full connected layer  n°1, input and 20 neurons-nodes of the first layer
        # one neuron can be the genre of the movie
        
        # Encode step 
            self.fc1 = nn.Linear(nb_movies,20)
        # Full connected layer n°2 
            self.fc2 = nn.Linear(20,10)
        
        # Decode step 
        # Full connected layer n°3
            self.fc3 = nn.Linear(10,20) 
        # Full connected layer n°4
            self.fc4 = nn.Linear(20,nb_movies) 
        # Sigmoid activation function 
            self.activation = nn.Sigmoid()
    
    # Action : activation of the neurons
    def forward(self, x) : 
            x = self.activation(self.fc1(x))
            x = self.activation(self.fc2(x))
            x = self.activation(self.fc3(x))
            # dont's use the activation function 
            # use the linear function only 
            x = self.fc4(x)
            # x is th evector of predicted ratings
            return x 
    
    # Create the AutoEncoder object 
    sae=SAE()
    #MSE Loss : imported from torch.nn 
    criterion=nn.MSELoss() 
    # RMSProp optimizer (update the weights) imported from torch.optim 
    #sea.parameters() are weights and bias adjusted during the training
    optimizer=optim.RMSProp(sae.parameters(),lr=0.01, weight_decay=0.5)
    
    ### Part 2 : Training of the SAE 
    # number of epochs 
    nb_epochs = 200 
    # Epoch forloop 
    for epoch in range(1, nb_epoch+1): 
            # at the beginning the loss is at zero
            s=0.
            train_loss = 0 
    
            #Users forloop 
            for id_user in range(nb_users)
                # add one dimension to make a two dimension vector.
                # create a new dimension and put it the first position .unsqueeze[0]
                input = Variable(training_set[id_user].unsqueeze[0])
                
                # clone the input to obtain the target  
                target= input.clone()
                
                # target.data are all the ratings 
                # ratings &gt; 0
                if torch.sum(target.data &gt;0) &gt; 0
                    output = sae(input)
                    # don't compute the gradients regarding the target
                    target.require_grad=False 
                    # only deal with true ratings 
                    output[target==0]=0
                    
                    # Loss Criterion 
                    loss =criterion(output,target)
                    
                    # Average the error of the movies that don't have zero ratings
                    mean_corrector=nb_movies/float(torch.sum(target.data&gt;0)+1e-10)
                    
                    # Direction of the backpropagation 
                    loss.backward()
                    train_loss+=np.sqrt(loss.data[0]*mean_corrector)
                    s+=1.
                    
                    # Intensity of the backpropagation 
                    optimizer.step()
            
        print('epoch:' +str (epoch)+'loss:' +str(train_loss/s)

If I want to train using the MLP model. How can I implement this class model? Also, What other deep learning model(Beside MLP) that I can use to compare with Stacked-Autoencoder?

Thanks.",t2_78bby4nc,False,,0,False,Creating MLP model to predict the ratings that a user will give to an unseen movie using PyTorch,[],r/pytorch,False,6,,0,,,False,t3_hoz011,False,dark,0.63,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1594450175.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For my project , i’m trying to predict the ratings that a user will give to an unseen movie, based on the ratings he gave to other movies. I’m using the &lt;strong&gt;movielens dataset&lt;/strong&gt; .The Main folder, which is ml-100k contains informations about &lt;strong&gt;100,000 movies&lt;/strong&gt; .&lt;/p&gt;

&lt;p&gt;Before processing the data, the main data (ratings data) contains &lt;strong&gt;user ID, movie ID, user rating&lt;/strong&gt; from 0 to 5 and &lt;strong&gt;timestamps&lt;/strong&gt; (not considered for this project).I then split the data into &lt;strong&gt;Training set(80%) and test data(20%) using sklearn Library.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To create the recommendation systems, the model ‘ &lt;strong&gt;Stacked-Autoencoder&lt;/strong&gt; ’ is being used. I’m using &lt;strong&gt;PyTorch&lt;/strong&gt; and the &lt;strong&gt;code is implemented on Google Colab&lt;/strong&gt; . The project is based on this &lt;a href=""https://towardsdatascience.com/stacked-auto-encoder-as-a-recommendation-system-for-movie-rating-prediction-33842386338""&gt;https://towardsdatascience.com/stacked-auto-encoder-as-a-recommendation-system-for-movie-rating-prediction-33842386338&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I’m new to deep Learning and I want to compare this model(Stacked_Autoencoder) to another Deep Learning model. For Instance,I want to use &lt;strong&gt;Multilayer Perception(MLP)&lt;/strong&gt; . This is for research purposes. This is the code below for creating Stacked-Autoencoder model and training the model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;### Part 1 : Archirecture of the AutoEncoder 

#nn.Module is a parent class 
# SAE is a child class of the parent class nn.Module
class SAE(nn.Module): 
# self is the object of the SAE class 

# Archirecture 
    def __init__(self, ): 
    # self can use alll the methods of the class nn.Module
        super(SAE,self).__init__()
    # Full connected layer  n°1, input and 20 neurons-nodes of the first layer
    # one neuron can be the genre of the movie

    # Encode step 
        self.fc1 = nn.Linear(nb_movies,20)
    # Full connected layer n°2 
        self.fc2 = nn.Linear(20,10)

    # Decode step 
    # Full connected layer n°3
        self.fc3 = nn.Linear(10,20) 
    # Full connected layer n°4
        self.fc4 = nn.Linear(20,nb_movies) 
    # Sigmoid activation function 
        self.activation = nn.Sigmoid()

# Action : activation of the neurons
def forward(self, x) : 
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.activation(self.fc3(x))
        # dont&amp;#39;s use the activation function 
        # use the linear function only 
        x = self.fc4(x)
        # x is th evector of predicted ratings
        return x 

# Create the AutoEncoder object 
sae=SAE()
#MSE Loss : imported from torch.nn 
criterion=nn.MSELoss() 
# RMSProp optimizer (update the weights) imported from torch.optim 
#sea.parameters() are weights and bias adjusted during the training
optimizer=optim.RMSProp(sae.parameters(),lr=0.01, weight_decay=0.5)

### Part 2 : Training of the SAE 
# number of epochs 
nb_epochs = 200 
# Epoch forloop 
for epoch in range(1, nb_epoch+1): 
        # at the beginning the loss is at zero
        s=0.
        train_loss = 0 

        #Users forloop 
        for id_user in range(nb_users)
            # add one dimension to make a two dimension vector.
            # create a new dimension and put it the first position .unsqueeze[0]
            input = Variable(training_set[id_user].unsqueeze[0])

            # clone the input to obtain the target  
            target= input.clone()

            # target.data are all the ratings 
            # ratings &amp;gt; 0
            if torch.sum(target.data &amp;gt;0) &amp;gt; 0
                output = sae(input)
                # don&amp;#39;t compute the gradients regarding the target
                target.require_grad=False 
                # only deal with true ratings 
                output[target==0]=0

                # Loss Criterion 
                loss =criterion(output,target)

                # Average the error of the movies that don&amp;#39;t have zero ratings
                mean_corrector=nb_movies/float(torch.sum(target.data&amp;gt;0)+1e-10)

                # Direction of the backpropagation 
                loss.backward()
                train_loss+=np.sqrt(loss.data[0]*mean_corrector)
                s+=1.

                # Intensity of the backpropagation 
                optimizer.step()

    print(&amp;#39;epoch:&amp;#39; +str (epoch)+&amp;#39;loss:&amp;#39; +str(train_loss/s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If I want to train using the MLP model. How can I implement this class model? Also, What other deep learning model(Beside MLP) that I can use to compare with Stacked-Autoencoder?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/1MpAk_aPBKFCEkvnQ2Ioh0SkD5ZrgG2llr2MNV4nHIc.jpg?auto=webp&amp;s=06eb8232e0fe01d722e33200c4aa455164fa891e', 'width': 601, 'height': 407}, 'resolutions': [{'url': 'https://external-preview.redd.it/1MpAk_aPBKFCEkvnQ2Ioh0SkD5ZrgG2llr2MNV4nHIc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=791b1c01fc308eaf76663b3f5499413dc312c9e5', 'width': 108, 'height': 73}, {'url': 'https://external-preview.redd.it/1MpAk_aPBKFCEkvnQ2Ioh0SkD5ZrgG2llr2MNV4nHIc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=690bc6b4f017576cb25c399a9139f49bccd2ae18', 'width': 216, 'height': 146}, {'url': 'https://external-preview.redd.it/1MpAk_aPBKFCEkvnQ2Ioh0SkD5ZrgG2llr2MNV4nHIc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc55554d9864aacc276e6b0bdb87f54ec12f1471', 'width': 320, 'height': 216}], 'variants': {}, 'id': 'SjwjLFk6GsWOjdZgEXl_4kMeYd2_81OHptoFevo5EMI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hoz011,True,,Coat_Responsible,,1,True,all_ads,False,[],False,,/r/pytorch/comments/hoz011/creating_mlp_model_to_predict_the_ratings_that_a/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hoz011/creating_mlp_model_to_predict_the_ratings_that_a/,7135,1594421375.0,0,,False,,,,,,,,
457,,pytorch,"Training with artificial images is becoming increasingly important to address the lack of real data sets in various niche areas. Yet, many today’s approaches write 2D/3D simulations from scratch.

To improve this situation and make better use of existing pipelines, we’ve been working towards an integration between Blender, an open-source real-time physics enabled animation software, and PyTorch. 

Today we announce **blendtorch**, an open-source Python library that seamlessly integrates distributed Blender renderings into PyTorch data pipelines at 60FPS (640x480 RGBA).

 [https://github.com/cheind/pytorch-blender](https://github.com/cheind/pytorch-blender) 

[Batch visualization from 4 Blender instances running a physics enabled falling cubes scene. Images and annotations \(corner pixels\) are streamed into PyTorch dataset. A Dataloader then collates individual samples and generates images as shown.](https://preview.redd.it/drlsddhvzt951.png?width=960&amp;format=png&amp;auto=webp&amp;s=dd96a9ff7dbf8a002a0d176bf2f8ce013f3ebfcd)",t2_2cm37mcd,False,,0,False,blendtorch: seamless PyTorch - Blender integration,[],r/pytorch,False,6,,0,105.0,,False,t3_ho2vcp,False,dark,1.0,,public,32,0,{},140.0,,False,[],,False,False,,{},,False,32,,False,https://b.thumbs.redditmedia.com/USsSluuBA7pA2_zF2JydXutXiIHPzB-FPXtC_qRdqHk.jpg,False,,[],{},self,,True,,1594329306.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Training with artificial images is becoming increasingly important to address the lack of real data sets in various niche areas. Yet, many today’s approaches write 2D/3D simulations from scratch.&lt;/p&gt;

&lt;p&gt;To improve this situation and make better use of existing pipelines, we’ve been working towards an integration between Blender, an open-source real-time physics enabled animation software, and PyTorch. &lt;/p&gt;

&lt;p&gt;Today we announce &lt;strong&gt;blendtorch&lt;/strong&gt;, an open-source Python library that seamlessly integrates distributed Blender renderings into PyTorch data pipelines at 60FPS (640x480 RGBA).&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/cheind/pytorch-blender""&gt;https://github.com/cheind/pytorch-blender&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/drlsddhvzt951.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dd96a9ff7dbf8a002a0d176bf2f8ce013f3ebfcd""&gt;Batch visualization from 4 Blender instances running a physics enabled falling cubes scene. Images and annotations (corner pixels) are streamed into PyTorch dataset. A Dataloader then collates individual samples and generates images as shown.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/bhsaEyy-Nl3_4eSsPWRwFDUATrQ1fDcsYC8CGuJji10.jpg?auto=webp&amp;s=ec51a9ad53a549d1d1c16e2d140a814e10df4da1', 'width': 358, 'height': 358}, 'resolutions': [{'url': 'https://external-preview.redd.it/bhsaEyy-Nl3_4eSsPWRwFDUATrQ1fDcsYC8CGuJji10.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f64017958a16f3e88bd5ceab43e696b63b8cd04', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/bhsaEyy-Nl3_4eSsPWRwFDUATrQ1fDcsYC8CGuJji10.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44f708723c2b0224c29430dbcce41b083908846c', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/bhsaEyy-Nl3_4eSsPWRwFDUATrQ1fDcsYC8CGuJji10.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0efc91b878dfeabece042d6a2e9c8a24ec9d3d44', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'YB9bJGYXKwYzklRmc0A942TXn48WyZIo2CAoqvTrsPQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ho2vcp,True,,chrisheind,,10,True,all_ads,False,[],False,,/r/pytorch/comments/ho2vcp/blendtorch_seamless_pytorch_blender_integration/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ho2vcp/blendtorch_seamless_pytorch_blender_integration/,7135,1594300506.0,5,,False,,,,"{'drlsddhvzt951': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 81, 'x': 108, 'u': 'https://preview.redd.it/drlsddhvzt951.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c83f7770bc82ca2ae8f19c4831162fc800928dad'}, {'y': 162, 'x': 216, 'u': 'https://preview.redd.it/drlsddhvzt951.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa565123995e8988f4adc414d4de9b56fb727a8c'}, {'y': 240, 'x': 320, 'u': 'https://preview.redd.it/drlsddhvzt951.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fad80f23a8e562ebbf14c2940b667d6df223ffc1'}, {'y': 480, 'x': 640, 'u': 'https://preview.redd.it/drlsddhvzt951.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c29ae3c2b527a27f5bbcd2b6942c52985676da15'}, {'y': 720, 'x': 960, 'u': 'https://preview.redd.it/drlsddhvzt951.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8284b02b2f41fed699b02272667deaae34268fa5'}], 's': {'y': 720, 'x': 960, 'u': 'https://preview.redd.it/drlsddhvzt951.png?width=960&amp;format=png&amp;auto=webp&amp;s=dd96a9ff7dbf8a002a0d176bf2f8ce013f3ebfcd'}, 'id': 'drlsddhvzt951'}}",,,,
458,,pytorch,,t2_44mbtmjy,False,,0,False,Latest from Adobe and UC Berkeley researchers: State of the art in deep image manipulation.,[],r/pytorch,False,6,,0,46.0,,False,t3_hoa9bv,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/iBQDL03JsDtHk3nP6shMObaqsSEVdSZed307gExK48U.jpg,False,,[],{},link,,False,,1594354015.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/mSHeIQeVhvCSDourK2p1t8dnuby35BgFr7IOqxrT63c.jpg?auto=webp&amp;s=78d142d4349cbc81ff17389810767c5ab2eb185d', 'width': 1100, 'height': 330}, 'resolutions': [{'url': 'https://external-preview.redd.it/mSHeIQeVhvCSDourK2p1t8dnuby35BgFr7IOqxrT63c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=77a022a35c96dfa20b664862d86f16f2ae425f46', 'width': 108, 'height': 32}, {'url': 'https://external-preview.redd.it/mSHeIQeVhvCSDourK2p1t8dnuby35BgFr7IOqxrT63c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=61d52c75792c1054b5dbc1538701c850a5c14457', 'width': 216, 'height': 64}, {'url': 'https://external-preview.redd.it/mSHeIQeVhvCSDourK2p1t8dnuby35BgFr7IOqxrT63c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=67fb2c2bd71be399bfc6790bba173d3a8468497d', 'width': 320, 'height': 96}, {'url': 'https://external-preview.redd.it/mSHeIQeVhvCSDourK2p1t8dnuby35BgFr7IOqxrT63c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7450420c14269d49731bcf1367f4e0a7c148e8f0', 'width': 640, 'height': 192}, {'url': 'https://external-preview.redd.it/mSHeIQeVhvCSDourK2p1t8dnuby35BgFr7IOqxrT63c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=201251aadc99d56efe97b7649cdbeb7c7d08beea', 'width': 960, 'height': 288}, {'url': 'https://external-preview.redd.it/mSHeIQeVhvCSDourK2p1t8dnuby35BgFr7IOqxrT63c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4db62cf0f23ebb2f9522190d7036334fb10ff76b', 'width': 1080, 'height': 324}], 'variants': {}, 'id': 'sDCKOGmWDZjyXZ5E1vBQPXREc8mu8ekeULHdBepedkc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hoa9bv,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hoa9bv/latest_from_adobe_and_uc_berkeley_researchers/,all_ads,False,/r/LatestInML/comments/hn9mlb/latest_from_adobe_and_uc_berkeley_researchers/,7135,1594325215.0,0,,False,/r/LatestInML/comments/hn9mlb/latest_from_adobe_and_uc_berkeley_researchers/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/dataset requests: [click here](https://www.catalyzex.com/paper/arxiv:2007.00653)\n\nhttps://preview.redd.it/wnnry2lbzj951.jpg?width=1572&amp;format=pjpg&amp;auto=webp&amp;s=532e659cdb22eb4579231a4cc8e758263cdfdd65\n\nThe key idea is to encode an image into two independent components and enforce that any swapped combination maps to a realistic image.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Latest from Adobe and UC Berkeley researchers: State of the art in deep image manipulation.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 42, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'wnnry2lbzj951': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 35, 'x': 108, 'u': 'https://preview.redd.it/wnnry2lbzj951.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6351a13c8bbae0316435399edc9e339cc45aecbe'}, {'y': 71, 'x': 216, 'u': 'https://preview.redd.it/wnnry2lbzj951.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a0eb3dbc6b4ffd901158a74bbd28028da19c3b'}, {'y': 105, 'x': 320, 'u': 'https://preview.redd.it/wnnry2lbzj951.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7534c73dbe1aa18d16fa07148ba84ffc63d3773e'}, {'y': 211, 'x': 640, 'u': 'https://preview.redd.it/wnnry2lbzj951.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7287406e0caf77d10147f6dfe077eeadf05dea9c'}, {'y': 317, 'x': 960, 'u': 'https://preview.redd.it/wnnry2lbzj951.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=995988803df580edbb281300cdcb5ee8351a8e2f'}, {'y': 357, 'x': 1080, 'u': 'https://preview.redd.it/wnnry2lbzj951.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4ef9272a21bd0f4b9dd8ea84dd43c47408b2ebee'}], 's': {'y': 520, 'x': 1572, 'u': 'https://preview.redd.it/wnnry2lbzj951.jpg?width=1572&amp;format=pjpg&amp;auto=webp&amp;s=532e659cdb22eb4579231a4cc8e758263cdfdd65'}, 'id': 'wnnry2lbzj951'}}, 'name': 't3_hn9mlb', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 46, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 46, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/lGsjWDIXd_s1rcVMYVpl-2bzyPcnjOMOklH-Hf7MA-c.jpg', 'edited': 1594328342.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1594207871.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/dataset requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2007.00653""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/wnnry2lbzj951.jpg?width=1572&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=532e659cdb22eb4579231a4cc8e758263cdfdd65""&gt;https://preview.redd.it/wnnry2lbzj951.jpg?width=1572&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=532e659cdb22eb4579231a4cc8e758263cdfdd65&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The key idea is to encode an image into two independent components and enforce that any swapped combination maps to a realistic image.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/mSHeIQeVhvCSDourK2p1t8dnuby35BgFr7IOqxrT63c.jpg?auto=webp&amp;s=78d142d4349cbc81ff17389810767c5ab2eb185d', 'width': 1100, 'height': 330}, 'resolutions': [{'url': 'https://external-preview.redd.it/mSHeIQeVhvCSDourK2p1t8dnuby35BgFr7IOqxrT63c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=77a022a35c96dfa20b664862d86f16f2ae425f46', 'width': 108, 'height': 32}, {'url': 'https://external-preview.redd.it/mSHeIQeVhvCSDourK2p1t8dnuby35BgFr7IOqxrT63c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=61d52c75792c1054b5dbc1538701c850a5c14457', 'width': 216, 'height': 64}, {'url': 'https://external-preview.redd.it/mSHeIQeVhvCSDourK2p1t8dnuby35BgFr7IOqxrT63c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=67fb2c2bd71be399bfc6790bba173d3a8468497d', 'width': 320, 'height': 96}, {'url': 'https://external-preview.redd.it/mSHeIQeVhvCSDourK2p1t8dnuby35BgFr7IOqxrT63c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7450420c14269d49731bcf1367f4e0a7c148e8f0', 'width': 640, 'height': 192}, {'url': 'https://external-preview.redd.it/mSHeIQeVhvCSDourK2p1t8dnuby35BgFr7IOqxrT63c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=201251aadc99d56efe97b7649cdbeb7c7d08beea', 'width': 960, 'height': 288}, {'url': 'https://external-preview.redd.it/mSHeIQeVhvCSDourK2p1t8dnuby35BgFr7IOqxrT63c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4db62cf0f23ebb2f9522190d7036334fb10ff76b', 'width': 1080, 'height': 324}], 'variants': {}, 'id': 'sDCKOGmWDZjyXZ5E1vBQPXREc8mu8ekeULHdBepedkc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'hn9mlb', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/hn9mlb/latest_from_adobe_and_uc_berkeley_researchers/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/hn9mlb/latest_from_adobe_and_uc_berkeley_researchers/', 'subreddit_subscribers': 6676, 'created_utc': 1594179071.0, 'num_crossposts': 14, 'media': None, 'is_video': False}]",t3_hn9mlb,,,,,
459,,pytorch,"I had a model in keras which has MSE loss and then I recreated the same model in pytorch. When I trained it in keras, it came to the minimum loss in 5 epochs. In case of pytorch I have completed over 20 epochs now.. The loss has just reached halfway there. I am thinking about changing the optimizer or criterion. Currently I am using SGD optimizer and MSE loss. And I have also saved the optimizer state dict and model parameters. So is it possible for me to change this for faster training? Also any suggestions on criterion and optimizer is appreciated",t2_4lkm36m5,False,,0,False,Can I change the criterion and optimizer for a model and continue training?,[],r/pytorch,False,6,,0,,,False,t3_ho4voi,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1594336624.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I had a model in keras which has MSE loss and then I recreated the same model in pytorch. When I trained it in keras, it came to the minimum loss in 5 epochs. In case of pytorch I have completed over 20 epochs now.. The loss has just reached halfway there. I am thinking about changing the optimizer or criterion. Currently I am using SGD optimizer and MSE loss. And I have also saved the optimizer state dict and model parameters. So is it possible for me to change this for faster training? Also any suggestions on criterion and optimizer is appreciated&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ho4voi,True,,Viper213567,,5,True,all_ads,False,[],False,,/r/pytorch/comments/ho4voi/can_i_change_the_criterion_and_optimizer_for_a/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ho4voi/can_i_change_the_criterion_and_optimizer_for_a/,7135,1594307824.0,0,,False,,,,,,,,
460,,pytorch,,t2_40d0zt4s,False,,0,False,How to Implement Convolutional Autoencoder in PyTorch with CUDA,[],r/pytorch,False,6,,0,87.0,,False,t3_hnyfll,False,dark,0.71,,public,4,0,{},140.0,,False,[],,False,False,,{},,False,4,,False,https://b.thumbs.redditmedia.com/ZMCap8b_ghSlPq0Tr3VxfUsB7s9924fW0eYkmX1xe-g.jpg,False,,[],{},link,,False,,1594306404.0,text,6,,,text,analyticsindiamag.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/_W2M4YsmKCIcFRaSJOoZPd2_wZDnpFRTj8fCqiRPfWI.jpg?auto=webp&amp;s=516ed4041611b74689b47a3d53c2bcb25d8de879', 'width': 1000, 'height': 624}, 'resolutions': [{'url': 'https://external-preview.redd.it/_W2M4YsmKCIcFRaSJOoZPd2_wZDnpFRTj8fCqiRPfWI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f32d1216822376e7af28513dfb4d398015c6d804', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/_W2M4YsmKCIcFRaSJOoZPd2_wZDnpFRTj8fCqiRPfWI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e02a08af472c3398178153a2b8da3d5f8b20a2f3', 'width': 216, 'height': 134}, {'url': 'https://external-preview.redd.it/_W2M4YsmKCIcFRaSJOoZPd2_wZDnpFRTj8fCqiRPfWI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8a60b08be1bca979d3c1d91d44cf3175c0265b35', 'width': 320, 'height': 199}, {'url': 'https://external-preview.redd.it/_W2M4YsmKCIcFRaSJOoZPd2_wZDnpFRTj8fCqiRPfWI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f8147dc31dddabb5aeb528dd750cd4b505065287', 'width': 640, 'height': 399}, {'url': 'https://external-preview.redd.it/_W2M4YsmKCIcFRaSJOoZPd2_wZDnpFRTj8fCqiRPfWI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=937cdadde0c201a4e0852f9e726d09d5dd09c120', 'width': 960, 'height': 599}], 'variants': {}, 'id': 'Fqo2e-bFbF3i_LIRgovTAvPUZ9mb9FHkDkzkZguOEAA'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hnyfll,True,,analyticsindiam,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hnyfll/how_to_implement_convolutional_autoencoder_in/,all_ads,False,https://analyticsindiamag.com/how-to-implement-convolutional-autoencoder-in-pytorch-with-cuda/,7135,1594277604.0,0,,False,https://analyticsindiamag.com/how-to-implement-convolutional-autoencoder-in-pytorch-with-cuda/,,,,,,,
461,,pytorch,[https://analyticsindiamag.com/hands-on-guide-to-implement-deep-autoencoder-in-pytorch-for-image-reconstruction/](https://analyticsindiamag.com/hands-on-guide-to-implement-deep-autoencoder-in-pytorch-for-image-reconstruction/),t2_40d0zt4s,False,,0,False,Hands-On Guide to Implement Deep Autoencoder in PyTorch,[],r/pytorch,False,6,,0,,,False,t3_hng5mw,False,dark,0.81,,public,12,0,{},,,False,[],,False,False,,{},,False,12,,False,self,False,,[],{},self,,True,,1594240062.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://analyticsindiamag.com/hands-on-guide-to-implement-deep-autoencoder-in-pytorch-for-image-reconstruction/""&gt;https://analyticsindiamag.com/hands-on-guide-to-implement-deep-autoencoder-in-pytorch-for-image-reconstruction/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/RO4gk_0YOa3jDFNNQGfla43ZP-53FssICBHnOfhpIQQ.jpg?auto=webp&amp;s=1401728e207d90bfa8b390ee014ccdfcc7f62b3f', 'width': 1920, 'height': 1280}, 'resolutions': [{'url': 'https://external-preview.redd.it/RO4gk_0YOa3jDFNNQGfla43ZP-53FssICBHnOfhpIQQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=07a46ce420c31c6f73a312ef40164ac82ac42d17', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/RO4gk_0YOa3jDFNNQGfla43ZP-53FssICBHnOfhpIQQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=51aaa5451200f7d89d5921c94d69998e01022895', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/RO4gk_0YOa3jDFNNQGfla43ZP-53FssICBHnOfhpIQQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc2c083000f25c1d7c1a114e3b8d5fad67c75a36', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/RO4gk_0YOa3jDFNNQGfla43ZP-53FssICBHnOfhpIQQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2724e2cb82bcbec65f9ec9ffe2c18f87fede9ef4', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/RO4gk_0YOa3jDFNNQGfla43ZP-53FssICBHnOfhpIQQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fa6f9dfff6b6bfe20935f94b96c8ccac9137e787', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/RO4gk_0YOa3jDFNNQGfla43ZP-53FssICBHnOfhpIQQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4c18d922d46838d98e13e9fe9da7832d695a0065', 'width': 1080, 'height': 720}], 'variants': {}, 'id': '_hgaylf9yLF-jyxqC1spy_8v2zSg2rQvXa5FuE7xh7I'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hng5mw,True,,analyticsindiam,,1,True,all_ads,False,[],False,,/r/pytorch/comments/hng5mw/handson_guide_to_implement_deep_autoencoder_in/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hng5mw/handson_guide_to_implement_deep_autoencoder_in/,7135,1594211262.0,0,,False,,,,,,,,
462,,pytorch,"Hi, everyone! 

I've a simple model setup as shown in the figure below.

Can you please help set weights manually for each layer given the current configuration?

(I want to use Xavier/Glorot Uniform initialization to compare the output with a counterpart TensorFlow model)

&amp;#x200B;

Thank you!

https://preview.redd.it/pgm3ju0s1o951.png?width=553&amp;format=png&amp;auto=webp&amp;s=df9c4c372ad67f227d900bcd25ccdc91ec57ade7",t2_4ecq42y4,False,,0,False,Initializing weights for all layers,[],r/pytorch,False,6,,0,109.0,,False,t3_hnl20u,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/4xWR4KsOohBSlPSeTDht-h5wjpj5AbgKOsevNI0ipkc.jpg,False,,[],{},,,True,,1594257262.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, everyone! &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve a simple model setup as shown in the figure below.&lt;/p&gt;

&lt;p&gt;Can you please help set weights manually for each layer given the current configuration?&lt;/p&gt;

&lt;p&gt;(I want to use Xavier/Glorot Uniform initialization to compare the output with a counterpart TensorFlow model)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/pgm3ju0s1o951.png?width=553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df9c4c372ad67f227d900bcd25ccdc91ec57ade7""&gt;https://preview.redd.it/pgm3ju0s1o951.png?width=553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df9c4c372ad67f227d900bcd25ccdc91ec57ade7&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hnl20u,True,,ncuxomun,,4,True,all_ads,False,[],False,,/r/pytorch/comments/hnl20u/initializing_weights_for_all_layers/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hnl20u/initializing_weights_for_all_layers/,7135,1594228462.0,0,,False,,,,"{'pgm3ju0s1o951': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 84, 'x': 108, 'u': 'https://preview.redd.it/pgm3ju0s1o951.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ab0bb5d126320ded03482e641d6f34b83761065'}, {'y': 169, 'x': 216, 'u': 'https://preview.redd.it/pgm3ju0s1o951.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9820afdab8a48fd6456a9dc0194d3ecb1e9f939'}, {'y': 251, 'x': 320, 'u': 'https://preview.redd.it/pgm3ju0s1o951.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=66ba3f984a345285f313b03a4a807ef7feb7f955'}], 's': {'y': 434, 'x': 553, 'u': 'https://preview.redd.it/pgm3ju0s1o951.png?width=553&amp;format=png&amp;auto=webp&amp;s=df9c4c372ad67f227d900bcd25ccdc91ec57ade7'}, 'id': 'pgm3ju0s1o951'}}",,,,
463,,pytorch,,t2_uoabgr3,False,,0,False,"Pytorch released free book - ""Deep Learning with Pytorch""",[],r/pytorch,False,6,,0,140.0,,False,t3_hmv2e0,False,dark,1.0,,public,60,0,{},140.0,,False,[],,False,False,,{},,False,60,,False,https://b.thumbs.redditmedia.com/ZlS3F0-kujM0wEOjkQQsE7OlIAzkNWNPoantWpSt06w.jpg,False,,[],{},link,,False,,1594159285.0,text,6,,,text,pytorch.org,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?auto=webp&amp;s=d0da5dbb7aff983a5173aba7d2bdd36fa28abe08', 'width': 2500, 'height': 2500}, 'resolutions': [{'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fef05ec37ddf39032979380cc7acc403e05834be', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e1de8e408f33ef3c93425e30e0bb5afffd5c631', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89562dde9cea0af5ebaaba50a9437e484d732513', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d6125bcd02c2fed8d84dac49cc8d73dbe205d81', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=40d7ca8d88566f27054acb632514012b5026acc9', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=757464a70e69532931dcbf0fbd397b9d433cf205', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'Mg0HAz_PmYkYWkaGTVyhCEzp15PpLW_TWWqmn9VNVFs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hmv2e0,True,,depiloda,,5,True,all_ads,False,[],False,,/r/pytorch/comments/hmv2e0/pytorch_released_free_book_deep_learning_with/,all_ads,False,https://pytorch.org/deep-learning-with-pytorch,7135,1594130485.0,0,,False,https://pytorch.org/deep-learning-with-pytorch,,,,,,,
464,,pytorch,"Lately I have been looking into web frameworks that allow me to deploy pytorch models in web applications, but I have not found any definitive answers yet on which is the most suitable. On the pytorch website a tutorial is provided using flask: [https://pytorch.org/tutorials/intermediate/flask\_rest\_api\_tutorial.html](https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html). However, a framework like django seems much more scalable. 

Does anyone have experience with this type of deployment and are there clear winners and losers when it comes to deep learning applications? Or does it just come down to preference in the end?",t2_76e7yl0p,False,,0,False,Most suitable web framework for production?,[],r/pytorch,False,6,,0,,,False,t3_hn9l8o,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1594207725.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Lately I have been looking into web frameworks that allow me to deploy pytorch models in web applications, but I have not found any definitive answers yet on which is the most suitable. On the pytorch website a tutorial is provided using flask: &lt;a href=""https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html""&gt;https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html&lt;/a&gt;. However, a framework like django seems much more scalable. &lt;/p&gt;

&lt;p&gt;Does anyone have experience with this type of deployment and are there clear winners and losers when it comes to deep learning applications? Or does it just come down to preference in the end?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hn9l8o,True,,TopOfEverest,,6,True,all_ads,False,[],False,,/r/pytorch/comments/hn9l8o/most_suitable_web_framework_for_production/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hn9l8o/most_suitable_web_framework_for_production/,7135,1594178925.0,0,,False,,,,,,,,
465,,pytorch,"Hey Everyone, 

We've been working away on a framework to eliminate or decrease racism in machine learning. Today we have a demo setup and first version release of the library. Would love if anyone has any vision problems classifying people to check out our library to balance out the dataset and keep it private. Any feedback on ways to improve are greatly appreciated. 

Demo Site: [https://privyfilter.herokuapp.com/](https://privyfilter.herokuapp.com/)

Repo: [https://github.com/Deamoner/privyfilter](https://github.com/Deamoner/privyfilter)

Architecture and Methodology Article: [https://medium.com/@mdavis\_71283/hard-problems-racial-bias-in-machine-learning-bf631c9d680c](https://medium.com/@mdavis_71283/hard-problems-racial-bias-in-machine-learning-bf631c9d680c)

&amp;#x200B;

Things to learn in this project: 

\- Creating a pipeline for new model creation - first create pipeline to generate dataset and then create model. 

\- 

Current Features:

\- Face Detection 

\- Demographic Information Extraction 

\- Synthetic Face Generation 

\- Face Swapping 

&amp;#x200B;

Future Features: 

\- Skin Detection 

\- Skin Hue Manipulation

\- Full Pipeline Process for the entire directory 

\- Multi-person photo support  

\- Remove Pipeline and Train pix2pix model 

&amp;#x200B;

This is an open source initiative, and open to any constructive feedback or even better actually getting your hands dirty by helping code. Looking for any feedback on ways we can improve it for any of your specific use cases.",t2_541pptuw,False,,0,False,Hard Problems: Eliminating Racism in Machine Learning,[],r/pytorch,False,6,,0,,,False,t3_hniap6,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1594248269.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey Everyone, &lt;/p&gt;

&lt;p&gt;We&amp;#39;ve been working away on a framework to eliminate or decrease racism in machine learning. Today we have a demo setup and first version release of the library. Would love if anyone has any vision problems classifying people to check out our library to balance out the dataset and keep it private. Any feedback on ways to improve are greatly appreciated. &lt;/p&gt;

&lt;p&gt;Demo Site: &lt;a href=""https://privyfilter.herokuapp.com/""&gt;https://privyfilter.herokuapp.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Repo: &lt;a href=""https://github.com/Deamoner/privyfilter""&gt;https://github.com/Deamoner/privyfilter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Architecture and Methodology Article: &lt;a href=""https://medium.com/@mdavis_71283/hard-problems-racial-bias-in-machine-learning-bf631c9d680c""&gt;https://medium.com/@mdavis_71283/hard-problems-racial-bias-in-machine-learning-bf631c9d680c&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Things to learn in this project: &lt;/p&gt;

&lt;p&gt;- Creating a pipeline for new model creation - first create pipeline to generate dataset and then create model. &lt;/p&gt;

&lt;p&gt;- &lt;/p&gt;

&lt;p&gt;Current Features:&lt;/p&gt;

&lt;p&gt;- Face Detection &lt;/p&gt;

&lt;p&gt;- Demographic Information Extraction &lt;/p&gt;

&lt;p&gt;- Synthetic Face Generation &lt;/p&gt;

&lt;p&gt;- Face Swapping &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Future Features: &lt;/p&gt;

&lt;p&gt;- Skin Detection &lt;/p&gt;

&lt;p&gt;- Skin Hue Manipulation&lt;/p&gt;

&lt;p&gt;- Full Pipeline Process for the entire directory &lt;/p&gt;

&lt;p&gt;- Multi-person photo support  &lt;/p&gt;

&lt;p&gt;- Remove Pipeline and Train pix2pix model &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;This is an open source initiative, and open to any constructive feedback or even better actually getting your hands dirty by helping code. Looking for any feedback on ways we can improve it for any of your specific use cases.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/NjKqUCmDfju3K6Z7_E7NbI4CSbQzqU0woWAIrXzSnxI.jpg?auto=webp&amp;s=7dc3f320a7987d3839956f871d1e6dd63d5c7abf', 'width': 1728, 'height': 2304}, 'resolutions': [{'url': 'https://external-preview.redd.it/NjKqUCmDfju3K6Z7_E7NbI4CSbQzqU0woWAIrXzSnxI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=96b62af9b321dcadd90a0f61f67a48565939ae33', 'width': 108, 'height': 144}, {'url': 'https://external-preview.redd.it/NjKqUCmDfju3K6Z7_E7NbI4CSbQzqU0woWAIrXzSnxI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=239f8e940176ddf6eeb9e705b6dc1b9c32957f4c', 'width': 216, 'height': 288}, {'url': 'https://external-preview.redd.it/NjKqUCmDfju3K6Z7_E7NbI4CSbQzqU0woWAIrXzSnxI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e56c1146db94bd7a1b05118f04f1e7f1152a1b7', 'width': 320, 'height': 426}, {'url': 'https://external-preview.redd.it/NjKqUCmDfju3K6Z7_E7NbI4CSbQzqU0woWAIrXzSnxI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e6e2719dc31e10f0ccf1c631f6891c9e2651f71', 'width': 640, 'height': 853}, {'url': 'https://external-preview.redd.it/NjKqUCmDfju3K6Z7_E7NbI4CSbQzqU0woWAIrXzSnxI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=279235a76b85d4656bf863f8dc60a9889a1f5443', 'width': 960, 'height': 1280}, {'url': 'https://external-preview.redd.it/NjKqUCmDfju3K6Z7_E7NbI4CSbQzqU0woWAIrXzSnxI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0d0612cb30fc35fca548031dffaa786f1ae13ddb', 'width': 1080, 'height': 1440}], 'variants': {}, 'id': 'IASxc4CqdFw-JR9X83jPUwWlf60yjiFbuXhQth3terQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hniap6,True,,namenomatter85,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hniap6/hard_problems_eliminating_racism_in_machine/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hniap6/hard_problems_eliminating_racism_in_machine/,7135,1594219469.0,0,,False,,,,,,,,
466,,pytorch,,t2_3itl62k7,False,,0,False,[Project] cflearn - a minimal Automatic Machine Learning (AutoML) solution for tabular datasets based on PyTorch,[],r/pytorch,False,6,,0,78.0,,False,t3_hm6a4v,False,dark,1.0,,public,17,0,{},140.0,,False,[],"{'reddit_video': {'fallback_url': 'https://v.redd.it/jxojqzg0c6951/DASH_1080?source=fallback', 'height': 1080, 'width': 1920, 'scrubber_media_url': 'https://v.redd.it/jxojqzg0c6951/DASH_96', 'dash_url': 'https://v.redd.it/jxojqzg0c6951/DASHPlaylist.mpd?a=1618044220%2CZjJkNmVlMGI4YWRhMjUzOThkYmJjOWM2ZmYxNzJlOGFkYjFjODBiNTM1ZDdjOWQ2ZDA0MzJkOGQ4OGE1ZDNiOA%3D%3D&amp;v=1&amp;f=sd', 'duration': 89, 'hls_url': 'https://v.redd.it/jxojqzg0c6951/HLSPlaylist.m3u8?a=1618044220%2CZDczZDI0NzQ1NTYyNzgwNmZkN2QxMzZmYWRhYWFlNzIwYzE2ZmY4YjExYmE0ZGQ5MDQ2NTMxNGEzNTA4OTQ3Zg%3D%3D&amp;v=1&amp;f=sd', 'is_gif': False, 'transcoding_status': 'completed'}}",True,False,,{},,False,17,,False,https://b.thumbs.redditmedia.com/nC7W70xOm4fzHwMWxn24G6dVyXf_6xO4cEBfASpS0_s.jpg,False,,[],{},hosted:video,,False,,1594065374.0,text,6,,,text,v.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/0rH8oU5BRcqziWDDu1aNoF_FNjzmsXvq0htwpDCcapg.png?format=pjpg&amp;auto=webp&amp;s=2fa7f109ba27fe24882af6e157202ff7afae6571', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://external-preview.redd.it/0rH8oU5BRcqziWDDu1aNoF_FNjzmsXvq0htwpDCcapg.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9be716ef820c2da329adb27032099facda10a500', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/0rH8oU5BRcqziWDDu1aNoF_FNjzmsXvq0htwpDCcapg.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c0b8d2bb36801f73ff1da460f18e5a1e69a59fcc', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/0rH8oU5BRcqziWDDu1aNoF_FNjzmsXvq0htwpDCcapg.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a9651bcd507ca72568fc764175736e67b8bb901f', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/0rH8oU5BRcqziWDDu1aNoF_FNjzmsXvq0htwpDCcapg.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=eb4f06a9ca866410a151fd0ab901c605ffb96192', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/0rH8oU5BRcqziWDDu1aNoF_FNjzmsXvq0htwpDCcapg.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=58796b42741dd771a90cf56373e0a7ea4c2dca56', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/0rH8oU5BRcqziWDDu1aNoF_FNjzmsXvq0htwpDCcapg.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1c244672f170f7355e10eb25711456ca35a14214', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'KUDy033hACjf1I1jj0fAeFz4eRIoCWeTwT9cjqwh9CU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hm6a4v,True,,carefree0910,,5,True,all_ads,False,[],False,,/r/pytorch/comments/hm6a4v/project_cflearn_a_minimal_automatic_machine/,all_ads,False,https://v.redd.it/jxojqzg0c6951,7135,1594036574.0,0,"{'reddit_video': {'fallback_url': 'https://v.redd.it/jxojqzg0c6951/DASH_1080?source=fallback', 'height': 1080, 'width': 1920, 'scrubber_media_url': 'https://v.redd.it/jxojqzg0c6951/DASH_96', 'dash_url': 'https://v.redd.it/jxojqzg0c6951/DASHPlaylist.mpd?a=1618044220%2CZjJkNmVlMGI4YWRhMjUzOThkYmJjOWM2ZmYxNzJlOGFkYjFjODBiNTM1ZDdjOWQ2ZDA0MzJkOGQ4OGE1ZDNiOA%3D%3D&amp;v=1&amp;f=sd', 'duration': 89, 'hls_url': 'https://v.redd.it/jxojqzg0c6951/HLSPlaylist.m3u8?a=1618044220%2CZDczZDI0NzQ1NTYyNzgwNmZkN2QxMzZmYWRhYWFlNzIwYzE2ZmY4YjExYmE0ZGQ5MDQ2NTMxNGEzNTA4OTQ3Zg%3D%3D&amp;v=1&amp;f=sd', 'is_gif': False, 'transcoding_status': 'completed'}}",True,https://v.redd.it/jxojqzg0c6951,,,,,,,
467,,pytorch,[https://analyticsindiamag.com/hands-on-guide-to-implement-resnet50-in-pytorch-with-tpu/](https://analyticsindiamag.com/hands-on-guide-to-implement-resnet50-in-pytorch-with-tpu/),t2_40d0zt4s,False,,0,False,Hands-On Guide to Implement ResNet50 in PyTorch with TPU,[],r/pytorch,False,6,,0,,,False,t3_hm2z3q,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},self,,True,,1594048386.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://analyticsindiamag.com/hands-on-guide-to-implement-resnet50-in-pytorch-with-tpu/""&gt;https://analyticsindiamag.com/hands-on-guide-to-implement-resnet50-in-pytorch-with-tpu/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/k32HP648RBIatQbMOl3V_J7HtOfv13mhzMSEAXynZUk.jpg?auto=webp&amp;s=fb2011bbd210df9b8eee89ca4a4f785e3859f0e1', 'width': 720, 'height': 412}, 'resolutions': [{'url': 'https://external-preview.redd.it/k32HP648RBIatQbMOl3V_J7HtOfv13mhzMSEAXynZUk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=56c95e381c2daa6a0d11a8b3cf552e96cf252ff4', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/k32HP648RBIatQbMOl3V_J7HtOfv13mhzMSEAXynZUk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=767367646c744b38ca78bd08655755f9cbd7a517', 'width': 216, 'height': 123}, {'url': 'https://external-preview.redd.it/k32HP648RBIatQbMOl3V_J7HtOfv13mhzMSEAXynZUk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27060d58849da65dbcf3c4928fb34934eddc7ff3', 'width': 320, 'height': 183}, {'url': 'https://external-preview.redd.it/k32HP648RBIatQbMOl3V_J7HtOfv13mhzMSEAXynZUk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=077aae1353497757558a77ecff2069fc0ed9ce66', 'width': 640, 'height': 366}], 'variants': {}, 'id': 'ouKpZD99MThxnuMczVYlAGmIO92p8FgQnjNzO3nP4Fg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hm2z3q,True,,analyticsindiam,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hm2z3q/handson_guide_to_implement_resnet50_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hm2z3q/handson_guide_to_implement_resnet50_in_pytorch/,7135,1594019586.0,0,,False,,,,,,,,
468,,pytorch,"I'm trying to train a deep learning model without loading the entire dataset into memory. My main question is, what's the best way of doing this?

It seems like HDF5 is a common method that people accomplish this, and is what I tried first. However when using pytorch's dataloader class, this ran extremely slowly. I created my own iterator which ran faster, however the data is not randomized every batch. I'm trying to understand why the pytorch dataloader is running slowly and if there is something I can do about it.


Below is my code

First I defined a dataset class that takes in a filepath to an HDF5 dataset. My understanding of this code is that it reads from disk whenever __getitem__ is called.



    class My_H5Dataset(torch.utils.data.Dataset):

    def __init__(self, file_path):
        super(My_H5Dataset, self).__init__()
        h5_file = h5py.File(file_path , 'r')
        self.features = h5_file['features']
        self.labels = h5_file['labels']
        self.index = h5_file['index']
        self.labels_values = h5_file['labels_values']
        self.index_values = h5_file['index_values']
        

    def __getitem__(self, index): 
        

        return (torch.from_numpy(self.features[index,:]).float(),
                torch.from_numpy(self.labels[index,:]).float(),
               torch.from_numpy(self.index[index,:]).float(),
               torch.from_numpy(np.array(self.labels_values[index])),
               torch.from_numpy(np.array(self.index_values[index])))

    def __len__(self):
        return self.features.shape[0]


Then I simply pass this into a pytorch dataloader as follows



    train_dataset = My_H5Dataset(hdf5_data_folder_train)
    train_ms = MySampler(train_dataset)
    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, 
    sampler=train_ms,num_workers=2)



My other method was to manually define an iterator. And this does run much faster. However I have not noticed a speed difference between the data stored in an HDD versus an SSD which makes me worried that there is a bottleneck somewhere that I am missing.

```
def hdf5_loader_generator(dataset, batch_size, as_tensor=True, n_samples = 10000):
    """"""Given an h5 path to a file that holds the arrays, returns a generator
    that can get certain data at a time.""""""
    stop = n_samples
    curr_index = start = 0
    while 1:
        stop_index = min([curr_index + batch_size, stop])
        ft, sp, index, sp_values, index_values = dataset[curr_index:stop_index]
        curr_index += batch_size
        if curr_index &gt;= stop:
            curr_index = start
            continue

        yield ft, sp, index, sp_values, index_values
        
def hdf5_data_iterator(dataset, batch_size, as_tensor=True):
    return iter(hdf5_loader_generator(dataset, batch_size, as_tensor))

```

I timed pytorch's iterator as follows:


```
start = time.time()
for i, data in enumerate(trainloader, 0):
    ft, sp, index, sp_values, index_values = data
    ft, sp, index = ft.to(device), sp.to(device), index.to(device)
    if i &gt; 500:
        break
        
end = time.time()

print(end-start)

```

I timed my iterator as follows:

```
train_dataset = My_H5Dataset(hdf5_data_folder_train)
samples = hdf5_data_iterator(train_dataset, batch_sizes)
start = time.time()
for i, data in enumerate(samples):
    ft, sp, index, sp_values, index_values = data
    ft, sp, index = ft.to(device), sp.to(device), index.to(device)
    if i &gt; 500:
        break
    
end = time.time()

print(end-start)
```


Is there a better approach than what I am currently trying?

Thank you for the help!",t2_10mp017i,False,,0,False,What's the best way to use HDF5 data in a dataloader with pytorch?,[],r/pytorch,False,6,,0,,,False,t3_hlrjxg,False,dark,1.0,,public,8,0,{},,,False,[],,False,False,,{},,False,8,,False,self,1593974241.0,,[],{},,,True,,1594002737.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to train a deep learning model without loading the entire dataset into memory. My main question is, what&amp;#39;s the best way of doing this?&lt;/p&gt;

&lt;p&gt;It seems like HDF5 is a common method that people accomplish this, and is what I tried first. However when using pytorch&amp;#39;s dataloader class, this ran extremely slowly. I created my own iterator which ran faster, however the data is not randomized every batch. I&amp;#39;m trying to understand why the pytorch dataloader is running slowly and if there is something I can do about it.&lt;/p&gt;

&lt;p&gt;Below is my code&lt;/p&gt;

&lt;p&gt;First I defined a dataset class that takes in a filepath to an HDF5 dataset. My understanding of this code is that it reads from disk whenever &lt;strong&gt;getitem&lt;/strong&gt; is called.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class My_H5Dataset(torch.utils.data.Dataset):

def __init__(self, file_path):
    super(My_H5Dataset, self).__init__()
    h5_file = h5py.File(file_path , &amp;#39;r&amp;#39;)
    self.features = h5_file[&amp;#39;features&amp;#39;]
    self.labels = h5_file[&amp;#39;labels&amp;#39;]
    self.index = h5_file[&amp;#39;index&amp;#39;]
    self.labels_values = h5_file[&amp;#39;labels_values&amp;#39;]
    self.index_values = h5_file[&amp;#39;index_values&amp;#39;]


def __getitem__(self, index): 


    return (torch.from_numpy(self.features[index,:]).float(),
            torch.from_numpy(self.labels[index,:]).float(),
           torch.from_numpy(self.index[index,:]).float(),
           torch.from_numpy(np.array(self.labels_values[index])),
           torch.from_numpy(np.array(self.index_values[index])))

def __len__(self):
    return self.features.shape[0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then I simply pass this into a pytorch dataloader as follows&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_dataset = My_H5Dataset(hdf5_data_folder_train)
train_ms = MySampler(train_dataset)
trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, 
sampler=train_ms,num_workers=2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My other method was to manually define an iterator. And this does run much faster. However I have not noticed a speed difference between the data stored in an HDD versus an SSD which makes me worried that there is a bottleneck somewhere that I am missing.&lt;/p&gt;

&lt;p&gt;```
def hdf5_loader_generator(dataset, batch_size, as_tensor=True, n_samples = 10000):
    &amp;quot;&amp;quot;&amp;quot;Given an h5 path to a file that holds the arrays, returns a generator
    that can get certain data at a time.&amp;quot;&amp;quot;&amp;quot;
    stop = n_samples
    curr_index = start = 0
    while 1:
        stop_index = min([curr_index + batch_size, stop])
        ft, sp, index, sp_values, index_values = dataset[curr_index:stop_index]
        curr_index += batch_size
        if curr_index &amp;gt;= stop:
            curr_index = start
            continue&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    yield ft, sp, index, sp_values, index_values
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;def hdf5_data_iterator(dataset, batch_size, as_tensor=True):
    return iter(hdf5_loader_generator(dataset, batch_size, as_tensor))&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;I timed pytorch&amp;#39;s iterator as follows:&lt;/p&gt;

&lt;p&gt;```
start = time.time()
for i, data in enumerate(trainloader, 0):
    ft, sp, index, sp_values, index_values = data
    ft, sp, index = ft.to(device), sp.to(device), index.to(device)
    if i &amp;gt; 500:
        break&lt;/p&gt;

&lt;p&gt;end = time.time()&lt;/p&gt;

&lt;p&gt;print(end-start)&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;I timed my iterator as follows:&lt;/p&gt;

&lt;p&gt;```
train_dataset = My_H5Dataset(hdf5_data_folder_train)
samples = hdf5_data_iterator(train_dataset, batch_sizes)
start = time.time()
for i, data in enumerate(samples):
    ft, sp, index, sp_values, index_values = data
    ft, sp, index = ft.to(device), sp.to(device), index.to(device)
    if i &amp;gt; 500:
        break&lt;/p&gt;

&lt;p&gt;end = time.time()&lt;/p&gt;

&lt;p&gt;print(end-start)
```&lt;/p&gt;

&lt;p&gt;Is there a better approach than what I am currently trying?&lt;/p&gt;

&lt;p&gt;Thank you for the help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hlrjxg,True,,eclectic_scientist,,8,True,all_ads,False,[],False,,/r/pytorch/comments/hlrjxg/whats_the_best_way_to_use_hdf5_data_in_a/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hlrjxg/whats_the_best_way_to_use_hdf5_data_in_a/,7135,1593973937.0,0,,False,,,,,,,,
469,,pytorch,,t2_57s4p771,False,,0,False,Interested in nonlinear time series prediction with NNs? Check out how LSTM nets can be used in the forecasting of chaotic dynamical systems.,[],r/pytorch,False,6,,0,,,False,t3_hl0a78,False,dark,0.92,,public,10,0,{},,,False,[],,False,False,,{},,False,10,,False,default,False,,[],{},,,False,,1593881758.0,text,6,,,text,researchgate.net,False,,,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hl0a78,True,,_Mat_San_,,2,True,all_ads,False,[],False,,/r/pytorch/comments/hl0a78/interested_in_nonlinear_time_series_prediction/,all_ads,False,https://www.researchgate.net/publication/342569402_Robustness_of_LSTM_neural_networks_for_multi-step_forecasting_of_chaotic_time_series,7135,1593852958.0,0,,False,https://www.researchgate.net/publication/342569402_Robustness_of_LSTM_neural_networks_for_multi-step_forecasting_of_chaotic_time_series,,,,,,,
470,,pytorch,"I am planning to use Data Loader to load a csv data for training a model. When I loaded csv using dataloader and tried to iterate through it, it is showing a key error and I have no idea what that is now",t2_4lkm36m5,False,,0,False,"""[Discussion]"" How to load csv data using DataLoader?",[],r/pytorch,False,6,,0,,,False,t3_hl4eyk,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1593874052.0,,[],{},,,True,,1593902664.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am planning to use Data Loader to load a csv data for training a model. When I loaded csv using dataloader and tried to iterate through it, it is showing a key error and I have no idea what that is now&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hl4eyk,True,,Viper213567,,3,True,all_ads,False,[],False,,/r/pytorch/comments/hl4eyk/discussion_how_to_load_csv_data_using_dataloader/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hl4eyk/discussion_how_to_load_csv_data_using_dataloader/,7135,1593873864.0,0,,False,,,,,,,,
471,,pytorch,Does anyone in this sub has any experience with porting PyTorch DNNs/CNNs after compression/optimization on Android? I need some help.,t2_r23e1g3,False,,0,False,Inference on Android,[],r/pytorch,False,6,,0,,,False,t3_hl1dsn,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1593888265.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone in this sub has any experience with porting PyTorch DNNs/CNNs after compression/optimization on Android? I need some help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hl1dsn,True,,tranquil_af,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hl1dsn/inference_on_android/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hl1dsn/inference_on_android/,7135,1593859465.0,0,,False,,,,,,,,
472,,pytorch,,t2_44mbtmjy,False,,0,False,"A browser extension that automatically finds code implementations for machine learning papers anywhere on the web (Google, Arxiv, Twitter, Scholar, and other sites)!",[],r/pytorch,False,6,,0,,,False,t3_hkrj9v,False,dark,0.88,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,default,False,,[],{},link,,False,,1593841226.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/wNOK4uFSjn0_VLV9GzzwcWxl8N-Cjs1NgKKI98QDVv8.jpg?auto=webp&amp;s=6d2c995ff6e0e752289c28367c6de1532772648d', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/wNOK4uFSjn0_VLV9GzzwcWxl8N-Cjs1NgKKI98QDVv8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d81abbb07ddce350d7a7603a6d590802e9aa0ca', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'I-n5sa7QCjcBfo5kRd7bssElIH2v7WE5U8qgV0NDZuI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hkrj9v,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hkrj9v/a_browser_extension_that_automatically_finds_code/,all_ads,False,/r/LatestInML/comments/hkrcdd/a_browser_extension_that_automatically_finds_code/,7135,1593812426.0,0,,False,/r/LatestInML/comments/hkrcdd/a_browser_extension_that_automatically_finds_code/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'ICYMI: A browser extension that automatically finds code implementations for machine learning papers anywhere on the web (Google, Arxiv, Twitter, Scholar, and other sites)! \n\nhttps://chrome.google.com/webstore/detail/mlai-code-implementation/aikkeehnlfpamidigaffhfmgbkdeheil \n\nhttps://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/\n\nFeedback and requests are very welcome! :)', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'A browser extension that automatically finds code implementations for machine learning papers anywhere on the web (Google, Arxiv, Twitter, Scholar, and other sites)!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_hkrcdd', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 94, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 94, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1593840515.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': True, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;ICYMI: A browser extension that automatically finds code implementations for machine learning papers anywhere on the web (Google, Arxiv, Twitter, Scholar, and other sites)! &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://chrome.google.com/webstore/detail/mlai-code-implementation/aikkeehnlfpamidigaffhfmgbkdeheil""&gt;https://chrome.google.com/webstore/detail/mlai-code-implementation/aikkeehnlfpamidigaffhfmgbkdeheil&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/""&gt;https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Feedback and requests are very welcome! :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/wNOK4uFSjn0_VLV9GzzwcWxl8N-Cjs1NgKKI98QDVv8.jpg?auto=webp&amp;s=6d2c995ff6e0e752289c28367c6de1532772648d', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/wNOK4uFSjn0_VLV9GzzwcWxl8N-Cjs1NgKKI98QDVv8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d81abbb07ddce350d7a7603a6d590802e9aa0ca', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'I-n5sa7QCjcBfo5kRd7bssElIH2v7WE5U8qgV0NDZuI'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'hkrcdd', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 11, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/hkrcdd/a_browser_extension_that_automatically_finds_code/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/hkrcdd/a_browser_extension_that_automatically_finds_code/', 'subreddit_subscribers': 6676, 'created_utc': 1593811715.0, 'num_crossposts': 26, 'media': None, 'is_video': False}]",t3_hkrcdd,,,,,
473,,pytorch,,t2_44mbtmjy,False,,0,False,ML/AI Code Implementation Finder (free browser extension),[],r/pytorch,False,6,,0,,,False,t3_hjom9d,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,default,False,,[],{},link,,False,,1593685615.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/wNOK4uFSjn0_VLV9GzzwcWxl8N-Cjs1NgKKI98QDVv8.jpg?auto=webp&amp;s=6d2c995ff6e0e752289c28367c6de1532772648d', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/wNOK4uFSjn0_VLV9GzzwcWxl8N-Cjs1NgKKI98QDVv8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d81abbb07ddce350d7a7603a6d590802e9aa0ca', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'I-n5sa7QCjcBfo5kRd7bssElIH2v7WE5U8qgV0NDZuI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hjom9d,True,,MLtinkerer,,1,True,all_ads,False,[],False,,/r/pytorch/comments/hjom9d/mlai_code_implementation_finder_free_browser/,all_ads,False,/r/LatestInML/comments/hjokbt/mlai_code_implementation_finder_free_browser/,7135,1593656815.0,0,,False,/r/LatestInML/comments/hjokbt/mlai_code_implementation_finder_free_browser/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': '\\[Update\\] Just out: highly recommend this free chrome/firefox extension as a must-have! It automatically finds code implementations for machine learning papers anywhere on the web (Google, Arxiv, Twitter, Scholar, and other sites)  \n\n\n[link to chrome extension](https://chrome.google.com/webstore/detail/mlai-code-implementation/aikkeehnlfpamidigaffhfmgbkdeheil)  \nor  \n[link to firefox extension](https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/)  \n\n\nFeedback and requests are very welcome!', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'ML/AI Code Implementation Finder (free browser extension)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_hjokbt', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 50, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 50, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1593685390.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[Update] Just out: highly recommend this free chrome/firefox extension as a must-have! It automatically finds code implementations for machine learning papers anywhere on the web (Google, Arxiv, Twitter, Scholar, and other sites)  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://chrome.google.com/webstore/detail/mlai-code-implementation/aikkeehnlfpamidigaffhfmgbkdeheil""&gt;link to chrome extension&lt;/a&gt;&lt;br/&gt;\nor&lt;br/&gt;\n&lt;a href=""https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/""&gt;link to firefox extension&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Feedback and requests are very welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/wNOK4uFSjn0_VLV9GzzwcWxl8N-Cjs1NgKKI98QDVv8.jpg?auto=webp&amp;s=6d2c995ff6e0e752289c28367c6de1532772648d', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/wNOK4uFSjn0_VLV9GzzwcWxl8N-Cjs1NgKKI98QDVv8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d81abbb07ddce350d7a7603a6d590802e9aa0ca', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'I-n5sa7QCjcBfo5kRd7bssElIH2v7WE5U8qgV0NDZuI'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': 'moderator', 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'hjokbt', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/hjokbt/mlai_code_implementation_finder_free_browser/', 'parent_whitelist_status': 'all_ads', 'stickied': True, 'url': 'https://www.reddit.com/r/LatestInML/comments/hjokbt/mlai_code_implementation_finder_free_browser/', 'subreddit_subscribers': 6676, 'created_utc': 1593656590.0, 'num_crossposts': 35, 'media': None, 'is_video': False}]",t3_hjokbt,,,,,
474,,pytorch,"I am trying to create confusion for my NN.  My test data consists of 300 images of cats, 300 images of dogs, and 100 images of horses. After running with a batch size of 64 I am getting this:

&amp;#x200B;

https://preview.redd.it/9oy3d8kfz4851.png?width=339&amp;format=png&amp;auto=webp&amp;s=59c9ab9e43cb4107e9fba74351005b6c8d35f751

The values in my matrix are not adding up to the number of images in my test data. Another issue I am having is that the batch size does affect the output of my matrix, causing it to sometimes be a 2x2 instead of a 3x3 depending on the batch size. Can someone help me debug my code?

    class_names = ['cats', '  dogs', ' horses']
     with torch.no_grad():
            for b, (pics, names) in enumerate(test_loader):
                
                if torch.cuda.is_available():
                    pics = pics.cuda()
                    names = names.cuda()
                    
                b+=1
    
                # Apply the model
                y_val = MobileNet(pics)
                val_loss = criterion(y_val, names)
                # Tally the number of correct predictions
                test_predicted = torch.max(y_val.data, 1)[1] 
                tst_corr += (test_predicted == names).sum()
                
                test_correct.append(tst_corr)
                Val_accuracy = tst_corr.item()*100/(b*batch)
                # Print Validation results
                if b%4 == 0:
                    print(f'test_epoch: {i:2}  batch: {b:4}  Val_loss: {val_loss.item():10.8f}  \Val_accuracy: {Val_accuracy:7.3f}%')
                    test_acc.append(Val_accuracy)
                    test_losses.append(val_loss)
                    print(""\n"")
    
    names = names.cpu()
    test_predicted = test_predicted.cpu()
    arr = confusion_matrix(names.view(-1), test_predicted.view(-1))
    #df_cm = pd.DataFrame(arr, class_names, class_names)
    plt.figure(figsize = (9,6))
    sn.heatmap(df_cm, annot=True, fmt=""d"", cmap='BuGn')
    plt.xlabel(""prediction"")
    plt.ylabel(""label (ground truth)"")
    plt.show();",t2_48egj5xw,False,,0,False,Help Debugging my confusion matrix,[],r/pytorch,False,6,,0,32.0,,False,t3_hizgod,False,dark,1.0,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://a.thumbs.redditmedia.com/uF6Yyi-DDEMaWAawMCFebqJvkQSrEZKtNTsJDvbkMH4.jpg,False,,[],{},,,True,,1593590786.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to create confusion for my NN.  My test data consists of 300 images of cats, 300 images of dogs, and 100 images of horses. After running with a batch size of 64 I am getting this:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/9oy3d8kfz4851.png?width=339&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59c9ab9e43cb4107e9fba74351005b6c8d35f751""&gt;https://preview.redd.it/9oy3d8kfz4851.png?width=339&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59c9ab9e43cb4107e9fba74351005b6c8d35f751&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The values in my matrix are not adding up to the number of images in my test data. Another issue I am having is that the batch size does affect the output of my matrix, causing it to sometimes be a 2x2 instead of a 3x3 depending on the batch size. Can someone help me debug my code?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class_names = [&amp;#39;cats&amp;#39;, &amp;#39;  dogs&amp;#39;, &amp;#39; horses&amp;#39;]
 with torch.no_grad():
        for b, (pics, names) in enumerate(test_loader):

            if torch.cuda.is_available():
                pics = pics.cuda()
                names = names.cuda()

            b+=1

            # Apply the model
            y_val = MobileNet(pics)
            val_loss = criterion(y_val, names)
            # Tally the number of correct predictions
            test_predicted = torch.max(y_val.data, 1)[1] 
            tst_corr += (test_predicted == names).sum()

            test_correct.append(tst_corr)
            Val_accuracy = tst_corr.item()*100/(b*batch)
            # Print Validation results
            if b%4 == 0:
                print(f&amp;#39;test_epoch: {i:2}  batch: {b:4}  Val_loss: {val_loss.item():10.8f}  \Val_accuracy: {Val_accuracy:7.3f}%&amp;#39;)
                test_acc.append(Val_accuracy)
                test_losses.append(val_loss)
                print(&amp;quot;\n&amp;quot;)

names = names.cpu()
test_predicted = test_predicted.cpu()
arr = confusion_matrix(names.view(-1), test_predicted.view(-1))
#df_cm = pd.DataFrame(arr, class_names, class_names)
plt.figure(figsize = (9,6))
sn.heatmap(df_cm, annot=True, fmt=&amp;quot;d&amp;quot;, cmap=&amp;#39;BuGn&amp;#39;)
plt.xlabel(&amp;quot;prediction&amp;quot;)
plt.ylabel(&amp;quot;label (ground truth)&amp;quot;)
plt.show();
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hizgod,True,,stunbomb1,,3,True,all_ads,False,[],False,,/r/pytorch/comments/hizgod/help_debugging_my_confusion_matrix/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hizgod/help_debugging_my_confusion_matrix/,7135,1593561986.0,0,,False,,,,"{'9oy3d8kfz4851': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 25, 'x': 108, 'u': 'https://preview.redd.it/9oy3d8kfz4851.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2ce6b70db5601339f41f0a5a12e40f71b697063'}, {'y': 50, 'x': 216, 'u': 'https://preview.redd.it/9oy3d8kfz4851.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=39c99db85a5cc512ebd0e673ff4484abeb7f52a5'}, {'y': 74, 'x': 320, 'u': 'https://preview.redd.it/9oy3d8kfz4851.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1d97d8867ceae80a4f320c290e19b042db64dda'}], 's': {'y': 79, 'x': 339, 'u': 'https://preview.redd.it/9oy3d8kfz4851.png?width=339&amp;format=png&amp;auto=webp&amp;s=59c9ab9e43cb4107e9fba74351005b6c8d35f751'}, 'id': '9oy3d8kfz4851'}}",,,,
475,,pytorch,,t2_44mbtmjy,False,,0,False,"Generate photo-realistic images given the input geometry and basic intrinsic properties, OR decompose real images back into their intrinsic components.",[],r/pytorch,False,6,,0,140.0,,False,t3_hijsw3,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/x-Pvs0yA5ybpbzOGwh6G93IIkJLYlefRLgIq7X_aeLg.jpg,False,,[],{},link,,False,,1593536994.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hijsw3,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hijsw3/generate_photorealistic_images_given_the_input/,all_ads,False,/r/LatestInML/comments/hijdkl/generate_photorealistic_images_given_the_input/,7135,1593508194.0,0,,False,/r/LatestInML/comments/hijdkl/generate_photorealistic_images_given_the_input/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/dataset requests: [click here](https://www.catalyzex.com/paper/arxiv:2006.16011)\n\nhttps://preview.redd.it/7j4hv5rzd0851.png?width=2410&amp;format=png&amp;auto=webp&amp;s=031202d07dafef6501356edace42ecab84172e64\n\nExperiments confirm that a joint treatment of rendering and decomposition is indeed beneficial and that our approach outperforms state-of-the-art image-to-image translation baselines both qualitatively and quantitatively.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Generate photo-realistic images given the input geometry and basic intrinsic properties, OR decompose real images back into their intrinsic components.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'7j4hv5rzd0851': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 36, 'x': 108, 'u': 'https://preview.redd.it/7j4hv5rzd0851.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=68dfbe738176173ce8e92b1a66ea962f831cfa48'}, {'y': 73, 'x': 216, 'u': 'https://preview.redd.it/7j4hv5rzd0851.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=580c80c19513ed068899bbd839381aaabb6882de'}, {'y': 108, 'x': 320, 'u': 'https://preview.redd.it/7j4hv5rzd0851.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2fc233cf7cef6f4ff5d96b0209d2c6081e8acb3b'}, {'y': 217, 'x': 640, 'u': 'https://preview.redd.it/7j4hv5rzd0851.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e3f6b5993a659c9cb0ab2ec04f46be2f36ecf0a'}, {'y': 325, 'x': 960, 'u': 'https://preview.redd.it/7j4hv5rzd0851.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a095e0e90763eaca526e6e77affd51bb889ab5d'}, {'y': 366, 'x': 1080, 'u': 'https://preview.redd.it/7j4hv5rzd0851.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71c20f911119eff87545684c742a3eb8cd8fc386'}], 's': {'y': 818, 'x': 2410, 'u': 'https://preview.redd.it/7j4hv5rzd0851.png?width=2410&amp;format=png&amp;auto=webp&amp;s=031202d07dafef6501356edace42ecab84172e64'}, 'id': '7j4hv5rzd0851'}}, 'name': 't3_hijdkl', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 27, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 27, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/x-Pvs0yA5ybpbzOGwh6G93IIkJLYlefRLgIq7X_aeLg.jpg', 'edited': 1593506264.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1593534876.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/dataset requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2006.16011""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/7j4hv5rzd0851.png?width=2410&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=031202d07dafef6501356edace42ecab84172e64""&gt;https://preview.redd.it/7j4hv5rzd0851.png?width=2410&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=031202d07dafef6501356edace42ecab84172e64&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Experiments confirm that a joint treatment of rendering and decomposition is indeed beneficial and that our approach outperforms state-of-the-art image-to-image translation baselines both qualitatively and quantitatively.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?auto=webp&amp;s=4433271552041dbb48641eeab7fc92329936ded0', 'width': 466, 'height': 466}, 'resolutions': [{'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b7cabdef09304877213eb836fa68de9ba16b40', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38a6011d4229584591143498e125f4d4ca61d27d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/VrDWnS5ZDDoFvQICvZOWZ881K5C96wjYsp-cQ33a-jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=624b9fd4d910a83d7d2cc43056a728e8ee1a0969', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'X4W47ha8bZrbpA5MgJZUopc3rjrPepNW7LlZStVXlZc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'hijdkl', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/hijdkl/generate_photorealistic_images_given_the_input/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/hijdkl/generate_photorealistic_images_given_the_input/', 'subreddit_subscribers': 6676, 'created_utc': 1593506076.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_hijdkl,,,,,
476,,pytorch,"I have a time series tabular dataset stored as many CSVs that are simply too large to fit into memory. I am curious what the best way to batch load and train using this data. 

I have been reading a lot about custom datasets but haven't really found an example related to using a more tabular dataset. Should I create the sequences as I load the data or should it be done before hand? Sample code would be really helpful.",t2_5l0i1cin,False,,0,False,Training PyTorch on larger dataset,[],r/pytorch,False,6,,0,,,False,t3_hiaw8g,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1593471596.0,,[],{},,,True,,1593500093.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a time series tabular dataset stored as many CSVs that are simply too large to fit into memory. I am curious what the best way to batch load and train using this data. &lt;/p&gt;

&lt;p&gt;I have been reading a lot about custom datasets but haven&amp;#39;t really found an example related to using a more tabular dataset. Should I create the sequences as I load the data or should it be done before hand? Sample code would be really helpful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hiaw8g,True,,rchinny,,7,True,all_ads,False,[],False,,/r/pytorch/comments/hiaw8g/training_pytorch_on_larger_dataset/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hiaw8g/training_pytorch_on_larger_dataset/,7135,1593471293.0,0,,False,,,,,,,,
477,,pytorch,"Hey guys, I am new to DL, and curious about the differences among the pytorch frameworks under the same pytorch version, but different cuda version.  

For example, what is the exact difference between pytorch1.3.1-cuda10.0 and pytorch1.3.1-cuda9.2?

I assume that for a pytorch update, say from 1.2 to 1.3 there will be updates related to the operations, and enable the operation, the team has to come up with different solution for different cuda version? As I know there will be some lib difference between different version of CUDA

It looks like a stupid question, any help would be appreciated!",t2_11s43k,False,,0,False,A question about the pytorch version and cuda version,[],r/pytorch,False,6,,0,,,False,t3_hhyt0v,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1593461874.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, I am new to DL, and curious about the differences among the pytorch frameworks under the same pytorch version, but different cuda version.  &lt;/p&gt;

&lt;p&gt;For example, what is the exact difference between pytorch1.3.1-cuda10.0 and pytorch1.3.1-cuda9.2?&lt;/p&gt;

&lt;p&gt;I assume that for a pytorch update, say from 1.2 to 1.3 there will be updates related to the operations, and enable the operation, the team has to come up with different solution for different cuda version? As I know there will be some lib difference between different version of CUDA&lt;/p&gt;

&lt;p&gt;It looks like a stupid question, any help would be appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hhyt0v,True,,heydwane3,,3,True,all_ads,False,[],False,,/r/pytorch/comments/hhyt0v/a_question_about_the_pytorch_version_and_cuda/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hhyt0v/a_question_about_the_pytorch_version_and_cuda/,7135,1593433074.0,0,,False,,,,,,,,
478,,pytorch,"Do anyone know how to use VOC2007 dataset instead of the CIFAR-10 dataset on the [CUDA convolution kernel](https://github.com/jdnie/AdderNetCuda/blob/master/train.py#L80) ?

Note that [torchvision.dataset](https://pytorch.org/docs/stable/torchvision/datasets.html) does not have VOC2007",t2_bpftl,False,,0,False,CUDA convolution kernel with VOC2007 dataset,[],r/pytorch,False,6,,0,,,False,t3_hi48qi,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1593480408.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Do anyone know how to use VOC2007 dataset instead of the CIFAR-10 dataset on the &lt;a href=""https://github.com/jdnie/AdderNetCuda/blob/master/train.py#L80""&gt;CUDA convolution kernel&lt;/a&gt; ?&lt;/p&gt;

&lt;p&gt;Note that &lt;a href=""https://pytorch.org/docs/stable/torchvision/datasets.html""&gt;torchvision.dataset&lt;/a&gt; does not have VOC2007&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/1oJELh2pyTu6gRTe3DGgCJBpi6Y3RD_H_8xOxF2TddA.jpg?auto=webp&amp;s=19e61b3b9eeeb20ee7e20703e5c5c18266a45a21', 'width': 420, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/1oJELh2pyTu6gRTe3DGgCJBpi6Y3RD_H_8xOxF2TddA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=15f237bfaebcb0d809189e635acb8004e4cdc046', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/1oJELh2pyTu6gRTe3DGgCJBpi6Y3RD_H_8xOxF2TddA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c5a7779c702ee0db2f10ff5bdc67f388172fed3', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/1oJELh2pyTu6gRTe3DGgCJBpi6Y3RD_H_8xOxF2TddA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89ff9ee69cf09a6ec179ae3b66d4f3fd5d46bff4', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'wmrjIuaBYHhuOtK6iYSNLUsvMmzkBld3QUJyGapSWUk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hi48qi,True,,promach,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hi48qi/cuda_convolution_kernel_with_voc2007_dataset/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hi48qi/cuda_convolution_kernel_with_voc2007_dataset/,7135,1593451608.0,0,,False,,,,,,,,
479,,pytorch,"I am trying to implement a super resolution architecture from fast.ai and run out of RAM. Why did this happen during training? What might be the problem that I run out o RAM during training, but only after some epochs and not directly after the first. 

I am using some sort of ResNet Architecture or maybe someone knows the fast.ai implementation of a super resolution Network from one of their tutourials. 

I only use PyTorch and not fast.ai, but I use the same architecture he proposed in his Videos.",t2_4zpj0bxn,False,,0,False,Out of RAM,[],r/pytorch,False,6,,0,,,False,t3_hi2oto,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1593475778.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to implement a super resolution architecture from fast.ai and run out of RAM. Why did this happen during training? What might be the problem that I run out o RAM during training, but only after some epochs and not directly after the first. &lt;/p&gt;

&lt;p&gt;I am using some sort of ResNet Architecture or maybe someone knows the fast.ai implementation of a super resolution Network from one of their tutourials. &lt;/p&gt;

&lt;p&gt;I only use PyTorch and not fast.ai, but I use the same architecture he proposed in his Videos.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hi2oto,True,,SeucheAchat9115,,3,True,all_ads,False,[],False,,/r/pytorch/comments/hi2oto/out_of_ram/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hi2oto/out_of_ram/,7135,1593446978.0,0,,False,,,,,,,,
480,,pytorch,"Title says it all. 
I accidentally ran the output of layer 1 through relu then passed it back to the first layer and ran softmax.

The network still learned with ""better"" results. Is this a correct method what's happening if I do that. If I try to do relu on layer 1 then pass output to layer 2 with softmax the results are worse. 

What might be happening with the incorrect syntax of passing the output back to the first layer?",t2_2hf57lyk,False,,0,False,Running relu on first conv layer and then softmax after passing relu back to first layer,[],r/pytorch,False,6,,0,,,False,t3_hi1200,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1593470410.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Title says it all. 
I accidentally ran the output of layer 1 through relu then passed it back to the first layer and ran softmax.&lt;/p&gt;

&lt;p&gt;The network still learned with &amp;quot;better&amp;quot; results. Is this a correct method what&amp;#39;s happening if I do that. If I try to do relu on layer 1 then pass output to layer 2 with softmax the results are worse. &lt;/p&gt;

&lt;p&gt;What might be happening with the incorrect syntax of passing the output back to the first layer?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hi1200,True,,HoustonWarlock,,2,True,all_ads,False,[],False,,/r/pytorch/comments/hi1200/running_relu_on_first_conv_layer_and_then_softmax/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hi1200/running_relu_on_first_conv_layer_and_then_softmax/,7135,1593441610.0,0,,False,,,,,,,,
481,,pytorch,"Hello there!

I just approached PyTorch (coming from a short beginning with TensorFlow).

I'm currently having fun with this repo [https://github.com/mar-muel/artificial-self-AMLD-2020/blob/master/3/task\_3.ipynb](https://github.com/mar-muel/artificial-self-AMLD-2020/blob/master/3/task_3.ipynb) that creates sort of like a chatbot trained on chat logs. I followed the Colab and I ended up saving the mode on my drive.

I'm now struggling to run the model in colab (later I want to do it locally too).

At the moment this is what I did after copying the model from my drive to a `runs` folder inside of `/content/artificial-self-AMLD-2020/3` :

    !git clone https://github.com/mar-muel/artificial-self-AMLD-2020.git
    %cd /content/artificial-self-AMLD-2020/3
    pip install -r requirements.txt
    !python interact.py --run_name run4

But I'm far from getting the logic here, and this is the output I'm getting: [https://pastebin.com/Eyfya98b](https://pastebin.com/Eyfya98b) . Note that at the end, it doesn't allow me to send an input as it should. what is the problem here?

Thanks you in advance for any help and tips.",t2_1etn20,False,,0,False,Seeking advices on how to re-run my language model,[],r/pytorch,False,6,,0,,,False,t3_hi08ih,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1593619537.0,,[],{},self,,True,,1593467590.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello there!&lt;/p&gt;

&lt;p&gt;I just approached PyTorch (coming from a short beginning with TensorFlow).&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently having fun with this repo &lt;a href=""https://github.com/mar-muel/artificial-self-AMLD-2020/blob/master/3/task_3.ipynb""&gt;https://github.com/mar-muel/artificial-self-AMLD-2020/blob/master/3/task_3.ipynb&lt;/a&gt; that creates sort of like a chatbot trained on chat logs. I followed the Colab and I ended up saving the mode on my drive.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m now struggling to run the model in colab (later I want to do it locally too).&lt;/p&gt;

&lt;p&gt;At the moment this is what I did after copying the model from my drive to a &lt;code&gt;runs&lt;/code&gt; folder inside of &lt;code&gt;/content/artificial-self-AMLD-2020/3&lt;/code&gt; :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;!git clone https://github.com/mar-muel/artificial-self-AMLD-2020.git
%cd /content/artificial-self-AMLD-2020/3
pip install -r requirements.txt
!python interact.py --run_name run4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But I&amp;#39;m far from getting the logic here, and this is the output I&amp;#39;m getting: &lt;a href=""https://pastebin.com/Eyfya98b""&gt;https://pastebin.com/Eyfya98b&lt;/a&gt; . Note that at the end, it doesn&amp;#39;t allow me to send an input as it should. what is the problem here?&lt;/p&gt;

&lt;p&gt;Thanks you in advance for any help and tips.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/C3MdEZFTNHT0opcB01SlgWKViW4LrzfukANKZMfJS3Y.jpg?auto=webp&amp;s=e37261eb30778cfbd11cf884a9ae860c7a74ba42', 'width': 368, 'height': 368}, 'resolutions': [{'url': 'https://external-preview.redd.it/C3MdEZFTNHT0opcB01SlgWKViW4LrzfukANKZMfJS3Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ef9c94eb8b9e3c184d16aa146ada7ad037acf38d', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/C3MdEZFTNHT0opcB01SlgWKViW4LrzfukANKZMfJS3Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c59b98af2220a599902802323833c88a20e9e7f5', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/C3MdEZFTNHT0opcB01SlgWKViW4LrzfukANKZMfJS3Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=067419a0bcd82bb71350355803110a4e4958cd7b', 'width': 320, 'height': 320}], 'variants': {}, 'id': '6SYfN7YOGCNm-_d---5g_pbqJ6_HNgh74HiNbNJPszc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hi08ih,True,,Goretx,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hi08ih/seeking_advices_on_how_to_rerun_my_language_model/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hi08ih/seeking_advices_on_how_to_rerun_my_language_model/,7135,1593438790.0,2,,False,,,,,,,,
482,,pytorch,"Given a tensor with the shape of \[NxCxDxHxW\], one can merge the 2nd and 3rd dims into one dim, after which the tensor will be like \[Nx(CxD)xHxW\]. The process above is just a reshape changing tensor from 5d to 4d without size reduction. In my case, 3d convolution applied to the \[NxCxDxHxW\] runs slower than 2d convolution applied to \[Nx(CxD)xHxW\]. We can easily calculate flops of the two processes above, in which 2d case has more flops than 3d case. My guess is that it's related to the GEMM written in cuDNN. But I cannot understand what makes this happen. Does anyone know what mechanism impacts this?

My inference time evaluation code is:

&amp;#x200B;

`cost4x = torch.rand(1,4,5, 312, 96).cuda().float()`

`cost4x_merge = torch.rand(1,20, 312, 96).cuda().float()`

`class conv3d(nn.Module):`

`def __init__(self, args):`

`super(conv3d, self).__init__()`

`self.conv = nn.Conv3d(4, 4, 3, 1, 1, bias=False)`

`def forward(self, in_t):`

`return self.conv(in_t)`

&amp;#x200B;

`class conv3d_merge(nn.Module):`

`def __init__(self, args):`

`super(conv3d_merge, self).__init__()`

`self.conv = nn.Conv2d(20, 20, 3, 1, 1, bias=False)`

`def forward(self, in_t):`

`return self.conv(in_t)`

&amp;#x200B;

`model3 = conv3d(args).eval().cuda()`

`model4 = conv3d_merge(args).eval().cuda()`

`starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)`

`repetitions = 1000`

`with torch.no_grad():`

`for i in range(repetitions):`

`starter.record()`

`out = model3(cost4x)`

`# out = model4(cost4x_merge)`

`ender.record()`

`torch.cuda.synchronize()`

`curr_time = starter.elapsed_time(ender)`

`times.append(curr_time / 1000)`

`print('Mean time: {:6f}'.format(sum(times[200:]) / (len(times) - 200)))`",t2_6bo10j7,False,,0,False,nn.Conv2d is faster than nn.Conv3d for the same data size and flops,[],r/pytorch,False,6,,0,,,False,t3_hhrsbb,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1593419028.0,,[],{},,,True,,1593427280.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Given a tensor with the shape of [NxCxDxHxW], one can merge the 2nd and 3rd dims into one dim, after which the tensor will be like [Nx(CxD)xHxW]. The process above is just a reshape changing tensor from 5d to 4d without size reduction. In my case, 3d convolution applied to the [NxCxDxHxW] runs slower than 2d convolution applied to [Nx(CxD)xHxW]. We can easily calculate flops of the two processes above, in which 2d case has more flops than 3d case. My guess is that it&amp;#39;s related to the GEMM written in cuDNN. But I cannot understand what makes this happen. Does anyone know what mechanism impacts this?&lt;/p&gt;

&lt;p&gt;My inference time evaluation code is:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;cost4x = torch.rand(1,4,5, 312, 96).cuda().float()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;cost4x_merge = torch.rand(1,20, 312, 96).cuda().float()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;class conv3d(nn.Module):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def __init__(self, args):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;super(conv3d, self).__init__()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.conv = nn.Conv3d(4, 4, 3, 1, 1, bias=False)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def forward(self, in_t):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;return self.conv(in_t)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;class conv3d_merge(nn.Module):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def __init__(self, args):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;super(conv3d_merge, self).__init__()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.conv = nn.Conv2d(20, 20, 3, 1, 1, bias=False)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def forward(self, in_t):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;return self.conv(in_t)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model3 = conv3d(args).eval().cuda()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model4 = conv3d_merge(args).eval().cuda()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;repetitions = 1000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;with torch.no_grad():&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for i in range(repetitions):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;starter.record()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;out = model3(cost4x)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# out = model4(cost4x_merge)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ender.record()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;torch.cuda.synchronize()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;curr_time = starter.elapsed_time(ender)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;times.append(curr_time / 1000)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(&amp;#39;Mean time: {:6f}&amp;#39;.format(sum(times[200:]) / (len(times) - 200)))&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hhrsbb,True,,MJITG,,8,True,all_ads,False,[],False,,/r/pytorch/comments/hhrsbb/nnconv2d_is_faster_than_nnconv3d_for_the_same/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hhrsbb/nnconv2d_is_faster_than_nnconv3d_for_the_same/,7135,1593398480.0,1,,False,,,,,,,,
483,,pytorch,"When should I use nn.sequential and when rewriting the model class? I mostly use sequential, but maybe the other way is easier/better for some situations.",t2_4zpj0bxn,False,,0,False,Sequential vs Class,[],r/pytorch,False,6,,0,,,False,t3_hhercz,False,dark,0.76,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1593381334.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When should I use nn.sequential and when rewriting the model class? I mostly use sequential, but maybe the other way is easier/better for some situations.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hhercz,True,,SeucheAchat9115,,9,True,all_ads,False,[],False,,/r/pytorch/comments/hhercz/sequential_vs_class/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hhercz/sequential_vs_class/,7135,1593352534.0,0,,False,,,,,,,,
484,,pytorch,"Hello,  


How do you create a custom loss function using a combination of losses in Pytorch? For example, how do I define something like:   


custom\_loss = 0.3 \* L1 + 0.7 \* L2?",t2_28a57i1o,False,,0,False,Defining combined loss functions,[],r/pytorch,False,6,,0,,,False,t3_hgc6l6,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1593222074.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,  &lt;/p&gt;

&lt;p&gt;How do you create a custom loss function using a combination of losses in Pytorch? For example, how do I define something like:   &lt;/p&gt;

&lt;p&gt;custom_loss = 0.3 * L1 + 0.7 * L2?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hgc6l6,True,,Alex-S-S,,3,True,all_ads,False,[],False,,/r/pytorch/comments/hgc6l6/defining_combined_loss_functions/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hgc6l6/defining_combined_loss_functions/,7135,1593193274.0,0,,False,,,,,,,,
485,,pytorch," I am attempting to modify this particular section of code in the mobilenetv2 model :

    (17): InvertedResidual(
    (conv): Sequential(         
    (0): ConvBNReLU(           
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)           
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                            
     (2) : ReLU6(inplace=True)         
      )

I want to add a max-pooling layer after the convolutional layer. I was thinking of something like:

    MobileNet.features[17].conv[0] = nn.ConvBRELU(nn.Conv2d(),nn.maxpool,nn.BatchNorm(),nn.ReLU())

but that won't work because there is no nn.ConvBNReLU(). How can I make the desired changed?",t2_48egj5xw,False,,0,False,Help with modifying a pretrained model?,[],r/pytorch,False,6,,0,,,False,t3_hgffup,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1593232396.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am attempting to modify this particular section of code in the mobilenetv2 model :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(17): InvertedResidual(
(conv): Sequential(         
(0): ConvBNReLU(           
    (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)           
    (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                            
 (2) : ReLU6(inplace=True)         
  )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I want to add a max-pooling layer after the convolutional layer. I was thinking of something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MobileNet.features[17].conv[0] = nn.ConvBRELU(nn.Conv2d(),nn.maxpool,nn.BatchNorm(),nn.ReLU())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;but that won&amp;#39;t work because there is no nn.ConvBNReLU(). How can I make the desired changed?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hgffup,True,,stunbomb1,,3,True,all_ads,False,[],False,,/r/pytorch/comments/hgffup/help_with_modifying_a_pretrained_model/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hgffup/help_with_modifying_a_pretrained_model/,7135,1593203596.0,0,,False,,,,,,,,
486,,pytorch,"Ran one of my first big training sessions, \~18hrs, via a tutorial. I'm feeling pretty awesome. Had fun tinkering with hyper-parameters to cram it all into my gpu.

I'm left with a .bin file.   [https://imgur.com/FrrlCoL](https://imgur.com/FrrlCoL)

&amp;#x200B;

Let's say I restart my kernel, how do I reload my precious model?

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

(the tutorial, if interested:  [https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/](https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/) )",t2_g1ws1,False,,0,False,"Stupid new, with a stupid question, how to re-load a trained model?",[],r/pytorch,False,6,,0,,,False,t3_hg9f1x,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1593213260.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Ran one of my first big training sessions, ~18hrs, via a tutorial. I&amp;#39;m feeling pretty awesome. Had fun tinkering with hyper-parameters to cram it all into my gpu.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m left with a .bin file.   &lt;a href=""https://imgur.com/FrrlCoL""&gt;https://imgur.com/FrrlCoL&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s say I restart my kernel, how do I reload my precious model?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;(the tutorial, if interested:  &lt;a href=""https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/""&gt;https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/&lt;/a&gt; )&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BOJHAawjEzEiiRhsIE9mo6K68ZDdjkDHS6LxJYVwaTk.jpg?auto=webp&amp;s=16257d762efaeafc1d5d9631b515d7f740683baa', 'width': 3859, 'height': 2159}, 'resolutions': [{'url': 'https://external-preview.redd.it/BOJHAawjEzEiiRhsIE9mo6K68ZDdjkDHS6LxJYVwaTk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=11300aaa7c407f9a083a25a00c8a068b7372ad2e', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/BOJHAawjEzEiiRhsIE9mo6K68ZDdjkDHS6LxJYVwaTk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d760ee8fbc95ec9f49f2d3a928774f053f7111e', 'width': 216, 'height': 120}, {'url': 'https://external-preview.redd.it/BOJHAawjEzEiiRhsIE9mo6K68ZDdjkDHS6LxJYVwaTk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9090bfb97ba45add04638ccfffa2978c417dae6', 'width': 320, 'height': 179}, {'url': 'https://external-preview.redd.it/BOJHAawjEzEiiRhsIE9mo6K68ZDdjkDHS6LxJYVwaTk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=518ec56aa832bf1ecccac5893061fa1e6e3a54e7', 'width': 640, 'height': 358}, {'url': 'https://external-preview.redd.it/BOJHAawjEzEiiRhsIE9mo6K68ZDdjkDHS6LxJYVwaTk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=793c17ddb0718d060efe1617f22e87f506f895a5', 'width': 960, 'height': 537}, {'url': 'https://external-preview.redd.it/BOJHAawjEzEiiRhsIE9mo6K68ZDdjkDHS6LxJYVwaTk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e23a37cc6f8dfb8462d7ac037b82e8fe5d2cf08e', 'width': 1080, 'height': 604}], 'variants': {}, 'id': 'NM-b_-7cT5DpfvisZAfgeeATwOoZIx_77cGre0p1AEE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hg9f1x,True,,polandtown,,5,True,all_ads,False,[],False,,/r/pytorch/comments/hg9f1x/stupid_new_with_a_stupid_question_how_to_reload_a/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hg9f1x/stupid_new_with_a_stupid_question_how_to_reload_a/,7135,1593184460.0,0,,False,,,,,,,,
487,,pytorch,,t2_44mbtmjy,False,,0,False,Latest from Adobe and KAIST researchers: A novel video panoptic segmentation network - VPSNet.,[],r/pytorch,False,6,,0,39.0,,False,t3_hg0f8m,False,dark,0.88,,public,6,0,{},140.0,,False,[],,False,False,,{},,False,6,,False,https://a.thumbs.redditmedia.com/TKe1WlBXODtlwKPdbhBgaI7Tlz4TSrrg0M75s55Asy8.jpg,False,,[],{},link,,False,,1593172009.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/R1EAtm-KD57XmqQ8QfOvHkNZxiEYwNgq9RZBoBekKkw.jpg?auto=webp&amp;s=d415a0b6c396b3b63ba3ef74d4bd321ebb26e890', 'width': 586, 'height': 260}, 'resolutions': [{'url': 'https://external-preview.redd.it/R1EAtm-KD57XmqQ8QfOvHkNZxiEYwNgq9RZBoBekKkw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=43a88ec90fc1d8aa0820ee9593c76408a2173735', 'width': 108, 'height': 47}, {'url': 'https://external-preview.redd.it/R1EAtm-KD57XmqQ8QfOvHkNZxiEYwNgq9RZBoBekKkw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80866399c208f65c91c4165906e54b6ecb5583fd', 'width': 216, 'height': 95}, {'url': 'https://external-preview.redd.it/R1EAtm-KD57XmqQ8QfOvHkNZxiEYwNgq9RZBoBekKkw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b22620c9f7c78304f0a13be4b4039e20ea9b816', 'width': 320, 'height': 141}], 'variants': {}, 'id': 'NMB5lOZJ6n1Q4wNAP6yQDdo-T-k_Dd7cteqcYMwKwx4'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hg0f8m,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hg0f8m/latest_from_adobe_and_kaist_researchers_a_novel/,all_ads,False,/r/LatestInML/comments/hg0d9h/latest_from_adobe_and_kaist_researchers_a_novel/,7135,1593143209.0,0,,False,/r/LatestInML/comments/hg0d9h/latest_from_adobe_and_kaist_researchers_a_novel/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests:\xa0[click here](https://www.catalyzex.com/paper/arxiv:2006.11339)\n\nhttps://i.redd.it/ut2wodq7e6751.gif\n\nIt jointly predicts object classes, bounding boxes, masks, instance id tracking, and semantic segmentation in video frames.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Latest from Adobe and KAIST researchers: A novel video panoptic segmentation network - VPSNet.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 39, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'ut2wodq7e6751': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 30, 'x': 108, 'u': 'https://preview.redd.it/ut2wodq7e6751.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=e21d79cc0771fe161285953bf164afb0af842984'}, {'y': 60, 'x': 216, 'u': 'https://preview.redd.it/ut2wodq7e6751.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=38ed4ee08d0815903130658bf40ce0cd55848804'}, {'y': 90, 'x': 320, 'u': 'https://preview.redd.it/ut2wodq7e6751.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=b25d07ddde55568aff053eea2763079f7968d5b9'}, {'y': 180, 'x': 640, 'u': 'https://preview.redd.it/ut2wodq7e6751.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=829572449f3b785e7c47828fb82bbdb935f78f24'}], 's': {'y': 240, 'gif': 'https://i.redd.it/ut2wodq7e6751.gif', 'mp4': 'https://preview.redd.it/ut2wodq7e6751.gif?format=mp4&amp;s=1cbc4069f0d2efd833256327e8434d19f4aa8915', 'x': 852}, 'id': 'ut2wodq7e6751'}}, 'name': 't3_hg0d9h', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 22, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 22, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/TKe1WlBXODtlwKPdbhBgaI7Tlz4TSrrg0M75s55Asy8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1593171759.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests:\xa0&lt;a href=""https://www.catalyzex.com/paper/arxiv:2006.11339""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://i.redd.it/ut2wodq7e6751.gif""&gt;https://i.redd.it/ut2wodq7e6751.gif&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It jointly predicts object classes, bounding boxes, masks, instance id tracking, and semantic segmentation in video frames.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/R1EAtm-KD57XmqQ8QfOvHkNZxiEYwNgq9RZBoBekKkw.jpg?auto=webp&amp;s=d415a0b6c396b3b63ba3ef74d4bd321ebb26e890', 'width': 586, 'height': 260}, 'resolutions': [{'url': 'https://external-preview.redd.it/R1EAtm-KD57XmqQ8QfOvHkNZxiEYwNgq9RZBoBekKkw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=43a88ec90fc1d8aa0820ee9593c76408a2173735', 'width': 108, 'height': 47}, {'url': 'https://external-preview.redd.it/R1EAtm-KD57XmqQ8QfOvHkNZxiEYwNgq9RZBoBekKkw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80866399c208f65c91c4165906e54b6ecb5583fd', 'width': 216, 'height': 95}, {'url': 'https://external-preview.redd.it/R1EAtm-KD57XmqQ8QfOvHkNZxiEYwNgq9RZBoBekKkw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b22620c9f7c78304f0a13be4b4039e20ea9b816', 'width': 320, 'height': 141}], 'variants': {}, 'id': 'NMB5lOZJ6n1Q4wNAP6yQDdo-T-k_Dd7cteqcYMwKwx4'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'hg0d9h', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/hg0d9h/latest_from_adobe_and_kaist_researchers_a_novel/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/hg0d9h/latest_from_adobe_and_kaist_researchers_a_novel/', 'subreddit_subscribers': 6676, 'created_utc': 1593142959.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_hg0d9h,,,,,
488,,pytorch,"Most NLP examples and tutorials that use a pre-trained nn.embedding layer put it inside the model, as the first layer. Since this layer is frozen anyway, would it make sense to instead put it in the data loader,  so that the words are converted into float vectors when the batches are  created? Or is there some reason, performance or otherwise, why this shouldn't be done?

For me, it would just make things a bit neater when swapping between different data loaders and models, if the embeddings were a part of the data loader.",t2_76ogqos,False,,0,False,Does it make sense to deal with embeddings in the data loader?,[],r/pytorch,False,6,,0,,,False,t3_hftu30,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,1593134146.0,,[],{},,,True,,1593146829.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Most NLP examples and tutorials that use a pre-trained nn.embedding layer put it inside the model, as the first layer. Since this layer is frozen anyway, would it make sense to instead put it in the data loader,  so that the words are converted into float vectors when the batches are  created? Or is there some reason, performance or otherwise, why this shouldn&amp;#39;t be done?&lt;/p&gt;

&lt;p&gt;For me, it would just make things a bit neater when swapping between different data loaders and models, if the embeddings were a part of the data loader.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hftu30,True,,karlpoppery,,5,True,all_ads,False,[],False,,/r/pytorch/comments/hftu30/does_it_make_sense_to_deal_with_embeddings_in_the/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hftu30/does_it_make_sense_to_deal_with_embeddings_in_the/,7135,1593118029.0,0,,False,,,,,,,,
489,,pytorch,"The title, basically. I only have a single 1080Ti, so hardware might pose an issue
  
Is it a straight forward process of just describing a model and then a simple training loop or are there other more involving steps necessary?",,False,,0,False,How difficult is it to train a speech to text model using torchaudio and a dataset like Common Voice?,[],r/pytorch,False,6,,0,,,False,t3_hfo84f,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,,self,1593099712.0,,,{},,,True,,1593128266.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The title, basically. I only have a single 1080Ti, so hardware might pose an issue&lt;/p&gt;

&lt;p&gt;Is it a straight forward process of just describing a model and then a simple training loop or are there other more involving steps necessary?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hfo84f,True,,[deleted],,0,True,all_ads,False,[],,dark,/r/pytorch/comments/hfo84f/how_difficult_is_it_to_train_a_speech_to_text/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hfo84f/how_difficult_is_it_to_train_a_speech_to_text/,7135,1593099466.0,0,,False,,,,,,,,
490,,pytorch,,t2_6hroi9yk,False,,0,False,Using PyTorch C++ API (LibTorch) in Visual Studio,[],r/pytorch,False,6,,0,140.0,,False,t3_hf332u,False,dark,0.92,,public,10,0,{},140.0,,False,[],,False,False,,{},,False,10,,False,https://b.thumbs.redditmedia.com/GXQHKY_EXolWTibGRo2KwYKfhXVZ4eePMu8lu8-XICc.jpg,False,,[],{},link,,False,,1593041568.0,text,6,,,text,mrnabati.github.io,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/mfOKMU_cvd9viqxpgC3ayQ7R4sKLaEGt_nydY7IJgCY.jpg?auto=webp&amp;s=48c0d9b95e5fbed1dfe1c1d24acb2bdb18161cb1', 'width': 600, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/mfOKMU_cvd9viqxpgC3ayQ7R4sKLaEGt_nydY7IJgCY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5d0d81525ff569f416c919c83fd2187d6a00480', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/mfOKMU_cvd9viqxpgC3ayQ7R4sKLaEGt_nydY7IJgCY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6618863b5cb81fdef41cfc4a6b4161527da481a', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/mfOKMU_cvd9viqxpgC3ayQ7R4sKLaEGt_nydY7IJgCY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=51a3c85b00ed6788ff156d4b8962f48c773205c2', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'NpQY79iQp_O7d-B8NpLpoZUosJOQDlGStN-R_Dj_394'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hf332u,True,,RamiNoob,,3,True,all_ads,False,[],False,,/r/pytorch/comments/hf332u/using_pytorch_c_api_libtorch_in_visual_studio/,all_ads,False,https://mrnabati.github.io/post/004_adv_pytorch_integrating_pytorch_cpp_frontend_in_visual_studio_on_windows/,7135,1593012768.0,0,,False,https://mrnabati.github.io/post/004_adv_pytorch_integrating_pytorch_cpp_frontend_in_visual_studio_on_windows/,,,,,,,
491,,pytorch,"I have made one model in Pytorch, seen tutorials and googling but I want to buy a book to understand well pytorch. Anyone recommend a book for beginners?

Thanks",t2_65dd1bcd,False,,0,False,Pytoch Books,[],r/pytorch,False,6,,0,,,False,t3_hf475m,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1593094399.0,,[],{},,,True,,1593045117.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have made one model in Pytorch, seen tutorials and googling but I want to buy a book to understand well pytorch. Anyone recommend a book for beginners?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hf475m,True,,huasin,,3,True,all_ads,False,[],False,,/r/pytorch/comments/hf475m/pytoch_books/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hf475m/pytoch_books/,7135,1593016317.0,0,,False,,,,,,,,
492,,pytorch,,t2_44mbtmjy,False,,0,False,"Latest from Max Planck researchers: Estimate the clothing deformations with fine details from input body shape, body pose and garment style.",[],r/pytorch,False,6,,0,40.0,,False,t3_heqmc5,False,dark,0.86,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/4eC6gFpdWaIQkpp_ADPZ7OifhxbbPqBPOukoczSyF4s.jpg,False,,[],{},link,,False,,1592987238.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/TehKZy-laMhCDvDPRA5ty9NwaaFUHfJTW2JlQRdKoIM.jpg?auto=webp&amp;s=f94d034e24c9a4547cad6d570b678dec5290382d', 'width': 1246, 'height': 362}, 'resolutions': [{'url': 'https://external-preview.redd.it/TehKZy-laMhCDvDPRA5ty9NwaaFUHfJTW2JlQRdKoIM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dd6267e27473629f8b992892ef5db33532928ec8', 'width': 108, 'height': 31}, {'url': 'https://external-preview.redd.it/TehKZy-laMhCDvDPRA5ty9NwaaFUHfJTW2JlQRdKoIM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f26873a90341bf2152a51f3487ba05518f3f41c6', 'width': 216, 'height': 62}, {'url': 'https://external-preview.redd.it/TehKZy-laMhCDvDPRA5ty9NwaaFUHfJTW2JlQRdKoIM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2612809b5dd15dbbb983d15ba52656a569484e2', 'width': 320, 'height': 92}, {'url': 'https://external-preview.redd.it/TehKZy-laMhCDvDPRA5ty9NwaaFUHfJTW2JlQRdKoIM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e46fe38ef9a01e7d70527348de2582dcfa1fda0', 'width': 640, 'height': 185}, {'url': 'https://external-preview.redd.it/TehKZy-laMhCDvDPRA5ty9NwaaFUHfJTW2JlQRdKoIM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1be974b39ea9882b1aed9c396607e34f808a264f', 'width': 960, 'height': 278}, {'url': 'https://external-preview.redd.it/TehKZy-laMhCDvDPRA5ty9NwaaFUHfJTW2JlQRdKoIM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=161305d235a87359ea1e4bd0ca9abc2a4e4c564e', 'width': 1080, 'height': 313}], 'variants': {}, 'id': 'xHfYl-PE0-s0zi8-AECDWFtupOC0Byxv5mp8bh31wbE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,heqmc5,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/heqmc5/latest_from_max_planck_researchers_estimate_the/,all_ads,False,/r/LatestInML/comments/heq0l0/latest_from_max_planck_researchers_estimate_the/,7135,1592958438.0,0,,False,/r/LatestInML/comments/heq0l0/latest_from_max_planck_researchers_estimate_the/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2003.04583)\n\nhttps://reddit.com/link/heq0l0/video/cwv1r4juyq651/player', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Latest from Max Planck researchers: Estimate the clothing deformations with fine details from input body shape, body pose and garment style.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 40, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'cwv1r4juyq651': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/heq0l0/asset/cwv1r4juyq651/DASHPlaylist.mpd?a=1618044223%2CN2RjOTM0NDA1MzUyYTRiN2NiMzZiYmM1YjJhNzEzZmNjODdiNzhhOTBlNGUwY2VmNzAwM2M2N2JkZWNkMGQ1ZQ%3D%3D&amp;v=1&amp;f=sd', 'x': 426, 'y': 240, 'hlsUrl': 'https://v.redd.it/link/heq0l0/asset/cwv1r4juyq651/HLSPlaylist.m3u8?a=1618044223%2COWIzNTg5ODc1MDA2NGI5Yzk5YTVlZjNkMmY4YmQ1ZjgyZGJlODBlZDhlYTU5YjNmNmMyMDNmMzA3OTExY2FjNQ%3D%3D&amp;v=1&amp;f=sd', 'id': 'cwv1r4juyq651', 'isGif': False}}, 'name': 't3_heq0l0', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 22, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 22, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/4eC6gFpdWaIQkpp_ADPZ7OifhxbbPqBPOukoczSyF4s.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1592984997.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2003.04583""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/heq0l0/video/cwv1r4juyq651/player""&gt;https://reddit.com/link/heq0l0/video/cwv1r4juyq651/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/TehKZy-laMhCDvDPRA5ty9NwaaFUHfJTW2JlQRdKoIM.jpg?auto=webp&amp;s=f94d034e24c9a4547cad6d570b678dec5290382d', 'width': 1246, 'height': 362}, 'resolutions': [{'url': 'https://external-preview.redd.it/TehKZy-laMhCDvDPRA5ty9NwaaFUHfJTW2JlQRdKoIM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dd6267e27473629f8b992892ef5db33532928ec8', 'width': 108, 'height': 31}, {'url': 'https://external-preview.redd.it/TehKZy-laMhCDvDPRA5ty9NwaaFUHfJTW2JlQRdKoIM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f26873a90341bf2152a51f3487ba05518f3f41c6', 'width': 216, 'height': 62}, {'url': 'https://external-preview.redd.it/TehKZy-laMhCDvDPRA5ty9NwaaFUHfJTW2JlQRdKoIM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2612809b5dd15dbbb983d15ba52656a569484e2', 'width': 320, 'height': 92}, {'url': 'https://external-preview.redd.it/TehKZy-laMhCDvDPRA5ty9NwaaFUHfJTW2JlQRdKoIM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e46fe38ef9a01e7d70527348de2582dcfa1fda0', 'width': 640, 'height': 185}, {'url': 'https://external-preview.redd.it/TehKZy-laMhCDvDPRA5ty9NwaaFUHfJTW2JlQRdKoIM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1be974b39ea9882b1aed9c396607e34f808a264f', 'width': 960, 'height': 278}, {'url': 'https://external-preview.redd.it/TehKZy-laMhCDvDPRA5ty9NwaaFUHfJTW2JlQRdKoIM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=161305d235a87359ea1e4bd0ca9abc2a4e4c564e', 'width': 1080, 'height': 313}], 'variants': {}, 'id': 'xHfYl-PE0-s0zi8-AECDWFtupOC0Byxv5mp8bh31wbE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'heq0l0', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/heq0l0/latest_from_max_planck_researchers_estimate_the/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/heq0l0/latest_from_max_planck_researchers_estimate_the/', 'subreddit_subscribers': 6676, 'created_utc': 1592956197.0, 'num_crossposts': 11, 'media': None, 'is_video': False}]",t3_heq0l0,,,,,
493,,pytorch,"Hello everyone!

I made a project template for PyTorch users. 

My template supports logging with Tensorboard and wandb, dataloader with background generator, **distributed learning** with PyTorch DDP, configuring with yaml, code lint &amp; test.

Feel free to use my template and to make the issue to my repo.

[https://github.com/ryul99/pytorch-project-template](https://github.com/ryul99/pytorch-project-template)",t2_4gto164d,False,,0,False,Template for PyTorch Project!,[],r/pytorch,False,6,,0,,,False,t3_hecstw,False,dark,0.67,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},self,,True,,1592940002.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;

&lt;p&gt;I made a project template for PyTorch users. &lt;/p&gt;

&lt;p&gt;My template supports logging with Tensorboard and wandb, dataloader with background generator, &lt;strong&gt;distributed learning&lt;/strong&gt; with PyTorch DDP, configuring with yaml, code lint &amp;amp; test.&lt;/p&gt;

&lt;p&gt;Feel free to use my template and to make the issue to my repo.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/ryul99/pytorch-project-template""&gt;https://github.com/ryul99/pytorch-project-template&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4Lw3JkD55CWwkRMI9jE6GpDRUDqg_2eeCCW-IqWURF4.jpg?auto=webp&amp;s=717acf592fd9c332af7abe30ba73395e3150236a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/4Lw3JkD55CWwkRMI9jE6GpDRUDqg_2eeCCW-IqWURF4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7281894a5e60daf8721b0dd41793d18374f5e180', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/4Lw3JkD55CWwkRMI9jE6GpDRUDqg_2eeCCW-IqWURF4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=684e96a30ac342e3279800a092bf54568aa97a88', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/4Lw3JkD55CWwkRMI9jE6GpDRUDqg_2eeCCW-IqWURF4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e482f3edfdb1506cd24f263f1e58cbe2a96ce89d', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'Ok4O2PmLgHvtofHupMjnvGzZNtSyakb70qYLu4CTurc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hecstw,True,,ryul99,,9,True,all_ads,False,[],False,,/r/pytorch/comments/hecstw/template_for_pytorch_project/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hecstw/template_for_pytorch_project/,7135,1592911202.0,0,,False,,,,,,,,
494,,pytorch,"Hi, I am trying to quantize a MobileNetV3 for use in a pytorch mobile/android application.

However, the MobileNetV3 architecture contains nn.AdaptiveAvgPool2d(1) layers, which can not be quantized, so I would like to replace them with functionally equivalent nn.AvgPool2d() layers.

How can this be achieved?",t2_jexci0,False,,0,False,Looking for nn.AdaptiveAvgPool2d(1) alternative,[],r/pytorch,False,6,,0,,,False,t3_he0rmd,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1592888562.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am trying to quantize a MobileNetV3 for use in a pytorch mobile/android application.&lt;/p&gt;

&lt;p&gt;However, the MobileNetV3 architecture contains nn.AdaptiveAvgPool2d(1) layers, which can not be quantized, so I would like to replace them with functionally equivalent nn.AvgPool2d() layers.&lt;/p&gt;

&lt;p&gt;How can this be achieved?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,he0rmd,True,,ElectronicAvocado,,2,True,all_ads,False,[],False,,/r/pytorch/comments/he0rmd/looking_for_nnadaptiveavgpool2d1_alternative/,all_ads,False,https://www.reddit.com/r/pytorch/comments/he0rmd/looking_for_nnadaptiveavgpool2d1_alternative/,7135,1592859762.0,0,,False,,,,,,,,
495,,pytorch,"Does anyone know if there is a way to select the number of threads for pytorch to use? The only way I have found is to run shorts tests to benchmark my code and then select. Doing this seems to do much better than however the default number of threads is selected, getting a 3x speed up for what I'm currently working on.",t2_8oamaj1,False,,0,False,Selecting number of threads,[],r/pytorch,False,6,,0,,,False,t3_hdz24w,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1592883090.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone know if there is a way to select the number of threads for pytorch to use? The only way I have found is to run shorts tests to benchmark my code and then select. Doing this seems to do much better than however the default number of threads is selected, getting a 3x speed up for what I&amp;#39;m currently working on.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hdz24w,True,,fail_daily,,3,True,all_ads,False,[],False,,/r/pytorch/comments/hdz24w/selecting_number_of_threads/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hdz24w/selecting_number_of_threads/,7135,1592854290.0,0,,False,,,,,,,,
496,,pytorch,"Hi, i am struggling to get my quantized pytorch mobile model running on android. I have followed this tutorial [https://pytorch.org/tutorials/advanced/static\_quantization\_tutorial.html](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html) and I managed to quantize my model without any problems. However, wehen I try to load the model via. Module.*load*(*)* I get the following Exception:

""Unknown builtin op: quantized::linear\_unpack\_fp16.

Could not find any similar ops to quantized::linear\_unpack\_fp16. This op may not exist or may not be currently supported in TorchScript.""

Any ideas on how this can be fixed?  


\---

this is how I quantized the model:

\# quantize model  
torch.quantization.get\_default\_qconfig(backend='qnnpack')  
model.qconfig = torch.quantization.default\_qconfig  
torch.quantization.prepare(model, inplace=True)  
torch.quantization.convert(model, inplace=True)  
traced\_script\_module = torch.jit.trace(model,dummyInput)  
traced\_script\_module.save(""model/modelQuantized.pt"")",t2_jexci0,False,,0,False,Load quantized pytorch model on android,[],r/pytorch,False,6,,0,,,False,t3_hdjiwo,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1592821544.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, i am struggling to get my quantized pytorch mobile model running on android. I have followed this tutorial &lt;a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html""&gt;https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html&lt;/a&gt; and I managed to quantize my model without any problems. However, wehen I try to load the model via. Module.&lt;em&gt;load&lt;/em&gt;(&lt;em&gt;)&lt;/em&gt; I get the following Exception:&lt;/p&gt;

&lt;p&gt;&amp;quot;Unknown builtin op: quantized::linear_unpack_fp16.&lt;/p&gt;

&lt;p&gt;Could not find any similar ops to quantized::linear_unpack_fp16. This op may not exist or may not be currently supported in TorchScript.&amp;quot;&lt;/p&gt;

&lt;p&gt;Any ideas on how this can be fixed?  &lt;/p&gt;

&lt;p&gt;---&lt;/p&gt;

&lt;p&gt;this is how I quantized the model:&lt;/p&gt;

&lt;p&gt;# quantize model&lt;br/&gt;
torch.quantization.get_default_qconfig(backend=&amp;#39;qnnpack&amp;#39;)&lt;br/&gt;
model.qconfig = torch.quantization.default_qconfig&lt;br/&gt;
torch.quantization.prepare(model, inplace=True)&lt;br/&gt;
torch.quantization.convert(model, inplace=True)&lt;br/&gt;
traced_script_module = torch.jit.trace(model,dummyInput)&lt;br/&gt;
traced_script_module.save(&amp;quot;model/modelQuantized.pt&amp;quot;)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hdjiwo,True,,ElectronicAvocado,,1,True,all_ads,False,[],False,,/r/pytorch/comments/hdjiwo/load_quantized_pytorch_model_on_android/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hdjiwo/load_quantized_pytorch_model_on_android/,7135,1592792744.0,0,,False,,,,,,,,
497,,pytorch,"Hello, I'm new to PyTorch and I come from Tensorflow. In Tensorflow the most efficient way to store your dataset would be using a TFRecord. As I seem to understand, in PyTorch you can make a dataset from pretty much anything, is there a preferable file format to store arrays?

Which is the best way to store a dataset which is composed of pairs of np.arrays (the sample and the features to predict)? The whole dataset would not fit in memory, so I need to be able to read a batch of samples from the disk. 

Thank you very much for your help!",t2_26t6gid3,False,,0,False,Best format for dataset,[],r/pytorch,False,6,,0,,,False,t3_hde1sc,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1592800645.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I&amp;#39;m new to PyTorch and I come from Tensorflow. In Tensorflow the most efficient way to store your dataset would be using a TFRecord. As I seem to understand, in PyTorch you can make a dataset from pretty much anything, is there a preferable file format to store arrays?&lt;/p&gt;

&lt;p&gt;Which is the best way to store a dataset which is composed of pairs of np.arrays (the sample and the features to predict)? The whole dataset would not fit in memory, so I need to be able to read a batch of samples from the disk. &lt;/p&gt;

&lt;p&gt;Thank you very much for your help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hde1sc,True,,TrPhantom8,,15,True,all_ads,False,[],False,,/r/pytorch/comments/hde1sc/best_format_for_dataset/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hde1sc/best_format_for_dataset/,7135,1592771845.0,0,,False,,,,,,,,
498,,pytorch,"I'm trying to build a CNN based classifier for which the dataset has the training and test images stored in a separate folder, with the picture names and respective classes stored in a .csv file. Below is my custom dataset implementation: 

class CustomDataset(Dataset):

def \_\_init\_\_(self, csv\_file, root, transform = None):

self.annotations = pd.read\_csv(csv\_file)

self.root = root

self.transform = transform  

def \_\_length\_\_(self):

return len(self.annotations)    

def \_\_getitem\_\_(self, index):

if torch.is\_tensor(index):

index = index.tolist()         

img\_path = os.path.join(self.root, self.annotations.iloc\[index, 0\])

img = io.imread(img\_path)

onehot = preprocessing.OneHotEncoder(handle\_unknown='ignore')

y\_label\_onehot = onehot.fit\_transform(self.annotations\[\['target'\]\]).toarray()

if self.transforms:

img = self.transform(img)          

return (img, y\_label\_onehot)  

\------------------------------------------------------------------------------------------------------------------------------------------------- 

data\_set = CustomDataset(csv\_file = 'train.csv', root = '/train/')    

Data\_Loader = DataLoader(data\_set, batch\_size = 10, shuffle = True

\------------------------------------------------------------------------------------------------------------------------------------------------- 

On running this, it throws an error: 

Traceback (most recent call last):

&amp;#x200B;

  File ""&lt;ipython-input-227-e32ed8ba2148&gt;"", line 2, in &lt;module&gt;

Data\_Loader = DataLoader(data\_set, batch\_size = 10, shuffle = True, sampler = None)

&amp;#x200B;

  File ""C:\\Users\\monse\\Anaconda3\\envs\\practice\_env\\lib\\site-packages\\torch\\utils\\data\\[dataloader.py](https://dataloader.py)"", line 213, in \_\_init\_\_

sampler = RandomSampler(dataset)

&amp;#x200B;

  File ""C:\\Users\\monse\\Anaconda3\\envs\\practice\_env\\lib\\site-packages\\torch\\utils\\data\\[sampler.py](https://sampler.py)"", line 92, in \_\_init\_\_

if not isinstance(self.num\_samples, int) or self.num\_samples &lt;= 0:

&amp;#x200B;

  File ""C:\\Users\\monse\\Anaconda3\\envs\\practice\_env\\lib\\site-packages\\torch\\utils\\data\\[sampler.py](https://sampler.py)"", line 100, in num\_samples

return len(self.data\_source)

&amp;#x200B;

TypeError: object of type 'CustomDataset' has no len()

\------------------------------------------------------------------------------------------------------------------------------------------------- 

Anyone know any fix for this, or some kind of workaround? Any help would be greatly appreciated.",t2_sls3yk5,False,,0,False,DataLoader not instantiating for custom datasets,[],r/pytorch,False,6,,0,,,False,t3_hdcqfv,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1592796008.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to build a CNN based classifier for which the dataset has the training and test images stored in a separate folder, with the picture names and respective classes stored in a .csv file. Below is my custom dataset implementation: &lt;/p&gt;

&lt;p&gt;class CustomDataset(Dataset):&lt;/p&gt;

&lt;p&gt;def __init__(self, csv_file, root, transform = None):&lt;/p&gt;

&lt;p&gt;self.annotations = pd.read_csv(csv_file)&lt;/p&gt;

&lt;p&gt;self.root = root&lt;/p&gt;

&lt;p&gt;self.transform = transform  &lt;/p&gt;

&lt;p&gt;def __length__(self):&lt;/p&gt;

&lt;p&gt;return len(self.annotations)    &lt;/p&gt;

&lt;p&gt;def __getitem__(self, index):&lt;/p&gt;

&lt;p&gt;if torch.is_tensor(index):&lt;/p&gt;

&lt;p&gt;index = index.tolist()         &lt;/p&gt;

&lt;p&gt;img_path = os.path.join(self.root, self.annotations.iloc[index, 0])&lt;/p&gt;

&lt;p&gt;img = io.imread(img_path)&lt;/p&gt;

&lt;p&gt;onehot = preprocessing.OneHotEncoder(handle_unknown=&amp;#39;ignore&amp;#39;)&lt;/p&gt;

&lt;p&gt;y_label_onehot = onehot.fit_transform(self.annotations[[&amp;#39;target&amp;#39;]]).toarray()&lt;/p&gt;

&lt;p&gt;if self.transforms:&lt;/p&gt;

&lt;p&gt;img = self.transform(img)          &lt;/p&gt;

&lt;p&gt;return (img, y_label_onehot)  &lt;/p&gt;

&lt;p&gt;------------------------------------------------------------------------------------------------------------------------------------------------- &lt;/p&gt;

&lt;p&gt;data_set = CustomDataset(csv_file = &amp;#39;train.csv&amp;#39;, root = &amp;#39;/train/&amp;#39;)    &lt;/p&gt;

&lt;p&gt;Data_Loader = DataLoader(data_set, batch_size = 10, shuffle = True&lt;/p&gt;

&lt;p&gt;------------------------------------------------------------------------------------------------------------------------------------------------- &lt;/p&gt;

&lt;p&gt;On running this, it throws an error: &lt;/p&gt;

&lt;p&gt;Traceback (most recent call last):&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;File &amp;quot;&amp;lt;ipython-input-227-e32ed8ba2148&amp;gt;&amp;quot;, line 2, in &amp;lt;module&amp;gt;&lt;/p&gt;

&lt;p&gt;Data_Loader = DataLoader(data_set, batch_size = 10, shuffle = True, sampler = None)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;File &amp;quot;C:\Users\monse\Anaconda3\envs\practice_env\lib\site-packages\torch\utils\data\&lt;a href=""https://dataloader.py""&gt;dataloader.py&lt;/a&gt;&amp;quot;, line 213, in __init__&lt;/p&gt;

&lt;p&gt;sampler = RandomSampler(dataset)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;File &amp;quot;C:\Users\monse\Anaconda3\envs\practice_env\lib\site-packages\torch\utils\data\&lt;a href=""https://sampler.py""&gt;sampler.py&lt;/a&gt;&amp;quot;, line 92, in __init__&lt;/p&gt;

&lt;p&gt;if not isinstance(self.num_samples, int) or self.num_samples &amp;lt;= 0:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;File &amp;quot;C:\Users\monse\Anaconda3\envs\practice_env\lib\site-packages\torch\utils\data\&lt;a href=""https://sampler.py""&gt;sampler.py&lt;/a&gt;&amp;quot;, line 100, in num_samples&lt;/p&gt;

&lt;p&gt;return len(self.data_source)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;TypeError: object of type &amp;#39;CustomDataset&amp;#39; has no len()&lt;/p&gt;

&lt;p&gt;------------------------------------------------------------------------------------------------------------------------------------------------- &lt;/p&gt;

&lt;p&gt;Anyone know any fix for this, or some kind of workaround? Any help would be greatly appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hdcqfv,True,,the_dawmbreaker,,4,True,all_ads,False,[],False,,/r/pytorch/comments/hdcqfv/dataloader_not_instantiating_for_custom_datasets/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hdcqfv/dataloader_not_instantiating_for_custom_datasets/,7135,1592767208.0,0,,False,,,,,,,,
499,,pytorch,"So I have a rather weird problem. 
I have a PDF in this Indian language that I need to learn embeddings for.  
 

But the problem is that copying text from the PDF gives me gibberish, and I've established that there's no way to fix that because the error was introduced when the PDF was made.   
 

For example the text ""భావిస్తుంటాం"" gets copied as ""uÛ≤$düTÔ+{≤+"".  
I've run OCR on the pages and while it is mostly accurate, it is a tedious process that I cannot use to convert the entire PDF.   
 

The gibberish is consistent, everything maps to some unicode character but the length's aren't the same. Quite a lot of unicode characters are actually mapped to a pair of gibberish characters. There's an insane amount of tokens this way so manually finding the mapping for everything would take very long.   
 

So would it be possible to train a character level RNN to do the conversions? I'm probably getting the hammer for something that isn't a nail but it seems fun if nothing else.   
 
I've got around 200 odd sentences in unicode and their gibberish counterparts.",,False,,0,False,Need some help for a character level seq2seq model.,[],r/pytorch,False,6,,0,,,False,t3_hcmob3,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,,self,False,,,{},,,True,,1592690171.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I have a rather weird problem. 
I have a PDF in this Indian language that I need to learn embeddings for.  &lt;/p&gt;

&lt;p&gt;But the problem is that copying text from the PDF gives me gibberish, and I&amp;#39;ve established that there&amp;#39;s no way to fix that because the error was introduced when the PDF was made.   &lt;/p&gt;

&lt;p&gt;For example the text &amp;quot;భావిస్తుంటాం&amp;quot; gets copied as &amp;quot;uÛ≤$düTÔ+{≤+&amp;quot;.&lt;br/&gt;
I&amp;#39;ve run OCR on the pages and while it is mostly accurate, it is a tedious process that I cannot use to convert the entire PDF.   &lt;/p&gt;

&lt;p&gt;The gibberish is consistent, everything maps to some unicode character but the length&amp;#39;s aren&amp;#39;t the same. Quite a lot of unicode characters are actually mapped to a pair of gibberish characters. There&amp;#39;s an insane amount of tokens this way so manually finding the mapping for everything would take very long.   &lt;/p&gt;

&lt;p&gt;So would it be possible to train a character level RNN to do the conversions? I&amp;#39;m probably getting the hammer for something that isn&amp;#39;t a nail but it seems fun if nothing else.   &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve got around 200 odd sentences in unicode and their gibberish counterparts.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hcmob3,True,,[deleted],,2,True,all_ads,False,[],,dark,/r/pytorch/comments/hcmob3/need_some_help_for_a_character_level_seq2seq_model/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hcmob3/need_some_help_for_a_character_level_seq2seq_model/,7135,1592661371.0,0,,False,,,,,,,,
500,,pytorch,What are folks using for hyperparameter optimization for PyTorch?,t2_dl2dx,False,,0,False,Hyperparameter optimization for PyTorch?,[],r/pytorch,False,6,,0,,,False,t3_hcakrz,False,dark,1.0,,public,11,0,{},,,False,[],,False,False,,{},,False,11,,False,self,False,,[],{},,,True,,1592635603.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What are folks using for hyperparameter optimization for PyTorch?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hcakrz,True,,ddebarr,,5,True,all_ads,False,[],False,,/r/pytorch/comments/hcakrz/hyperparameter_optimization_for_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hcakrz/hyperparameter_optimization_for_pytorch/,7135,1592606803.0,0,,False,,,,,,,,
501,,pytorch,Could anyone explain how this [CUDA version of convolution](https://github.com/jdnie/AdderNetCuda/blob/master/unoptimized/unoptimized.cu#L419-L554) works ?,t2_bpftl,False,,0,False,CUDA version of convolution,[],r/pytorch,False,6,,0,,,False,t3_hckezj,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1592679860.0,text,6,,,text,self.pytorch,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Could anyone explain how this &lt;a href=""https://github.com/jdnie/AdderNetCuda/blob/master/unoptimized/unoptimized.cu#L419-L554""&gt;CUDA version of convolution&lt;/a&gt; works ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/1oJELh2pyTu6gRTe3DGgCJBpi6Y3RD_H_8xOxF2TddA.jpg?auto=webp&amp;s=19e61b3b9eeeb20ee7e20703e5c5c18266a45a21', 'width': 420, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/1oJELh2pyTu6gRTe3DGgCJBpi6Y3RD_H_8xOxF2TddA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=15f237bfaebcb0d809189e635acb8004e4cdc046', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/1oJELh2pyTu6gRTe3DGgCJBpi6Y3RD_H_8xOxF2TddA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c5a7779c702ee0db2f10ff5bdc67f388172fed3', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/1oJELh2pyTu6gRTe3DGgCJBpi6Y3RD_H_8xOxF2TddA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89ff9ee69cf09a6ec179ae3b66d4f3fd5d46bff4', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'wmrjIuaBYHhuOtK6iYSNLUsvMmzkBld3QUJyGapSWUk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hckezj,True,,promach,,3,True,all_ads,False,[],False,,/r/pytorch/comments/hckezj/cuda_version_of_convolution/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hckezj/cuda_version_of_convolution/,7135,1592651060.0,2,,False,,,,,,,,
502,,pytorch,"Hey y'all,

I recently published a major revision for a library that I have been developing to ease training PyTorch models. It has been built based on experiences that I made as research assistant at university.

The library does not only make it markedly easier to train simple models such as image classifiers but is highly extensible to e.g. train complex generative adversarial networks for graphs.

In comparison to PyTorch Lightning and PyTorch Ignite, *PyBlaze* is much less obtrusive and thus makes it very easy to simplify code written in plain PyTorch.

Check it out and I'm happy about any feedback!

[https://github.com/borchero/pyblaze](https://github.com/borchero/pyblaze)",t2_22mmlgir,False,,0,False,High-Level Library for Large-Scale Machine Learning in PyTorch,[],r/pytorch,False,6,,0,,,False,t3_hc571y,False,dark,1.0,,public,11,0,{},,,False,[],,False,False,,{},,False,11,,False,self,False,,[],{},self,,True,,1592617505.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey y&amp;#39;all,&lt;/p&gt;

&lt;p&gt;I recently published a major revision for a library that I have been developing to ease training PyTorch models. It has been built based on experiences that I made as research assistant at university.&lt;/p&gt;

&lt;p&gt;The library does not only make it markedly easier to train simple models such as image classifiers but is highly extensible to e.g. train complex generative adversarial networks for graphs.&lt;/p&gt;

&lt;p&gt;In comparison to PyTorch Lightning and PyTorch Ignite, &lt;em&gt;PyBlaze&lt;/em&gt; is much less obtrusive and thus makes it very easy to simplify code written in plain PyTorch.&lt;/p&gt;

&lt;p&gt;Check it out and I&amp;#39;m happy about any feedback!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/borchero/pyblaze""&gt;https://github.com/borchero/pyblaze&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/oIsqnqmVWIgJS4V5TPoI9Jw1DetPTG2dZRkDZ0tvuIQ.jpg?auto=webp&amp;s=fd5e3715961d6e9effaf57c0b0cf5eacaf9cf11b', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/oIsqnqmVWIgJS4V5TPoI9Jw1DetPTG2dZRkDZ0tvuIQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=81527106bfb0b76c67ff73839c54fed0b77362b7', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/oIsqnqmVWIgJS4V5TPoI9Jw1DetPTG2dZRkDZ0tvuIQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=82cf1a30a1492cb8f6f28e6004ba33fe3521cbc3', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/oIsqnqmVWIgJS4V5TPoI9Jw1DetPTG2dZRkDZ0tvuIQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9aaa7a291924da3784119e6f940eb16d4fa2bf9e', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'EUnquBhItSlykbHdYCdN151xtSjjdWeL6zScFjYcEmc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hc571y,True,,borchero,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hc571y/highlevel_library_for_largescale_machine_learning/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hc571y/highlevel_library_for_largescale_machine_learning/,7135,1592588705.0,0,,False,,,,,,,,
503,,pytorch,,t2_58sia09x,False,,0,False,We're building a labeling platform for image segmentation. Looking for feedback!,[],r/pytorch,False,6,,0,78.0,,False,t3_hc2kur,False,dark,1.0,,public,15,0,{},140.0,,False,[],"{'reddit_video': {'fallback_url': 'https://v.redd.it/tm625p5lwv551/DASH_720?source=fallback', 'height': 720, 'width': 1280, 'scrubber_media_url': 'https://v.redd.it/tm625p5lwv551/DASH_96', 'dash_url': 'https://v.redd.it/tm625p5lwv551/DASHPlaylist.mpd?a=1618044231%2CMWYxNDYyNWYzMThiM2ViZWFiMGYwNjZmMzUxZmM0OTkxZDAyNjQ4YTkzNTMzMWNhMzRhMWE0NDRmMmI2MTBjYw%3D%3D&amp;v=1&amp;f=sd', 'duration': 56, 'hls_url': 'https://v.redd.it/tm625p5lwv551/HLSPlaylist.m3u8?a=1618044231%2CMjRkZjhiNDcyZmRmNTIzNmUxMjBjNTA2MzM5ZWMyY2FhYTgxZTQ2MTkzMWM0N2YyYWEwYTQzYmRjNDdiYjNiYg%3D%3D&amp;v=1&amp;f=sd', 'is_gif': False, 'transcoding_status': 'completed'}}",True,False,,{},,False,15,,False,https://b.thumbs.redditmedia.com/Jyz-Mvj-ZgGwbON6wRxSWpVGUdd4G6QYZGAZ8mKd31Q.jpg,False,,[],{},hosted:video,,False,,1592608869.0,text,6,,,text,v.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vsiubxmPW-T4abWzzHIb7Wwx7vDbDWQqBoULfx--JW8.png?format=pjpg&amp;auto=webp&amp;s=750f3157d00b35ba11dc7e48c5bfa544f8c43d2c', 'width': 1280, 'height': 720}, 'resolutions': [{'url': 'https://external-preview.redd.it/vsiubxmPW-T4abWzzHIb7Wwx7vDbDWQqBoULfx--JW8.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=de9fbd029c69719d34f90ae71e9e4763b6fe483f', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/vsiubxmPW-T4abWzzHIb7Wwx7vDbDWQqBoULfx--JW8.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2365b943cde5198bf27c3c0864de8055cf95cbc9', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/vsiubxmPW-T4abWzzHIb7Wwx7vDbDWQqBoULfx--JW8.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3760df621b7bca1e353632d90e135378c1f94fa8', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/vsiubxmPW-T4abWzzHIb7Wwx7vDbDWQqBoULfx--JW8.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=53f69460b1878377e150875c088d70ab393acc10', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/vsiubxmPW-T4abWzzHIb7Wwx7vDbDWQqBoULfx--JW8.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=96dd30d592280766d603b430b842bcd83a17e1c7', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/vsiubxmPW-T4abWzzHIb7Wwx7vDbDWQqBoULfx--JW8.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5612d05fe607fcd4540a8ec7b02dcaf121053b5c', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'V44ETkOpnJPH-fqgrGXs9hOCOITKB42yiE_GN-9iC5M'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hc2kur,True,,segments-bert,,3,True,all_ads,False,[],False,,/r/pytorch/comments/hc2kur/were_building_a_labeling_platform_for_image/,all_ads,False,https://v.redd.it/tm625p5lwv551,7135,1592580069.0,0,"{'reddit_video': {'fallback_url': 'https://v.redd.it/tm625p5lwv551/DASH_720?source=fallback', 'height': 720, 'width': 1280, 'scrubber_media_url': 'https://v.redd.it/tm625p5lwv551/DASH_96', 'dash_url': 'https://v.redd.it/tm625p5lwv551/DASHPlaylist.mpd?a=1618044231%2CMWYxNDYyNWYzMThiM2ViZWFiMGYwNjZmMzUxZmM0OTkxZDAyNjQ4YTkzNTMzMWNhMzRhMWE0NDRmMmI2MTBjYw%3D%3D&amp;v=1&amp;f=sd', 'duration': 56, 'hls_url': 'https://v.redd.it/tm625p5lwv551/HLSPlaylist.m3u8?a=1618044231%2CMjRkZjhiNDcyZmRmNTIzNmUxMjBjNTA2MzM5ZWMyY2FhYTgxZTQ2MTkzMWM0N2YyYWEwYTQzYmRjNDdiYjNiYg%3D%3D&amp;v=1&amp;f=sd', 'is_gif': False, 'transcoding_status': 'completed'}}",True,https://v.redd.it/tm625p5lwv551,,,,,,,
504,,pytorch,"I have 2 .PTH files that are supposedly a finished midi generator and midi descriminator.  How can I use these PTH files to generate an actual file?

original GitHub for the GAN training was: [https://github.com/cjbayron/c-rnn-gan.pytorch](https://github.com/cjbayron/c-rnn-gan.pytorch)",t2_35kvmhpu,False,,0,False,What is .PTH file in the scope of Pytorch or tensorflow?,[],r/pytorch,False,6,,0,,,False,t3_hbqnnn,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},self,,True,,1592555986.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have 2 .PTH files that are supposedly a finished midi generator and midi descriminator.  How can I use these PTH files to generate an actual file?&lt;/p&gt;

&lt;p&gt;original GitHub for the GAN training was: &lt;a href=""https://github.com/cjbayron/c-rnn-gan.pytorch""&gt;https://github.com/cjbayron/c-rnn-gan.pytorch&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jZUDN28URrEHac4v1Foyl535FjjYz6-v0_9VR3HMohA.jpg?auto=webp&amp;s=07a3a4b3be7142a834300ff53e8d30c7e4406279', 'width': 420, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/jZUDN28URrEHac4v1Foyl535FjjYz6-v0_9VR3HMohA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b69beeb18bead7c9308764a6fe05e27f5e4e4e8d', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/jZUDN28URrEHac4v1Foyl535FjjYz6-v0_9VR3HMohA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=08592a3c6ab5d4b5ef10499b8dae4bdaf43bd04c', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/jZUDN28URrEHac4v1Foyl535FjjYz6-v0_9VR3HMohA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=035ed698b21d6b2a056765ef9f6c4cea37ccbcaf', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'bR2OYgSLUNDyfdNldI7Apts3BuKLZBQ4VIRAGGE40JM'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hbqnnn,True,,SalvosMachina,,4,True,all_ads,False,[],False,,/r/pytorch/comments/hbqnnn/what_is_pth_file_in_the_scope_of_pytorch_or/,all_ads,False,https://www.reddit.com/r/pytorch/comments/hbqnnn/what_is_pth_file_in_the_scope_of_pytorch_or/,7135,1592527186.0,0,,False,,,,,,,,
505,,pytorch,,t2_15n8fb,False,,0,False,How does one implemented a parametrized meta-learner in Pytorch's higher library?,[],r/pytorch,False,6,,0,140.0,,False,t3_hbp1n5,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/-4TNpMnFGKT32sp8J5VQ-EyOeVVALWkjvH8gCcrbRVA.jpg,False,,[],{},link,,False,,1592550093.0,text,6,,,text,stackoverflow.com,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,hbp1n5,True,,real_pinocchio,,0,True,all_ads,False,[],False,,/r/pytorch/comments/hbp1n5/how_does_one_implemented_a_parametrized/,all_ads,False,https://stackoverflow.com/questions/62459891/how-does-one-implemented-a-parametrized-meta-learner-in-pytorchs-higher-library,7135,1592521293.0,0,,False,https://stackoverflow.com/questions/62459891/how-does-one-implemented-a-parametrized-meta-learner-in-pytorchs-higher-library,,,,,,,
506,,pytorch," I am completely new to Pytorch and I created my first model. I made a similar model in Keras and use this code to test it on data it never has seen before: 

    from keras.models import load_model
    import numpy as np
    from keras.preprocessing import image
    import time
    import sys
    import PIL
       
    start_time =time.time()
    classifier = load_model(""C:/Users/deonh/Pictures/Summer Work/model/Covid-19.h5"")
    #classifier.summary()
    
    test_image = image.load_img('C:/Users/deonh/Pictures/Summer Work/Prediction/pnemonia_Covid_or_Normal_1.jpeg', target_size = (64, 64))
    #test_image.show()
    test_image = image.img_to_array(test_image)
    
    test_image = np.expand_dims(test_image, axis = 0)
    result = classifier.predict(test_image)
    print(result)
    #training_set.class_indices
    print(""%s seconds"" %(time.time() - start_time) + "" run time"")

 My goal is to do something similar with my Pytorch model. This is what I have so far, but it is not working. 

    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import DataLoader
    from torchvision import datasets, transforms, models # add models to the list
    import os
    import time
    from PIL import Image
    from IPython.display import display
    
    
    
    model = torch.load('C:/Users/deonh/Pictures/Summer Work/xrays/PythonApplication1/scans.pth')
    
    
    photo = Image.open('C:/Users/deonh/Pictures/Summer Work/prediction/Covid_Normal_or_Pnemonia.jpg')
    
    
    #photo.show()
    
    transform = transforms.Compose([
            
            transforms.Resize(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                                 [0.229, 0.224, 0.225])
        ])
    
    im = transform(photo)
    
    
    #with torch.no_grad():
    prediction = model(im)

 I am getting a **‘collections.OrderedDict’ object is not callable** error",t2_48egj5xw,False,,0,False,How to load and use a trained model?,[],r/pytorch,False,6,,0,,,False,t3_haap0y,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1592364021.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am completely new to Pytorch and I created my first model. I made a similar model in Keras and use this code to test it on data it never has seen before: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from keras.models import load_model
import numpy as np
from keras.preprocessing import image
import time
import sys
import PIL

start_time =time.time()
classifier = load_model(&amp;quot;C:/Users/deonh/Pictures/Summer Work/model/Covid-19.h5&amp;quot;)
#classifier.summary()

test_image = image.load_img(&amp;#39;C:/Users/deonh/Pictures/Summer Work/Prediction/pnemonia_Covid_or_Normal_1.jpeg&amp;#39;, target_size = (64, 64))
#test_image.show()
test_image = image.img_to_array(test_image)

test_image = np.expand_dims(test_image, axis = 0)
result = classifier.predict(test_image)
print(result)
#training_set.class_indices
print(&amp;quot;%s seconds&amp;quot; %(time.time() - start_time) + &amp;quot; run time&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My goal is to do something similar with my Pytorch model. This is what I have so far, but it is not working. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models # add models to the list
import os
import time
from PIL import Image
from IPython.display import display



model = torch.load(&amp;#39;C:/Users/deonh/Pictures/Summer Work/xrays/PythonApplication1/scans.pth&amp;#39;)


photo = Image.open(&amp;#39;C:/Users/deonh/Pictures/Summer Work/prediction/Covid_Normal_or_Pnemonia.jpg&amp;#39;)


#photo.show()

transform = transforms.Compose([

        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ])

im = transform(photo)


#with torch.no_grad():
prediction = model(im)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I am getting a &lt;strong&gt;‘collections.OrderedDict’ object is not callable&lt;/strong&gt; error&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,haap0y,True,,stunbomb1,,4,True,all_ads,False,[],False,,/r/pytorch/comments/haap0y/how_to_load_and_use_a_trained_model/,all_ads,False,https://www.reddit.com/r/pytorch/comments/haap0y/how_to_load_and_use_a_trained_model/,7135,1592335221.0,0,,False,,,,,,,,
507,,pytorch,"I am extremely new to pytorch, so please bear with me !!!!

&amp;#x200B;

My dataset is basically a series of 512x512 jpg images. For context, there are several objects in each image, and in a separate JSON file, I have coordinates of the bounding boxes for each type of object. Can anyone give me advice as to how to instruct pytorch to train on the specific objects in my images? Thanks so much in advance",t2_qhiad,False,,0,False,How do i preprocess subimages from my larger images?,[],r/pytorch,False,6,,0,,,False,t3_h9xv0m,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1592315892.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am extremely new to pytorch, so please bear with me !!!!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;My dataset is basically a series of 512x512 jpg images. For context, there are several objects in each image, and in a separate JSON file, I have coordinates of the bounding boxes for each type of object. Can anyone give me advice as to how to instruct pytorch to train on the specific objects in my images? Thanks so much in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,h9xv0m,True,,GeNiaaz,,4,True,all_ads,False,[],False,,/r/pytorch/comments/h9xv0m/how_do_i_preprocess_subimages_from_my_larger/,all_ads,False,https://www.reddit.com/r/pytorch/comments/h9xv0m/how_do_i_preprocess_subimages_from_my_larger/,7135,1592287092.0,0,,False,,,,,,,,
508,,pytorch,"I am doing time series analysis and i decided to use batches for training to try and speed up the training, when i tried to predict on the test dataset tho I ran into an obvious problem, the model was predicting in batches yet i need t-1 to predict t ...

Is there any way to use batches for training and yet predict single instances?",t2_3w9zrapb,False,,0,False,Question about batches on LSTM net,[],r/pytorch,False,6,,0,,,False,t3_h9e5l3,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1592247924.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am doing time series analysis and i decided to use batches for training to try and speed up the training, when i tried to predict on the test dataset tho I ran into an obvious problem, the model was predicting in batches yet i need t-1 to predict t ...&lt;/p&gt;

&lt;p&gt;Is there any way to use batches for training and yet predict single instances?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,h9e5l3,True,,---Morgan---,,6,True,all_ads,False,[],False,,/r/pytorch/comments/h9e5l3/question_about_batches_on_lstm_net/,all_ads,False,https://www.reddit.com/r/pytorch/comments/h9e5l3/question_about_batches_on_lstm_net/,7135,1592219124.0,0,,False,,,,,,,,
509,,pytorch,"Hello. I deployed a trained model via flask on heroku. I noticed my predict method keeps failing at the point of applying a transform to an input image.

&amp;#x200B;

This is the error message

 RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0  
 

&amp;#x200B;

    def transform_image(infile):
        my_transforms = transforms.Compose([transforms.Resize(256),
                                            transforms.CenterCrop(224),
                                            transforms.ToTensor(),
                                            transforms.Normalize(
                                                [0.485, 0.456, 0.406],
                                                [0.229, 0.224, 0.225])])
        image = Image.open(infile)
        timg = my_transforms(image)
        timg.unsqueeze_(0)
        return timg

It fails at 

     timg = my_transforms(image)

I am at a loss what to do to fix this issue",t2_rb4u3,False,,0,False,Error when running prediction,[],r/pytorch,False,6,,0,,,False,t3_h8rj6b,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1592159963.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello. I deployed a trained model via flask on heroku. I noticed my predict method keeps failing at the point of applying a transform to an input image.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;This is the error message&lt;/p&gt;

&lt;p&gt;RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0  &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def transform_image(infile):
    my_transforms = transforms.Compose([transforms.Resize(256),
                                        transforms.CenterCrop(224),
                                        transforms.ToTensor(),
                                        transforms.Normalize(
                                            [0.485, 0.456, 0.406],
                                            [0.229, 0.224, 0.225])])
    image = Image.open(infile)
    timg = my_transforms(image)
    timg.unsqueeze_(0)
    return timg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It fails at &lt;/p&gt;

&lt;pre&gt;&lt;code&gt; timg = my_transforms(image)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I am at a loss what to do to fix this issue&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,h8rj6b,True,,Pranklyspeaking,,3,True,all_ads,False,[],False,,/r/pytorch/comments/h8rj6b/error_when_running_prediction/,all_ads,False,https://www.reddit.com/r/pytorch/comments/h8rj6b/error_when_running_prediction/,7135,1592131163.0,0,,False,,,,,,,,
510,,pytorch,"hello. I am trying to make custom PyTorch dataset from the [Dota2 match dataset](https://www.kaggle.com/deepakkn/dota-2016) 

I have inherited Dataset class like this.  


    class dotaset(Dataset):
        def __init__(self, train=True):
            if (train == False):
                self.dataframe = pd.read_csv(test_path)
            else:
                self.dataframe = pd.read_csv(train_path)
    
                
        def __len__(self):
            return len(self.dataframe)
        
        
        def __getitem__(self, index):
            if torch.is_tensor(index):
                index = index.tolist()
            target = self.dataframe.iloc[index, 0]
            features = self.dataframe.iloc[index, 1:]
            features = torch.tensor(features, dtype=torch.float32, requires_grad=True)
            return features, target

This works if I want to access individual samples like `train_dl[69]`

However, if I try to use a DataLoader like this,

    train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)
    
    for features, label in train_dl:
        print(features, label)
        break

it gives the following error:

    RuntimeError: Caught RuntimeError in DataLoader worker process 0.
    Original Traceback (most recent call last):
      File ""/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py"", line 178, in _worker_loop
        data = fetcher.fetch(index)
      File ""/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 47, in fetch
        return self.collate_fn(data)
      File ""/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py"", line 79, in default_collate
        return [default_collate(samples) for samples in transposed]
      File ""/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py"", line 79, in &lt;listcomp&gt;
        return [default_collate(samples) for samples in transposed]
      File ""/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py"", line 55, in default_collate
        return torch.stack(batch, 0, out=out)
    RuntimeError: stack(): functions with out=... arguments don't support automatic differentiation, but one of the arguments requires grad.

However, if I remove the `num_workers=4` , the code runs fine. What seems to be the problem here?",t2_108f9l,False,,0,False,Custom PyTorch dataset not working properly,[],r/pytorch,False,6,,0,,,False,t3_h7r6dt,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1592018315.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;hello. I am trying to make custom PyTorch dataset from the &lt;a href=""https://www.kaggle.com/deepakkn/dota-2016""&gt;Dota2 match dataset&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;I have inherited Dataset class like this.  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class dotaset(Dataset):
    def __init__(self, train=True):
        if (train == False):
            self.dataframe = pd.read_csv(test_path)
        else:
            self.dataframe = pd.read_csv(train_path)


    def __len__(self):
        return len(self.dataframe)


    def __getitem__(self, index):
        if torch.is_tensor(index):
            index = index.tolist()
        target = self.dataframe.iloc[index, 0]
        features = self.dataframe.iloc[index, 1:]
        features = torch.tensor(features, dtype=torch.float32, requires_grad=True)
        return features, target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works if I want to access individual samples like &lt;code&gt;train_dl[69]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;However, if I try to use a DataLoader like this,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)

for features, label in train_dl:
    print(features, label)
    break
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;it gives the following error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File &amp;quot;/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py&amp;quot;, line 178, in _worker_loop
    data = fetcher.fetch(index)
  File &amp;quot;/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py&amp;quot;, line 47, in fetch
    return self.collate_fn(data)
  File &amp;quot;/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&amp;quot;, line 79, in default_collate
    return [default_collate(samples) for samples in transposed]
  File &amp;quot;/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&amp;quot;, line 79, in &amp;lt;listcomp&amp;gt;
    return [default_collate(samples) for samples in transposed]
  File &amp;quot;/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&amp;quot;, line 55, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack(): functions with out=... arguments don&amp;#39;t support automatic differentiation, but one of the arguments requires grad.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, if I remove the &lt;code&gt;num_workers=4&lt;/code&gt; , the code runs fine. What seems to be the problem here?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/YFXvo2UcFrLekqXlHQGaSTmSkU7EBaxY6kuHYAUjBms.jpg?auto=webp&amp;s=532ab8e01978518e47c9a6fc99061a2912d3f31f', 'width': 600, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/YFXvo2UcFrLekqXlHQGaSTmSkU7EBaxY6kuHYAUjBms.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=223ad2343c1f479c28332b13955e67298c065a21', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/YFXvo2UcFrLekqXlHQGaSTmSkU7EBaxY6kuHYAUjBms.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7b0b5d6bd36c5c3737210bb38411970548722632', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/YFXvo2UcFrLekqXlHQGaSTmSkU7EBaxY6kuHYAUjBms.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b96b3ee9501f06f032caf266e20e22315d19ac36', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'ovCNHH8EenbyqMCRO6qV4TwXgkGoVUodRZFSmIt31x4'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,h7r6dt,True,,s_basu,,8,True,all_ads,False,[],False,,/r/pytorch/comments/h7r6dt/custom_pytorch_dataset_not_working_properly/,all_ads,False,https://www.reddit.com/r/pytorch/comments/h7r6dt/custom_pytorch_dataset_not_working_properly/,7135,1591989515.0,0,,False,,,,,,,,
511,,pytorch,"This is Pytorch documentation for random image translation, but I don't understand what is ""range"" in this formula (I highlighted it)?

&amp;#x200B;

https://preview.redd.it/xykhadtmnh451.png?width=1104&amp;format=png&amp;auto=webp&amp;s=97e4ab297adfb2e906087148d8fa020e0df07c6a",t2_8iq8g3v,False,,0,False,Pytorch Data Augmentation - Random Image Translation Function,[],r/pytorch,False,6,,0,107.0,,False,t3_h7lk5m,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/Y6616phjbP9y9_KoTyDbSuxrLozAHX5bj3DtE1LZAog.jpg,False,,[],{},,,True,,1592000483.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is Pytorch documentation for random image translation, but I don&amp;#39;t understand what is &amp;quot;range&amp;quot; in this formula (I highlighted it)?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/xykhadtmnh451.png?width=1104&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=97e4ab297adfb2e906087148d8fa020e0df07c6a""&gt;https://preview.redd.it/xykhadtmnh451.png?width=1104&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=97e4ab297adfb2e906087148d8fa020e0df07c6a&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,h7lk5m,True,,brgreen25,,1,True,all_ads,False,[],False,,/r/pytorch/comments/h7lk5m/pytorch_data_augmentation_random_image/,all_ads,False,https://www.reddit.com/r/pytorch/comments/h7lk5m/pytorch_data_augmentation_random_image/,7135,1591971683.0,0,,False,,,,"{'xykhadtmnh451': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 83, 'x': 108, 'u': 'https://preview.redd.it/xykhadtmnh451.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b7442703c73a04326c47a26087947fbdc115c0f1'}, {'y': 166, 'x': 216, 'u': 'https://preview.redd.it/xykhadtmnh451.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=76ec637467f25c66e3cdd8e6bedc5fb11d1565a1'}, {'y': 246, 'x': 320, 'u': 'https://preview.redd.it/xykhadtmnh451.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8825d21fc8171a5b3a0c453acccc71d49b2bfaa8'}, {'y': 493, 'x': 640, 'u': 'https://preview.redd.it/xykhadtmnh451.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7753d7ce469dd5ef05da8c8178bb4ff1f6818751'}, {'y': 740, 'x': 960, 'u': 'https://preview.redd.it/xykhadtmnh451.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=551c0bc2c42ce1c90f14c932856cd7babbedd0a5'}, {'y': 832, 'x': 1080, 'u': 'https://preview.redd.it/xykhadtmnh451.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dff47676c30d9187ab54a8f85da095f1a0003be2'}], 's': {'y': 851, 'x': 1104, 'u': 'https://preview.redd.it/xykhadtmnh451.png?width=1104&amp;format=png&amp;auto=webp&amp;s=97e4ab297adfb2e906087148d8fa020e0df07c6a'}, 'id': 'xykhadtmnh451'}}",,,,
512,,pytorch,,t2_44mbtmjy,False,,0,False,From SIGGRAPH 2020: Method reconstructs the geometry of complex 3D thin structures in high quality from a color video captured by a handheld camera,[],r/pytorch,False,6,,0,63.0,,False,t3_h7ckuk,False,dark,1.0,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/R2MlNII9Eye1aIDO30UhiSAmRjl0HVS58eW4YBpfY6k.jpg,False,,[],{},,,False,,1591959055.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,h7ckuk,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/h7ckuk/from_siggraph_2020_method_reconstructs_the/,all_ads,False,/r/LatestInML/comments/h7bctu/from_siggraph_2020_method_reconstructs_the/,7135,1591930255.0,0,,False,/r/LatestInML/comments/h7bctu/from_siggraph_2020_method_reconstructs_the/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert request:\xa0[click here](https://www.catalyzex.com/paper/arxiv:2005.03372)\n\nhttps://preview.redd.it/h5eih01oud451.png?width=1902&amp;format=png&amp;auto=webp&amp;s=c1dad94446e8172ac4926d9fa44c94e2e961ae6f\n\nMethod achieves accurate camera pose estimation and faithful reconstruction of 3D thin structures with complex shape and topology at a level that has not been attained by other existing reconstruction methods.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 1, 'clicked': False, 'title': 'From SIGGRAPH 2020: Method reconstructs the geometry of complex 3D thin structures in high quality from a color video captured by a handheld camera', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 63, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'h5eih01oud451': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 49, 'x': 108, 'u': 'https://preview.redd.it/h5eih01oud451.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=201c86dc35d735140e6c4aaf15e32f18bc67dfb7'}, {'y': 98, 'x': 216, 'u': 'https://preview.redd.it/h5eih01oud451.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=23e415a1ad969c9cf9337003783d8b116c07beb6'}, {'y': 145, 'x': 320, 'u': 'https://preview.redd.it/h5eih01oud451.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e8eaafd6cefc7f95629e717153564b888c50492c'}, {'y': 290, 'x': 640, 'u': 'https://preview.redd.it/h5eih01oud451.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5413b0f073f1d7c8500ab6427c9ea8302598b4fb'}, {'y': 436, 'x': 960, 'u': 'https://preview.redd.it/h5eih01oud451.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cd90b82713620c98bb6734b0c52e936ce40e562f'}, {'y': 490, 'x': 1080, 'u': 'https://preview.redd.it/h5eih01oud451.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a2fd896aae36a16a20cf051e3421ddaaf465b462'}], 's': {'y': 864, 'x': 1902, 'u': 'https://preview.redd.it/h5eih01oud451.png?width=1902&amp;format=png&amp;auto=webp&amp;s=c1dad94446e8172ac4926d9fa44c94e2e961ae6f'}, 'id': 'h5eih01oud451'}}, 'name': 't3_h7bctu', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 34, 'total_awards_received': 1, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 34, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/R2MlNII9Eye1aIDO30UhiSAmRjl0HVS58eW4YBpfY6k.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {'gid_2': 1}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1591954427.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert request:\xa0&lt;a href=""https://www.catalyzex.com/paper/arxiv:2005.03372""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/h5eih01oud451.png?width=1902&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1dad94446e8172ac4926d9fa44c94e2e961ae6f""&gt;https://preview.redd.it/h5eih01oud451.png?width=1902&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1dad94446e8172ac4926d9fa44c94e2e961ae6f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Method achieves accurate camera pose estimation and faithful reconstruction of 3D thin structures with complex shape and topology at a level that has not been attained by other existing reconstruction methods.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 500, 'id': 'gid_2', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 100, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/gold_512.png', 'days_of_premium': 7, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/gold_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Gold', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/gold_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/gold_512.png'}], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'h7bctu', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/h7bctu/from_siggraph_2020_method_reconstructs_the/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/h7bctu/from_siggraph_2020_method_reconstructs_the/', 'subreddit_subscribers': 6676, 'created_utc': 1591925627.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_h7bctu,,,,,
513,,pytorch,"Hi everyone, 

I am a beginner in pytorch. I was using MXNET and TF so far but pytorch seems really simple. I am coding up some popular papers in NLP with pytorch for others to use. Please feel free to add issues, comments, suggestions (as critical and brutal you could be :) ). 

Code repo : [https://github.com/y12uc231/nlp\_paper\_implementations](https://github.com/y12uc231/nlp_paper_implementations)

I hope it helps some newbies.",t2_5bc0iet2,False,,0,False,NLP Paper implementations in pytorch,[],r/pytorch,False,6,,0,,,False,t3_h04h4i,False,dark,0.95,,public,15,0,{},,,False,[],,False,False,,{},,False,15,,False,self,False,,[],{},self,,True,,1591795483.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;

&lt;p&gt;I am a beginner in pytorch. I was using MXNET and TF so far but pytorch seems really simple. I am coding up some popular papers in NLP with pytorch for others to use. Please feel free to add issues, comments, suggestions (as critical and brutal you could be :) ). &lt;/p&gt;

&lt;p&gt;Code repo : &lt;a href=""https://github.com/y12uc231/nlp_paper_implementations""&gt;https://github.com/y12uc231/nlp_paper_implementations&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I hope it helps some newbies.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ne2M-OtxlUD33Yck0jYdi55oOcl3pMX442Zj4K0Zdt0.jpg?auto=webp&amp;s=c7cf46c072669b46fea1d2eddf57011174e118f6', 'width': 181, 'height': 181}, 'resolutions': [{'url': 'https://external-preview.redd.it/ne2M-OtxlUD33Yck0jYdi55oOcl3pMX442Zj4K0Zdt0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=66e863eae1688accf6498023d877dde38150b7be', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'wTqyt-h5g1XgVMY5LSNWcnzWEQWwXvioQposhCZat2s'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,h04h4i,True,,satyapk_invest,,2,True,all_ads,False,[],False,,/r/pytorch/comments/h04h4i/nlp_paper_implementations_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/h04h4i/nlp_paper_implementations_in_pytorch/,7135,1591766683.0,0,,False,,,,,,,,
514,,pytorch,,t2_44mbtmjy,False,,0,False,DeepFaceDrawing system allows users with little training in drawing to produce high-quality face images,[],r/pytorch,False,6,,0,55.0,,False,t3_h0141l,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/j4F2IuCHl2nebv_kNHfiPTlFtdVcZ6LPTVQRqEZdg2E.jpg,False,,[],{},link,,False,,1591781605.0,text,6,,,text,self.LatestInML,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vF44h8zojy4IL7VX9B28ZMR6TjgW5ULoCo0HHX5Q8mQ.jpg?auto=webp&amp;s=0735e62b72962f53e7ebb5ae36c8990f3ed549b2', 'width': 1416, 'height': 632}, 'resolutions': [{'url': 'https://external-preview.redd.it/vF44h8zojy4IL7VX9B28ZMR6TjgW5ULoCo0HHX5Q8mQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=10e76eb3b95cd0153d3cee356b95d72cc675b8db', 'width': 108, 'height': 48}, {'url': 'https://external-preview.redd.it/vF44h8zojy4IL7VX9B28ZMR6TjgW5ULoCo0HHX5Q8mQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d487a4b29c8ceaf24e60c5463e1d3e238b70dae7', 'width': 216, 'height': 96}, {'url': 'https://external-preview.redd.it/vF44h8zojy4IL7VX9B28ZMR6TjgW5ULoCo0HHX5Q8mQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3213d02cc95f645e405444e70b41483ebe655a3', 'width': 320, 'height': 142}, {'url': 'https://external-preview.redd.it/vF44h8zojy4IL7VX9B28ZMR6TjgW5ULoCo0HHX5Q8mQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ccda282569ac85860c19fb3431e45d00286927e6', 'width': 640, 'height': 285}, {'url': 'https://external-preview.redd.it/vF44h8zojy4IL7VX9B28ZMR6TjgW5ULoCo0HHX5Q8mQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f0ec622eaaa537e3fc5b50f89c58e2f336608ad1', 'width': 960, 'height': 428}, {'url': 'https://external-preview.redd.it/vF44h8zojy4IL7VX9B28ZMR6TjgW5ULoCo0HHX5Q8mQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf84579ecf23ebcffb128c704081239a526afbf0', 'width': 1080, 'height': 482}], 'variants': {}, 'id': 'Q3i1cSUvyTWVvGH08hmCDExTvdqZ2AEr4hbiJNF96Dg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,h0141l,True,,MLtinkerer,,1,True,all_ads,False,[],False,,/r/pytorch/comments/h0141l/deepfacedrawing_system_allows_users_with_little/,all_ads,False,/r/LatestInML/comments/h00ve4/deepfacedrawing_system_allows_users_with_little/,7135,1591752805.0,0,,False,/r/LatestInML/comments/h00ve4/deepfacedrawing_system_allows_users_with_little/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code/API/expert requests: [click here](https://www.catalyzex.com/paper/arxiv:2006.01047)\n\nhttps://preview.redd.it/vkjdtkh4iz351.jpg?width=10929&amp;format=pjpg&amp;auto=webp&amp;s=6d756d2ca075d51f9f818d97ca0b8e4cf7ed6576\n\nFrom rough or even incomplete freehand sketches. Note that the method faithfully respects user intentions in input strokes, which serve more like soft constraints to guide image synthesis.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'DeepFaceDrawing system allows users with little training in drawing to produce high-quality face images', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 55, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'vkjdtkh4iz351': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 42, 'x': 108, 'u': 'https://preview.redd.it/vkjdtkh4iz351.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cfa66a58728339b343afaa9f0ea2af4c6bc4b867'}, {'y': 85, 'x': 216, 'u': 'https://preview.redd.it/vkjdtkh4iz351.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a20f98c6afc694b6f0404c51dafbf91f6cf6ab0a'}, {'y': 126, 'x': 320, 'u': 'https://preview.redd.it/vkjdtkh4iz351.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6757da3584724eed41025a0fed12a9bc0da5ed7'}, {'y': 253, 'x': 640, 'u': 'https://preview.redd.it/vkjdtkh4iz351.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d37dd5499cddd86be2361f52d5abdf379b1a0fb'}, {'y': 380, 'x': 960, 'u': 'https://preview.redd.it/vkjdtkh4iz351.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2842a91c86883d01e939102e1197f3cef3540e0e'}, {'y': 427, 'x': 1080, 'u': 'https://preview.redd.it/vkjdtkh4iz351.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9f2a7d4c9a30afbb18444b3cce01762c2eeb1769'}], 's': {'y': 4330, 'x': 10929, 'u': 'https://preview.redd.it/vkjdtkh4iz351.jpg?width=10929&amp;format=pjpg&amp;auto=webp&amp;s=6d756d2ca075d51f9f818d97ca0b8e4cf7ed6576'}, 'id': 'vkjdtkh4iz351'}}, 'name': 't3_h00ve4', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 76, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 76, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/j4F2IuCHl2nebv_kNHfiPTlFtdVcZ6LPTVQRqEZdg2E.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1591780712.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code/API/expert requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2006.01047""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/vkjdtkh4iz351.jpg?width=10929&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6d756d2ca075d51f9f818d97ca0b8e4cf7ed6576""&gt;https://preview.redd.it/vkjdtkh4iz351.jpg?width=10929&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6d756d2ca075d51f9f818d97ca0b8e4cf7ed6576&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;From rough or even incomplete freehand sketches. Note that the method faithfully respects user intentions in input strokes, which serve more like soft constraints to guide image synthesis.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/vF44h8zojy4IL7VX9B28ZMR6TjgW5ULoCo0HHX5Q8mQ.jpg?auto=webp&amp;s=0735e62b72962f53e7ebb5ae36c8990f3ed549b2', 'width': 1416, 'height': 632}, 'resolutions': [{'url': 'https://external-preview.redd.it/vF44h8zojy4IL7VX9B28ZMR6TjgW5ULoCo0HHX5Q8mQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=10e76eb3b95cd0153d3cee356b95d72cc675b8db', 'width': 108, 'height': 48}, {'url': 'https://external-preview.redd.it/vF44h8zojy4IL7VX9B28ZMR6TjgW5ULoCo0HHX5Q8mQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d487a4b29c8ceaf24e60c5463e1d3e238b70dae7', 'width': 216, 'height': 96}, {'url': 'https://external-preview.redd.it/vF44h8zojy4IL7VX9B28ZMR6TjgW5ULoCo0HHX5Q8mQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3213d02cc95f645e405444e70b41483ebe655a3', 'width': 320, 'height': 142}, {'url': 'https://external-preview.redd.it/vF44h8zojy4IL7VX9B28ZMR6TjgW5ULoCo0HHX5Q8mQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ccda282569ac85860c19fb3431e45d00286927e6', 'width': 640, 'height': 285}, {'url': 'https://external-preview.redd.it/vF44h8zojy4IL7VX9B28ZMR6TjgW5ULoCo0HHX5Q8mQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f0ec622eaaa537e3fc5b50f89c58e2f336608ad1', 'width': 960, 'height': 428}, {'url': 'https://external-preview.redd.it/vF44h8zojy4IL7VX9B28ZMR6TjgW5ULoCo0HHX5Q8mQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf84579ecf23ebcffb128c704081239a526afbf0', 'width': 1080, 'height': 482}], 'variants': {}, 'id': 'Q3i1cSUvyTWVvGH08hmCDExTvdqZ2AEr4hbiJNF96Dg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'h00ve4', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/h00ve4/deepfacedrawing_system_allows_users_with_little/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/h00ve4/deepfacedrawing_system_allows_users_with_little/', 'subreddit_subscribers': 6676, 'created_utc': 1591751912.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_h00ve4,,,,,
515,,pytorch,"Hi all, before adding my model to the gpu I added the following code:

    def empty_cached():   
    gc.collect()   
    torch.cuda.empty_cache() 

The idea buying that it will clear out to GPU of the previous model I was playing with.

Here’s a scenario, I start training with a resnet18 and after a few epochs I notice the results are not that good so I interrupt training, change the model, run the function above.  
When I do it that way I get a:

    RuntimeError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 11.17 GiB total capacity; 10.56 GiB already allocated; 9.81 MiB free; 10.85 GiB reserved in total by PyTorch) 

However, if I interupt training, restart the kernel and run the same model that wouldn’t work before, it now works.

It seems like nothing works as good as restarting the kernel. What would come the closest to it?",t2_16u5s20q,False,,0,False,How to clear the GPU,[],r/pytorch,False,6,,0,,,False,t3_gzvzwj,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1591764969.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, before adding my model to the gpu I added the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def empty_cached():   
gc.collect()   
torch.cuda.empty_cache() 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The idea buying that it will clear out to GPU of the previous model I was playing with.&lt;/p&gt;

&lt;p&gt;Here’s a scenario, I start training with a resnet18 and after a few epochs I notice the results are not that good so I interrupt training, change the model, run the function above.&lt;br/&gt;
When I do it that way I get a:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 11.17 GiB total capacity; 10.56 GiB already allocated; 9.81 MiB free; 10.85 GiB reserved in total by PyTorch) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, if I interupt training, restart the kernel and run the same model that wouldn’t work before, it now works.&lt;/p&gt;

&lt;p&gt;It seems like nothing works as good as restarting the kernel. What would come the closest to it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gzvzwj,True,,RegularConstant,,2,True,all_ads,False,[],False,,/r/pytorch/comments/gzvzwj/how_to_clear_the_gpu/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gzvzwj/how_to_clear_the_gpu/,7135,1591736169.0,0,,False,,,,,,,,
516,,pytorch,"&amp;#x200B;

    import torch.nn.functional as F   is giving me the following error

**cannot import name 'PILLOW\_VERSION' from 'PIL'**

This is a new error that I am getting and I am honestly not sure why.  I recently uninstalled and re-install anaconda after having another issue, but before I uninstalled it last night I was not getting this error.",t2_48egj5xw,False,,0,False,Help with Pillow_version,[],r/pytorch,False,6,,0,,,False,t3_gz96t3,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1591682124.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import torch.nn.functional as F   is giving me the following error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;cannot import name &amp;#39;PILLOW_VERSION&amp;#39; from &amp;#39;PIL&amp;#39;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is a new error that I am getting and I am honestly not sure why.  I recently uninstalled and re-install anaconda after having another issue, but before I uninstalled it last night I was not getting this error.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gz96t3,True,,stunbomb1,,11,True,all_ads,False,[],False,,/r/pytorch/comments/gz96t3/help_with_pillow_version/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gz96t3/help_with_pillow_version/,7135,1591653324.0,0,,False,,,,,,,,
517,,pytorch,,t2_44mbtmjy,False,,0,False,Latest from Samsung researchers: State of the art in photo editing (Harmonization),[],r/pytorch,False,6,,0,51.0,,False,t3_gy12b0,False,dark,1.0,,public,7,0,{},140.0,,False,[],,False,False,,{},,False,7,,False,https://b.thumbs.redditmedia.com/_EWsxpczI_1YbeWAGE0NpqUnhD6Vf5A2rP4YPaO6tOs.jpg,False,,[],{},link,,False,,1591513810.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/YyPB_ZIoFkPmVfM203OA-j7VdWdufhUSmn2SgTlS4Mc.jpg?auto=webp&amp;s=ffc66469f6cffb453bac6b0445c07507ff5a0f99', 'width': 1018, 'height': 292}, 'resolutions': [{'url': 'https://external-preview.redd.it/YyPB_ZIoFkPmVfM203OA-j7VdWdufhUSmn2SgTlS4Mc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=75903d82e1d0f12d74c5196d7ae9a88c333476e0', 'width': 108, 'height': 30}, {'url': 'https://external-preview.redd.it/YyPB_ZIoFkPmVfM203OA-j7VdWdufhUSmn2SgTlS4Mc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e63b056b0fcddeffd6a9b6a62c67fd2fba9b4432', 'width': 216, 'height': 61}, {'url': 'https://external-preview.redd.it/YyPB_ZIoFkPmVfM203OA-j7VdWdufhUSmn2SgTlS4Mc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8d8bbb8a1077e9c3e7004ffeb3f5fe0f201e5716', 'width': 320, 'height': 91}, {'url': 'https://external-preview.redd.it/YyPB_ZIoFkPmVfM203OA-j7VdWdufhUSmn2SgTlS4Mc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0ca2384b2987d4473bc47fc07001966a7c9b6256', 'width': 640, 'height': 183}, {'url': 'https://external-preview.redd.it/YyPB_ZIoFkPmVfM203OA-j7VdWdufhUSmn2SgTlS4Mc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0ebb0d3b09e84b270e7c8cf7afbb8076fa33f84f', 'width': 960, 'height': 275}], 'variants': {}, 'id': '1331JclxlTOlTiVp4Wx42rGAPmj2H-NDERzo3p7it_g'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gy12b0,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gy12b0/latest_from_samsung_researchers_state_of_the_art/,all_ads,False,/r/LatestInML/comments/gw7v2i/latest_from_samsung_researchers_state_of_the_art/,7135,1591485010.0,0,,False,/r/LatestInML/comments/gw7v2i/latest_from_samsung_researchers_state_of_the_art/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2006.00809)\n\nhttps://preview.redd.it/uuqevxvcvs251.png?width=2124&amp;format=png&amp;auto=webp&amp;s=09581885683a2bb94013965399bc81f9f1ee9c18\n\nThey create the models as a combination of existing encoder-decoder architectures and a pre-trained foreground-aware deep high-resolution network.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Latest from Samsung researchers: State of the art in photo editing (Harmonization)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 51, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'uuqevxvcvs251': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 40, 'x': 108, 'u': 'https://preview.redd.it/uuqevxvcvs251.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=201d668e803749c1b78e2f217d12842a7190ed58'}, {'y': 80, 'x': 216, 'u': 'https://preview.redd.it/uuqevxvcvs251.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d5986d90acbab060ce0eb06d74194bb2253e7ff'}, {'y': 118, 'x': 320, 'u': 'https://preview.redd.it/uuqevxvcvs251.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d9c3ab7709fdf2489a0d55169b2e1d8dc68c3e0a'}, {'y': 237, 'x': 640, 'u': 'https://preview.redd.it/uuqevxvcvs251.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f97b6deec6c9699b8c3579d31b423a208ef6f1b'}, {'y': 356, 'x': 960, 'u': 'https://preview.redd.it/uuqevxvcvs251.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6c2f735c616d05ef5126725b728d706bb9feb8ff'}, {'y': 400, 'x': 1080, 'u': 'https://preview.redd.it/uuqevxvcvs251.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6d2440d60a9a13ea020026cc97c362d657ac7db6'}], 's': {'y': 788, 'x': 2124, 'u': 'https://preview.redd.it/uuqevxvcvs251.png?width=2124&amp;format=png&amp;auto=webp&amp;s=09581885683a2bb94013965399bc81f9f1ee9c18'}, 'id': 'uuqevxvcvs251'}}, 'name': 't3_gw7v2i', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 25, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 25, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/_EWsxpczI_1YbeWAGE0NpqUnhD6Vf5A2rP4YPaO6tOs.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1591264565.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2006.00809""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/uuqevxvcvs251.png?width=2124&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09581885683a2bb94013965399bc81f9f1ee9c18""&gt;https://preview.redd.it/uuqevxvcvs251.png?width=2124&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09581885683a2bb94013965399bc81f9f1ee9c18&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They create the models as a combination of existing encoder-decoder architectures and a pre-trained foreground-aware deep high-resolution network.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/YyPB_ZIoFkPmVfM203OA-j7VdWdufhUSmn2SgTlS4Mc.jpg?auto=webp&amp;s=ffc66469f6cffb453bac6b0445c07507ff5a0f99', 'width': 1018, 'height': 292}, 'resolutions': [{'url': 'https://external-preview.redd.it/YyPB_ZIoFkPmVfM203OA-j7VdWdufhUSmn2SgTlS4Mc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=75903d82e1d0f12d74c5196d7ae9a88c333476e0', 'width': 108, 'height': 30}, {'url': 'https://external-preview.redd.it/YyPB_ZIoFkPmVfM203OA-j7VdWdufhUSmn2SgTlS4Mc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e63b056b0fcddeffd6a9b6a62c67fd2fba9b4432', 'width': 216, 'height': 61}, {'url': 'https://external-preview.redd.it/YyPB_ZIoFkPmVfM203OA-j7VdWdufhUSmn2SgTlS4Mc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8d8bbb8a1077e9c3e7004ffeb3f5fe0f201e5716', 'width': 320, 'height': 91}, {'url': 'https://external-preview.redd.it/YyPB_ZIoFkPmVfM203OA-j7VdWdufhUSmn2SgTlS4Mc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0ca2384b2987d4473bc47fc07001966a7c9b6256', 'width': 640, 'height': 183}, {'url': 'https://external-preview.redd.it/YyPB_ZIoFkPmVfM203OA-j7VdWdufhUSmn2SgTlS4Mc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0ebb0d3b09e84b270e7c8cf7afbb8076fa33f84f', 'width': 960, 'height': 275}], 'variants': {}, 'id': '1331JclxlTOlTiVp4Wx42rGAPmj2H-NDERzo3p7it_g'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gw7v2i', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gw7v2i/latest_from_samsung_researchers_state_of_the_art/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gw7v2i/latest_from_samsung_researchers_state_of_the_art/', 'subreddit_subscribers': 6676, 'created_utc': 1591235765.0, 'num_crossposts': 13, 'media': None, 'is_video': False}]",t3_gw7v2i,,,,,
518,,pytorch,,t2_44mbtmjy,False,,0,False,Latest from Mitsubishi: State of the art in Joint Perception and Motion Prediction for Autonomous Driving!(Based on Bird's Eye View Maps),[],r/pytorch,False,6,,0,,,False,t3_gy1d62,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,default,False,,[],{},,,False,,1591514870.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gy1d62,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gy1d62/latest_from_mitsubishi_state_of_the_art_in_joint/,all_ads,False,/r/LatestInML/comments/gy17m8/latest_from_mitsubishi_state_of_the_art_in_joint/,7135,1591486070.0,0,,False,/r/LatestInML/comments/gy17m8/latest_from_mitsubishi_state_of_the_art_in_joint/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""For project and code or API requests: [click here](https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.catalyzex.com%2Fpaper%2Farxiv%3A2003.06754%3Ffbclid%3DIwAR3tYzYiIO5GvzzRfziY2aoxAHbX99rq_NO9Omw-sOAZyrLryx5W9Wh-oNs&amp;h=AT0BGlVrVse7VBrU2TEekV7JpQmDl8-4FpBg-R3FdelbgjIfA_35x5KdB0OfppY1qSHgPii0DOsQQA2XhkETkmae8BqiPrqWcePE6E9b7WxZyDh7xibFf-hxMDksUSw8mRLSKXa2mDlEe-GWKrsts2feLFYrgVjzqM5q9N26Xz-YP5BImWhlD23z6A9UowGwLS1OgVHxB_WoZvpgFv0fyqFBvtgMslMd4OSVH2HkObufENQWxoG_NpMh4yW3Rwtg7Tr9Hcv9qZ5GTiX_bPv7jiIeDymQ_8VeZMbXDGgdUj-dIzarnnFGw437tN4KAhmK65TKKVavbqeSFOZX9-je9FTIPuhK3oqqeaLP9l-YVmC28un2lWXISJy7obbsYDtUSZUazMWmhw5FvaGvEnLLN0MKqAwPWqasmgT6afIsY_PlbgXF6l8b0fHD9nJz_zAq07PF_4G3vGgKuRDK-0OK8iZR1-QopyuNIHnZzvBdpuQidE-OwMxfDCDDrogxfQu9Iw-v8_yzbJLkuIiCZ_8TcTb-lo4tvnf5SJo9-WbyKx13rW6gqoqMXmCpHJYM3ZneGUW1fkwn_TPoCYjSUjscfIzjscBVQO6M636QE9KXjiz6NKxN8uEh6oLlrGcOJLoc_E2An5LbSyhtZEGJ2EZyVSmxVW8)\n\nhttps://reddit.com/link/gy17m8/video/e96u7x13id351/player\n\nMotionNet takes a sequence of LiDAR sweeps as input and outputs a bird's eye view (BEV) map, which encodes the object category and motion information in each grid cell"", 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""Latest from Mitsubishi: State of the art in Joint Perception and Motion Prediction for Autonomous Driving!(Based on Bird's Eye View Maps)"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'e96u7x13id351': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/gy17m8/asset/e96u7x13id351/DASHPlaylist.mpd?a=1618044232%2CMjQ5Yjc0NzEyODNkY2EyNmQ1ZjZjYWU2NTg5YzUwNGQ0ODJiZTI1NGYzMTkyMTIwNjE0NWMyMzI5YmJjNzJjZQ%3D%3D&amp;v=1&amp;f=sd', 'x': 640, 'y': 216, 'hlsUrl': 'https://v.redd.it/link/gy17m8/asset/e96u7x13id351/HLSPlaylist.m3u8?a=1618044232%2COGU3ZjFhN2NmMjE3MDA2YzgxMTYxOThmNmE0OTM4NGUyYzAzMDFiZjYzNmYzZjFmMDVhN2U1Y2EwODk1ZjViMw%3D%3D&amp;v=1&amp;f=sd', 'id': 'e96u7x13id351', 'isGif': False}}, 'name': 't3_gy17m8', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 2, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': 1591486379.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1591514341.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code or API requests: &lt;a href=""https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.catalyzex.com%2Fpaper%2Farxiv%3A2003.06754%3Ffbclid%3DIwAR3tYzYiIO5GvzzRfziY2aoxAHbX99rq_NO9Omw-sOAZyrLryx5W9Wh-oNs&amp;amp;h=AT0BGlVrVse7VBrU2TEekV7JpQmDl8-4FpBg-R3FdelbgjIfA_35x5KdB0OfppY1qSHgPii0DOsQQA2XhkETkmae8BqiPrqWcePE6E9b7WxZyDh7xibFf-hxMDksUSw8mRLSKXa2mDlEe-GWKrsts2feLFYrgVjzqM5q9N26Xz-YP5BImWhlD23z6A9UowGwLS1OgVHxB_WoZvpgFv0fyqFBvtgMslMd4OSVH2HkObufENQWxoG_NpMh4yW3Rwtg7Tr9Hcv9qZ5GTiX_bPv7jiIeDymQ_8VeZMbXDGgdUj-dIzarnnFGw437tN4KAhmK65TKKVavbqeSFOZX9-je9FTIPuhK3oqqeaLP9l-YVmC28un2lWXISJy7obbsYDtUSZUazMWmhw5FvaGvEnLLN0MKqAwPWqasmgT6afIsY_PlbgXF6l8b0fHD9nJz_zAq07PF_4G3vGgKuRDK-0OK8iZR1-QopyuNIHnZzvBdpuQidE-OwMxfDCDDrogxfQu9Iw-v8_yzbJLkuIiCZ_8TcTb-lo4tvnf5SJo9-WbyKx13rW6gqoqMXmCpHJYM3ZneGUW1fkwn_TPoCYjSUjscfIzjscBVQO6M636QE9KXjiz6NKxN8uEh6oLlrGcOJLoc_E2An5LbSyhtZEGJ2EZyVSmxVW8""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/gy17m8/video/e96u7x13id351/player""&gt;https://reddit.com/link/gy17m8/video/e96u7x13id351/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;MotionNet takes a sequence of LiDAR sweeps as input and outputs a bird&amp;#39;s eye view (BEV) map, which encodes the object category and motion information in each grid cell&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gy17m8', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gy17m8/latest_from_mitsubishi_state_of_the_art_in_joint/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gy17m8/latest_from_mitsubishi_state_of_the_art_in_joint/', 'subreddit_subscribers': 6676, 'created_utc': 1591485541.0, 'num_crossposts': 10, 'media': None, 'is_video': False}]",t3_gy17m8,,,,,
519,,pytorch,,t2_44mbtmjy,False,,0,False,"Recapture your portrait photos with desired posture/view, figure, and clothing style!",[],r/pytorch,False,6,,0,27.0,,False,t3_gy3q14,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/DzA6yUzK5JM4nyetQeb2uEK38s7msr_mQPmy77pP9qY.jpg,False,,[],{},link,,False,,1591523569.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/alM7295wnBGAuNCunephHXQhbf1DyMbh-XMzPjbRWGI.jpg?auto=webp&amp;s=524b2c0df9809a636ff4304ac9d7b01ce268ae98', 'width': 670, 'height': 484}, 'resolutions': [{'url': 'https://external-preview.redd.it/alM7295wnBGAuNCunephHXQhbf1DyMbh-XMzPjbRWGI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd7ccda7046195886c823ddf63c6b3b27d3d742d', 'width': 108, 'height': 78}, {'url': 'https://external-preview.redd.it/alM7295wnBGAuNCunephHXQhbf1DyMbh-XMzPjbRWGI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3decf5f9cf427b7161d67a22abe2882821d98070', 'width': 216, 'height': 156}, {'url': 'https://external-preview.redd.it/alM7295wnBGAuNCunephHXQhbf1DyMbh-XMzPjbRWGI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=46f41766d055c98ecc13bfa590bfa1672dc7b9cd', 'width': 320, 'height': 231}, {'url': 'https://external-preview.redd.it/alM7295wnBGAuNCunephHXQhbf1DyMbh-XMzPjbRWGI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5cdf61fa647293fefb98899f65fd9a530e2b7343', 'width': 640, 'height': 462}], 'variants': {}, 'id': '2nV7GOexnmf3TbvTwLWopQGh0v2NE3WckDh1uQxnl7U'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gy3q14,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gy3q14/recapture_your_portrait_photos_with_desired/,all_ads,False,/r/LatestInML/comments/gy3opz/recapture_your_portrait_photos_with_desired/,7135,1591494769.0,0,,False,/r/LatestInML/comments/gy3opz/recapture_your_portrait_photos_with_desired/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and API or code request: [click here](https://www.catalyzex.com/paper/arxiv:2006.01435?fbclid=IwAR1mOIxV0J8-NHeO-sOCIt0zuTHnZwzOX16fkqjs0F4_tiGwxdJ9iZQLiYU)\n\nhttps://preview.redd.it/lzwokwl39e351.png?width=2000&amp;format=png&amp;auto=webp&amp;s=cf4f88cba4ebd37f1609fe5960e1a1e9aacc6dd3\n\nIt can properly infer invisible body parts and clothes in original portraits, e.g. the lower body, and meanwhile guarantee global coherency of different regions in recaptured portraits.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Recapture your portrait photos with desired posture/view, figure, and clothing style!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 101, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'lzwokwl39e351': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 21, 'x': 108, 'u': 'https://preview.redd.it/lzwokwl39e351.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6e13eeffa38f5cef7d34e116a29c7ae5fe07c50'}, {'y': 42, 'x': 216, 'u': 'https://preview.redd.it/lzwokwl39e351.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=113e39e88dc6381f37fb4d6ec9e8026773cbb531'}, {'y': 62, 'x': 320, 'u': 'https://preview.redd.it/lzwokwl39e351.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e73e985f494569a9c17372427234e198e86d2fb4'}, {'y': 125, 'x': 640, 'u': 'https://preview.redd.it/lzwokwl39e351.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=aa985c069a1b128298f81d3b42bf5b56be5f645d'}, {'y': 188, 'x': 960, 'u': 'https://preview.redd.it/lzwokwl39e351.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f791a191f4a261c2d0a13832be31f4081165a4ce'}, {'y': 211, 'x': 1080, 'u': 'https://preview.redd.it/lzwokwl39e351.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aea6553e3cfd69e6837f206bb8927037e9ae528d'}], 's': {'y': 392, 'x': 2000, 'u': 'https://preview.redd.it/lzwokwl39e351.png?width=2000&amp;format=png&amp;auto=webp&amp;s=cf4f88cba4ebd37f1609fe5960e1a1e9aacc6dd3'}, 'id': 'lzwokwl39e351'}}, 'name': 't3_gy3opz', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 21, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 21, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/t2Urn5r7FftiJeWqUEOx0j824yldJS55l6Mjz6iIlnk.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1591523434.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and API or code request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2006.01435?fbclid=IwAR1mOIxV0J8-NHeO-sOCIt0zuTHnZwzOX16fkqjs0F4_tiGwxdJ9iZQLiYU""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/lzwokwl39e351.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf4f88cba4ebd37f1609fe5960e1a1e9aacc6dd3""&gt;https://preview.redd.it/lzwokwl39e351.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf4f88cba4ebd37f1609fe5960e1a1e9aacc6dd3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It can properly infer invisible body parts and clothes in original portraits, e.g. the lower body, and meanwhile guarantee global coherency of different regions in recaptured portraits.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/alM7295wnBGAuNCunephHXQhbf1DyMbh-XMzPjbRWGI.jpg?auto=webp&amp;s=524b2c0df9809a636ff4304ac9d7b01ce268ae98', 'width': 670, 'height': 484}, 'resolutions': [{'url': 'https://external-preview.redd.it/alM7295wnBGAuNCunephHXQhbf1DyMbh-XMzPjbRWGI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd7ccda7046195886c823ddf63c6b3b27d3d742d', 'width': 108, 'height': 78}, {'url': 'https://external-preview.redd.it/alM7295wnBGAuNCunephHXQhbf1DyMbh-XMzPjbRWGI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3decf5f9cf427b7161d67a22abe2882821d98070', 'width': 216, 'height': 156}, {'url': 'https://external-preview.redd.it/alM7295wnBGAuNCunephHXQhbf1DyMbh-XMzPjbRWGI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=46f41766d055c98ecc13bfa590bfa1672dc7b9cd', 'width': 320, 'height': 231}, {'url': 'https://external-preview.redd.it/alM7295wnBGAuNCunephHXQhbf1DyMbh-XMzPjbRWGI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5cdf61fa647293fefb98899f65fd9a530e2b7343', 'width': 640, 'height': 462}], 'variants': {}, 'id': '2nV7GOexnmf3TbvTwLWopQGh0v2NE3WckDh1uQxnl7U'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gy3opz', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gy3opz/recapture_your_portrait_photos_with_desired/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gy3opz/recapture_your_portrait_photos_with_desired/', 'subreddit_subscribers': 6676, 'created_utc': 1591494634.0, 'num_crossposts': 11, 'media': None, 'is_video': False}]",t3_gy3opz,,,,,
520,,pytorch,"I'm a master student and in my thesis I'm going to use a machine learning algorithm to learn and detect driving behavior using Hidden Markov Model.

I have programming experience in several languages, but never programmed any Machine learning (I'm researching on the topic) I also never programmed in Lua, but interest in learning the language.

1.) Is PyTorch a good choice for a beginner to learn machine learning?

2.) Is there a Hidden Markov Model library for PyTorch? I don't want to make my own, I want to use one.

3.) Is there a way to use PyTorch in a browser / Server environment? I read ONNE has support for PyTorch and JavaScript?

4.) where would be the best place to ask for help related PyTorch, is it here? some forum or other place?

Thanks for your help!",t2_a8l8hzq,False,,0,False,"Is PyTorch suitable for a machine learning beginner, making a Hidden Markov Model for a behavior analysis?",[],r/pytorch,False,6,,0,,,False,t3_gxqg0z,False,dark,0.82,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1591476783.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a master student and in my thesis I&amp;#39;m going to use a machine learning algorithm to learn and detect driving behavior using Hidden Markov Model.&lt;/p&gt;

&lt;p&gt;I have programming experience in several languages, but never programmed any Machine learning (I&amp;#39;m researching on the topic) I also never programmed in Lua, but interest in learning the language.&lt;/p&gt;

&lt;p&gt;1.) Is PyTorch a good choice for a beginner to learn machine learning?&lt;/p&gt;

&lt;p&gt;2.) Is there a Hidden Markov Model library for PyTorch? I don&amp;#39;t want to make my own, I want to use one.&lt;/p&gt;

&lt;p&gt;3.) Is there a way to use PyTorch in a browser / Server environment? I read ONNE has support for PyTorch and JavaScript?&lt;/p&gt;

&lt;p&gt;4.) where would be the best place to ask for help related PyTorch, is it here? some forum or other place?&lt;/p&gt;

&lt;p&gt;Thanks for your help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gxqg0z,True,,Splicex42,,7,True,all_ads,False,[],False,,/r/pytorch/comments/gxqg0z/is_pytorch_suitable_for_a_machine_learning/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gxqg0z/is_pytorch_suitable_for_a_machine_learning/,7135,1591447983.0,0,,False,,,,,,,,
521,,pytorch,"I've just started using it and it seems rather straightforward for many cases, but I just can't figure out how to initialize it on a cycleGAN where there are 4 networks and 2 optimizers. Here are the lines initializing the the networks:

	self.Gab = define_Gen3d(input_nc=1, output_nc=1, ngf=args.ngf, netG=args.gen_net, norm=args.norm, 
												use_dropout= not args.no_dropout, gpu_ids=args.gpu_ids).type(self.t_type)
	self.Gba = define_Gen3d(input_nc=1, output_nc=1, ngf=args.ngf, netG=args.gen_net, norm=args.norm, 
												use_dropout= not args.no_dropout, gpu_ids=args.gpu_ids).type(self.t_type)
	self.Da = define_Dis3d(input_nc=1, ndf=args.ndf, netD= args.dis_net, n_layers_D=3, norm=args.norm, gpu_ids=args.gpu_ids).type(self.t_type)
	self.Db = define_Dis3d(input_nc=1, ndf=args.ndf, netD= args.dis_net, n_layers_D=3, norm=args.norm, gpu_ids=args.gpu_ids).type(self.t_type)

and here are the lines initializing the optimizers (notice that the parameters are joined by itertools.chain()):

    self.g_optimizer = torch.optim.Adam(itertools.chain(self.Gab.parameters(),self.Gba.parameters()), lr=args.lr, betas=(0.5, 0.999))
    self.d_optimizer = torch.optim.Adam(itertools.chain(self.Da.parameters(),self.Db.parameters()), lr=args.lr, betas=(0.5, 0.999))

The amp module API calls for:

    model, optimizer = amp.initialize(model, optimizer)

so I am unsure whether this is somehow possible to combine or not?",t2_j5t09,False,,0,False,Need help using apex on CycleGAN,[],r/pytorch,False,6,,0,,,False,t3_gx5gv9,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1591395666.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve just started using it and it seems rather straightforward for many cases, but I just can&amp;#39;t figure out how to initialize it on a cycleGAN where there are 4 networks and 2 optimizers. Here are the lines initializing the the networks:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;self.Gab = define_Gen3d(input_nc=1, output_nc=1, ngf=args.ngf, netG=args.gen_net, norm=args.norm, 
                                            use_dropout= not args.no_dropout, gpu_ids=args.gpu_ids).type(self.t_type)
self.Gba = define_Gen3d(input_nc=1, output_nc=1, ngf=args.ngf, netG=args.gen_net, norm=args.norm, 
                                            use_dropout= not args.no_dropout, gpu_ids=args.gpu_ids).type(self.t_type)
self.Da = define_Dis3d(input_nc=1, ndf=args.ndf, netD= args.dis_net, n_layers_D=3, norm=args.norm, gpu_ids=args.gpu_ids).type(self.t_type)
self.Db = define_Dis3d(input_nc=1, ndf=args.ndf, netD= args.dis_net, n_layers_D=3, norm=args.norm, gpu_ids=args.gpu_ids).type(self.t_type)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and here are the lines initializing the optimizers (notice that the parameters are joined by itertools.chain()):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;self.g_optimizer = torch.optim.Adam(itertools.chain(self.Gab.parameters(),self.Gba.parameters()), lr=args.lr, betas=(0.5, 0.999))
self.d_optimizer = torch.optim.Adam(itertools.chain(self.Da.parameters(),self.Db.parameters()), lr=args.lr, betas=(0.5, 0.999))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The amp module API calls for:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model, optimizer = amp.initialize(model, optimizer)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;so I am unsure whether this is somehow possible to combine or not?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gx5gv9,True,,HudPesjan,,1,True,all_ads,False,[],False,,/r/pytorch/comments/gx5gv9/need_help_using_apex_on_cyclegan/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gx5gv9/need_help_using_apex_on_cyclegan/,7135,1591366866.0,0,,False,,,,,,,,
522,,pytorch,"Watch a couple of pytorch tutorials and they all used either Mnist or Cifar. I want to be able to load my own dataset from my computer into my network, but my not sure how as I found no example of how it is done. Im hoping its a simple as tensor flow.",t2_48egj5xw,False,,0,False,How to upload custom datasets?,[],r/pytorch,False,6,,0,,,False,t3_gwmv1r,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1591323327.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Watch a couple of pytorch tutorials and they all used either Mnist or Cifar. I want to be able to load my own dataset from my computer into my network, but my not sure how as I found no example of how it is done. Im hoping its a simple as tensor flow.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gwmv1r,True,,stunbomb1,,2,True,all_ads,False,[],False,,/r/pytorch/comments/gwmv1r/how_to_upload_custom_datasets/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gwmv1r/how_to_upload_custom_datasets/,7135,1591294527.0,0,,False,,,,,,,,
523,,pytorch,"Hey all, 

I wish to implement a custom DL framework, but it's nowhere near structural. I'm working on graphs, where nodes contain time series data and edges represent the spatial connection strength. My idea is to apply convolutions where for each node, k strongest edges will define the neighborhood for convolution operation. It's really messy, but I want to give it a shot, even if it doesn't work I think I will learn valuable things. 

So, my question is, can PyTorch support such kind of frameworks? At first I got into implementing it by myself, but I realized that it would take so much time.

Thank you :)",t2_1cui2d3k,False,,0,False,Custom model implementation,[],r/pytorch,False,6,,0,,,False,t3_gvxgmd,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1591231584.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all, &lt;/p&gt;

&lt;p&gt;I wish to implement a custom DL framework, but it&amp;#39;s nowhere near structural. I&amp;#39;m working on graphs, where nodes contain time series data and edges represent the spatial connection strength. My idea is to apply convolutions where for each node, k strongest edges will define the neighborhood for convolution operation. It&amp;#39;s really messy, but I want to give it a shot, even if it doesn&amp;#39;t work I think I will learn valuable things. &lt;/p&gt;

&lt;p&gt;So, my question is, can PyTorch support such kind of frameworks? At first I got into implementing it by myself, but I realized that it would take so much time.&lt;/p&gt;

&lt;p&gt;Thank you :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gvxgmd,True,,cheeky_bastard__,,2,True,all_ads,False,[],False,,/r/pytorch/comments/gvxgmd/custom_model_implementation/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gvxgmd/custom_model_implementation/,7135,1591202784.0,0,,False,,,,,,,,
524,,pytorch,,t2_44mbtmjy,False,,0,False,Latest from apple researchers: Deep learning approach for driving animated faces using both acoustic and visual information.,[],r/pytorch,False,6,,0,94.0,,False,t3_gvi1xm,False,dark,0.9,,public,7,0,{},140.0,,False,[],,False,False,,{},,False,7,,False,https://b.thumbs.redditmedia.com/NKWqlXROkLYuZm6TZlSfrUY3aWFOY4dEp7xjupjDvcI.jpg,False,,[],{},link,,False,,1591167804.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/OzjvSTkSSmNfDSlca5f7FhykyIFTPqy5Y04LJJ7w9jA.jpg?auto=webp&amp;s=5f62cc05f6a453b5328f6bfb9b51081cc3115514', 'width': 832, 'height': 570}, 'resolutions': [{'url': 'https://external-preview.redd.it/OzjvSTkSSmNfDSlca5f7FhykyIFTPqy5Y04LJJ7w9jA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a39c751e3053b3b9d315785dc3e0ce4381857775', 'width': 108, 'height': 73}, {'url': 'https://external-preview.redd.it/OzjvSTkSSmNfDSlca5f7FhykyIFTPqy5Y04LJJ7w9jA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d2e6c3ec3b4d5e152c56e2388232bac1cd2046b4', 'width': 216, 'height': 147}, {'url': 'https://external-preview.redd.it/OzjvSTkSSmNfDSlca5f7FhykyIFTPqy5Y04LJJ7w9jA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d60551d1ca1c376eaf0b667a8e6bf5e8386bef22', 'width': 320, 'height': 219}, {'url': 'https://external-preview.redd.it/OzjvSTkSSmNfDSlca5f7FhykyIFTPqy5Y04LJJ7w9jA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dc398b278813c24f29130dd6d91222bf5ba19bd5', 'width': 640, 'height': 438}], 'variants': {}, 'id': 'DBWFFHM3zEqtRyst5399owBz0nwonHoouHYAPZ5bfY0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gvi1xm,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gvi1xm/latest_from_apple_researchers_deep_learning/,all_ads,False,/r/LatestInML/comments/gvhfln/latest_from_apple_researchers_deep_learning/,7135,1591139004.0,0,,False,/r/LatestInML/comments/gvhfln/latest_from_apple_researchers_deep_learning/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code or API requests: [**https://lnkd.in/g25QSyW**](https://lnkd.in/g25QSyW)  \nTo ensure that the model exploits both modalities during training, batches are generated that contain audio-only, video-only, and audiovisual input features\n\nhttps://preview.redd.it/pao1768jpk251.png?width=1256&amp;format=png&amp;auto=webp&amp;s=b57f5ca729bb57201b5aa344148bad6cb5748ec0', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Latest from apple researchers: Deep learning approach for driving animated faces using both acoustic and visual information.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 94, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'pao1768jpk251': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 73, 'x': 108, 'u': 'https://preview.redd.it/pao1768jpk251.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ea9e8f515486a4eacbb2646e1934e18d979c0e65'}, {'y': 146, 'x': 216, 'u': 'https://preview.redd.it/pao1768jpk251.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1232e30f9c8ed358a77a0d9315fd36b1713f0504'}, {'y': 217, 'x': 320, 'u': 'https://preview.redd.it/pao1768jpk251.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7b282e507f95b91c5d05044e0b8410fec149757'}, {'y': 434, 'x': 640, 'u': 'https://preview.redd.it/pao1768jpk251.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bfc496b7d809baff4237e74783fe17d8a0ffb525'}, {'y': 651, 'x': 960, 'u': 'https://preview.redd.it/pao1768jpk251.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=158a15fa50c34838b9f07104bb69d34a7330c03e'}, {'y': 732, 'x': 1080, 'u': 'https://preview.redd.it/pao1768jpk251.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=108f05ba45b9dc1320d0a95c0f5600b4d24ae844'}], 's': {'y': 852, 'x': 1256, 'u': 'https://preview.redd.it/pao1768jpk251.png?width=1256&amp;format=png&amp;auto=webp&amp;s=b57f5ca729bb57201b5aa344148bad6cb5748ec0'}, 'id': 'pao1768jpk251'}}, 'name': 't3_gvhfln', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 31, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 31, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/NKWqlXROkLYuZm6TZlSfrUY3aWFOY4dEp7xjupjDvcI.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1591165753.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code or API requests: &lt;a href=""https://lnkd.in/g25QSyW""&gt;&lt;strong&gt;https://lnkd.in/g25QSyW&lt;/strong&gt;&lt;/a&gt;&lt;br/&gt;\nTo ensure that the model exploits both modalities during training, batches are generated that contain audio-only, video-only, and audiovisual input features&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/pao1768jpk251.png?width=1256&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b57f5ca729bb57201b5aa344148bad6cb5748ec0""&gt;https://preview.redd.it/pao1768jpk251.png?width=1256&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b57f5ca729bb57201b5aa344148bad6cb5748ec0&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/OzjvSTkSSmNfDSlca5f7FhykyIFTPqy5Y04LJJ7w9jA.jpg?auto=webp&amp;s=5f62cc05f6a453b5328f6bfb9b51081cc3115514', 'width': 832, 'height': 570}, 'resolutions': [{'url': 'https://external-preview.redd.it/OzjvSTkSSmNfDSlca5f7FhykyIFTPqy5Y04LJJ7w9jA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a39c751e3053b3b9d315785dc3e0ce4381857775', 'width': 108, 'height': 73}, {'url': 'https://external-preview.redd.it/OzjvSTkSSmNfDSlca5f7FhykyIFTPqy5Y04LJJ7w9jA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d2e6c3ec3b4d5e152c56e2388232bac1cd2046b4', 'width': 216, 'height': 147}, {'url': 'https://external-preview.redd.it/OzjvSTkSSmNfDSlca5f7FhykyIFTPqy5Y04LJJ7w9jA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d60551d1ca1c376eaf0b667a8e6bf5e8386bef22', 'width': 320, 'height': 219}, {'url': 'https://external-preview.redd.it/OzjvSTkSSmNfDSlca5f7FhykyIFTPqy5Y04LJJ7w9jA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dc398b278813c24f29130dd6d91222bf5ba19bd5', 'width': 640, 'height': 438}], 'variants': {}, 'id': 'DBWFFHM3zEqtRyst5399owBz0nwonHoouHYAPZ5bfY0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gvhfln', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gvhfln/latest_from_apple_researchers_deep_learning/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gvhfln/latest_from_apple_researchers_deep_learning/', 'subreddit_subscribers': 6676, 'created_utc': 1591136953.0, 'num_crossposts': 10, 'media': None, 'is_video': False}]",t3_gvhfln,,,,,
525,,pytorch,"Hello, I'm trying to move from tensorflow/keras to pytorch, as many new models are implemented in pytorch for which there is no equivalent in tensorflow and implementing everything again would be too long and difficult. 
I'm used to converting my dataset to a tfrecord and then loading it as a tf dataset. I've seen that it is possible to use tfrecords in pytorch, are they efficient? Is there a better way to store data to be used with pytorch? My data is in the form of arrays (npz files if you want) and I usually bake them into some tfrecords for convenience and speed.

Is it possible to work with gcp buckets? In tf I simply need to specify the bucket url and then I can easily retrieve my files. 

Thank you very much!",t2_26t6gid3,False,,0,False,"[Q] TFRecord in pytorch, is it a good format?",[],r/pytorch,False,6,,0,,,False,t3_gv7epu,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1591133569.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I&amp;#39;m trying to move from tensorflow/keras to pytorch, as many new models are implemented in pytorch for which there is no equivalent in tensorflow and implementing everything again would be too long and difficult. 
I&amp;#39;m used to converting my dataset to a tfrecord and then loading it as a tf dataset. I&amp;#39;ve seen that it is possible to use tfrecords in pytorch, are they efficient? Is there a better way to store data to be used with pytorch? My data is in the form of arrays (npz files if you want) and I usually bake them into some tfrecords for convenience and speed.&lt;/p&gt;

&lt;p&gt;Is it possible to work with gcp buckets? In tf I simply need to specify the bucket url and then I can easily retrieve my files. &lt;/p&gt;

&lt;p&gt;Thank you very much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gv7epu,True,,TrPhantom8,,2,True,all_ads,False,[],False,,/r/pytorch/comments/gv7epu/q_tfrecord_in_pytorch_is_it_a_good_format/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gv7epu/q_tfrecord_in_pytorch_is_it_a_good_format/,7135,1591104769.0,0,,False,,,,,,,,
526,,pytorch,,t2_2o7eaff,False,,0,False,Top 15 AI Articles You Should Read This Month - May 2020,[],r/pytorch,False,6,,0,78.0,,False,t3_gv2fb3,False,dark,0.8,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://a.thumbs.redditmedia.com/1Exr-XPGSwO2qsETmi3RDj5JJtAX_u1-rVpz8--gcd0.jpg,False,,[],{},link,,False,,1591109993.0,text,6,,,text,rubikscode.net,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/09QyNs-XqcG8AdC4ilCEGIyZ61EW9P9bNMITevoNJ4o.jpg?auto=webp&amp;s=db461a6c9842fb9b6e2b988285e39ba0cfebc5f3', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/09QyNs-XqcG8AdC4ilCEGIyZ61EW9P9bNMITevoNJ4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c82d87f2bcdc052f166d60692a127e383bb57495', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/09QyNs-XqcG8AdC4ilCEGIyZ61EW9P9bNMITevoNJ4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d5637a83315435a65f2057c324746191efe9fe69', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/09QyNs-XqcG8AdC4ilCEGIyZ61EW9P9bNMITevoNJ4o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=476116a99d4873bf9a8c88beb85de62940d37892', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/09QyNs-XqcG8AdC4ilCEGIyZ61EW9P9bNMITevoNJ4o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aab3f9c57e24247b83d580d8864baafa8c284515', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/09QyNs-XqcG8AdC4ilCEGIyZ61EW9P9bNMITevoNJ4o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=73d7f7e00adc49e72d8299249e58e24106e92455', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/09QyNs-XqcG8AdC4ilCEGIyZ61EW9P9bNMITevoNJ4o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9844238e4f60c4883d586293a9d54e70c621591c', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'J0jeCpogDytWR5bqwuBRHufPnwitn6H1e2WRDw8T_kk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gv2fb3,True,,RubiksCodeNMZ,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gv2fb3/top_15_ai_articles_you_should_read_this_month_may/,all_ads,False,https://rubikscode.net/2020/06/02/top-15-ai-articles-you-should-read-this-month-may-2020/,7135,1591081193.0,0,,False,https://rubikscode.net/2020/06/02/top-15-ai-articles-you-should-read-this-month-may-2020/,,,,,,,
527,,pytorch,"I guess my question is how would I go about doing this.
I know you define a model by creating layers, but NEAT does not have a simple dense layered structure.

The only way I can think of doing it is creating a layer  for each individual node and connecting these layers as if I am connecting individual nodes together.

This seems like it is probably wrong though so was wondering if there was another approach I should be looking at?",t2_6up5y,False,,0,False,Implementing NEAT in pytorch,[],r/pytorch,False,6,,0,,,False,t3_gutmrk,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1591076206.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I guess my question is how would I go about doing this.
I know you define a model by creating layers, but NEAT does not have a simple dense layered structure.&lt;/p&gt;

&lt;p&gt;The only way I can think of doing it is creating a layer  for each individual node and connecting these layers as if I am connecting individual nodes together.&lt;/p&gt;

&lt;p&gt;This seems like it is probably wrong though so was wondering if there was another approach I should be looking at?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gutmrk,True,,PantsMcShirt,,4,True,all_ads,False,[],False,,/r/pytorch/comments/gutmrk/implementing_neat_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gutmrk/implementing_neat_in_pytorch/,7135,1591047406.0,0,,False,,,,,,,,
528,,pytorch,"How to speedup the [back-propagation function of AdderNet](https://github.com/promach/PyTorch-YOLOv3/blob/addernet/adder.py#L67=L100) because training time is extremely long ?

Note: More context could be found at [https://github.com/huawei-noah/AdderNet/issues/16#issuecomment-627446321](https://github.com/huawei-noah/AdderNet/issues/16#issuecomment-627446321)",t2_bpftl,False,,0,False,How to speedup the back-propagation function of AdderNet,[],r/pytorch,False,6,,0,,,False,t3_gty2ga,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1590956143.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How to speedup the &lt;a href=""https://github.com/promach/PyTorch-YOLOv3/blob/addernet/adder.py#L67=L100""&gt;back-propagation function of AdderNet&lt;/a&gt; because training time is extremely long ?&lt;/p&gt;

&lt;p&gt;Note: More context could be found at &lt;a href=""https://github.com/huawei-noah/AdderNet/issues/16#issuecomment-627446321""&gt;https://github.com/huawei-noah/AdderNet/issues/16#issuecomment-627446321&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/9d05FBviry4xZbtj5hrL39dEeESpYm-eW_O8NwWaMJc.jpg?auto=webp&amp;s=a71ee0a57683973328fb511934f60e94e7bf4a88', 'width': 420, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/9d05FBviry4xZbtj5hrL39dEeESpYm-eW_O8NwWaMJc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=56fe2c0f3efd18dc06f39795737c2aa1d6c57fbc', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/9d05FBviry4xZbtj5hrL39dEeESpYm-eW_O8NwWaMJc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=31badc65b6c5ce3ceccdf9a2ff55cb48d248bd11', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/9d05FBviry4xZbtj5hrL39dEeESpYm-eW_O8NwWaMJc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a2da7c378cffe706a1da92423226256192c7715f', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'Rjx6RIFbZs51PhPAFkKZU4Pl7b6dp5BwJZUq2CuhXCU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gty2ga,True,,promach,,5,True,all_ads,False,[],False,,/r/pytorch/comments/gty2ga/how_to_speedup_the_backpropagation_function_of/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gty2ga/how_to_speedup_the_backpropagation_function_of/,7135,1590927343.0,4,,False,,,,,,,,
529,,pytorch,"Hey Guys! I recently built a Convolutional Siamese Network for one shot learning with PyTorch. I wrote an article about how I implemented and experimented with it. Feel free to check it out!

Link: [https://medium.com/@taying.cheng/building-a-one-shot-learning-network-with-pytorch-d1c3a5fafa4a](https://medium.com/@taying.cheng/building-a-one-shot-learning-network-with-pytorch-d1c3a5fafa4a)",t2_jn2eq6v,False,,0,False,Deep Network for One Shot Learning Using PyTorch,[],r/pytorch,False,6,,0,,,False,t3_gthkjw,False,dark,0.88,,public,11,0,{},,,False,[],,False,False,,{},,False,11,,False,self,False,,[],{},self,,True,,1590887051.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey Guys! I recently built a Convolutional Siamese Network for one shot learning with PyTorch. I wrote an article about how I implemented and experimented with it. Feel free to check it out!&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=""https://medium.com/@taying.cheng/building-a-one-shot-learning-network-with-pytorch-d1c3a5fafa4a""&gt;https://medium.com/@taying.cheng/building-a-one-shot-learning-network-with-pytorch-d1c3a5fafa4a&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DZ_UQEZe0qM53TZbiwNpkrH3vG56tJDG4gwKjm89g2o.jpg?auto=webp&amp;s=2fc2ddd971bcac65bf6e120002084b65d9e6d382', 'width': 1200, 'height': 780}, 'resolutions': [{'url': 'https://external-preview.redd.it/DZ_UQEZe0qM53TZbiwNpkrH3vG56tJDG4gwKjm89g2o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1879aeca36beaebcbe3cf3908fe1eced2c391c8', 'width': 108, 'height': 70}, {'url': 'https://external-preview.redd.it/DZ_UQEZe0qM53TZbiwNpkrH3vG56tJDG4gwKjm89g2o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=91cf217e71585cebfbdffe79b9a9ff566e039058', 'width': 216, 'height': 140}, {'url': 'https://external-preview.redd.it/DZ_UQEZe0qM53TZbiwNpkrH3vG56tJDG4gwKjm89g2o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a83015ec7bbcfcb0aca17fafd38bad93db383794', 'width': 320, 'height': 208}, {'url': 'https://external-preview.redd.it/DZ_UQEZe0qM53TZbiwNpkrH3vG56tJDG4gwKjm89g2o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec08475ce4bda818a2e631fcb321545188f9c9fe', 'width': 640, 'height': 416}, {'url': 'https://external-preview.redd.it/DZ_UQEZe0qM53TZbiwNpkrH3vG56tJDG4gwKjm89g2o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3bf210d65d5eb22f1947e15726c0057dc14674ac', 'width': 960, 'height': 624}, {'url': 'https://external-preview.redd.it/DZ_UQEZe0qM53TZbiwNpkrH3vG56tJDG4gwKjm89g2o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ec6406c8431ebbdc3d21d538817a6053f1ca404', 'width': 1080, 'height': 702}], 'variants': {}, 'id': 'XgSpKfAjTfPcdj2fzEH-pI4JmMZ2tuQ9T7rSjiUZwQE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gthkjw,True,,tt12343,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gthkjw/deep_network_for_one_shot_learning_using_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gthkjw/deep_network_for_one_shot_learning_using_pytorch/,7135,1590858251.0,0,,False,,,,,,,,
530,,pytorch,,t2_44mbtmjy,False,,0,False,"State of the art in instance segmentation: higher speed, more precise detection",[],r/pytorch,False,6,,0,55.0,,False,t3_gtoaey,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/vPWm_WDb_s1GN9ieGxQuoz9AyVo1DXt4mxs3uk3-Hpk.jpg,False,,[],{},link,,False,,1590910074.0,text,6,,,text,self.ObjectSegmentation,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/OTba2S_1hvkeqW2nbFlQhQZQ1TYzV1Y4MEKt3lY_h_o.jpg?auto=webp&amp;s=4316a67a40b610bb535bd27baadea1b8e6952fe8', 'width': 1300, 'height': 398}, 'resolutions': [{'url': 'https://external-preview.redd.it/OTba2S_1hvkeqW2nbFlQhQZQ1TYzV1Y4MEKt3lY_h_o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c5e721accbd85c735c93c4e9b4bbb5423058f19f', 'width': 108, 'height': 33}, {'url': 'https://external-preview.redd.it/OTba2S_1hvkeqW2nbFlQhQZQ1TYzV1Y4MEKt3lY_h_o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ebc48c229bce4dfc19db072ea00709107ec7b48', 'width': 216, 'height': 66}, {'url': 'https://external-preview.redd.it/OTba2S_1hvkeqW2nbFlQhQZQ1TYzV1Y4MEKt3lY_h_o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=71e97a02b0ce4aed80a10b7c1abd25051b2ae8f0', 'width': 320, 'height': 97}, {'url': 'https://external-preview.redd.it/OTba2S_1hvkeqW2nbFlQhQZQ1TYzV1Y4MEKt3lY_h_o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=343cbd1da608d6f2cea30683feef62e1e41a9c28', 'width': 640, 'height': 195}, {'url': 'https://external-preview.redd.it/OTba2S_1hvkeqW2nbFlQhQZQ1TYzV1Y4MEKt3lY_h_o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1329acdb7bc792fb5aa184b2ee6e186a02ccea8b', 'width': 960, 'height': 293}, {'url': 'https://external-preview.redd.it/OTba2S_1hvkeqW2nbFlQhQZQ1TYzV1Y4MEKt3lY_h_o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3d5642fae9f10102a36eedcd20b08c6a9b34c213', 'width': 1080, 'height': 330}], 'variants': {}, 'id': 'Cd9GoWOeXtgBF9CHZH-PwGxrLX63QHvvt-47C5fD_Tc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gtoaey,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gtoaey/state_of_the_art_in_instance_segmentation_higher/,all_ads,False,/r/ObjectSegmentation/comments/gto8i9/state_of_the_art_in_instance_segmentation_higher/,7135,1590881274.0,0,,False,/r/ObjectSegmentation/comments/gto8i9/state_of_the_art_in_instance_segmentation_higher/,"[{'approved_at_utc': None, 'subreddit': 'ObjectSegmentation', 'selftext': 'State of the art in instance segmentation: higher speed, more precise detection   \n\nFor project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2005.13243)\n\nhttps://preview.redd.it/nwkdfp1dkz151.png?width=1810&amp;format=png&amp;auto=webp&amp;s=10020be1dde30a82795980951b2d093df93e27b4\n\nPoly-YOLO performs instance segmentation using bounding polygons. The network is trained to detect size-independent polygons defined on a polar grid', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'State of the art in instance segmentation: higher speed, more precise detection', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ObjectSegmentation', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 55, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'nwkdfp1dkz151': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 42, 'x': 108, 'u': 'https://preview.redd.it/nwkdfp1dkz151.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa8d37d60b9e1d95255cffce6dc9c9a62c8cc09d'}, {'y': 85, 'x': 216, 'u': 'https://preview.redd.it/nwkdfp1dkz151.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9833a1fc4b8b010d802d20a0d49c0e96cdd3a515'}, {'y': 126, 'x': 320, 'u': 'https://preview.redd.it/nwkdfp1dkz151.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ffc06d8cb2bbc7869fdc9f2d9b2dac9ae2be1ad6'}, {'y': 252, 'x': 640, 'u': 'https://preview.redd.it/nwkdfp1dkz151.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e3679b7f2b4c78a1558b0fff3fdbaf643859820'}, {'y': 378, 'x': 960, 'u': 'https://preview.redd.it/nwkdfp1dkz151.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2522c4769e0c523dd16f5fa4a42bb911f20a1a3b'}, {'y': 426, 'x': 1080, 'u': 'https://preview.redd.it/nwkdfp1dkz151.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1e4d262f3bcf20141898eb45bd61be83f9a75f60'}], 's': {'y': 714, 'x': 1810, 'u': 'https://preview.redd.it/nwkdfp1dkz151.png?width=1810&amp;format=png&amp;auto=webp&amp;s=10020be1dde30a82795980951b2d093df93e27b4'}, 'id': 'nwkdfp1dkz151'}}, 'name': 't3_gto8i9', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 7, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/vPWm_WDb_s1GN9ieGxQuoz9AyVo1DXt4mxs3uk3-Hpk.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1590909887.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ObjectSegmentation', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;State of the art in instance segmentation: higher speed, more precise detection   &lt;/p&gt;\n\n&lt;p&gt;For project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2005.13243""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/nwkdfp1dkz151.png?width=1810&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=10020be1dde30a82795980951b2d093df93e27b4""&gt;https://preview.redd.it/nwkdfp1dkz151.png?width=1810&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=10020be1dde30a82795980951b2d093df93e27b4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Poly-YOLO performs instance segmentation using bounding polygons. The network is trained to detect size-independent polygons defined on a polar grid&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/OTba2S_1hvkeqW2nbFlQhQZQ1TYzV1Y4MEKt3lY_h_o.jpg?auto=webp&amp;s=4316a67a40b610bb535bd27baadea1b8e6952fe8', 'width': 1300, 'height': 398}, 'resolutions': [{'url': 'https://external-preview.redd.it/OTba2S_1hvkeqW2nbFlQhQZQ1TYzV1Y4MEKt3lY_h_o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c5e721accbd85c735c93c4e9b4bbb5423058f19f', 'width': 108, 'height': 33}, {'url': 'https://external-preview.redd.it/OTba2S_1hvkeqW2nbFlQhQZQ1TYzV1Y4MEKt3lY_h_o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ebc48c229bce4dfc19db072ea00709107ec7b48', 'width': 216, 'height': 66}, {'url': 'https://external-preview.redd.it/OTba2S_1hvkeqW2nbFlQhQZQ1TYzV1Y4MEKt3lY_h_o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=71e97a02b0ce4aed80a10b7c1abd25051b2ae8f0', 'width': 320, 'height': 97}, {'url': 'https://external-preview.redd.it/OTba2S_1hvkeqW2nbFlQhQZQ1TYzV1Y4MEKt3lY_h_o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=343cbd1da608d6f2cea30683feef62e1e41a9c28', 'width': 640, 'height': 195}, {'url': 'https://external-preview.redd.it/OTba2S_1hvkeqW2nbFlQhQZQ1TYzV1Y4MEKt3lY_h_o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1329acdb7bc792fb5aa184b2ee6e186a02ccea8b', 'width': 960, 'height': 293}, {'url': 'https://external-preview.redd.it/OTba2S_1hvkeqW2nbFlQhQZQ1TYzV1Y4MEKt3lY_h_o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3d5642fae9f10102a36eedcd20b08c6a9b34c213', 'width': 1080, 'height': 330}], 'variants': {}, 'id': 'Cd9GoWOeXtgBF9CHZH-PwGxrLX63QHvvt-47C5fD_Tc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2nxy3x', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gto8i9', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ObjectSegmentation/comments/gto8i9/state_of_the_art_in_instance_segmentation_higher/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ObjectSegmentation/comments/gto8i9/state_of_the_art_in_instance_segmentation_higher/', 'subreddit_subscribers': 117, 'created_utc': 1590881087.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_gto8i9,,,,,
531,,pytorch,"As many of you might know, keras's ImageDataGenerator enables it to:
1. Image augmentation
2. Automatic label creation (with respect to its directories)
3. Obviously, loading data.

As a user who've just got started with learning Pytorch, I've come to wonder why such function does not exist (I might've not found it yet though). Any advices? Is the best way is to create my custom data loaded from scratch?",t2_6lynm2wy,False,,0,False,Pytorch version of ImageDataGenerator?,[],r/pytorch,False,6,,0,,,False,t3_gte4na,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1590873388.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As many of you might know, keras&amp;#39;s ImageDataGenerator enables it to:
1. Image augmentation
2. Automatic label creation (with respect to its directories)
3. Obviously, loading data.&lt;/p&gt;

&lt;p&gt;As a user who&amp;#39;ve just got started with learning Pytorch, I&amp;#39;ve come to wonder why such function does not exist (I might&amp;#39;ve not found it yet though). Any advices? Is the best way is to create my custom data loaded from scratch?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gte4na,True,,aus_ge_zeich_net,,1,True,all_ads,False,[],False,,/r/pytorch/comments/gte4na/pytorch_version_of_imagedatagenerator/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gte4na/pytorch_version_of_imagedatagenerator/,7135,1590844588.0,0,,False,,,,,,,,
532,,pytorch,,t2_44mbtmjy,False,,0,False,Extracting editable 3D objects directly from a single photograph.,[],r/pytorch,False,6,,0,60.0,,False,t3_gt6smk,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/U7vfhhON2MporZsBozhwBSlER5rYi0kK7_HjPR_GGlo.jpg,False,,[],{},link,,False,,1590836136.0,text,6,,,text,self.LatestInML,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/nf882LtHa4Bw7oEIBR8N4o6VimX-3DckWwTExMPRoaw.jpg?auto=webp&amp;s=df18ef39ed9747ca6a2493d56c490161d803de5a', 'width': 700, 'height': 302}, 'resolutions': [{'url': 'https://external-preview.redd.it/nf882LtHa4Bw7oEIBR8N4o6VimX-3DckWwTExMPRoaw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c515e4843017aea419fbfd3b50ac96d3c6c96536', 'width': 108, 'height': 46}, {'url': 'https://external-preview.redd.it/nf882LtHa4Bw7oEIBR8N4o6VimX-3DckWwTExMPRoaw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=759e674416d1b19c04535f7c6091635d57e04698', 'width': 216, 'height': 93}, {'url': 'https://external-preview.redd.it/nf882LtHa4Bw7oEIBR8N4o6VimX-3DckWwTExMPRoaw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a1c444c2c00c555e7b6ee490e4434c12c9488bf', 'width': 320, 'height': 138}, {'url': 'https://external-preview.redd.it/nf882LtHa4Bw7oEIBR8N4o6VimX-3DckWwTExMPRoaw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=edec50bc26d7380bba4c7bcadfcf3baa8ed388b0', 'width': 640, 'height': 276}], 'variants': {}, 'id': '9aN1fpLI4cBSZvWluMA61uf-q3g25CwBL-q-20GHq_g'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gt6smk,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gt6smk/extracting_editable_3d_objects_directly_from_a/,all_ads,False,/r/LatestInML/comments/gt6qib/extracting_editable_3d_objects_directly_from_a/,7135,1590807336.0,0,,False,/r/LatestInML/comments/gt6qib/extracting_editable_3d_objects_directly_from_a/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code or API request:\xa0[click here](https://www.catalyzex.com/paper/arxiv:2005.13312)\n\nhttps://reddit.com/link/gt6qib/video/1tehurhngt151/player\n\nThey simultaneously identify profile-body relations and recover 3D parts by sweeping the recognized profile along their body contour and jointly optimize the geometry to align with the recovered masks. Qualitative and quantitative experiments show that our algorithm can recover high-quality 3D models and outperforms existing methods in both instance segmentation and 3D reconstruction', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Extracting editable 3D objects directly from a single photograph.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 60, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'1tehurhngt151': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/gt6qib/asset/1tehurhngt151/DASHPlaylist.mpd?a=1618044237%2CM2ViNGE3OGEzYTYxZjExMjM5ZjBkYzg1MGFlNzE3NjE0YmYyOWQ4ZmE0NDlkYjMyNjVhY2U1NjRkYzIxMWI4YQ%3D%3D&amp;v=1&amp;f=sd', 'x': 600, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/gt6qib/asset/1tehurhngt151/HLSPlaylist.m3u8?a=1618044237%2CYWE4YmE5YzE1YTcwYmE4OTAyNDc5YWQ3YTM2NzhjMTNiOGZhZDA1YzQ0NmNiMGQyNDAxOTY1ZGRmYTNiODY4YQ%3D%3D&amp;v=1&amp;f=sd', 'id': '1tehurhngt151', 'isGif': False}}, 'name': 't3_gt6qib', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.85, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 23, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 23, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/U7vfhhON2MporZsBozhwBSlER5rYi0kK7_HjPR_GGlo.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1590835888.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code or API request:\xa0&lt;a href=""https://www.catalyzex.com/paper/arxiv:2005.13312""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/gt6qib/video/1tehurhngt151/player""&gt;https://reddit.com/link/gt6qib/video/1tehurhngt151/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They simultaneously identify profile-body relations and recover 3D parts by sweeping the recognized profile along their body contour and jointly optimize the geometry to align with the recovered masks. Qualitative and quantitative experiments show that our algorithm can recover high-quality 3D models and outperforms existing methods in both instance segmentation and 3D reconstruction&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/nf882LtHa4Bw7oEIBR8N4o6VimX-3DckWwTExMPRoaw.jpg?auto=webp&amp;s=df18ef39ed9747ca6a2493d56c490161d803de5a', 'width': 700, 'height': 302}, 'resolutions': [{'url': 'https://external-preview.redd.it/nf882LtHa4Bw7oEIBR8N4o6VimX-3DckWwTExMPRoaw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c515e4843017aea419fbfd3b50ac96d3c6c96536', 'width': 108, 'height': 46}, {'url': 'https://external-preview.redd.it/nf882LtHa4Bw7oEIBR8N4o6VimX-3DckWwTExMPRoaw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=759e674416d1b19c04535f7c6091635d57e04698', 'width': 216, 'height': 93}, {'url': 'https://external-preview.redd.it/nf882LtHa4Bw7oEIBR8N4o6VimX-3DckWwTExMPRoaw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a1c444c2c00c555e7b6ee490e4434c12c9488bf', 'width': 320, 'height': 138}, {'url': 'https://external-preview.redd.it/nf882LtHa4Bw7oEIBR8N4o6VimX-3DckWwTExMPRoaw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=edec50bc26d7380bba4c7bcadfcf3baa8ed388b0', 'width': 640, 'height': 276}], 'variants': {}, 'id': '9aN1fpLI4cBSZvWluMA61uf-q3g25CwBL-q-20GHq_g'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gt6qib', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gt6qib/extracting_editable_3d_objects_directly_from_a/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gt6qib/extracting_editable_3d_objects_directly_from_a/', 'subreddit_subscribers': 6676, 'created_utc': 1590807088.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_gt6qib,,,,,
533,,pytorch,"I am coming here with exceptional frustration.  


I am currently doing an AI Practicum where we explore different models and their uses on data.  


I have an LNN I have constructed which looks at my sequence of 6 by 150 data (6 items in a row, 150 rows), and that specific sequence has a rank assigned to it. Any NN achieves accuracy by being within a specific margin of that rank. (There is only one output, just trying to achieve that rank). The LNN's I built and trained often easily achieved an accuracy in the 90's roughly.  


The RNN I built however seems to do this really weird thing during training where it just finds pretty much the average of all the ranks and goes for that, then guesses that for EVERYTHING.  


It's giving me such a headache and I cannot understand it.",t2_f18ju,False,,0,False,RNN Giving me grief,[],r/pytorch,False,6,,0,,,False,t3_gskzjp,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1590751216.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am coming here with exceptional frustration.  &lt;/p&gt;

&lt;p&gt;I am currently doing an AI Practicum where we explore different models and their uses on data.  &lt;/p&gt;

&lt;p&gt;I have an LNN I have constructed which looks at my sequence of 6 by 150 data (6 items in a row, 150 rows), and that specific sequence has a rank assigned to it. Any NN achieves accuracy by being within a specific margin of that rank. (There is only one output, just trying to achieve that rank). The LNN&amp;#39;s I built and trained often easily achieved an accuracy in the 90&amp;#39;s roughly.  &lt;/p&gt;

&lt;p&gt;The RNN I built however seems to do this really weird thing during training where it just finds pretty much the average of all the ranks and goes for that, then guesses that for EVERYTHING.  &lt;/p&gt;

&lt;p&gt;It&amp;#39;s giving me such a headache and I cannot understand it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gskzjp,True,,jokojoko22,,6,True,all_ads,False,[],False,,/r/pytorch/comments/gskzjp/rnn_giving_me_grief/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gskzjp/rnn_giving_me_grief/,7135,1590722416.0,0,,False,,,,,,,,
534,,pytorch,,t2_44mbtmjy,False,,0,False,Latest from Facebook and CMU researchers: Navigating to the location indicated by a goal image in a novel previously unseen environment!,[],r/pytorch,False,6,,0,93.0,,False,t3_gsewit,False,dark,0.88,,public,6,0,{},140.0,,False,[],,False,False,,{},,False,6,,False,https://a.thumbs.redditmedia.com/2pzxomkPHoB4vyMvq1hjRjv4HeV0P0lS-UkrNr_ezn0.jpg,False,,[],{},link,,False,,1590729023.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/zNRdYjgER_6zigtCiiK0NsHMqwWNXW96Mvhi3xrPE1Y.jpg?auto=webp&amp;s=03f65d332f0981ddb4094295394eb4615c388ae3', 'width': 656, 'height': 440}, 'resolutions': [{'url': 'https://external-preview.redd.it/zNRdYjgER_6zigtCiiK0NsHMqwWNXW96Mvhi3xrPE1Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f21904d500559796073250b054d1d9f9acedc6c', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/zNRdYjgER_6zigtCiiK0NsHMqwWNXW96Mvhi3xrPE1Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb4b7f2d28baa72b9d0e2f859c998f9a41b2f7b2', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/zNRdYjgER_6zigtCiiK0NsHMqwWNXW96Mvhi3xrPE1Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c221477ba35690b1a18afd1a1fd9d4b59fec2c6', 'width': 320, 'height': 214}, {'url': 'https://external-preview.redd.it/zNRdYjgER_6zigtCiiK0NsHMqwWNXW96Mvhi3xrPE1Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4f32478f583e1489e422cfbe6f139af58f9a54e', 'width': 640, 'height': 429}], 'variants': {}, 'id': 'OVSh2roS9znVLVuq6BzIqsUl_vAUYDWrWj8n7CIpBLg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gsewit,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gsewit/latest_from_facebook_and_cmu_researchers/,all_ads,False,/r/LatestInML/comments/gseu6w/latest_from_facebook_and_cmu_researchers/,7135,1590700223.0,0,,False,/r/LatestInML/comments/gseu6w/latest_from_facebook_and_cmu_researchers/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'Latest from Facebook and CMU researchers: Navigating to the location indicated by a goal image in a novel previously unseen environment!\n\nFor project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2005.12256)\n\nhttps://reddit.com/link/gseu6w/video/ewq25iz9mk151/player\n\nThey design topological representations for space that effectively leverage semantics and afford approximate geometric reasoning. Their method builds effective representations that capture structural regularities and efficiently solve long-horizon navigation problems.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Latest from Facebook and CMU researchers: Navigating to the location indicated by a goal image in a novel previously unseen environment!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 93, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'ewq25iz9mk151': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/gseu6w/asset/ewq25iz9mk151/DASHPlaylist.mpd?a=1618044238%2CMmYyOWQ4YzFhYmVkYzUyNDY4Zjk1Y2UxYmJiZDMxNzQ1ZDNmZDM1NWZiMTM5NmEwZTRlMmQ5OWUzY2YwMzczZg%3D%3D&amp;v=1&amp;f=sd', 'x': 426, 'y': 160, 'hlsUrl': 'https://v.redd.it/link/gseu6w/asset/ewq25iz9mk151/HLSPlaylist.m3u8?a=1618044238%2CMGY3OTkyZWNjMWY1N2FkODliZThlNjNlZWQ5NGVjNTM2YzVkNmQ0ZjI3MTRjMDZhOTFjNzdlZGYxMzFjYWQ1ZA%3D%3D&amp;v=1&amp;f=sd', 'id': 'ewq25iz9mk151', 'isGif': False}}, 'name': 't3_gseu6w', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 16, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 16, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/2pzxomkPHoB4vyMvq1hjRjv4HeV0P0lS-UkrNr_ezn0.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1590728812.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Latest from Facebook and CMU researchers: Navigating to the location indicated by a goal image in a novel previously unseen environment!&lt;/p&gt;\n\n&lt;p&gt;For project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2005.12256""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/gseu6w/video/ewq25iz9mk151/player""&gt;https://reddit.com/link/gseu6w/video/ewq25iz9mk151/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They design topological representations for space that effectively leverage semantics and afford approximate geometric reasoning. Their method builds effective representations that capture structural regularities and efficiently solve long-horizon navigation problems.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/zNRdYjgER_6zigtCiiK0NsHMqwWNXW96Mvhi3xrPE1Y.jpg?auto=webp&amp;s=03f65d332f0981ddb4094295394eb4615c388ae3', 'width': 656, 'height': 440}, 'resolutions': [{'url': 'https://external-preview.redd.it/zNRdYjgER_6zigtCiiK0NsHMqwWNXW96Mvhi3xrPE1Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f21904d500559796073250b054d1d9f9acedc6c', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/zNRdYjgER_6zigtCiiK0NsHMqwWNXW96Mvhi3xrPE1Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb4b7f2d28baa72b9d0e2f859c998f9a41b2f7b2', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/zNRdYjgER_6zigtCiiK0NsHMqwWNXW96Mvhi3xrPE1Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c221477ba35690b1a18afd1a1fd9d4b59fec2c6', 'width': 320, 'height': 214}, {'url': 'https://external-preview.redd.it/zNRdYjgER_6zigtCiiK0NsHMqwWNXW96Mvhi3xrPE1Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4f32478f583e1489e422cfbe6f139af58f9a54e', 'width': 640, 'height': 429}], 'variants': {}, 'id': 'OVSh2roS9znVLVuq6BzIqsUl_vAUYDWrWj8n7CIpBLg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gseu6w', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gseu6w/latest_from_facebook_and_cmu_researchers/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gseu6w/latest_from_facebook_and_cmu_researchers/', 'subreddit_subscribers': 6676, 'created_utc': 1590700012.0, 'num_crossposts': 13, 'media': None, 'is_video': False}]",t3_gseu6w,,,,,
535,,pytorch,"I'm new to pytorch and i have few questions.

while defining a model, we have to provide ""super().\_\_init\_\_()"" inside constructor. then what is difference between

1. super(Model, self).\_\_init\_\_()
2. super().\_\_init\_\_(x, y)",,False,,0,False,"super().__init__(**kwargs) vs super(Model, self).__init__()",[],r/pytorch,False,6,,0,,,False,t3_gs4ptl,False,dark,0.82,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,,self,False,,,{},,,True,,1590694889.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m new to pytorch and i have few questions.&lt;/p&gt;

&lt;p&gt;while defining a model, we have to provide &amp;quot;super().__init__()&amp;quot; inside constructor. then what is difference between&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;super(Model, self).__init__()&lt;/li&gt;
&lt;li&gt;super().__init__(x, y)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gs4ptl,True,,[deleted],,2,True,all_ads,False,[],,dark,/r/pytorch/comments/gs4ptl/super_init_kwargs_vs_supermodel_self_init/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gs4ptl/super_init_kwargs_vs_supermodel_self_init/,7135,1590666089.0,0,,False,,,,,,,,
536,,pytorch,"Hello all,

I just want to work with PyTorch tensors on GPU using Google Colab, since I do many matrix multiplications in my project and NumPy is way too slow. But I can't seem to do it for some reason. I think that following line of code must give me a matrix on GPU, and operations between such tensors must run on GPU:

mat = torch.zeros(dim1, dim2).type(torch.cuda.FloatTensor) 

But this does not utilize the GPU on Google Colab. Can someone point me my mistake?

Thanks!",t2_1cui2d3k,False,,0,False,Using GPU for tensor operations,[],r/pytorch,False,6,,0,,,False,t3_gs8jmu,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1590709204.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all,&lt;/p&gt;

&lt;p&gt;I just want to work with PyTorch tensors on GPU using Google Colab, since I do many matrix multiplications in my project and NumPy is way too slow. But I can&amp;#39;t seem to do it for some reason. I think that following line of code must give me a matrix on GPU, and operations between such tensors must run on GPU:&lt;/p&gt;

&lt;p&gt;mat = torch.zeros(dim1, dim2).type(torch.cuda.FloatTensor) &lt;/p&gt;

&lt;p&gt;But this does not utilize the GPU on Google Colab. Can someone point me my mistake?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gs8jmu,True,,cheeky_bastard__,,3,True,all_ads,False,[],False,,/r/pytorch/comments/gs8jmu/using_gpu_for_tensor_operations/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gs8jmu/using_gpu_for_tensor_operations/,7135,1590680404.0,0,,False,,,,,,,,
537,,pytorch,,t2_44mbtmjy,False,,0,False,From Adobe researchers: State of the art in High-Resolution Image Inpainting,[],r/pytorch,False,6,,0,48.0,,False,t3_grxt7m,False,dark,1.0,,public,6,0,{},140.0,,False,[],,False,False,,{},,False,6,,False,https://b.thumbs.redditmedia.com/BMySi0F7fOS1iSIMIqAVhDSpcQjIRBrHqLbAM0IqWvc.jpg,False,,[],{},link,,False,,1590662192.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/TrrDb6mPzyO_PtO9_aYcIF9YZVCIGiLg9bTNoTyGYuQ.jpg?auto=webp&amp;s=485c3f9555e3454949001634ed970443733d4a00', 'width': 960, 'height': 330}, 'resolutions': [{'url': 'https://external-preview.redd.it/TrrDb6mPzyO_PtO9_aYcIF9YZVCIGiLg9bTNoTyGYuQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a8a4b8a65be3888323a0f7fadb606d49f9f181a', 'width': 108, 'height': 37}, {'url': 'https://external-preview.redd.it/TrrDb6mPzyO_PtO9_aYcIF9YZVCIGiLg9bTNoTyGYuQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5235bda8e40785d8fb41451a2e78b43f403a0b6e', 'width': 216, 'height': 74}, {'url': 'https://external-preview.redd.it/TrrDb6mPzyO_PtO9_aYcIF9YZVCIGiLg9bTNoTyGYuQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f00554c8bf045fdd0261ea9f02e7dbd4827d0ef', 'width': 320, 'height': 110}, {'url': 'https://external-preview.redd.it/TrrDb6mPzyO_PtO9_aYcIF9YZVCIGiLg9bTNoTyGYuQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0727243fdc1a2227b40796439df3757e92491b1d', 'width': 640, 'height': 220}, {'url': 'https://external-preview.redd.it/TrrDb6mPzyO_PtO9_aYcIF9YZVCIGiLg9bTNoTyGYuQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a32a6a8ace75a8b0e686848cfc9211bb8d83ca7c', 'width': 960, 'height': 330}], 'variants': {}, 'id': '5xDjvMNK3RSZ1ptT1BibKcSZ2LUYuyvLegK_TMKBO5M'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,grxt7m,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/grxt7m/from_adobe_researchers_state_of_the_art_in/,all_ads,False,/r/LatestInML/comments/grxr83/from_adobe_researchers_state_of_the_art_in/,7135,1590633392.0,0,,False,/r/LatestInML/comments/grxr83/from_adobe_researchers_state_of_the_art_in/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2005.11742)\n\nhttps://preview.redd.it/a07pf5lk3f151.png?width=1966&amp;format=png&amp;auto=webp&amp;s=63825a0f67aa812f867e9ba09387177afb9bcdf3\n\nTo mimic real object removal scenarios, they collect a large object mask dataset and synthesize more realistic training data that better simulates user inputs', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'From Adobe researchers: State of the art in High-Resolution Image Inpainting', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 48, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'a07pf5lk3f151': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 53, 'x': 108, 'u': 'https://preview.redd.it/a07pf5lk3f151.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb3fa09fb3bfbe52d47a6c5db4505a93aba8cb6a'}, {'y': 107, 'x': 216, 'u': 'https://preview.redd.it/a07pf5lk3f151.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b2af7a41782b1f82454549b25216601ffc2d08f'}, {'y': 158, 'x': 320, 'u': 'https://preview.redd.it/a07pf5lk3f151.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1a450c1dd448677f7573c0e3b8628c8251155c2'}, {'y': 317, 'x': 640, 'u': 'https://preview.redd.it/a07pf5lk3f151.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=636539e1697e76d45f40be13a6019b713a3e55ee'}, {'y': 476, 'x': 960, 'u': 'https://preview.redd.it/a07pf5lk3f151.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=60ea31553a8a695e9c993a4217bd79d1bb5b5094'}, {'y': 536, 'x': 1080, 'u': 'https://preview.redd.it/a07pf5lk3f151.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5242064a0d08c63162edd0ddcb1ef9a6c32f1419'}], 's': {'y': 976, 'x': 1966, 'u': 'https://preview.redd.it/a07pf5lk3f151.png?width=1966&amp;format=png&amp;auto=webp&amp;s=63825a0f67aa812f867e9ba09387177afb9bcdf3'}, 'id': 'a07pf5lk3f151'}}, 'name': 't3_grxr83', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 28, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 28, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/BMySi0F7fOS1iSIMIqAVhDSpcQjIRBrHqLbAM0IqWvc.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1590661985.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2005.11742""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/a07pf5lk3f151.png?width=1966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63825a0f67aa812f867e9ba09387177afb9bcdf3""&gt;https://preview.redd.it/a07pf5lk3f151.png?width=1966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63825a0f67aa812f867e9ba09387177afb9bcdf3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To mimic real object removal scenarios, they collect a large object mask dataset and synthesize more realistic training data that better simulates user inputs&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/TrrDb6mPzyO_PtO9_aYcIF9YZVCIGiLg9bTNoTyGYuQ.jpg?auto=webp&amp;s=485c3f9555e3454949001634ed970443733d4a00', 'width': 960, 'height': 330}, 'resolutions': [{'url': 'https://external-preview.redd.it/TrrDb6mPzyO_PtO9_aYcIF9YZVCIGiLg9bTNoTyGYuQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a8a4b8a65be3888323a0f7fadb606d49f9f181a', 'width': 108, 'height': 37}, {'url': 'https://external-preview.redd.it/TrrDb6mPzyO_PtO9_aYcIF9YZVCIGiLg9bTNoTyGYuQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5235bda8e40785d8fb41451a2e78b43f403a0b6e', 'width': 216, 'height': 74}, {'url': 'https://external-preview.redd.it/TrrDb6mPzyO_PtO9_aYcIF9YZVCIGiLg9bTNoTyGYuQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f00554c8bf045fdd0261ea9f02e7dbd4827d0ef', 'width': 320, 'height': 110}, {'url': 'https://external-preview.redd.it/TrrDb6mPzyO_PtO9_aYcIF9YZVCIGiLg9bTNoTyGYuQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0727243fdc1a2227b40796439df3757e92491b1d', 'width': 640, 'height': 220}, {'url': 'https://external-preview.redd.it/TrrDb6mPzyO_PtO9_aYcIF9YZVCIGiLg9bTNoTyGYuQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a32a6a8ace75a8b0e686848cfc9211bb8d83ca7c', 'width': 960, 'height': 330}], 'variants': {}, 'id': '5xDjvMNK3RSZ1ptT1BibKcSZ2LUYuyvLegK_TMKBO5M'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'grxr83', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/grxr83/from_adobe_researchers_state_of_the_art_in/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/grxr83/from_adobe_researchers_state_of_the_art_in/', 'subreddit_subscribers': 6676, 'created_utc': 1590633185.0, 'num_crossposts': 14, 'media': None, 'is_video': False}]",t3_grxr83,,,,,
538,,pytorch,,t2_44mbtmjy,False,,0,False,"Deep Fashion3D, the largest collection to date of 3D garment models",[],r/pytorch,False,6,,0,53.0,,False,t3_gr93u5,False,dark,1.0,,public,8,0,{},140.0,,False,[],,False,False,,{},,False,8,,False,https://b.thumbs.redditmedia.com/YLGQU6jnIZlYs4gw52JJWwCV0G7bOtC0xEtvAf0r9ZE.jpg,False,,[],{},link,,False,,1590567814.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Evy-W-plYOij_gWYblHSAKY_qdChJFoUGIfdZhlaLoc.png?auto=webp&amp;s=dbedf5167f04d140565aed6abeed07fd9e2bdffe', 'width': 2026, 'height': 776}, 'resolutions': [{'url': 'https://external-preview.redd.it/Evy-W-plYOij_gWYblHSAKY_qdChJFoUGIfdZhlaLoc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=337c2c7757c53fa2b9c64f1e03e1893062fe9d0e', 'width': 108, 'height': 41}, {'url': 'https://external-preview.redd.it/Evy-W-plYOij_gWYblHSAKY_qdChJFoUGIfdZhlaLoc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=98e78b5091ccd3318e2e3d757f0498feecd477c8', 'width': 216, 'height': 82}, {'url': 'https://external-preview.redd.it/Evy-W-plYOij_gWYblHSAKY_qdChJFoUGIfdZhlaLoc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5a74bf3a9b47f38b21eb7fe45da40caa0944f5a', 'width': 320, 'height': 122}, {'url': 'https://external-preview.redd.it/Evy-W-plYOij_gWYblHSAKY_qdChJFoUGIfdZhlaLoc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e79eb0445dcea900693adf0dff7b22236fb4e9fa', 'width': 640, 'height': 245}, {'url': 'https://external-preview.redd.it/Evy-W-plYOij_gWYblHSAKY_qdChJFoUGIfdZhlaLoc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5d8120e832f5b92886ac821ccbb26789ac6aa56', 'width': 960, 'height': 367}, {'url': 'https://external-preview.redd.it/Evy-W-plYOij_gWYblHSAKY_qdChJFoUGIfdZhlaLoc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bea2018e4ecee484669eafcc0161646490ad31b8', 'width': 1080, 'height': 413}], 'variants': {}, 'id': '--Cya8Tsq4HK9e5Rt2Vvk9kyi2uSl72qjW0RYyLKkNI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gr93u5,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gr93u5/deep_fashion3d_the_largest_collection_to_date_of/,all_ads,False,/r/LatestInML/comments/gr9042/deep_fashion3d_the_largest_collection_to_date_of/,7135,1590539014.0,0,,False,/r/LatestInML/comments/gr9042/deep_fashion3d_the_largest_collection_to_date_of/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'Deep Fashion3D, the largest collection to date of 3D garment models  \n\nFor project and dataset: [click here](https://www.catalyzex.com/paper/arxiv:2003.12753)\n\nhttps://preview.redd.it/zzuhjb1ea7151.png?width=2026&amp;format=png&amp;auto=webp&amp;s=dbf76012b42f230b8c24f64d4dac68365369ad5c\n\nIt has the goal of establishing a novel benchmark and dataset for the evaluation of image-based garment reconstruction systems. It contains 2078 models reconstructed from real garments, which covers 10 different categories and 563 garment instances', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Deep Fashion3D, the largest collection to date of 3D garment models', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 53, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'zzuhjb1ea7151': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 41, 'x': 108, 'u': 'https://preview.redd.it/zzuhjb1ea7151.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec7699724d9b6ad3f6a7b80f827087df1706e962'}, {'y': 82, 'x': 216, 'u': 'https://preview.redd.it/zzuhjb1ea7151.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c68112cd9d961199eed2e2cb33e98d73027e1c2e'}, {'y': 122, 'x': 320, 'u': 'https://preview.redd.it/zzuhjb1ea7151.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1786684deac7036aebc546318448a5409a99b658'}, {'y': 245, 'x': 640, 'u': 'https://preview.redd.it/zzuhjb1ea7151.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0ec71f9dac8cf1c553acaad666e93fa6404d2d4'}, {'y': 367, 'x': 960, 'u': 'https://preview.redd.it/zzuhjb1ea7151.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dd5281616f109a9b01d5ca067c20e74722eca409'}, {'y': 413, 'x': 1080, 'u': 'https://preview.redd.it/zzuhjb1ea7151.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=34c6e8cff8c93ab767265ee085428528fdbd69d0'}], 's': {'y': 776, 'x': 2026, 'u': 'https://preview.redd.it/zzuhjb1ea7151.png?width=2026&amp;format=png&amp;auto=webp&amp;s=dbf76012b42f230b8c24f64d4dac68365369ad5c'}, 'id': 'zzuhjb1ea7151'}}, 'name': 't3_gr9042', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 20, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 20, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/YLGQU6jnIZlYs4gw52JJWwCV0G7bOtC0xEtvAf0r9ZE.jpg', 'edited': 1594328562.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1590567420.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Deep Fashion3D, the largest collection to date of 3D garment models  &lt;/p&gt;\n\n&lt;p&gt;For project and dataset: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2003.12753""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/zzuhjb1ea7151.png?width=2026&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dbf76012b42f230b8c24f64d4dac68365369ad5c""&gt;https://preview.redd.it/zzuhjb1ea7151.png?width=2026&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dbf76012b42f230b8c24f64d4dac68365369ad5c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It has the goal of establishing a novel benchmark and dataset for the evaluation of image-based garment reconstruction systems. It contains 2078 models reconstructed from real garments, which covers 10 different categories and 563 garment instances&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/Evy-W-plYOij_gWYblHSAKY_qdChJFoUGIfdZhlaLoc.png?auto=webp&amp;s=dbedf5167f04d140565aed6abeed07fd9e2bdffe', 'width': 2026, 'height': 776}, 'resolutions': [{'url': 'https://external-preview.redd.it/Evy-W-plYOij_gWYblHSAKY_qdChJFoUGIfdZhlaLoc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=337c2c7757c53fa2b9c64f1e03e1893062fe9d0e', 'width': 108, 'height': 41}, {'url': 'https://external-preview.redd.it/Evy-W-plYOij_gWYblHSAKY_qdChJFoUGIfdZhlaLoc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=98e78b5091ccd3318e2e3d757f0498feecd477c8', 'width': 216, 'height': 82}, {'url': 'https://external-preview.redd.it/Evy-W-plYOij_gWYblHSAKY_qdChJFoUGIfdZhlaLoc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5a74bf3a9b47f38b21eb7fe45da40caa0944f5a', 'width': 320, 'height': 122}, {'url': 'https://external-preview.redd.it/Evy-W-plYOij_gWYblHSAKY_qdChJFoUGIfdZhlaLoc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e79eb0445dcea900693adf0dff7b22236fb4e9fa', 'width': 640, 'height': 245}, {'url': 'https://external-preview.redd.it/Evy-W-plYOij_gWYblHSAKY_qdChJFoUGIfdZhlaLoc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5d8120e832f5b92886ac821ccbb26789ac6aa56', 'width': 960, 'height': 367}, {'url': 'https://external-preview.redd.it/Evy-W-plYOij_gWYblHSAKY_qdChJFoUGIfdZhlaLoc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bea2018e4ecee484669eafcc0161646490ad31b8', 'width': 1080, 'height': 413}], 'variants': {}, 'id': '--Cya8Tsq4HK9e5Rt2Vvk9kyi2uSl72qjW0RYyLKkNI'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gr9042', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gr9042/deep_fashion3d_the_largest_collection_to_date_of/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gr9042/deep_fashion3d_the_largest_collection_to_date_of/', 'subreddit_subscribers': 6676, 'created_utc': 1590538620.0, 'num_crossposts': 14, 'media': None, 'is_video': False}]",t3_gr9042,,,,,
539,,pytorch,"Hey guys have a look at my blog....I am new to machine learning and I will be going to post more blogs soon!

[https://medium.com/@ojastyagi122/fun-with-tensors-in-pytorch-fdd7043cf690](https://medium.com/@ojastyagi122/fun-with-tensors-in-pytorch-fdd7043cf690)

Thanks for your support!",t2_f7rcnka,False,,0,False,Check it OUT,[],r/pytorch,False,6,,0,,,False,t3_grj8fk,False,dark,0.13,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1590613070.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys have a look at my blog....I am new to machine learning and I will be going to post more blogs soon!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://medium.com/@ojastyagi122/fun-with-tensors-in-pytorch-fdd7043cf690""&gt;https://medium.com/@ojastyagi122/fun-with-tensors-in-pytorch-fdd7043cf690&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks for your support!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/aCKNqoquKWBhg11DW_5g5nUA7PzuNMBTFiEccL0ztOk.jpg?auto=webp&amp;s=abde59e3f2fcd7c0560b58e2239541fa2071c7a3', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/aCKNqoquKWBhg11DW_5g5nUA7PzuNMBTFiEccL0ztOk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d0b2aeb36a58a49af16b838deda85fb8e660221', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/aCKNqoquKWBhg11DW_5g5nUA7PzuNMBTFiEccL0ztOk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9704d465b8599c685f9a4ab4530b9eca74f87697', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/aCKNqoquKWBhg11DW_5g5nUA7PzuNMBTFiEccL0ztOk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=95d70df7e0da88b5029d65f9acf1fbccda631012', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/aCKNqoquKWBhg11DW_5g5nUA7PzuNMBTFiEccL0ztOk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=68bec18815368b998f6e8493078f222a69e9f638', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/aCKNqoquKWBhg11DW_5g5nUA7PzuNMBTFiEccL0ztOk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f1105978a7c95f89685dbddbb347b82ad6967449', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/aCKNqoquKWBhg11DW_5g5nUA7PzuNMBTFiEccL0ztOk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=163cd9b6b03b2312b96b72a377e16c37d12a654e', 'width': 1080, 'height': 607}], 'variants': {}, 'id': '6Q-6D1ppRmrfuNOVGE8Bqq4xoGZRFEs2NJr1XFuH96s'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,grj8fk,True,,ojastyagi,,0,True,all_ads,False,[],False,,/r/pytorch/comments/grj8fk/check_it_out/,all_ads,False,https://www.reddit.com/r/pytorch/comments/grj8fk/check_it_out/,7135,1590584270.0,0,,False,,,,,,,,
540,,pytorch,"Here's a previous project we did, it may give insight to our group 

https://github.com/Santosh-Gupta/NaturalLanguageRecommendations",t2_10efjmjx,False,,0,False,"Anyone interested in using NLP for making search engines for Covid-19 papers? We have a casual group working on this, looking for others who have an interest in IR / scientific texts.",[],r/pytorch,False,6,,0,,,False,t3_graryh,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1590574226.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Here&amp;#39;s a previous project we did, it may give insight to our group &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/Santosh-Gupta/NaturalLanguageRecommendations""&gt;https://github.com/Santosh-Gupta/NaturalLanguageRecommendations&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ekuBOtSHhv88LIngi7mHdtvquw_XVp3mVUJUAGFGfbQ.jpg?auto=webp&amp;s=7ed0c1aa5b932c8947956789ff8bf076e1897367', 'width': 211, 'height': 211}, 'resolutions': [{'url': 'https://external-preview.redd.it/ekuBOtSHhv88LIngi7mHdtvquw_XVp3mVUJUAGFGfbQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=62805bb6eeea6ec5453290ec4da037cf4a3e1159', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'wzRKJJ0kJscV4F1Bl5fDB_oI36nc-NFGKIJn0GEHiXI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,graryh,True,,BatmantoshReturns,,2,True,all_ads,False,[],False,,/r/pytorch/comments/graryh/anyone_interested_in_using_nlp_for_making_search/,all_ads,False,https://www.reddit.com/r/pytorch/comments/graryh/anyone_interested_in_using_nlp_for_making_search/,7135,1590545426.0,0,,False,,,,,,,,
541,,pytorch,"In many AI applications, we need a lot of data to train a model. The more data we have, the better the model becomes. This is especially true in areas like healthcare where a good AI model can be immensely useful to humanity as a whole. At the same time, people may not (and should not) be willing to share their data because of privacy concerns.  
This points to a need for a training system that improves a model without a remote server ever getting access to private/personal/local data.  
In today's post, we go over the idea of Federated Learning tries to solve this problem with an example (code)  


[https://www.learnopencv.com/federated-learning-using-pytorch-and-pysyft/](https://www.learnopencv.com/federated-learning-using-pytorch-and-pysyft/)  
Please find the relevant code at:

[https://github.com/spmallick/learnopencv/tree/master/Federated-Learning-Intro](https://github.com/spmallick/learnopencv/tree/master/Federated-Learning-Intro) 

&amp;#x200B;

https://preview.redd.it/6t1yx1vpb2151.png?width=600&amp;format=png&amp;auto=webp&amp;s=9f67a5f597dbdc46a5b8cea77eb1aa87c0415f4c",t2_cvc9f,False,,0,False,Federated Learning using PyTorch and PySyft,[],r/pytorch,False,6,,0,93.0,,False,t3_gqsj99,False,dark,0.86,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/tcer8HP93DoUU6BZ9ZuG_e0wKaUWtc0XllRBhrYpgZI.jpg,False,,[],{},,,True,,1590507328.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In many AI applications, we need a lot of data to train a model. The more data we have, the better the model becomes. This is especially true in areas like healthcare where a good AI model can be immensely useful to humanity as a whole. At the same time, people may not (and should not) be willing to share their data because of privacy concerns.&lt;br/&gt;
This points to a need for a training system that improves a model without a remote server ever getting access to private/personal/local data.&lt;br/&gt;
In today&amp;#39;s post, we go over the idea of Federated Learning tries to solve this problem with an example (code)  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.learnopencv.com/federated-learning-using-pytorch-and-pysyft/""&gt;https://www.learnopencv.com/federated-learning-using-pytorch-and-pysyft/&lt;/a&gt;&lt;br/&gt;
Please find the relevant code at:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/spmallick/learnopencv/tree/master/Federated-Learning-Intro""&gt;https://github.com/spmallick/learnopencv/tree/master/Federated-Learning-Intro&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/6t1yx1vpb2151.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9f67a5f597dbdc46a5b8cea77eb1aa87c0415f4c""&gt;https://preview.redd.it/6t1yx1vpb2151.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9f67a5f597dbdc46a5b8cea77eb1aa87c0415f4c&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gqsj99,True,,spmallick,,1,True,all_ads,False,[],False,,/r/pytorch/comments/gqsj99/federated_learning_using_pytorch_and_pysyft/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gqsj99/federated_learning_using_pytorch_and_pysyft/,7135,1590478528.0,0,,False,,,,"{'6t1yx1vpb2151': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 72, 'x': 108, 'u': 'https://preview.redd.it/6t1yx1vpb2151.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b516e21a714b7d4d066a8cd7b0774e3428ef9d8'}, {'y': 144, 'x': 216, 'u': 'https://preview.redd.it/6t1yx1vpb2151.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=89843709ab7ad5447d7ad25b631fd25a0a9027b3'}, {'y': 213, 'x': 320, 'u': 'https://preview.redd.it/6t1yx1vpb2151.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1ca599f492b75826804b766bc33678e8ea057c3'}], 's': {'y': 400, 'x': 600, 'u': 'https://preview.redd.it/6t1yx1vpb2151.png?width=600&amp;format=png&amp;auto=webp&amp;s=9f67a5f597dbdc46a5b8cea77eb1aa87c0415f4c'}, 'id': '6t1yx1vpb2151'}}",,,,
542,,pytorch,"This is my current model:

    class pcNet(nn.Module):

    def __init__(self, input_size, hidden_size, output_size):
    super(pcNet, self).__init__()
    self.fc1 = nn.Linear(input_size, hidden_size)
    self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
    out = F.leaky_relu(self.fc1(x))
    out = F.relu(self.fc2(out))
    return out

The input size is 1220, and I would like to change the model so in the first fc1 layer takes only the first 1024 of those and connects them to 32 nodes, which than get connected back with the other 196.

In the dataloader the two sets are actually separated witch these exact sizes, and I concatenate them, so if it's only possible by passing an extra argument that would work as well.",t2_6cgfmz1c,False,,0,False,"How do I alter this model, so it takes only part of the input value, and include the rest later?",[],r/pytorch,False,6,,0,,,False,t3_gqyrym,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1590534528.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is my current model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class pcNet(nn.Module):

def __init__(self, input_size, hidden_size, output_size):
super(pcNet, self).__init__()
self.fc1 = nn.Linear(input_size, hidden_size)
self.fc2 = nn.Linear(hidden_size, output_size)

def forward(self, x):
out = F.leaky_relu(self.fc1(x))
out = F.relu(self.fc2(out))
return out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The input size is 1220, and I would like to change the model so in the first fc1 layer takes only the first 1024 of those and connects them to 32 nodes, which than get connected back with the other 196.&lt;/p&gt;

&lt;p&gt;In the dataloader the two sets are actually separated witch these exact sizes, and I concatenate them, so if it&amp;#39;s only possible by passing an extra argument that would work as well.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gqyrym,True,,BewitchedHare,,1,True,all_ads,False,[],False,,/r/pytorch/comments/gqyrym/how_do_i_alter_this_model_so_it_takes_only_part/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gqyrym/how_do_i_alter_this_model_so_it_takes_only_part/,7135,1590505728.0,0,,False,,,,,,,,
543,,pytorch,"The documentation has a tutorial [(link)](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) on transfer learning with resnet18. But that one just involves changing the last layer to have as many units as required.  
   
    model_ft = models.resnet18(pretrained=True)
    num_ftrs = model_ft.fc.in_features   
    #Here the size of each output sample is set to 2.
    #Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).
    model_ft.fc = nn.Linear(num_ftrs, 2)   
 

Looking at the source code of models like [mobilenet](https://pytorch.org/docs/stable/_modules/torchvision/models/mobilenet.html#mobilenet_v2)   and [squeezenet](https://pytorch.org/docs/stable/_modules/torchvision/models/squeezenet.html#squeezenet1_1), I am not sure how to implement this in a straightforward manner",,False,,0,False,Transfer learning with pretrained torchvision models like mobilenet and squeezenet?,[],r/pytorch,False,6,,0,,,False,t3_gp0e7w,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,,self,False,,,{},self,,True,,1590245317.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The documentation has a tutorial &lt;a href=""https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html""&gt;(link)&lt;/a&gt; on transfer learning with resnet18. But that one just involves changing the last layer to have as many units as required.  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model_ft = models.resnet18(pretrained=True)
num_ftrs = model_ft.fc.in_features   
#Here the size of each output sample is set to 2.
#Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).
model_ft.fc = nn.Linear(num_ftrs, 2)   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looking at the source code of models like &lt;a href=""https://pytorch.org/docs/stable/_modules/torchvision/models/mobilenet.html#mobilenet_v2""&gt;mobilenet&lt;/a&gt;   and &lt;a href=""https://pytorch.org/docs/stable/_modules/torchvision/models/squeezenet.html#squeezenet1_1""&gt;squeezenet&lt;/a&gt;, I am not sure how to implement this in a straightforward manner&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/3VFMdWynqTSOoHxn13w1bVUSgKb3BYkeol9k1WzRsv4.jpg?auto=webp&amp;s=6eefc3e1bb0e1dbf439f159bce296d878cae81d0', 'width': 640, 'height': 480}, 'resolutions': [{'url': 'https://external-preview.redd.it/3VFMdWynqTSOoHxn13w1bVUSgKb3BYkeol9k1WzRsv4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c9f6b4d9aeb219d0f29e1aed9a2b4920d9ab990', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/3VFMdWynqTSOoHxn13w1bVUSgKb3BYkeol9k1WzRsv4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa6691f2e0cfcc5b91213dcbe0f86ae646c841ce', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/3VFMdWynqTSOoHxn13w1bVUSgKb3BYkeol9k1WzRsv4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=67a87d8753b4de88c91dfae6198a440ee9752111', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/3VFMdWynqTSOoHxn13w1bVUSgKb3BYkeol9k1WzRsv4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=716700f446840779292e61f5054c5f5dd37c94a7', 'width': 640, 'height': 480}], 'variants': {}, 'id': 'fqY7PDHqrDgI_IUYIfwzM0uCSXpvjmCO92dBbkkoPcg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gp0e7w,True,,[deleted],,4,True,all_ads,False,[],,dark,/r/pytorch/comments/gp0e7w/transfer_learning_with_pretrained_torchvision/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gp0e7w/transfer_learning_with_pretrained_torchvision/,7135,1590216517.0,0,,False,,,,,,,,
544,,pytorch,,t2_44mbtmjy,False,,0,False,Improving semantic segmentation for urban-scene images,[],r/pytorch,False,6,,0,46.0,,False,t3_goxmnx,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://a.thumbs.redditmedia.com/e4GTCQHKvQS-vIsfUix0L32PTZpe2rKqvResVdpUmR4.jpg,False,,[],{},link,,False,,1590232126.0,text,6,,,text,self.ObjectSegmentation,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/wvYhqth8OC0cZ15GbIYVz9mwpIwL0ODXMJWd79l5XsE.jpg?auto=webp&amp;s=f9512beed88360ffba48d21c01acf912afe5fd57', 'width': 1360, 'height': 456}, 'resolutions': [{'url': 'https://external-preview.redd.it/wvYhqth8OC0cZ15GbIYVz9mwpIwL0ODXMJWd79l5XsE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2aece80ee9f1c7d454fc4ef70b49e141428f718c', 'width': 108, 'height': 36}, {'url': 'https://external-preview.redd.it/wvYhqth8OC0cZ15GbIYVz9mwpIwL0ODXMJWd79l5XsE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e938098b038349cbbaaa4622cf2a86619e84ea41', 'width': 216, 'height': 72}, {'url': 'https://external-preview.redd.it/wvYhqth8OC0cZ15GbIYVz9mwpIwL0ODXMJWd79l5XsE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=733036124eb949ee203feb823fab63fb5077cca3', 'width': 320, 'height': 107}, {'url': 'https://external-preview.redd.it/wvYhqth8OC0cZ15GbIYVz9mwpIwL0ODXMJWd79l5XsE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a64a4a6a19dab4bde85867edef591d6bbe1d51d3', 'width': 640, 'height': 214}, {'url': 'https://external-preview.redd.it/wvYhqth8OC0cZ15GbIYVz9mwpIwL0ODXMJWd79l5XsE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ac6482b5812543520fffcc8c7607428ecca28027', 'width': 960, 'height': 321}, {'url': 'https://external-preview.redd.it/wvYhqth8OC0cZ15GbIYVz9mwpIwL0ODXMJWd79l5XsE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df9affb88aad4216a76657f7f2c8339dc8ff9582', 'width': 1080, 'height': 362}], 'variants': {}, 'id': 'T9mrYIlHK3q1t0vzU7awuXPYLqroVHimxLeUvZtDwyE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,goxmnx,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/goxmnx/improving_semantic_segmentation_for_urbanscene/,all_ads,False,/r/ObjectSegmentation/comments/goxlvq/improving_semantic_segmentation_for_urbanscene/,7135,1590203326.0,0,,False,/r/ObjectSegmentation/comments/goxlvq/improving_semantic_segmentation_for_urbanscene/,"[{'approved_at_utc': None, 'subreddit': 'ObjectSegmentation', 'selftext': 'For project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2003.05128)\n\n&amp;#x200B;\n\nhttps://reddit.com/link/goxlvq/video/tvz57eb4lf051/player\n\nMethod achieves a new state-of-the-art performance on the Cityscapes benchmark with a large margin among ResNet101 based segmentation models. Also, they show that the proposed model is coherent with the facts observed in the urban scene by visualizing and interpreting the attention map.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Improving semantic segmentation for urban-scene images', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ObjectSegmentation', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 46, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'tvz57eb4lf051': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/goxlvq/asset/tvz57eb4lf051/DASHPlaylist.mpd?a=1618044237%2CNzUxNWE0OWJjMWFjNTUwYmE2YzFlNDU4Nzc3YTM4OWE0MTRiNmI1NTJlNDZjNGYxOWUwZGZhZWQ3YWY5MjExNA%3D%3D&amp;v=1&amp;f=sd', 'x': 550, 'y': 480, 'hlsUrl': 'https://v.redd.it/link/goxlvq/asset/tvz57eb4lf051/HLSPlaylist.m3u8?a=1618044237%2CODgwMWJkMDM2OTFjNzZjYWMxZTEwNzNkYTgyNmU3OTZhNzlhOWJmZjBlYTI4NTdiNmYzZWYwYzU1OWQ3NmU1Nw%3D%3D&amp;v=1&amp;f=sd', 'id': 'tvz57eb4lf051', 'isGif': False}}, 'name': 't3_goxlvq', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 6, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/e4GTCQHKvQS-vIsfUix0L32PTZpe2rKqvResVdpUmR4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1590232033.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ObjectSegmentation', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2003.05128""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/goxlvq/video/tvz57eb4lf051/player""&gt;https://reddit.com/link/goxlvq/video/tvz57eb4lf051/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Method achieves a new state-of-the-art performance on the Cityscapes benchmark with a large margin among ResNet101 based segmentation models. Also, they show that the proposed model is coherent with the facts observed in the urban scene by visualizing and interpreting the attention map.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/wvYhqth8OC0cZ15GbIYVz9mwpIwL0ODXMJWd79l5XsE.jpg?auto=webp&amp;s=f9512beed88360ffba48d21c01acf912afe5fd57', 'width': 1360, 'height': 456}, 'resolutions': [{'url': 'https://external-preview.redd.it/wvYhqth8OC0cZ15GbIYVz9mwpIwL0ODXMJWd79l5XsE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2aece80ee9f1c7d454fc4ef70b49e141428f718c', 'width': 108, 'height': 36}, {'url': 'https://external-preview.redd.it/wvYhqth8OC0cZ15GbIYVz9mwpIwL0ODXMJWd79l5XsE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e938098b038349cbbaaa4622cf2a86619e84ea41', 'width': 216, 'height': 72}, {'url': 'https://external-preview.redd.it/wvYhqth8OC0cZ15GbIYVz9mwpIwL0ODXMJWd79l5XsE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=733036124eb949ee203feb823fab63fb5077cca3', 'width': 320, 'height': 107}, {'url': 'https://external-preview.redd.it/wvYhqth8OC0cZ15GbIYVz9mwpIwL0ODXMJWd79l5XsE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a64a4a6a19dab4bde85867edef591d6bbe1d51d3', 'width': 640, 'height': 214}, {'url': 'https://external-preview.redd.it/wvYhqth8OC0cZ15GbIYVz9mwpIwL0ODXMJWd79l5XsE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ac6482b5812543520fffcc8c7607428ecca28027', 'width': 960, 'height': 321}, {'url': 'https://external-preview.redd.it/wvYhqth8OC0cZ15GbIYVz9mwpIwL0ODXMJWd79l5XsE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df9affb88aad4216a76657f7f2c8339dc8ff9582', 'width': 1080, 'height': 362}], 'variants': {}, 'id': 'T9mrYIlHK3q1t0vzU7awuXPYLqroVHimxLeUvZtDwyE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2nxy3x', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'goxlvq', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ObjectSegmentation/comments/goxlvq/improving_semantic_segmentation_for_urbanscene/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ObjectSegmentation/comments/goxlvq/improving_semantic_segmentation_for_urbanscene/', 'subreddit_subscribers': 117, 'created_utc': 1590203233.0, 'num_crossposts': 13, 'media': None, 'is_video': False}]",t3_goxlvq,,,,,
545,,pytorch,"I have a multi-label classification problem, and so I’ve been using the Pytorch's [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss). I’d like to optimize my model for a higher F2 score, and so want to bias it to have greater recall (with decent precision too of course). Are there any guidelines for setting the values of `pos_weight` to be able to do this?",t2_2dgpjfe9,False,,0,False,Using pos_weight parameter in BCEWithLogitsLoss to improve recall in a multi-label problem,[],r/pytorch,False,6,,0,,,False,t3_goo6ol,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1590198740.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a multi-label classification problem, and so I’ve been using the Pytorch&amp;#39;s &lt;a href=""https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss""&gt;BCEWithLogitsLoss&lt;/a&gt;. I’d like to optimize my model for a higher F2 score, and so want to bias it to have greater recall (with decent precision too of course). Are there any guidelines for setting the values of &lt;code&gt;pos_weight&lt;/code&gt; to be able to do this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jYGjd9HNUsAXXNvMaRvzvi_5jJoBwaVOADjbz_QDVEU.jpg?auto=webp&amp;s=ed5ad7c35d75dbd9bca66c22aead098893e1b6b5', 'width': 640, 'height': 480}, 'resolutions': [{'url': 'https://external-preview.redd.it/jYGjd9HNUsAXXNvMaRvzvi_5jJoBwaVOADjbz_QDVEU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b04a6ebf2f01a6a2cc5dcdd50d21ea570a7ca70c', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/jYGjd9HNUsAXXNvMaRvzvi_5jJoBwaVOADjbz_QDVEU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=87e6025523a4ed73f236c7fd22afc37f1b260e3f', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/jYGjd9HNUsAXXNvMaRvzvi_5jJoBwaVOADjbz_QDVEU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a9ee111b029dd387a95260262c71d42c22783adf', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/jYGjd9HNUsAXXNvMaRvzvi_5jJoBwaVOADjbz_QDVEU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60dd0ab3df5660b239f9a64b77c7d5874965ffeb', 'width': 640, 'height': 480}], 'variants': {}, 'id': 'JyYWpgJb1xoJMe3whGKaTZ6fqF7Fcnm9nxsRfDcNzOU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,goo6ol,True,,dudester_el,,1,True,all_ads,False,[],False,,/r/pytorch/comments/goo6ol/using_pos_weight_parameter_in_bcewithlogitsloss/,all_ads,False,https://www.reddit.com/r/pytorch/comments/goo6ol/using_pos_weight_parameter_in_bcewithlogitsloss/,7135,1590169940.0,0,,False,,,,,,,,
546,,pytorch,"I have a multi-label classification problem, and so I’ve been using the Pytorch's [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss). I’d like to optimize my model for a higher F2 score, and so want to bias it to have greater recall (with decent precision too of course). Are there any guidelines for setting the values of `pos_weight` to be able to do this?",t2_2dgpjfe9,False,,0,False,Using pos_weight parameter in BCEWithLogitsLoss to improve recall in a multi-class multi-label problem,[],r/pytorch,False,6,,0,,,False,t3_goo6l4,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1590198732.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a multi-label classification problem, and so I’ve been using the Pytorch&amp;#39;s &lt;a href=""https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss""&gt;BCEWithLogitsLoss&lt;/a&gt;. I’d like to optimize my model for a higher F2 score, and so want to bias it to have greater recall (with decent precision too of course). Are there any guidelines for setting the values of &lt;code&gt;pos_weight&lt;/code&gt; to be able to do this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,goo6l4,True,,dudester_el,,3,True,all_ads,False,[],False,,/r/pytorch/comments/goo6l4/using_pos_weight_parameter_in_bcewithlogitsloss/,all_ads,False,https://www.reddit.com/r/pytorch/comments/goo6l4/using_pos_weight_parameter_in_bcewithlogitsloss/,7135,1590169932.0,0,,False,,,,,,,,
547,,pytorch,,t2_44mbtmjy,False,,0,False,BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation: Paper and Code,[],r/pytorch,False,6,,0,76.0,,False,t3_gnnzmb,False,dark,1.0,,public,6,0,{},140.0,,False,[],,False,False,,{},,False,6,,False,https://b.thumbs.redditmedia.com/3lI5XfGJJckPb_huHVxoZOD6iu7wNxTBjE5WzGaynVU.jpg,False,,[],{},,,False,,1590054946.0,text,6,,,text,self.ObjectSegmentation,False,,,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gnnzmb,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gnnzmb/blendmask_topdown_meets_bottomup_for_instance/,all_ads,False,/r/ObjectSegmentation/comments/gnnt8v/blendmask_topdown_meets_bottomup_for_instance/,7135,1590026146.0,0,,False,/r/ObjectSegmentation/comments/gnnt8v/blendmask_topdown_meets_bottomup_for_instance/,"[{'approved_at_utc': None, 'subreddit': 'ObjectSegmentation', 'selftext': 'For project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2001.00309)\n\nBlog: [https://medium.com/@vlatamas/blendmask-a-neural-network-bypasses-state-of-the-art-on-object-segmentation-task-b3092379c068](https://medium.com/@vlatamas/blendmask-a-neural-network-bypasses-state-of-the-art-on-object-segmentation-task-b3092379c068)  \n\n\nhttps://preview.redd.it/ar3kuzvlw0051.png?width=1404&amp;format=png&amp;auto=webp&amp;s=d1c22d80c2e327d874fb4b0af7a80ea9ecb86bda\n\n&amp;#x200B;', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation: Paper and Code', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ObjectSegmentation', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 76, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'ar3kuzvlw0051': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 58, 'x': 108, 'u': 'https://preview.redd.it/ar3kuzvlw0051.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c307a15107ff9c809e41ec3121ec91ce4782e20d'}, {'y': 117, 'x': 216, 'u': 'https://preview.redd.it/ar3kuzvlw0051.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7593a799eb459941d8ec869ca5a13f62c658994d'}, {'y': 174, 'x': 320, 'u': 'https://preview.redd.it/ar3kuzvlw0051.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bad5595e615537033e023ea2178607084921360a'}, {'y': 349, 'x': 640, 'u': 'https://preview.redd.it/ar3kuzvlw0051.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f20bb7a5e698f9363e642be71953f52f96f6ad5a'}, {'y': 523, 'x': 960, 'u': 'https://preview.redd.it/ar3kuzvlw0051.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=592d7377f70f3b2d0d81aec18b6a66d73565bea4'}, {'y': 589, 'x': 1080, 'u': 'https://preview.redd.it/ar3kuzvlw0051.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6cbce9b7fc2cd71fb456fb429a86421db5b08580'}], 's': {'y': 766, 'x': 1404, 'u': 'https://preview.redd.it/ar3kuzvlw0051.png?width=1404&amp;format=png&amp;auto=webp&amp;s=d1c22d80c2e327d874fb4b0af7a80ea9ecb86bda'}, 'id': 'ar3kuzvlw0051'}}, 'name': 't3_gnnt8v', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 5, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/3lI5XfGJJckPb_huHVxoZOD6iu7wNxTBjE5WzGaynVU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1590054252.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ObjectSegmentation', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2001.00309""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Blog: &lt;a href=""https://medium.com/@vlatamas/blendmask-a-neural-network-bypasses-state-of-the-art-on-object-segmentation-task-b3092379c068""&gt;https://medium.com/@vlatamas/blendmask-a-neural-network-bypasses-state-of-the-art-on-object-segmentation-task-b3092379c068&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/ar3kuzvlw0051.png?width=1404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1c22d80c2e327d874fb4b0af7a80ea9ecb86bda""&gt;https://preview.redd.it/ar3kuzvlw0051.png?width=1404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1c22d80c2e327d874fb4b0af7a80ea9ecb86bda&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2nxy3x', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gnnt8v', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ObjectSegmentation/comments/gnnt8v/blendmask_topdown_meets_bottomup_for_instance/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ObjectSegmentation/comments/gnnt8v/blendmask_topdown_meets_bottomup_for_instance/', 'subreddit_subscribers': 117, 'created_utc': 1590025452.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_gnnt8v,,,,,
548,,pytorch,,t2_44mbtmjy,False,,0,False,Semantic Segmentation from Image Labels,[],r/pytorch,False,6,,0,140.0,,False,t3_gnnupz,False,dark,1.0,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/bq9Vm6qFbZLeKbO-bqk7QzDYrMsEgpj2X24FcGPRbYY.jpg,False,,[],{},,,False,,1590054411.0,text,6,,,text,self.ObjectSegmentation,False,,,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gnnupz,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gnnupz/semantic_segmentation_from_image_labels/,all_ads,False,/r/ObjectSegmentation/comments/gnnqud/semantic_segmentation_from_image_labels/,7135,1590025611.0,0,,False,/r/ObjectSegmentation/comments/gnnqud/semantic_segmentation_from_image_labels/,"[{'approved_at_utc': None, 'subreddit': 'ObjectSegmentation', 'selftext': 'Semantic Segmentation from Image Labels\n\nFor project and code or API request:\xa0[click here](https://www.catalyzex.com/paper/arxiv:2005.08104)\n\n&amp;#x200B;\n\nhttps://i.redd.it/qbbp1iqrv0051.gif\n\nThey develop a segmentation-based network model and a self-supervised training scheme to train for semantic masks from image-level annotations in a single-stage', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Semantic Segmentation from Image Labels', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ObjectSegmentation', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'qbbp1iqrv0051': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 108, 'x': 108, 'u': 'https://preview.redd.it/qbbp1iqrv0051.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=0df80b811a147a9294ce858b8f3f3941c6dd3c63'}, {'y': 216, 'x': 216, 'u': 'https://preview.redd.it/qbbp1iqrv0051.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=2f681047685d85b0a1cd64723f4f721497af97fc'}, {'y': 320, 'x': 320, 'u': 'https://preview.redd.it/qbbp1iqrv0051.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=d9dc50eb09965be13d91fe10e3b5f4b02a6f7642'}], 's': {'y': 512, 'gif': 'https://i.redd.it/qbbp1iqrv0051.gif', 'mp4': 'https://preview.redd.it/qbbp1iqrv0051.gif?format=mp4&amp;s=20deaf52edc456835232f043c196cab80f088323', 'x': 512}, 'id': 'qbbp1iqrv0051'}}, 'name': 't3_gnnqud', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 8, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/bq9Vm6qFbZLeKbO-bqk7QzDYrMsEgpj2X24FcGPRbYY.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1590054009.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ObjectSegmentation', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Semantic Segmentation from Image Labels&lt;/p&gt;\n\n&lt;p&gt;For project and code or API request:\xa0&lt;a href=""https://www.catalyzex.com/paper/arxiv:2005.08104""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://i.redd.it/qbbp1iqrv0051.gif""&gt;https://i.redd.it/qbbp1iqrv0051.gif&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They develop a segmentation-based network model and a self-supervised training scheme to train for semantic masks from image-level annotations in a single-stage&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2nxy3x', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gnnqud', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ObjectSegmentation/comments/gnnqud/semantic_segmentation_from_image_labels/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ObjectSegmentation/comments/gnnqud/semantic_segmentation_from_image_labels/', 'subreddit_subscribers': 117, 'created_utc': 1590025209.0, 'num_crossposts': 13, 'media': None, 'is_video': False}]",t3_gnnqud,,,,,
549,,pytorch,"I'm using the stack() built-in function to concatenate outputs from different branches from the neural network. My question is when I do the back-propagation step, will the gradients propagate along the specific branch the output came from? or does it propagate throughout the entire neural network? 

&amp;#x200B;

Thank you",t2_h887db1,False,,0,False,Does stack() preserve the computation graph of each element,[],r/pytorch,False,6,,0,,,False,t3_gnf83q,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1590023017.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m using the stack() built-in function to concatenate outputs from different branches from the neural network. My question is when I do the back-propagation step, will the gradients propagate along the specific branch the output came from? or does it propagate throughout the entire neural network? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gnf83q,True,,Nakhleh,,2,True,all_ads,False,[],False,,/r/pytorch/comments/gnf83q/does_stack_preserve_the_computation_graph_of_each/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gnf83q/does_stack_preserve_the_computation_graph_of_each/,7135,1589994217.0,0,,False,,,,,,,,
550,,pytorch," 

I have problem with long training time for YOLOv3 using AdderNet as backbone.

One epoch for VOC2007 dataset could take at least 5 hours.

The context could be found at [https://github.com/huawei-noah/AdderNet/issues/16#issuecomment-627446321](https://github.com/huawei-noah/AdderNet/issues/16#issuecomment-627446321)",t2_bpftl,False,,0,False,Modify YOLOv3 backbone from DarkNet to AdderNet,[],r/pytorch,False,6,,0,,,False,t3_gn891x,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1589995677.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have problem with long training time for YOLOv3 using AdderNet as backbone.&lt;/p&gt;

&lt;p&gt;One epoch for VOC2007 dataset could take at least 5 hours.&lt;/p&gt;

&lt;p&gt;The context could be found at &lt;a href=""https://github.com/huawei-noah/AdderNet/issues/16#issuecomment-627446321""&gt;https://github.com/huawei-noah/AdderNet/issues/16#issuecomment-627446321&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?auto=webp&amp;s=b134e75c125f5c05086645695c0f7dd3390375e9', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0220f379ccc6f8cf5817f79238538ba2ffe05c70', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70cc6291695445faa1333d0a07c41b31fd9dbfcb', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=45992e6fd013e395412db216c83ded62bc740c38', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'bAKE-mgRYAKYVYhfiJp-sr0yWP8hFmq8xaav2zgIqNI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gn891x,True,,promach,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gn891x/modify_yolov3_backbone_from_darknet_to_addernet/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gn891x/modify_yolov3_backbone_from_darknet_to_addernet/,7135,1589966877.0,3,,False,,,,,,,,
551,,pytorch,"I'm using the torch.multiprocessing module, and using the model as  an argument to the multiprocessing workers. I've called network.share_memory() on the module, yet it still appears to make multiple copies of the model when the workers spin up (according to nvidia-smi).

Is there anything special I need to do to stop the model being copied over multiple times?

Any help, or explanations why I'm terribly misguided are welcome.",t2_3z2dt,False,,0,False,Is there anything special you need to do to have models use shared CUDA memory in seperate processes (apart of model.share_memory()).,[],r/pytorch,False,6,,0,,,False,t3_gmlrm1,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1589911744.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m using the torch.multiprocessing module, and using the model as  an argument to the multiprocessing workers. I&amp;#39;ve called network.share_memory() on the module, yet it still appears to make multiple copies of the model when the workers spin up (according to nvidia-smi).&lt;/p&gt;

&lt;p&gt;Is there anything special I need to do to stop the model being copied over multiple times?&lt;/p&gt;

&lt;p&gt;Any help, or explanations why I&amp;#39;m terribly misguided are welcome.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gmlrm1,True,,qazadex,,2,True,all_ads,False,[],False,,/r/pytorch/comments/gmlrm1/is_there_anything_special_you_need_to_do_to_have/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gmlrm1/is_there_anything_special_you_need_to_do_to_have/,7135,1589882944.0,0,,False,,,,,,,,
552,,pytorch,,t2_44mbtmjy,False,,0,False,Separate a target speaker's speech from a mixture of two speakers,[],r/pytorch,False,6,,0,65.0,,False,t3_gmbpgu,False,dark,1.0,,public,6,0,{},140.0,,False,[],,False,False,,{},,False,6,,False,https://b.thumbs.redditmedia.com/pL5UkpxQjfyk0QXKhn96nvuxlitYPjmtQiklAdUqPDI.jpg,False,,[],{},link,,False,,1589869371.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/G9_pRWsC_ii4iPj5nZfn0GETSToY6jzEkB2tPQfMHYM.jpg?auto=webp&amp;s=a0886460a75f9082f93b2667ee219ebde74ca35c', 'width': 1278, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/G9_pRWsC_ii4iPj5nZfn0GETSToY6jzEkB2tPQfMHYM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c5df4ddb4a4eef1ac96bb48108f8e6bb4cd11d8d', 'width': 108, 'height': 50}, {'url': 'https://external-preview.redd.it/G9_pRWsC_ii4iPj5nZfn0GETSToY6jzEkB2tPQfMHYM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4093b550b43bd0ddc1d2cc4bc8f681208448220', 'width': 216, 'height': 101}, {'url': 'https://external-preview.redd.it/G9_pRWsC_ii4iPj5nZfn0GETSToY6jzEkB2tPQfMHYM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=24957715099dcc5dcf63ed6654da912f4ae74236', 'width': 320, 'height': 150}, {'url': 'https://external-preview.redd.it/G9_pRWsC_ii4iPj5nZfn0GETSToY6jzEkB2tPQfMHYM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7b5541ae3b68ed99ceefcc8cad85abd200066ed4', 'width': 640, 'height': 300}, {'url': 'https://external-preview.redd.it/G9_pRWsC_ii4iPj5nZfn0GETSToY6jzEkB2tPQfMHYM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=487d29bb35c9bea7801e9c114b2fad58373a7468', 'width': 960, 'height': 450}, {'url': 'https://external-preview.redd.it/G9_pRWsC_ii4iPj5nZfn0GETSToY6jzEkB2tPQfMHYM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb200af61247f18718532967f7e7bb9156bc72ea', 'width': 1080, 'height': 507}], 'variants': {}, 'id': 'T3XLUvFtYR7bmUNQoHzJc7A5Q2CHYUsF6AH0EQ68OU4'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gmbpgu,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gmbpgu/separate_a_target_speakers_speech_from_a_mixture/,all_ads,False,/r/LatestInML/comments/gmbny4/separate_a_target_speakers_speech_from_a_mixture/,7135,1589840571.0,0,,False,/r/LatestInML/comments/gmbny4/separate_a_target_speakers_speech_from_a_mixture/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""Separate a target speaker's speech from a mixture of two speakers\n\nFor project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2005.07074)\n\nhttps://reddit.com/link/gmbny4/video/q33qaynbmlz41/player\n\n(FaceFilter: Audio-visual speech separation using still images)\n\nDone using a deep audio-visual speech separation network. Unlike previous works that used lip movement on video clips or pre-enrolled speaker information as an auxiliary conditional feature, we use a single face image of the target speaker"", 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""Separate a target speaker's speech from a mixture of two speakers"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 65, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'q33qaynbmlz41': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/gmbny4/asset/q33qaynbmlz41/DASHPlaylist.mpd?a=1618044245%2CN2JlYTM0ZGNmM2Y5YjVhOTIxNjc2ZTc4YjY3ZDRkMTAxNjM0YTc1ZmJlZDc0YWFjZDI1MTdjYmMyNTM0ZDllNg%3D%3D&amp;v=1&amp;f=sd', 'x': 640, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/gmbny4/asset/q33qaynbmlz41/HLSPlaylist.m3u8?a=1618044245%2CYmUyOTFkNTRhZDBjNzQ2YTk3MTg4NDc5NWQ0NmE0MGQ2MDJkMDFiNjk4YjRkNDUwMTMzMzYzNWU2YzgxN2ZlNg%3D%3D&amp;v=1&amp;f=sd', 'id': 'q33qaynbmlz41', 'isGif': False}}, 'name': 't3_gmbny4', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.79, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 13, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/pL5UkpxQjfyk0QXKhn96nvuxlitYPjmtQiklAdUqPDI.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589869240.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Separate a target speaker&amp;#39;s speech from a mixture of two speakers&lt;/p&gt;\n\n&lt;p&gt;For project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2005.07074""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/gmbny4/video/q33qaynbmlz41/player""&gt;https://reddit.com/link/gmbny4/video/q33qaynbmlz41/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;(FaceFilter: Audio-visual speech separation using still images)&lt;/p&gt;\n\n&lt;p&gt;Done using a deep audio-visual speech separation network. Unlike previous works that used lip movement on video clips or pre-enrolled speaker information as an auxiliary conditional feature, we use a single face image of the target speaker&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/G9_pRWsC_ii4iPj5nZfn0GETSToY6jzEkB2tPQfMHYM.jpg?auto=webp&amp;s=a0886460a75f9082f93b2667ee219ebde74ca35c', 'width': 1278, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/G9_pRWsC_ii4iPj5nZfn0GETSToY6jzEkB2tPQfMHYM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c5df4ddb4a4eef1ac96bb48108f8e6bb4cd11d8d', 'width': 108, 'height': 50}, {'url': 'https://external-preview.redd.it/G9_pRWsC_ii4iPj5nZfn0GETSToY6jzEkB2tPQfMHYM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4093b550b43bd0ddc1d2cc4bc8f681208448220', 'width': 216, 'height': 101}, {'url': 'https://external-preview.redd.it/G9_pRWsC_ii4iPj5nZfn0GETSToY6jzEkB2tPQfMHYM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=24957715099dcc5dcf63ed6654da912f4ae74236', 'width': 320, 'height': 150}, {'url': 'https://external-preview.redd.it/G9_pRWsC_ii4iPj5nZfn0GETSToY6jzEkB2tPQfMHYM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7b5541ae3b68ed99ceefcc8cad85abd200066ed4', 'width': 640, 'height': 300}, {'url': 'https://external-preview.redd.it/G9_pRWsC_ii4iPj5nZfn0GETSToY6jzEkB2tPQfMHYM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=487d29bb35c9bea7801e9c114b2fad58373a7468', 'width': 960, 'height': 450}, {'url': 'https://external-preview.redd.it/G9_pRWsC_ii4iPj5nZfn0GETSToY6jzEkB2tPQfMHYM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb200af61247f18718532967f7e7bb9156bc72ea', 'width': 1080, 'height': 507}], 'variants': {}, 'id': 'T3XLUvFtYR7bmUNQoHzJc7A5Q2CHYUsF6AH0EQ68OU4'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gmbny4', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gmbny4/separate_a_target_speakers_speech_from_a_mixture/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gmbny4/separate_a_target_speakers_speech_from_a_mixture/', 'subreddit_subscribers': 6676, 'created_utc': 1589840440.0, 'num_crossposts': 15, 'media': None, 'is_video': False}]",t3_gmbny4,,,,,
553,,pytorch,"Hi, I want to train a big dataset with 1M images. I have a RTX2060 with 6Gbs of VRAM.  

I am using a pretrained Alexnet with some extra layers and once I upload my model to my GPU It uses approximately 1Gb from it leaving 4.4 Gbs free. 

&amp;#x200B;

Then I try to train my images but my model crashes at the first batch when updating the weights of the network due to lack of memory in my GPU. I tried even with batch\_size = 1. 

Am I doing something wrong or can I do something to fix this issue? 

Am I doing something wrong or simply my hardware isn't enough for such a problem?

How can I solve this issue?  

[ss](https://preview.redd.it/4mu06gijmlz41.png?width=872&amp;format=png&amp;auto=webp&amp;s=79ce9f9eb0883f509c5f658538cddab4f90c9731)",t2_21t2bpv,False,,0,False,Need guidance with GPU training,[],r/pytorch,False,6,,0,71.0,,False,t3_gmbr7b,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://a.thumbs.redditmedia.com/Tpmdbumqun-ALsMcZFr7NPRbLU_DnGy4Om8n_5d9KP0.jpg,False,,[],{},,,True,,1589869531.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I want to train a big dataset with 1M images. I have a RTX2060 with 6Gbs of VRAM.  &lt;/p&gt;

&lt;p&gt;I am using a pretrained Alexnet with some extra layers and once I upload my model to my GPU It uses approximately 1Gb from it leaving 4.4 Gbs free. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Then I try to train my images but my model crashes at the first batch when updating the weights of the network due to lack of memory in my GPU. I tried even with batch_size = 1. &lt;/p&gt;

&lt;p&gt;Am I doing something wrong or can I do something to fix this issue? &lt;/p&gt;

&lt;p&gt;Am I doing something wrong or simply my hardware isn&amp;#39;t enough for such a problem?&lt;/p&gt;

&lt;p&gt;How can I solve this issue?  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/4mu06gijmlz41.png?width=872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79ce9f9eb0883f509c5f658538cddab4f90c9731""&gt;ss&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gmbr7b,True,,peterlaanguila8,,5,True,all_ads,False,[],False,,/r/pytorch/comments/gmbr7b/need_guidance_with_gpu_training/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gmbr7b/need_guidance_with_gpu_training/,7135,1589840731.0,0,,False,,,,"{'4mu06gijmlz41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 55, 'x': 108, 'u': 'https://preview.redd.it/4mu06gijmlz41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=97eeb9207ef3b344b6e8d204b47730c09fadefbb'}, {'y': 110, 'x': 216, 'u': 'https://preview.redd.it/4mu06gijmlz41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=52d40391c204dd2bbe388e02bc7e524fbd5ae907'}, {'y': 164, 'x': 320, 'u': 'https://preview.redd.it/4mu06gijmlz41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c30d480a896d0e51b271671f957955d71f2bd4c3'}, {'y': 328, 'x': 640, 'u': 'https://preview.redd.it/4mu06gijmlz41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3354f9f7af87bdbdfb125234d8e50831b47c0db3'}], 's': {'y': 448, 'x': 872, 'u': 'https://preview.redd.it/4mu06gijmlz41.png?width=872&amp;format=png&amp;auto=webp&amp;s=79ce9f9eb0883f509c5f658538cddab4f90c9731'}, 'id': '4mu06gijmlz41'}}",,,,
554,,pytorch,"hi everyone, 
I am new to machine learning, I have very little idea about how things work and I'm supposed to be doing my final year project which will be related to text recognition, can someone here guide on where to start because my supervisor has recommended me to use PyTorch, I'm good with programming concepts such as OOP and I've completed a course on python which has given me a decent idea of the language.
I need to start from level 0, there is online content available but I'm not sure which to go for, I'd appreciate if someone could suggest me something that'll help me get a head start with PyTorch.",t2_spq3o,False,,0,False,Need guidance learning PyTorch,[],r/pytorch,False,6,,0,,,False,t3_glz5xk,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1589827417.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;hi everyone, 
I am new to machine learning, I have very little idea about how things work and I&amp;#39;m supposed to be doing my final year project which will be related to text recognition, can someone here guide on where to start because my supervisor has recommended me to use PyTorch, I&amp;#39;m good with programming concepts such as OOP and I&amp;#39;ve completed a course on python which has given me a decent idea of the language.
I need to start from level 0, there is online content available but I&amp;#39;m not sure which to go for, I&amp;#39;d appreciate if someone could suggest me something that&amp;#39;ll help me get a head start with PyTorch.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,glz5xk,True,,X3NOM,,12,True,all_ads,False,[],False,,/r/pytorch/comments/glz5xk/need_guidance_learning_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/glz5xk/need_guidance_learning_pytorch/,7135,1589798617.0,0,,False,,,,,,,,
555,,pytorch,I have a tensor on which i want to apply some transformation on each entries (or sub tensors). Looping over the entries is not a good solution because it will not be paralellizable. Is there a way to do this in pytorch?,t2_e4n3l,False,,0,False,How to apply a function to sub tensors of a tensor,[],r/pytorch,False,6,,0,,,False,t3_glmn1v,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1589775899.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a tensor on which i want to apply some transformation on each entries (or sub tensors). Looping over the entries is not a good solution because it will not be paralellizable. Is there a way to do this in pytorch?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,glmn1v,True,,Infinite_Explosion,,4,True,all_ads,False,[],False,,/r/pytorch/comments/glmn1v/how_to_apply_a_function_to_sub_tensors_of_a_tensor/,all_ads,False,https://www.reddit.com/r/pytorch/comments/glmn1v/how_to_apply_a_function_to_sub_tensors_of_a_tensor/,7135,1589747099.0,0,,False,,,,,,,,
556,,pytorch,"[gist of the code](https://gist.github.com/aditya-hari/3921f12ac34a0e9ea41d4e27fdef8aaa)  
 

There's a lot of trash there, I apologize for that. The relevant pieces of code is that of the model and the training loop.  
  
I don't know what is going wrong, the dimensions of everything seems to be fine, there's some other logical thing that I am messing up. I've tried different learning rates, doesn't help.   
 
     
The model -    

    class RNN(nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(RNN,self).__init__()
            
            self.hidden_size = hidden_size
            self.i2h = nn.Linear(input_size+hidden_size, hidden_size)
            self.i2o = nn.Linear(input_size+hidden_size, output_size)
            self.softmax = nn.LogSoftmax(dim = 1)
            
        def forward(self, x, hidden):
            combined = torch.cat((x, hidden), 1).to(device)
            hidden = self.i2h(combined)
            output = self.i2o(combined)
            output = self.softmax(output)
            return output, hidden
        
        def init_hidden(self):
            return torch.zeros(1, self.hidden_size)
        
    n_hidden = 128
    rnn = RNN(n_letters, n_hidden, n_cats)
    criterion = nn.NLLLoss()
    optimizer = optim.SGD(rnn.parameters(), lr = .001)
    print(rnn)        
         

 
  
The training loop    
 
    def train(model, optimizer, epochs):
        start = time.time()
    
        for epoch in range(epochs):
            running_loss = 0.0
            running_correct = 0
            
            e_start = time.time()
            print(f'Epoch - {epoch+1}/{epochs}')
            print('-'*20)    
            
            
            for data in train_data:
                inputs, labels = data
                hidden = model.init_hidden()
    
                optimizer.zero_grad()
                
                for i in range(inputs.size()[0]):
                    letter = inputs[i][0].unsqueeze(0)
                    output, hidden = model(letter, hidden)
                    
                _, pred = torch.max(output, dim = 1)
                loss = criterion(output, labels)
    
                loss.backward()
                optimizer.step()
    
                running_loss += loss.item()
                running_correct += torch.sum(pred == labels.data)
    
            epoch_loss = running_loss/len(train_data)
            acc = (running_correct.double()/len(train_data))*100
            e_end = time.time()-e_start
            print(f'loss - {epoch_loss:3.2f} acc - {acc:3.2f} , Time - {e_end:.2f}')
            
        end = time.time() - start
        print(f'Training time - {end:.2f}')
        return model   
 
  
The training data is in the form of [input_word, label].  
The dimensions for the input word are [seq_length, batch_size (==1), num_of_chars], so a one hot vector for each character    
  
The gist has examples of everything outputted in case something isn't clear.",,False,,0,False,"RNN thing that I've tried to make based on the PyTorch tutorial, using linear layer. Loss doesn't decrease at all.",[],r/pytorch,False,6,,0,,,False,t3_gkz1ac,False,dark,0.67,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,,self,False,,,{},,,True,,1589680572.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://gist.github.com/aditya-hari/3921f12ac34a0e9ea41d4e27fdef8aaa""&gt;gist of the code&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;There&amp;#39;s a lot of trash there, I apologize for that. The relevant pieces of code is that of the model and the training loop.  &lt;/p&gt;

&lt;p&gt;I don&amp;#39;t know what is going wrong, the dimensions of everything seems to be fine, there&amp;#39;s some other logical thing that I am messing up. I&amp;#39;ve tried different learning rates, doesn&amp;#39;t help.   &lt;/p&gt;

&lt;p&gt;The model -    &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN,self).__init__()

        self.hidden_size = hidden_size
        self.i2h = nn.Linear(input_size+hidden_size, hidden_size)
        self.i2o = nn.Linear(input_size+hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim = 1)

    def forward(self, x, hidden):
        combined = torch.cat((x, hidden), 1).to(device)
        hidden = self.i2h(combined)
        output = self.i2o(combined)
        output = self.softmax(output)
        return output, hidden

    def init_hidden(self):
        return torch.zeros(1, self.hidden_size)

n_hidden = 128
rnn = RNN(n_letters, n_hidden, n_cats)
criterion = nn.NLLLoss()
optimizer = optim.SGD(rnn.parameters(), lr = .001)
print(rnn)        
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The training loop    &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def train(model, optimizer, epochs):
    start = time.time()

    for epoch in range(epochs):
        running_loss = 0.0
        running_correct = 0

        e_start = time.time()
        print(f&amp;#39;Epoch - {epoch+1}/{epochs}&amp;#39;)
        print(&amp;#39;-&amp;#39;*20)    


        for data in train_data:
            inputs, labels = data
            hidden = model.init_hidden()

            optimizer.zero_grad()

            for i in range(inputs.size()[0]):
                letter = inputs[i][0].unsqueeze(0)
                output, hidden = model(letter, hidden)

            _, pred = torch.max(output, dim = 1)
            loss = criterion(output, labels)

            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            running_correct += torch.sum(pred == labels.data)

        epoch_loss = running_loss/len(train_data)
        acc = (running_correct.double()/len(train_data))*100
        e_end = time.time()-e_start
        print(f&amp;#39;loss - {epoch_loss:3.2f} acc - {acc:3.2f} , Time - {e_end:.2f}&amp;#39;)

    end = time.time() - start
    print(f&amp;#39;Training time - {end:.2f}&amp;#39;)
    return model   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The training data is in the form of [input_word, label].&lt;br/&gt;
The dimensions for the input word are [seq_length, batch_size (==1), num_of_chars], so a one hot vector for each character    &lt;/p&gt;

&lt;p&gt;The gist has examples of everything outputted in case something isn&amp;#39;t clear.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gkz1ac,True,,[deleted],,7,True,all_ads,False,[],,dark,/r/pytorch/comments/gkz1ac/rnn_thing_that_ive_tried_to_make_based_on_the/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gkz1ac/rnn_thing_that_ive_tried_to_make_based_on_the/,7135,1589651772.0,0,,False,,,,,,,,
557,,pytorch,,t2_44mbtmjy,False,,0,False,"LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands and Water from Aerial Imagery",[],r/pytorch,False,6,,0,129.0,,False,t3_gk0mn6,False,dark,1.0,,public,10,0,{},140.0,,False,[],,False,False,,{},,False,10,,False,https://a.thumbs.redditmedia.com/ob_g6nSM-jMns_5Q0zdDNg2QVPEh1qaS68eZuzlyec4.jpg,False,,[],{},link,,False,,1589538275.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/IsN5Rc3MGbiVt65El1N1-O8GQW16X8Zsv5KlEmvIhJI.jpg?auto=webp&amp;s=b299da90818ec62cc416cdf65eb39d800333068d', 'width': 474, 'height': 440}, 'resolutions': [{'url': 'https://external-preview.redd.it/IsN5Rc3MGbiVt65El1N1-O8GQW16X8Zsv5KlEmvIhJI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f78328eb1a1ce50bedfa0f6a3c33f968133b25d2', 'width': 108, 'height': 100}, {'url': 'https://external-preview.redd.it/IsN5Rc3MGbiVt65El1N1-O8GQW16X8Zsv5KlEmvIhJI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c7ddecade3501e7f39982a28f732c6359226bdc', 'width': 216, 'height': 200}, {'url': 'https://external-preview.redd.it/IsN5Rc3MGbiVt65El1N1-O8GQW16X8Zsv5KlEmvIhJI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6fb311b2be8185f09905249c5fa3607e9de5d22', 'width': 320, 'height': 297}], 'variants': {}, 'id': 'sTQHm2b57mxiq5m4bzOsFJygtYyaskWQZ5R4EM_MrBg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gk0mn6,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gk0mn6/landcoverai_dataset_for_automatic_mapping_of/,all_ads,False,/r/LatestInML/comments/gk0lbu/landcoverai_dataset_for_automatic_mapping_of/,7135,1589509475.0,0,,False,/r/LatestInML/comments/gk0lbu/landcoverai_dataset_for_automatic_mapping_of/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and dataset: [click here](https://www.catalyzex.com/paper/arxiv:2005.02264?fbclid=IwAR3QYrR6reIn0psKtqmc-Pn5mGvq7WiqXMdsq0otT2nwKNkuGSOkknlx4qo)\n\nhttps://preview.redd.it/riiozbjs9uy41.png?width=754&amp;format=png&amp;auto=webp&amp;s=1f3b9edde23cd7dfbbcb6ea4e479c79ca2937028\n\nThey collected images of 216.27 sq. km lands across Poland, a country in Central Europe, 39.51 sq. km with resolution 50 cm per pixel and 176.76 sq. km with resolution 25 cm per pixel and manually fine annotated three following classes of objects: buildings, woodlands, and water.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands and Water from Aerial Imagery', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 129, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'riiozbjs9uy41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 108, 'x': 108, 'u': 'https://preview.redd.it/riiozbjs9uy41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff6b86a5f4a247131a56c1ec9e9c171e3bc84b39'}, {'y': 217, 'x': 216, 'u': 'https://preview.redd.it/riiozbjs9uy41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f36d8368c18f8c04a7ed037385d0e771dd82b395'}, {'y': 321, 'x': 320, 'u': 'https://preview.redd.it/riiozbjs9uy41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b1d64796f4c3b34371535f6cdd5d5d1a9def2b5c'}, {'y': 643, 'x': 640, 'u': 'https://preview.redd.it/riiozbjs9uy41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3babe174ee91d417c89b87bf4144ce47eef246ca'}], 's': {'y': 758, 'x': 754, 'u': 'https://preview.redd.it/riiozbjs9uy41.png?width=754&amp;format=png&amp;auto=webp&amp;s=1f3b9edde23cd7dfbbcb6ea4e479c79ca2937028'}, 'id': 'riiozbjs9uy41'}}, 'name': 't3_gk0lbu', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 34, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 34, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/ob_g6nSM-jMns_5Q0zdDNg2QVPEh1qaS68eZuzlyec4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589538132.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and dataset: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2005.02264?fbclid=IwAR3QYrR6reIn0psKtqmc-Pn5mGvq7WiqXMdsq0otT2nwKNkuGSOkknlx4qo""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/riiozbjs9uy41.png?width=754&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f3b9edde23cd7dfbbcb6ea4e479c79ca2937028""&gt;https://preview.redd.it/riiozbjs9uy41.png?width=754&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f3b9edde23cd7dfbbcb6ea4e479c79ca2937028&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They collected images of 216.27 sq. km lands across Poland, a country in Central Europe, 39.51 sq. km with resolution 50 cm per pixel and 176.76 sq. km with resolution 25 cm per pixel and manually fine annotated three following classes of objects: buildings, woodlands, and water.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/IsN5Rc3MGbiVt65El1N1-O8GQW16X8Zsv5KlEmvIhJI.jpg?auto=webp&amp;s=b299da90818ec62cc416cdf65eb39d800333068d', 'width': 474, 'height': 440}, 'resolutions': [{'url': 'https://external-preview.redd.it/IsN5Rc3MGbiVt65El1N1-O8GQW16X8Zsv5KlEmvIhJI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f78328eb1a1ce50bedfa0f6a3c33f968133b25d2', 'width': 108, 'height': 100}, {'url': 'https://external-preview.redd.it/IsN5Rc3MGbiVt65El1N1-O8GQW16X8Zsv5KlEmvIhJI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c7ddecade3501e7f39982a28f732c6359226bdc', 'width': 216, 'height': 200}, {'url': 'https://external-preview.redd.it/IsN5Rc3MGbiVt65El1N1-O8GQW16X8Zsv5KlEmvIhJI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6fb311b2be8185f09905249c5fa3607e9de5d22', 'width': 320, 'height': 297}], 'variants': {}, 'id': 'sTQHm2b57mxiq5m4bzOsFJygtYyaskWQZ5R4EM_MrBg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gk0lbu', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gk0lbu/landcoverai_dataset_for_automatic_mapping_of/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gk0lbu/landcoverai_dataset_for_automatic_mapping_of/', 'subreddit_subscribers': 6676, 'created_utc': 1589509332.0, 'num_crossposts': 15, 'media': None, 'is_video': False}]",t3_gk0lbu,,,,,
558,,pytorch,"Hi, I am using a pretrained AlexNet to classify some images. I keep the feature module weights as they are and I train just train over the classifier layers. I want to save some checkpoints over the training, but in order to save time and storage, I just want to save the weights of the classifier module since the other ones don't change at all.

I have tried to save and load these weights, but it is necessary to specify the class of the model you want to load, and my weights are just from a module of my model class.

How could I load just the weights of these last layers?",t2_21t2bpv,False,,0,False,Loading layer weights from file,[],r/pytorch,False,6,,0,,,False,t3_gjqkcr,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589505286.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am using a pretrained AlexNet to classify some images. I keep the feature module weights as they are and I train just train over the classifier layers. I want to save some checkpoints over the training, but in order to save time and storage, I just want to save the weights of the classifier module since the other ones don&amp;#39;t change at all.&lt;/p&gt;

&lt;p&gt;I have tried to save and load these weights, but it is necessary to specify the class of the model you want to load, and my weights are just from a module of my model class.&lt;/p&gt;

&lt;p&gt;How could I load just the weights of these last layers?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gjqkcr,True,,peterlaanguila8,,1,True,all_ads,False,[],False,,/r/pytorch/comments/gjqkcr/loading_layer_weights_from_file/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gjqkcr/loading_layer_weights_from_file/,7135,1589476486.0,0,,False,,,,,,,,
559,,pytorch,,t2_44mbtmjy,False,,0,False,State of the art in lane detection!,[],r/pytorch,False,6,,0,84.0,,False,t3_gjcxt9,False,dark,1.0,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/ebesBZb-Za8oMYpvawDRxF2xzbUk6gtQQUj6a7MGobg.jpg,False,,[],{},link,,False,,1589450039.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AjdtIPUsaIHlyd1eTWelEF2ZTzvxea8BPdxgDkdn7yA.jpg?auto=webp&amp;s=d18b815aa12ecfa01bd8688c576797da430e3d84', 'width': 1406, 'height': 350}, 'resolutions': [{'url': 'https://external-preview.redd.it/AjdtIPUsaIHlyd1eTWelEF2ZTzvxea8BPdxgDkdn7yA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=568810d20212925b54c06a1e23d7a946f780016f', 'width': 108, 'height': 26}, {'url': 'https://external-preview.redd.it/AjdtIPUsaIHlyd1eTWelEF2ZTzvxea8BPdxgDkdn7yA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=62d8bc84ffbd58a5c51ddbcbb6005dc72c2191e6', 'width': 216, 'height': 53}, {'url': 'https://external-preview.redd.it/AjdtIPUsaIHlyd1eTWelEF2ZTzvxea8BPdxgDkdn7yA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1822b4f3808336f15da64b0d15943d31df62099b', 'width': 320, 'height': 79}, {'url': 'https://external-preview.redd.it/AjdtIPUsaIHlyd1eTWelEF2ZTzvxea8BPdxgDkdn7yA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cefaede7466fb9f0ed7df882f4c422b0e9e38266', 'width': 640, 'height': 159}, {'url': 'https://external-preview.redd.it/AjdtIPUsaIHlyd1eTWelEF2ZTzvxea8BPdxgDkdn7yA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=17591355ff235cfcf320a5ad5a354269fa0eee51', 'width': 960, 'height': 238}, {'url': 'https://external-preview.redd.it/AjdtIPUsaIHlyd1eTWelEF2ZTzvxea8BPdxgDkdn7yA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=003b1f491347161d50d8708710690fa1b94f9161', 'width': 1080, 'height': 268}], 'variants': {}, 'id': 'fLd2V7yqvnxOC3CbpOsgimxbk_ZakZjzkElA5La65nU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gjcxt9,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gjcxt9/state_of_the_art_in_lane_detection/,all_ads,False,/r/LatestInML/comments/gjct0d/state_of_the_art_in_lane_detection/,7135,1589421239.0,0,,False,/r/LatestInML/comments/gjct0d/state_of_the_art_in_lane_detection/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2004.10924)\n\nhttps://preview.redd.it/rj1tquk8ymy41.png?width=1184&amp;format=png&amp;auto=webp&amp;s=88ce351741ffa7333f516d6576fcf1092ef5a284\n\nNovel method for lane detection that uses as input an image from a forward-looking camera mounted in the vehicle and outputs polynomials representing each lane marking in the image, via deep polynomial regression', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'State of the art in lane detection!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 84, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'rj1tquk8ymy41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 64, 'x': 108, 'u': 'https://preview.redd.it/rj1tquk8ymy41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a872159a26781387d1ff59f380bf00db181d5d3c'}, {'y': 129, 'x': 216, 'u': 'https://preview.redd.it/rj1tquk8ymy41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=24d9b6def2c2b55774647d2e7a43b907e1b48962'}, {'y': 192, 'x': 320, 'u': 'https://preview.redd.it/rj1tquk8ymy41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f298bf860c999f8e126a0ea051a1fb5f1d8d91b'}, {'y': 384, 'x': 640, 'u': 'https://preview.redd.it/rj1tquk8ymy41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=60c299145affd0a7ebb2735d1b9d8f9484d1c1a7'}, {'y': 577, 'x': 960, 'u': 'https://preview.redd.it/rj1tquk8ymy41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1b239455362c59b525b324834589601219fec83f'}, {'y': 649, 'x': 1080, 'u': 'https://preview.redd.it/rj1tquk8ymy41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=925b230dc962a4fee453c10cfa379d138b205628'}], 's': {'y': 712, 'x': 1184, 'u': 'https://preview.redd.it/rj1tquk8ymy41.png?width=1184&amp;format=png&amp;auto=webp&amp;s=88ce351741ffa7333f516d6576fcf1092ef5a284'}, 'id': 'rj1tquk8ymy41'}}, 'name': 't3_gjct0d', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 12, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/ebesBZb-Za8oMYpvawDRxF2xzbUk6gtQQUj6a7MGobg.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589449507.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2004.10924""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/rj1tquk8ymy41.png?width=1184&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=88ce351741ffa7333f516d6576fcf1092ef5a284""&gt;https://preview.redd.it/rj1tquk8ymy41.png?width=1184&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=88ce351741ffa7333f516d6576fcf1092ef5a284&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Novel method for lane detection that uses as input an image from a forward-looking camera mounted in the vehicle and outputs polynomials representing each lane marking in the image, via deep polynomial regression&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/AjdtIPUsaIHlyd1eTWelEF2ZTzvxea8BPdxgDkdn7yA.jpg?auto=webp&amp;s=d18b815aa12ecfa01bd8688c576797da430e3d84', 'width': 1406, 'height': 350}, 'resolutions': [{'url': 'https://external-preview.redd.it/AjdtIPUsaIHlyd1eTWelEF2ZTzvxea8BPdxgDkdn7yA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=568810d20212925b54c06a1e23d7a946f780016f', 'width': 108, 'height': 26}, {'url': 'https://external-preview.redd.it/AjdtIPUsaIHlyd1eTWelEF2ZTzvxea8BPdxgDkdn7yA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=62d8bc84ffbd58a5c51ddbcbb6005dc72c2191e6', 'width': 216, 'height': 53}, {'url': 'https://external-preview.redd.it/AjdtIPUsaIHlyd1eTWelEF2ZTzvxea8BPdxgDkdn7yA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1822b4f3808336f15da64b0d15943d31df62099b', 'width': 320, 'height': 79}, {'url': 'https://external-preview.redd.it/AjdtIPUsaIHlyd1eTWelEF2ZTzvxea8BPdxgDkdn7yA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cefaede7466fb9f0ed7df882f4c422b0e9e38266', 'width': 640, 'height': 159}, {'url': 'https://external-preview.redd.it/AjdtIPUsaIHlyd1eTWelEF2ZTzvxea8BPdxgDkdn7yA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=17591355ff235cfcf320a5ad5a354269fa0eee51', 'width': 960, 'height': 238}, {'url': 'https://external-preview.redd.it/AjdtIPUsaIHlyd1eTWelEF2ZTzvxea8BPdxgDkdn7yA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=003b1f491347161d50d8708710690fa1b94f9161', 'width': 1080, 'height': 268}], 'variants': {}, 'id': 'fLd2V7yqvnxOC3CbpOsgimxbk_ZakZjzkElA5La65nU'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gjct0d', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gjct0d/state_of_the_art_in_lane_detection/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gjct0d/state_of_the_art_in_lane_detection/', 'subreddit_subscribers': 6676, 'created_utc': 1589420707.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_gjct0d,,,,,
560,,pytorch,"\[SOLVED\] Thank you everyone!

I am building a network for image classification using the MNIST dataset. I've managed to get the model to train but my loss is not decreasing over time. What is a good way to debug this?

I'm using an SGD optimizer,  learning rate of 0.01 and NLL Loss as my loss function.

Each input is of size (64, 1, 28, 28) and the architecture is as follows:

`class ImageClassifier(nn.Module):`

`def __init__(self):\`

`super(ImageClassifier, self).__init__()`

`self.conv1 = nn.Conv2d(1, 10, kernel_size=5)`

`self.conv2 = nn.Conv2d(10, 20, kernel_size=5)`

`self.dropout = nn.Dropout2d()`

`self.fc1 = nn.Linear(320, 50)`

`self.fc2 = nn.Linear(50, 10) # (num_features, num_classes)`

`def forward(self, x):`

`x = F.relu(F.max_pool2d(self.conv1(x), 2))`

`x = F.relu(F.max_pool2d(self.dropout(self.conv2(x)), 2))`

`x = x.view(-1, 320)`

`x = F.relu(self.fc1(x))`

`x = F.dropout(x,` [`training=self.training`](https://training=self.training)`)`

`x = F.relu(self.fc2(x))`

`return F.log_softmax(x)`

Some help would really be appreciated. It may help to know that I feel like this has happened with other projects of mine in the past. This might just be an issue with how I fundamentally build my networks.

Thanks in advance!

&amp;#x200B;

Edit: It may also be possible that my issue lies outside the model architecture. In that case I have added my training loop here:

`def train_func(epoch):`

`model.train()`

`for batch_idx, (image, label) in enumerate(train_loader):`

`image, label =` [`image.to`](https://image.to)`(device),` [`label.to`](https://label.to)`(device)`

`optimizer.zero_grad()`

`output = model(data)`

`loss = F.nll_loss(output, label).to(device)`

`loss.backward()`

`optimizer.step()`

`if batch_idx % 10 == 0:`

`train_losses.append(loss.item())`

`train_counter.append(`

`(batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))`

[`torch.save`](https://torch.save)`(model.state_dict(), 'results/model.pth')`

[`torch.save`](https://torch.save)`(optimizer.state_dict(), 'results/optimizer.pth')`",t2_ebu4m,False,,0,False,Loss not decreasing in Convolutional Network help,[],r/pytorch,False,6,,0,,,False,t3_gj8u9o,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1589437813.0,,[],{},,,True,,1589436036.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[SOLVED] Thank you everyone!&lt;/p&gt;

&lt;p&gt;I am building a network for image classification using the MNIST dataset. I&amp;#39;ve managed to get the model to train but my loss is not decreasing over time. What is a good way to debug this?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m using an SGD optimizer,  learning rate of 0.01 and NLL Loss as my loss function.&lt;/p&gt;

&lt;p&gt;Each input is of size (64, 1, 28, 28) and the architecture is as follows:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;class ImageClassifier(nn.Module):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def __init__(self):\&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;super(ImageClassifier, self).__init__()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.conv1 = nn.Conv2d(1, 10, kernel_size=5)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.conv2 = nn.Conv2d(10, 20, kernel_size=5)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.dropout = nn.Dropout2d()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.fc1 = nn.Linear(320, 50)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.fc2 = nn.Linear(50, 10) # (num_features, num_classes)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def forward(self, x):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = F.relu(F.max_pool2d(self.conv1(x), 2))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = F.relu(F.max_pool2d(self.dropout(self.conv2(x)), 2))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = x.view(-1, 320)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = F.relu(self.fc1(x))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = F.dropout(x,&lt;/code&gt; &lt;a href=""https://training=self.training""&gt;&lt;code&gt;training=self.training&lt;/code&gt;&lt;/a&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = F.relu(self.fc2(x))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;return F.log_softmax(x)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Some help would really be appreciated. It may help to know that I feel like this has happened with other projects of mine in the past. This might just be an issue with how I fundamentally build my networks.&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Edit: It may also be possible that my issue lies outside the model architecture. In that case I have added my training loop here:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def train_func(epoch):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model.train()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for batch_idx, (image, label) in enumerate(train_loader):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;image, label =&lt;/code&gt; &lt;a href=""https://image.to""&gt;&lt;code&gt;image.to&lt;/code&gt;&lt;/a&gt;&lt;code&gt;(device),&lt;/code&gt; &lt;a href=""https://label.to""&gt;&lt;code&gt;label.to&lt;/code&gt;&lt;/a&gt;&lt;code&gt;(device)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;optimizer.zero_grad()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;output = model(data)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;loss = F.nll_loss(output, label).to(device)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;loss.backward()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;optimizer.step()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;if batch_idx % 10 == 0:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;train_losses.append(loss.item())&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;train_counter.append(&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://torch.save""&gt;&lt;code&gt;torch.save&lt;/code&gt;&lt;/a&gt;&lt;code&gt;(model.state_dict(), &amp;#39;results/model.pth&amp;#39;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://torch.save""&gt;&lt;code&gt;torch.save&lt;/code&gt;&lt;/a&gt;&lt;code&gt;(optimizer.state_dict(), &amp;#39;results/optimizer.pth&amp;#39;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gj8u9o,True,,Pepipasta,,21,True,all_ads,False,[],False,,/r/pytorch/comments/gj8u9o/loss_not_decreasing_in_convolutional_network_help/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gj8u9o/loss_not_decreasing_in_convolutional_network_help/,7135,1589407236.0,0,,False,,,,,,,,
561,,pytorch,"Japanese artificial intelligence startup Preferred Networks (PFN) today debuted its PyTorch library[ *pytorch-pfn-extras*](https://github.com/pfnet/pytorch-pfn-extras) (PPE). This is the company’s first open-source library supporting research and development in deep learning using the PyTorch framework. The release follows PFN’s December 2019 announcement that it was migrating its deep learning research platform to PyTorch from its own open source deep learning framework, Chainer.

Here is a quick read: [Japanese Unicorn Preferred Networks Releases First PyTorch Library](https://medium.com/syncedreview/japanese-unicorn-preferred-networks-releases-first-pytorch-library-ff902f8477eb)",t2_2fv4yodo,False,,0,False,[N] Japanese Unicorn Preferred Networks Releases First PyTorch Library,[],r/pytorch,False,6,,0,,,False,t3_gj1l0y,False,dark,0.87,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},self,,True,,1589413792.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Japanese artificial intelligence startup Preferred Networks (PFN) today debuted its PyTorch library&lt;a href=""https://github.com/pfnet/pytorch-pfn-extras""&gt; &lt;em&gt;pytorch-pfn-extras&lt;/em&gt;&lt;/a&gt; (PPE). This is the company’s first open-source library supporting research and development in deep learning using the PyTorch framework. The release follows PFN’s December 2019 announcement that it was migrating its deep learning research platform to PyTorch from its own open source deep learning framework, Chainer.&lt;/p&gt;

&lt;p&gt;Here is a quick read: &lt;a href=""https://medium.com/syncedreview/japanese-unicorn-preferred-networks-releases-first-pytorch-library-ff902f8477eb""&gt;Japanese Unicorn Preferred Networks Releases First PyTorch Library&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/hlLnxwh-pkW9PmWHFysuHcAU_dBj7IQ141OU8d2YEMM.jpg?auto=webp&amp;s=3529118b3d6e18b13867cb8f3f58067d999ffb6f', 'width': 200, 'height': 200}, 'resolutions': [{'url': 'https://external-preview.redd.it/hlLnxwh-pkW9PmWHFysuHcAU_dBj7IQ141OU8d2YEMM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ba1fa1ad1fb685a77d2d6b9b098fdb3969ffa256', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'iY0ZZQfJ6Ld5ghlVmWRpyv5LAK0yusG3IEKuDJrHs6U'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gj1l0y,True,,Yuqing7,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gj1l0y/n_japanese_unicorn_preferred_networks_releases/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gj1l0y/n_japanese_unicorn_preferred_networks_releases/,7135,1589384992.0,0,,False,,,,,,,,
562,,pytorch,"I am pretty new to pytorch, and want to create a very simple model that takes the two described vectors as inputs. I understand that going from tensor to numpy and back is straightforward. Of course I also have an expected value between 0 and 10000 that represents how well the input vectors correlate. I get the theory, now I am only looking for a similar model that I can learn/copy the syntax from. Please don't write it yourself, just point me to tutorials that you believe to be similar to my problem.",t2_6cgfmz1c,False,,0,False,"Creating a simple model that takes two dictionaries with ids as keys. One has a tensor of size 1024 as values, the other a numpy vector of size 196.",[],r/pytorch,False,6,,0,,,False,t3_giwuwu,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589396449.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am pretty new to pytorch, and want to create a very simple model that takes the two described vectors as inputs. I understand that going from tensor to numpy and back is straightforward. Of course I also have an expected value between 0 and 10000 that represents how well the input vectors correlate. I get the theory, now I am only looking for a similar model that I can learn/copy the syntax from. Please don&amp;#39;t write it yourself, just point me to tutorials that you believe to be similar to my problem.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,giwuwu,True,,BewitchedHare,,4,True,all_ads,False,[],False,,/r/pytorch/comments/giwuwu/creating_a_simple_model_that_takes_two/,all_ads,False,https://www.reddit.com/r/pytorch/comments/giwuwu/creating_a_simple_model_that_takes_two/,7135,1589367649.0,0,,False,,,,,,,,
563,,pytorch,"[https://github.com/lab-ml/labml/blob/master/labml/helpers/pytorch/module.py](https://github.com/lab-ml/labml/blob/master/labml/helpers/pytorch/module.py)

This is a class I wrote than subclass `nn.Module` and let modules implement `__call__` instead of `forward`. I did this to get better *type checking* and *type inference* in python.

This module uses `__init_subclass__` set the `__call__` method of it's subclasses to `forward` so that it doesn't mess up with `nn.Module` hooks and stuff. I've been using this for a while and hasn't encountered problems.

Are there implications that I haven't thought of?",t2_1jyhaoq,False,,0,False,[D] __call__ signature of nn.Module,[],r/pytorch,False,6,,0,,,False,t3_girkue,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1589371619.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://github.com/lab-ml/labml/blob/master/labml/helpers/pytorch/module.py""&gt;https://github.com/lab-ml/labml/blob/master/labml/helpers/pytorch/module.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is a class I wrote than subclass &lt;code&gt;nn.Module&lt;/code&gt; and let modules implement &lt;code&gt;__call__&lt;/code&gt; instead of &lt;code&gt;forward&lt;/code&gt;. I did this to get better &lt;em&gt;type checking&lt;/em&gt; and &lt;em&gt;type inference&lt;/em&gt; in python.&lt;/p&gt;

&lt;p&gt;This module uses &lt;code&gt;__init_subclass__&lt;/code&gt; set the &lt;code&gt;__call__&lt;/code&gt; method of it&amp;#39;s subclasses to &lt;code&gt;forward&lt;/code&gt; so that it doesn&amp;#39;t mess up with &lt;code&gt;nn.Module&lt;/code&gt; hooks and stuff. I&amp;#39;ve been using this for a while and hasn&amp;#39;t encountered problems.&lt;/p&gt;

&lt;p&gt;Are there implications that I haven&amp;#39;t thought of?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Wyk2ZaFFadSlp1tc7k5arE_nmZKMnftTCT36vL5y218.jpg?auto=webp&amp;s=a636d4b7d6a8fb759cdd14a621f273122e4a0ec2', 'width': 4000, 'height': 2000}, 'resolutions': [{'url': 'https://external-preview.redd.it/Wyk2ZaFFadSlp1tc7k5arE_nmZKMnftTCT36vL5y218.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a8153802b90f936592b35cab133c1972e17d87c1', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/Wyk2ZaFFadSlp1tc7k5arE_nmZKMnftTCT36vL5y218.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e49a726bb52004c0074f8bdf0dac715731247356', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/Wyk2ZaFFadSlp1tc7k5arE_nmZKMnftTCT36vL5y218.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a0047664c67ee46169907530015f048b3bb6b45f', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/Wyk2ZaFFadSlp1tc7k5arE_nmZKMnftTCT36vL5y218.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3382ebb8680dd06eca9f9ec88bde355f7464e0e', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/Wyk2ZaFFadSlp1tc7k5arE_nmZKMnftTCT36vL5y218.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f918109570b8673ad41b5d53ac77a78e34acd68d', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/Wyk2ZaFFadSlp1tc7k5arE_nmZKMnftTCT36vL5y218.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28dae149d5cd0544c764c196db02f27df1656437', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'Yf_TQTLAaWX4-QGo4UivNwAS_DvsJz9xuNZ8OX3OfyM'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,girkue,True,,mlvpj,,2,True,all_ads,False,[],False,,/r/pytorch/comments/girkue/d_call_signature_of_nnmodule/,all_ads,False,https://www.reddit.com/r/pytorch/comments/girkue/d_call_signature_of_nnmodule/,7135,1589342819.0,0,,False,,,,,,,,
564,,pytorch,"This max\_size argument related error has come while I tried to resolve a runtime error faced during training the model.

I am giving below the code in detail so that I can get your advice on fixing the issue in hand:

I am using google colab to build my model.

I am building this transformer model using pytorch where I am using custom embedding layer made of two pre-trained embeddings: fasttext and glove.

Dimension of both the pretrained embedding matrix is 300.

    crawl-300d-2M.vec glove.6B.300d.txt

But I want to limit the dimension of my custom embedding to 256.

Wrote this function to limit the embedding dimention to 256:

`def load_embedding(embedding_file):`



`def get_coefs(word,*arr):` 

`return word, np.asarray(arr, dtype='float32')[:256]  # keeping the embedding size 256`

&amp;#x200B;

`embeddings_index = dict(get_coefs(*o.split("" "")) for o in open(embedding_file, encoding=""utf8"", errors='ignore') if len(o)&gt;100)`

&amp;#x200B;

`return embeddings_index`

&amp;#x200B;

`glove_file = '/glove.6B.300d.txt'`

`fasttext_file = '/crawl-300d-2M.vec'`

&amp;#x200B;

`glove_embeddings_index = load_embedding(glove_file)`

&amp;#x200B;

`fasttext_embeddings_index = load_embedding(fasttext_file)`

&amp;#x200B;

`# custom embedding`

&amp;#x200B;

`# creating a placeholder embedding matrix first`

&amp;#x200B;

`all_embs = np.stack(fasttext_embeddings_index.values())  #using fasttext embedding as base`

`emb_mean,emb_std = all_embs.mean(), all_embs.std()`

`embed_size = all_embs.shape[1]`

&amp;#x200B;

`nb_words = len(word_index)`

`embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))`

&amp;#x200B;

`print(embedding_matrix.shape[1])  #output: 256`

&amp;#x200B;

`# custom embedding creation which is a dictionary containing word and corresponding word vector`

&amp;#x200B;

`cust_embedding = {}`

&amp;#x200B;

`for word, indx in word_index.items():`

  `if indx &lt; nb_words:`

`embedding_vector = fasttext_embeddings_index.get(word)` 

`if embedding_vector is None:`

`embedding_vector = glove_embeddings_index.get(word)` 

`if embedding_vector is None:`

`embedding_vector = embedding_matrix[indx]`

&amp;#x200B;

`cust_embedding[word] = embedding_vector`

&amp;#x200B;

`# saving custom embedding in .txt file which will be later used during preprocessing using torchtext`

&amp;#x200B;

`with open('/custom_embeddings.txt', 'w+') as f:`

  `for token, vector in cust_embedding.items():`

`vector_str = ' '.join([str(v) for v in vector])`

`f.write(f'{token} {vector_str}\n')`

Next I am trying to create a torchtext.vocab.Vectors object

    import torchtext.vocab as vocab  custom_embeddings = vocab.Vectors(name = '/custom_embeddings.txt', max_size= 256)

Here using 'max\_size=' argument is throwing error:

    TypeError: __init__() got an unexpected keyword argument 'max_size'

But, I checked the torchtext.vocab.Vectors documentation where I could see this max\_size argument is present :

    class torchtext.vocab.Vocab(counter, max_size=None, min_freq=1, specials=['&lt;pad&gt;'], vectors=None, unk_init=None, vectors_cache=None, specials_first=True)

And I need to set the size of my custom embedding to 256 or else later during training my model I am getting run time error.

Code snippet of vocabulary building for encoder(ENC) and decoder(DEC) input using custom embedding:

    ENC_TEXT.build_vocab(train_data, vectors = custom_embeddings) DEC_TEXT.build_vocab(train_data, vectors = custom_embeddings)  model.embedding.weight.data.copy_(ENC_TEXT.vocab.vectors)  model.embedding.weight.data.copy_(DEC_TEXT.vocab.vectors)

Giving below the parameter setting and run time error received during training the model if I do not change the custom embedding dimension to 256:

&amp;#x200B;

`INPUT_DIM = len(ENC_TEXT.vocab)`

`OUTPUT_DIM = len(DEC_TEXT.vocab)`

`HIDDEN_DIM = 256  # size of each pretrained word vector in the embedding matrix ie size[1] of the embedding matrix`

`ENC_LAYERS = 3`

`DEC_LAYERS = 3`

`ENC_HEADS = 10`

`DEC_HEADS =10`

`ENC_PF_DIM = 512` 

`DEC_PF_DIM = 512` 

`ENC_DROPOUT = 0.1`

`DEC_DROPOUT = 0.1`

&amp;#x200B;

`enc = Encoder(INPUT_DIM,` 

`HIDDEN_DIM ,` 

`ENC_LAYERS,` 

`ENC_HEADS,` 

`ENC_PF_DIM,` 

`ENC_DROPOUT,` 

`device)`

&amp;#x200B;

`dec = Decoder(OUTPUT_DIM,` 

`HIDDEN_DIM ,` 

`DEC_LAYERS,`

`DEC_HEADS,` 

`DEC_PF_DIM,` 

`DEC_DROPOUT,` 

`device)`

&amp;#x200B;

`---------------------------------------------------------------------------`

`RuntimeError                              Traceback (most recent call last)`

`&lt;ipython-input-75-d4dd8304c0ad&gt; in &lt;module&gt;()`

`20               ENC_PF_DIM,`

`21               ENC_DROPOUT,`

`---&gt; 22               device)`

`23` 

`24 dec = Decoder(OUTPUT_DIM,` 

&amp;#x200B;

`&lt;ipython-input-67-f239fb126b0e&gt; in __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length)`

`18` 

`19         # step added for custom embedding`

`---&gt; 20         self.tok_embedding.weight.data.copy_(SRC.vocab.vectors)`

`21` 

`22         self.pos_embedding = nn.Embedding(max_length, hid_dim)`

&amp;#x200B;

`RuntimeError: The size of tensor a (256) must match the size of tensor b (300) at non-singleton dimension 1`

&amp;#x200B;

`-------------------------------------------------------------------------------`

&amp;#x200B;

The main issue I am facing is this run time error of size mismatch between tensors. Looking for the reason behind led to the size of the custom embedding being fixed to 300(instead of 256)

Really appreciate if you kindly help in resolve the issue.

Thanks in advance!",t2_3x6gro0i,False,,0,False,Pytorch: getting error while trying to set max_size argument for torchtext.vocab.Vocab object in Colab,[],r/pytorch,False,6,,0,,,False,t3_gia4wv,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589314719.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This max_size argument related error has come while I tried to resolve a runtime error faced during training the model.&lt;/p&gt;

&lt;p&gt;I am giving below the code in detail so that I can get your advice on fixing the issue in hand:&lt;/p&gt;

&lt;p&gt;I am using google colab to build my model.&lt;/p&gt;

&lt;p&gt;I am building this transformer model using pytorch where I am using custom embedding layer made of two pre-trained embeddings: fasttext and glove.&lt;/p&gt;

&lt;p&gt;Dimension of both the pretrained embedding matrix is 300.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;crawl-300d-2M.vec glove.6B.300d.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But I want to limit the dimension of my custom embedding to 256.&lt;/p&gt;

&lt;p&gt;Wrote this function to limit the embedding dimention to 256:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def load_embedding(embedding_file):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def get_coefs(word,*arr):&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;return word, np.asarray(arr, dtype=&amp;#39;float32&amp;#39;)[:256]  # keeping the embedding size 256&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;embeddings_index = dict(get_coefs(*o.split(&amp;quot; &amp;quot;)) for o in open(embedding_file, encoding=&amp;quot;utf8&amp;quot;, errors=&amp;#39;ignore&amp;#39;) if len(o)&amp;gt;100)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;return embeddings_index&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;glove_file = &amp;#39;/glove.6B.300d.txt&amp;#39;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;fasttext_file = &amp;#39;/crawl-300d-2M.vec&amp;#39;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;glove_embeddings_index = load_embedding(glove_file)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;fasttext_embeddings_index = load_embedding(fasttext_file)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# custom embedding&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# creating a placeholder embedding matrix first&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;all_embs = np.stack(fasttext_embeddings_index.values())  #using fasttext embedding as base&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;emb_mean,emb_std = all_embs.mean(), all_embs.std()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;embed_size = all_embs.shape[1]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nb_words = len(word_index)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(embedding_matrix.shape[1])  #output: 256&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# custom embedding creation which is a dictionary containing word and corresponding word vector&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;cust_embedding = {}&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for word, indx in word_index.items():&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;if indx &amp;lt; nb_words:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;embedding_vector = fasttext_embeddings_index.get(word)&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;if embedding_vector is None:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;embedding_vector = glove_embeddings_index.get(word)&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;if embedding_vector is None:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;embedding_vector = embedding_matrix[indx]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;cust_embedding[word] = embedding_vector&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# saving custom embedding in .txt file which will be later used during preprocessing using torchtext&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;with open(&amp;#39;/custom_embeddings.txt&amp;#39;, &amp;#39;w+&amp;#39;) as f:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for token, vector in cust_embedding.items():&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;vector_str = &amp;#39; &amp;#39;.join([str(v) for v in vector])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;f.write(f&amp;#39;{token} {vector_str}\n&amp;#39;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Next I am trying to create a torchtext.vocab.Vectors object&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import torchtext.vocab as vocab  custom_embeddings = vocab.Vectors(name = &amp;#39;/custom_embeddings.txt&amp;#39;, max_size= 256)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here using &amp;#39;max_size=&amp;#39; argument is throwing error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TypeError: __init__() got an unexpected keyword argument &amp;#39;max_size&amp;#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But, I checked the torchtext.vocab.Vectors documentation where I could see this max_size argument is present :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class torchtext.vocab.Vocab(counter, max_size=None, min_freq=1, specials=[&amp;#39;&amp;lt;pad&amp;gt;&amp;#39;], vectors=None, unk_init=None, vectors_cache=None, specials_first=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And I need to set the size of my custom embedding to 256 or else later during training my model I am getting run time error.&lt;/p&gt;

&lt;p&gt;Code snippet of vocabulary building for encoder(ENC) and decoder(DEC) input using custom embedding:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ENC_TEXT.build_vocab(train_data, vectors = custom_embeddings) DEC_TEXT.build_vocab(train_data, vectors = custom_embeddings)  model.embedding.weight.data.copy_(ENC_TEXT.vocab.vectors)  model.embedding.weight.data.copy_(DEC_TEXT.vocab.vectors)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Giving below the parameter setting and run time error received during training the model if I do not change the custom embedding dimension to 256:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;INPUT_DIM = len(ENC_TEXT.vocab)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;OUTPUT_DIM = len(DEC_TEXT.vocab)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;HIDDEN_DIM = 256  # size of each pretrained word vector in the embedding matrix ie size[1] of the embedding matrix&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ENC_LAYERS = 3&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;DEC_LAYERS = 3&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ENC_HEADS = 10&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;DEC_HEADS =10&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ENC_PF_DIM = 512&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;DEC_PF_DIM = 512&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;ENC_DROPOUT = 0.1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;DEC_DROPOUT = 0.1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;enc = Encoder(INPUT_DIM,&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;HIDDEN_DIM ,&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;ENC_LAYERS,&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;ENC_HEADS,&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;ENC_PF_DIM,&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;ENC_DROPOUT,&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;device)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;dec = Decoder(OUTPUT_DIM,&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;HIDDEN_DIM ,&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;DEC_LAYERS,&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;DEC_HEADS,&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;DEC_PF_DIM,&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;DEC_DROPOUT,&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;device)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;---------------------------------------------------------------------------&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;RuntimeError                              Traceback (most recent call last)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;ipython-input-75-d4dd8304c0ad&amp;gt; in &amp;lt;module&amp;gt;()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;20               ENC_PF_DIM,&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;21               ENC_DROPOUT,&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;---&amp;gt; 22               device)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;23&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;24 dec = Decoder(OUTPUT_DIM,&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;ipython-input-67-f239fb126b0e&amp;gt; in __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;18&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;19         # step added for custom embedding&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;---&amp;gt; 20         self.tok_embedding.weight.data.copy_(SRC.vocab.vectors)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;21&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;22         self.pos_embedding = nn.Embedding(max_length, hid_dim)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;RuntimeError: The size of tensor a (256) must match the size of tensor b (300) at non-singleton dimension 1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;-------------------------------------------------------------------------------&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The main issue I am facing is this run time error of size mismatch between tensors. Looking for the reason behind led to the size of the custom embedding being fixed to 300(instead of 256)&lt;/p&gt;

&lt;p&gt;Really appreciate if you kindly help in resolve the issue.&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gia4wv,True,,Mandala16180,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gia4wv/pytorch_getting_error_while_trying_to_set_max/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gia4wv/pytorch_getting_error_while_trying_to_set_max/,7135,1589285919.0,0,,False,,,,,,,,
565,,pytorch,"Dear folks,

hello, I am a quite newbie in deep learning with PyTorch...

I am trying to build an image classification model using PyTorch,

but in the process of pre-processing dataset, I've stuck with a problem.

&amp;#x200B;

Problem) 

\- I want to read a bunch of .npy files in a directory (e.g. '/training')

\- One of the .npy files consists of tensors with shape of (N, C, H, W), i.e. mini-batch of images

(e.g. file1: (1592, 3, 224, 224), file2: (1100, 3, 224, 224), ...)

\- The size of mini-batch is different from each other (1592, 1100, 683, ...)

&amp;#x200B;

Questions)

\- AFAIK, DatasetFolder or ImageFolder module of torchvision.datasets are used for the directory having a list of single image files. (e.g. 1592 separate .npy files, of which shape is (1, 3, 224, 224))

\- In my case, is there any recommended way to deal with the situation?  
(I finally want to use DataLoader after constructing a (custom) DatasetFolder/ImageFolder module) 

\- I want to avoid the situation of reading each mini-batch .npy file, split them by a sample (1, C, H, W), and save them again in a separate file...

&amp;#x200B;

Thank you, in advance.

Have a good day!",t2_1ois3ev5,False,,0,False,[Question] What is an appropriate usage of ImageFolder/DatasetFolder in this situation?,[],r/pytorch,False,6,,0,,,False,t3_gic6cn,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589322278.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Dear folks,&lt;/p&gt;

&lt;p&gt;hello, I am a quite newbie in deep learning with PyTorch...&lt;/p&gt;

&lt;p&gt;I am trying to build an image classification model using PyTorch,&lt;/p&gt;

&lt;p&gt;but in the process of pre-processing dataset, I&amp;#39;ve stuck with a problem.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Problem) &lt;/p&gt;

&lt;p&gt;- I want to read a bunch of .npy files in a directory (e.g. &amp;#39;/training&amp;#39;)&lt;/p&gt;

&lt;p&gt;- One of the .npy files consists of tensors with shape of (N, C, H, W), i.e. mini-batch of images&lt;/p&gt;

&lt;p&gt;(e.g. file1: (1592, 3, 224, 224), file2: (1100, 3, 224, 224), ...)&lt;/p&gt;

&lt;p&gt;- The size of mini-batch is different from each other (1592, 1100, 683, ...)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Questions)&lt;/p&gt;

&lt;p&gt;- AFAIK, DatasetFolder or ImageFolder module of torchvision.datasets are used for the directory having a list of single image files. (e.g. 1592 separate .npy files, of which shape is (1, 3, 224, 224))&lt;/p&gt;

&lt;p&gt;- In my case, is there any recommended way to deal with the situation?&lt;br/&gt;
(I finally want to use DataLoader after constructing a (custom) DatasetFolder/ImageFolder module) &lt;/p&gt;

&lt;p&gt;- I want to avoid the situation of reading each mini-batch .npy file, split them by a sample (1, C, H, W), and save them again in a separate file...&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you, in advance.&lt;/p&gt;

&lt;p&gt;Have a good day!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gic6cn,True,,vaseline555,,2,True,all_ads,False,[],False,,/r/pytorch/comments/gic6cn/question_what_is_an_appropriate_usage_of/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gic6cn/question_what_is_an_appropriate_usage_of/,7135,1589293478.0,0,,False,,,,,,,,
566,,pytorch,"Hi, I'm totally new to pytorch, so it might be a very basic question. I have two networks that should be trained together.

1. First one takes data as input and returns its embedding as output.
2. Second one takes pairs of embedded datapoints and returns their 'similarity' as output.
3. Partial loss is then computed for every datapoint, and then all the losses are combined.
4. This final loss should be backpropagated through both networks.

How should the code for that look like? I'm thinking something like this:

    def train_models(inputs, targets):
        network1.train()
        network2.train()
        embeddings = network1(inputs)
        paired_embeddings = pair_embeddings(embeddings)
        similarities = network2(similarities)
        
        """"""
            I don't know how the loss should be calculated here.
            I have a loss formula for every embedded datapoint, 
            but not for every similarity.
            But if I only calculate loss for every embedding (using similarites),
            won't backpropagate() only modify network1, 
            since embeddings are network1's outputs
            and have not been modified in network2?
        """"""        
        
        optimizer1.step()
        optimizer2.step()
        scheduler1.step()
        scheduler2.step()
        network1.eval()
        network2.eval()

I hope this specific enough. I'll gladly share more details if necessary. I'm just so inexperienced with pytorch and deep learning in general, that I'm not even sure how to ask this question.",t2_nsxhq,False,,0,False,How to train these two networks together?,[],r/pytorch,False,6,,0,,,False,t3_gi7u0m,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589304015.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m totally new to pytorch, so it might be a very basic question. I have two networks that should be trained together.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;First one takes data as input and returns its embedding as output.&lt;/li&gt;
&lt;li&gt;Second one takes pairs of embedded datapoints and returns their &amp;#39;similarity&amp;#39; as output.&lt;/li&gt;
&lt;li&gt;Partial loss is then computed for every datapoint, and then all the losses are combined.&lt;/li&gt;
&lt;li&gt;This final loss should be backpropagated through both networks.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;How should the code for that look like? I&amp;#39;m thinking something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def train_models(inputs, targets):
    network1.train()
    network2.train()
    embeddings = network1(inputs)
    paired_embeddings = pair_embeddings(embeddings)
    similarities = network2(similarities)

    &amp;quot;&amp;quot;&amp;quot;
        I don&amp;#39;t know how the loss should be calculated here.
        I have a loss formula for every embedded datapoint, 
        but not for every similarity.
        But if I only calculate loss for every embedding (using similarites),
        won&amp;#39;t backpropagate() only modify network1, 
        since embeddings are network1&amp;#39;s outputs
        and have not been modified in network2?
    &amp;quot;&amp;quot;&amp;quot;        

    optimizer1.step()
    optimizer2.step()
    scheduler1.step()
    scheduler2.step()
    network1.eval()
    network2.eval()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I hope this specific enough. I&amp;#39;ll gladly share more details if necessary. I&amp;#39;m just so inexperienced with pytorch and deep learning in general, that I&amp;#39;m not even sure how to ask this question.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gi7u0m,True,,Algabera,,2,True,all_ads,False,[],False,,/r/pytorch/comments/gi7u0m/how_to_train_these_two_networks_together/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gi7u0m/how_to_train_these_two_networks_together/,7135,1589275215.0,0,,False,,,,,,,,
567,,pytorch,,t2_44mbtmjy,False,,0,False,ICYMI: Novel approach to generating high-resolution images,[],r/pytorch,False,6,,0,74.0,,False,t3_gi2tv0,False,dark,0.75,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/lqMToA1hfdo1GHy4m0ScShYlKc8CETAoKJYUmCLPdpI.jpg,False,,[],{},link,,False,,1589281646.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vPzMKqP0_IRkFtq9qD8WpKpZblALMfpMsF6Iz2fSY6Y.jpg?auto=webp&amp;s=ccbadc0fef4ec32ba9efc4ff6540f055bac4f512', 'width': 1410, 'height': 676}, 'resolutions': [{'url': 'https://external-preview.redd.it/vPzMKqP0_IRkFtq9qD8WpKpZblALMfpMsF6Iz2fSY6Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=87cddf1bfafaceb966f1ee48f6ed0516ec89d7a5', 'width': 108, 'height': 51}, {'url': 'https://external-preview.redd.it/vPzMKqP0_IRkFtq9qD8WpKpZblALMfpMsF6Iz2fSY6Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=27f359c28ba2f84a5375deccbdc79af4ca52d22f', 'width': 216, 'height': 103}, {'url': 'https://external-preview.redd.it/vPzMKqP0_IRkFtq9qD8WpKpZblALMfpMsF6Iz2fSY6Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f3f7ccca827d28e838db9d3d683a4cf4407b0f2', 'width': 320, 'height': 153}, {'url': 'https://external-preview.redd.it/vPzMKqP0_IRkFtq9qD8WpKpZblALMfpMsF6Iz2fSY6Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=af70e405799a932937a5bea417d55217f7c28f2b', 'width': 640, 'height': 306}, {'url': 'https://external-preview.redd.it/vPzMKqP0_IRkFtq9qD8WpKpZblALMfpMsF6Iz2fSY6Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=70dfbca5af14ee47b7ded0aaf67c9dad0f716418', 'width': 960, 'height': 460}, {'url': 'https://external-preview.redd.it/vPzMKqP0_IRkFtq9qD8WpKpZblALMfpMsF6Iz2fSY6Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ba9f3984913d380441441ed8cc0ca2226a33193', 'width': 1080, 'height': 517}], 'variants': {}, 'id': 'wHSFYIhjkZOCYegUjg6VTXGFv2PKvRi7KZ5QY21rtNs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gi2tv0,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gi2tv0/icymi_novel_approach_to_generating_highresolution/,all_ads,False,/r/LatestInML/comments/gi2r09/icymi_novel_approach_to_generating_highresolution/,7135,1589252846.0,0,,False,/r/LatestInML/comments/gi2r09/icymi_novel_approach_to_generating_highresolution/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'From Ian Goodfellow and other Google researchers: A novel approach to generating high-resolution images, guided by small inputs, that results in perceptually convincing details (called Latent Adversarial Generator (LAG))\n\nFor project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2003.02365)\n\nhttps://preview.redd.it/i7d0uub729y41.png?width=2056&amp;format=png&amp;auto=webp&amp;s=2f0548cf39ea5dd6543b7508d29f40343228b923', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'ICYMI: Novel approach to generating high-resolution images', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 74, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'i7d0uub729y41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 57, 'x': 108, 'u': 'https://preview.redd.it/i7d0uub729y41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5b0af172db7c102d16a64fa1eeed28b922f99f69'}, {'y': 115, 'x': 216, 'u': 'https://preview.redd.it/i7d0uub729y41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=df5fccec4902b0b5ba175d32d0c285e6de9e183b'}, {'y': 171, 'x': 320, 'u': 'https://preview.redd.it/i7d0uub729y41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c92f5913f079b549cfc9c1d37db99ee2fe42b8fa'}, {'y': 342, 'x': 640, 'u': 'https://preview.redd.it/i7d0uub729y41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=744888ee783fa0f642ccd53c6151eff4ee17a92e'}, {'y': 513, 'x': 960, 'u': 'https://preview.redd.it/i7d0uub729y41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a7e91d3b349e82e59cc06f9820298a36cbb3aed2'}, {'y': 577, 'x': 1080, 'u': 'https://preview.redd.it/i7d0uub729y41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c3481e10c3ebf8803b9125c2f5ab1318bea200cd'}], 's': {'y': 1100, 'x': 2056, 'u': 'https://preview.redd.it/i7d0uub729y41.png?width=2056&amp;format=png&amp;auto=webp&amp;s=2f0548cf39ea5dd6543b7508d29f40343228b923'}, 'id': 'i7d0uub729y41'}}, 'name': 't3_gi2r09', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 19, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 19, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/lqMToA1hfdo1GHy4m0ScShYlKc8CETAoKJYUmCLPdpI.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589281349.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;From Ian Goodfellow and other Google researchers: A novel approach to generating high-resolution images, guided by small inputs, that results in perceptually convincing details (called Latent Adversarial Generator (LAG))&lt;/p&gt;\n\n&lt;p&gt;For project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2003.02365""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/i7d0uub729y41.png?width=2056&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2f0548cf39ea5dd6543b7508d29f40343228b923""&gt;https://preview.redd.it/i7d0uub729y41.png?width=2056&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2f0548cf39ea5dd6543b7508d29f40343228b923&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/vPzMKqP0_IRkFtq9qD8WpKpZblALMfpMsF6Iz2fSY6Y.jpg?auto=webp&amp;s=ccbadc0fef4ec32ba9efc4ff6540f055bac4f512', 'width': 1410, 'height': 676}, 'resolutions': [{'url': 'https://external-preview.redd.it/vPzMKqP0_IRkFtq9qD8WpKpZblALMfpMsF6Iz2fSY6Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=87cddf1bfafaceb966f1ee48f6ed0516ec89d7a5', 'width': 108, 'height': 51}, {'url': 'https://external-preview.redd.it/vPzMKqP0_IRkFtq9qD8WpKpZblALMfpMsF6Iz2fSY6Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=27f359c28ba2f84a5375deccbdc79af4ca52d22f', 'width': 216, 'height': 103}, {'url': 'https://external-preview.redd.it/vPzMKqP0_IRkFtq9qD8WpKpZblALMfpMsF6Iz2fSY6Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f3f7ccca827d28e838db9d3d683a4cf4407b0f2', 'width': 320, 'height': 153}, {'url': 'https://external-preview.redd.it/vPzMKqP0_IRkFtq9qD8WpKpZblALMfpMsF6Iz2fSY6Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=af70e405799a932937a5bea417d55217f7c28f2b', 'width': 640, 'height': 306}, {'url': 'https://external-preview.redd.it/vPzMKqP0_IRkFtq9qD8WpKpZblALMfpMsF6Iz2fSY6Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=70dfbca5af14ee47b7ded0aaf67c9dad0f716418', 'width': 960, 'height': 460}, {'url': 'https://external-preview.redd.it/vPzMKqP0_IRkFtq9qD8WpKpZblALMfpMsF6Iz2fSY6Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ba9f3984913d380441441ed8cc0ca2226a33193', 'width': 1080, 'height': 517}], 'variants': {}, 'id': 'wHSFYIhjkZOCYegUjg6VTXGFv2PKvRi7KZ5QY21rtNs'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gi2r09', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gi2r09/icymi_novel_approach_to_generating_highresolution/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gi2r09/icymi_novel_approach_to_generating_highresolution/', 'subreddit_subscribers': 6676, 'created_utc': 1589252549.0, 'num_crossposts': 13, 'media': None, 'is_video': False}]",t3_gi2r09,,,,,
568,,pytorch,,t2_44mbtmjy,False,,0,False,[News] Latest from MIT researchers: A new methodology for lidar super-resolution with ground vehicles,[],r/pytorch,False,6,,0,78.0,,False,t3_ghw7cr,False,dark,0.75,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/RAomtULrhieB0hPFmW3pMy6QO-vIKrCihd-1cVLDMQQ.jpg,False,,[],{},link,,False,,1589259085.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/-vW4I4RB8aG0xaVeWFUgI-FPNg5fqd-sqeMJZVzYGyc.jpg?auto=webp&amp;s=1df290554e1cadeeff6f7e624db62610a04d7f46', 'width': 1414, 'height': 272}, 'resolutions': [{'url': 'https://external-preview.redd.it/-vW4I4RB8aG0xaVeWFUgI-FPNg5fqd-sqeMJZVzYGyc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d549bdd97cbb7f9ff758578c26a9c1b25ece578', 'width': 108, 'height': 20}, {'url': 'https://external-preview.redd.it/-vW4I4RB8aG0xaVeWFUgI-FPNg5fqd-sqeMJZVzYGyc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2eba3b898a6f39ebb7ac0a27bb173c64ea28784e', 'width': 216, 'height': 41}, {'url': 'https://external-preview.redd.it/-vW4I4RB8aG0xaVeWFUgI-FPNg5fqd-sqeMJZVzYGyc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9048f6e49538f8f88abc1656befe4d79708aaccc', 'width': 320, 'height': 61}, {'url': 'https://external-preview.redd.it/-vW4I4RB8aG0xaVeWFUgI-FPNg5fqd-sqeMJZVzYGyc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd2b8912bce28f72228e13de3ffe8a65ec57d191', 'width': 640, 'height': 123}, {'url': 'https://external-preview.redd.it/-vW4I4RB8aG0xaVeWFUgI-FPNg5fqd-sqeMJZVzYGyc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b20a029ad14de35acb0fe0a3068ba0f2060682fa', 'width': 960, 'height': 184}, {'url': 'https://external-preview.redd.it/-vW4I4RB8aG0xaVeWFUgI-FPNg5fqd-sqeMJZVzYGyc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b7c556097f70a63d322cf4f7b4441264e0ffb908', 'width': 1080, 'height': 207}], 'variants': {}, 'id': 'F6xx6gG6vyQcZOVqh09um81M3QKMynTGGIr7xa-d1yM'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ghw7cr,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ghw7cr/news_latest_from_mit_researchers_a_new/,all_ads,False,/r/LatestInML/comments/ghw4y1/latest_from_mit_researchers_a_new_methodology_for/,7135,1589230285.0,0,,False,/r/LatestInML/comments/ghw4y1/latest_from_mit_researchers_a_new_methodology_for/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'Latest from MIT researchers: A new methodology for lidar super-resolution with ground vehicles\n\nFor project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2004.05242)\n\n&amp;#x200B;\n\nhttps://i.redd.it/bow9g8db77y41.gif\n\nTo increase the resolution of the point cloud captured by a sparse 3D lidar, they convert this problem from 3D Euclidean space into an image super-resolution problem in 2D image space, which is solved using a deep convolutional neural network', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Latest from MIT researchers: A new methodology for lidar super-resolution with ground vehicles', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 78, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'bow9g8db77y41': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/bow9g8db77y41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=99f8d3e486b35002085818690973033711ba9ffb'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/bow9g8db77y41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=c47510d2e00c09337494c537e6e854beacf98cc9'}, {'y': 180, 'x': 320, 'u': 'https://preview.redd.it/bow9g8db77y41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=2cee3535f7c91eaf51f53f4936af7ca65227d15b'}, {'y': 360, 'x': 640, 'u': 'https://preview.redd.it/bow9g8db77y41.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=cc969f33f38a7684ef0ba9f6461efec456938dc7'}, {'y': 540, 'x': 960, 'u': 'https://preview.redd.it/bow9g8db77y41.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=f6c508dd86a966bf9af35056f2de6d4326fca559'}, {'y': 607, 'x': 1080, 'u': 'https://preview.redd.it/bow9g8db77y41.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=a1265411fe37dafb5c9873549b2777d2f516477f'}], 's': {'y': 720, 'gif': 'https://i.redd.it/bow9g8db77y41.gif', 'mp4': 'https://preview.redd.it/bow9g8db77y41.gif?format=mp4&amp;s=fc3731d9d183e488b760708333e102be83d5de14', 'x': 1280}, 'id': 'bow9g8db77y41'}}, 'name': 't3_ghw4y1', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 37, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 37, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/RAomtULrhieB0hPFmW3pMy6QO-vIKrCihd-1cVLDMQQ.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589258844.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Latest from MIT researchers: A new methodology for lidar super-resolution with ground vehicles&lt;/p&gt;\n\n&lt;p&gt;For project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2004.05242""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://i.redd.it/bow9g8db77y41.gif""&gt;https://i.redd.it/bow9g8db77y41.gif&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To increase the resolution of the point cloud captured by a sparse 3D lidar, they convert this problem from 3D Euclidean space into an image super-resolution problem in 2D image space, which is solved using a deep convolutional neural network&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/-vW4I4RB8aG0xaVeWFUgI-FPNg5fqd-sqeMJZVzYGyc.jpg?auto=webp&amp;s=1df290554e1cadeeff6f7e624db62610a04d7f46', 'width': 1414, 'height': 272}, 'resolutions': [{'url': 'https://external-preview.redd.it/-vW4I4RB8aG0xaVeWFUgI-FPNg5fqd-sqeMJZVzYGyc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d549bdd97cbb7f9ff758578c26a9c1b25ece578', 'width': 108, 'height': 20}, {'url': 'https://external-preview.redd.it/-vW4I4RB8aG0xaVeWFUgI-FPNg5fqd-sqeMJZVzYGyc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2eba3b898a6f39ebb7ac0a27bb173c64ea28784e', 'width': 216, 'height': 41}, {'url': 'https://external-preview.redd.it/-vW4I4RB8aG0xaVeWFUgI-FPNg5fqd-sqeMJZVzYGyc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9048f6e49538f8f88abc1656befe4d79708aaccc', 'width': 320, 'height': 61}, {'url': 'https://external-preview.redd.it/-vW4I4RB8aG0xaVeWFUgI-FPNg5fqd-sqeMJZVzYGyc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd2b8912bce28f72228e13de3ffe8a65ec57d191', 'width': 640, 'height': 123}, {'url': 'https://external-preview.redd.it/-vW4I4RB8aG0xaVeWFUgI-FPNg5fqd-sqeMJZVzYGyc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b20a029ad14de35acb0fe0a3068ba0f2060682fa', 'width': 960, 'height': 184}, {'url': 'https://external-preview.redd.it/-vW4I4RB8aG0xaVeWFUgI-FPNg5fqd-sqeMJZVzYGyc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b7c556097f70a63d322cf4f7b4441264e0ffb908', 'width': 1080, 'height': 207}], 'variants': {}, 'id': 'F6xx6gG6vyQcZOVqh09um81M3QKMynTGGIr7xa-d1yM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ghw4y1', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/ghw4y1/latest_from_mit_researchers_a_new_methodology_for/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/ghw4y1/latest_from_mit_researchers_a_new_methodology_for/', 'subreddit_subscribers': 6676, 'created_utc': 1589230044.0, 'num_crossposts': 15, 'media': None, 'is_video': False}]",t3_ghw4y1,,,,,
569,,pytorch,,t2_44mbtmjy,False,,0,False,ICYMI: Real-world Masked Face Recognition Dataset (RMFRD) is currently the world's largest real-world masked face dataset,[],r/pytorch,False,6,,0,140.0,,False,t3_ghz9ny,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/98TytbbOsEj0_idcrgKBV1ZZyR4TULNC45zOvYtHbxE.jpg,False,,[],{},link,,False,,1589268992.0,text,6,,,text,self.LatestInML,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/UggCHdQzhz6lUoSLLgF4vUWqQK7FV05ojeB_RksUJgI.jpg?auto=webp&amp;s=549949998e8709161d43f631d0fa6fef3cf3f213', 'width': 698, 'height': 224}, 'resolutions': [{'url': 'https://external-preview.redd.it/UggCHdQzhz6lUoSLLgF4vUWqQK7FV05ojeB_RksUJgI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=989d92118436af9617768633c15cb725b0432c03', 'width': 108, 'height': 34}, {'url': 'https://external-preview.redd.it/UggCHdQzhz6lUoSLLgF4vUWqQK7FV05ojeB_RksUJgI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a3167073be512dc75cc90647a26c9e35629e4fe1', 'width': 216, 'height': 69}, {'url': 'https://external-preview.redd.it/UggCHdQzhz6lUoSLLgF4vUWqQK7FV05ojeB_RksUJgI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=173749d9bfaf753904a692432dbc76f325c99bfc', 'width': 320, 'height': 102}, {'url': 'https://external-preview.redd.it/UggCHdQzhz6lUoSLLgF4vUWqQK7FV05ojeB_RksUJgI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=64e727b0c2abe9e9e65d2991bdc84bef1719030c', 'width': 640, 'height': 205}], 'variants': {}, 'id': '48wiwKipo-t-bmZ24NK3JMWDVlavl_W_p6lXcgr7aeY'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ghz9ny,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ghz9ny/icymi_realworld_masked_face_recognition_dataset/,all_ads,False,/r/LatestInML/comments/ghyhc9/icymi_realworld_masked_face_recognition_dataset/,7135,1589240192.0,0,,False,/r/LatestInML/comments/ghyhc9/icymi_realworld_masked_face_recognition_dataset/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""ICYMI: Real-world Masked Face Recognition Dataset (RMFRD) is currently the world's largest real-world masked face dataset   \n\nFor project and dataset: [https://www.catalyzex.com/paper/arxiv:2003.09093](https://www.catalyzex.com/paper/arxiv:2003.09093)\n\n\n\nhttps://i.redd.it/qyyu2duyt7y41.gif\n\n The dataset includes 5,000 pictures of 525 people wearing masks and 90,000 images of the same 525 subjects without masks.\n\nThis can be used in grocery stores and other public places to check if people are wearing masks or not.\n\nConventional facial recognition technology is ineffective in many cases, such as community access control, face access control, facial attendance, facial security checks at train stations, etc."", 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""ICYMI: Real-world Masked Face Recognition Dataset (RMFRD) is currently the world's largest real-world masked face dataset"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'qyyu2duyt7y41': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 108, 'x': 108, 'u': 'https://preview.redd.it/qyyu2duyt7y41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=659aaf4733d3162f2ff72d0ce208bd19d5578114'}, {'y': 216, 'x': 216, 'u': 'https://preview.redd.it/qyyu2duyt7y41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=825e0628122a712c2c70d53308c6f18d2eb311ba'}], 's': {'y': 256, 'gif': 'https://i.redd.it/qyyu2duyt7y41.gif', 'mp4': 'https://preview.redd.it/qyyu2duyt7y41.gif?format=mp4&amp;s=0be90dd72053e7a9cc5d58e63dd64746533f5bbd', 'x': 256}, 'id': 'qyyu2duyt7y41'}}, 'name': 't3_ghyhc9', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.56, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 1, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/98TytbbOsEj0_idcrgKBV1ZZyR4TULNC45zOvYtHbxE.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589266466.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;ICYMI: Real-world Masked Face Recognition Dataset (RMFRD) is currently the world&amp;#39;s largest real-world masked face dataset   &lt;/p&gt;\n\n&lt;p&gt;For project and dataset: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2003.09093""&gt;https://www.catalyzex.com/paper/arxiv:2003.09093&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://i.redd.it/qyyu2duyt7y41.gif""&gt;https://i.redd.it/qyyu2duyt7y41.gif&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The dataset includes 5,000 pictures of 525 people wearing masks and 90,000 images of the same 525 subjects without masks.&lt;/p&gt;\n\n&lt;p&gt;This can be used in grocery stores and other public places to check if people are wearing masks or not.&lt;/p&gt;\n\n&lt;p&gt;Conventional facial recognition technology is ineffective in many cases, such as community access control, face access control, facial attendance, facial security checks at train stations, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/UggCHdQzhz6lUoSLLgF4vUWqQK7FV05ojeB_RksUJgI.jpg?auto=webp&amp;s=549949998e8709161d43f631d0fa6fef3cf3f213', 'width': 698, 'height': 224}, 'resolutions': [{'url': 'https://external-preview.redd.it/UggCHdQzhz6lUoSLLgF4vUWqQK7FV05ojeB_RksUJgI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=989d92118436af9617768633c15cb725b0432c03', 'width': 108, 'height': 34}, {'url': 'https://external-preview.redd.it/UggCHdQzhz6lUoSLLgF4vUWqQK7FV05ojeB_RksUJgI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a3167073be512dc75cc90647a26c9e35629e4fe1', 'width': 216, 'height': 69}, {'url': 'https://external-preview.redd.it/UggCHdQzhz6lUoSLLgF4vUWqQK7FV05ojeB_RksUJgI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=173749d9bfaf753904a692432dbc76f325c99bfc', 'width': 320, 'height': 102}, {'url': 'https://external-preview.redd.it/UggCHdQzhz6lUoSLLgF4vUWqQK7FV05ojeB_RksUJgI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=64e727b0c2abe9e9e65d2991bdc84bef1719030c', 'width': 640, 'height': 205}], 'variants': {}, 'id': '48wiwKipo-t-bmZ24NK3JMWDVlavl_W_p6lXcgr7aeY'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ghyhc9', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/ghyhc9/icymi_realworld_masked_face_recognition_dataset/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/ghyhc9/icymi_realworld_masked_face_recognition_dataset/', 'subreddit_subscribers': 6676, 'created_utc': 1589237666.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_ghyhc9,,,,,
570,,pytorch,,t2_2o7eaff,False,,0,False,Python 3.9 - The Shape of Things to Come,[],r/pytorch,False,6,,0,78.0,,False,t3_ghil1p,False,dark,0.58,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/29I1aeH0h_Q6Zr7ztb_16mH_-K-Q1x0l6xgynfQuvPg.jpg,False,,[],{},link,,False,,1589208757.0,text,6,,,text,rubikscode.net,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/f-1Q16uswj-ildS8G25TZvND9psGZvnEFp2hZryE444.jpg?auto=webp&amp;s=5710bb17ffaf9650058f9228f8e8f3ffde8f52ee', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/f-1Q16uswj-ildS8G25TZvND9psGZvnEFp2hZryE444.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9749ff03b667f0826133616b759629c8f633d11', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/f-1Q16uswj-ildS8G25TZvND9psGZvnEFp2hZryE444.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5fb645f1315fbc1a4ae39637b829735ba6cab0e', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/f-1Q16uswj-ildS8G25TZvND9psGZvnEFp2hZryE444.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c142976f63e3b3c04be4cb3ce5e5723eb32c5112', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/f-1Q16uswj-ildS8G25TZvND9psGZvnEFp2hZryE444.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c8a4479ceae50e6cc9dd586b0a3159818cedb9b5', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/f-1Q16uswj-ildS8G25TZvND9psGZvnEFp2hZryE444.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=202359f71d79c24c98404f6a3df81599ea319199', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/f-1Q16uswj-ildS8G25TZvND9psGZvnEFp2hZryE444.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=45a99e3fe6acc428b2c6d0bb576112019f223e74', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'Imm0LmjsPSFmb7dpjVcXCm_9Kdmq0uLlfAYV4pHRuZ8'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ghil1p,True,,RubiksCodeNMZ,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ghil1p/python_39_the_shape_of_things_to_come/,all_ads,False,https://rubikscode.net/2020/05/11/python-3-9-the-shape-of-things-to-come/,7135,1589179957.0,0,,False,https://rubikscode.net/2020/05/11/python-3-9-the-shape-of-things-to-come/,,,,,,,
571,,pytorch,"# MapExtrackt

**Pytorch feature Visualization**

I've created a python package that makes viewing the outputs of layers of a CNN easy.

You can extract layers at a time, single cells, or a range of cells.

Pretty interesting to see what might be going on inside your CNN.

If any one is interested it can be found here - [MapExtrackt](https://pypi.org/project/mapextrackt/)

&amp;#x200B;

https://preview.redd.it/vblfnchyh3y41.jpg?width=1500&amp;format=pjpg&amp;auto=webp&amp;s=85f2e3e7a5694114fc8141f2ccdb91d6223698be

https://preview.redd.it/5cf4bihyh3y41.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp;s=535f00a019c87f72cc56c455b480a24ec748698c

https://preview.redd.it/7q6r4fhyh3y41.jpg?width=400&amp;format=pjpg&amp;auto=webp&amp;s=ddc5d07d07dbd788773c34b8c9210e4068f950dc

https://preview.redd.it/g2vx0ohyh3y41.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=67e8042c027ec5f09dd7c518bf7c96c2bf13da2d

https://preview.redd.it/dmga3ohyh3y41.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp;s=78e6a312702c965ce6ff58e7be517f65fbaae2db

https://preview.redd.it/x4ledghyh3y41.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp;s=b1e326d7d95f1f8a1f6ecefb40bae754180b1de1

&amp;#x200B;",t2_3ytwd1a8,False,,0,False,Pytorch Feature Extraction - Visualization [P],[],r/pytorch,False,6,,0,140.0,,False,t3_ggmn21,False,dark,0.95,,public,14,0,{},140.0,,False,[],,False,False,,{},,False,14,,False,https://b.thumbs.redditmedia.com/7LvnwndXJ1XsLhQC-sXsZHNbwTcO8HXSaIRkm0qrVvY.jpg,1589185193.0,,[],{},self,,True,,1589083705.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h1&gt;MapExtrackt&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Pytorch feature Visualization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve created a python package that makes viewing the outputs of layers of a CNN easy.&lt;/p&gt;

&lt;p&gt;You can extract layers at a time, single cells, or a range of cells.&lt;/p&gt;

&lt;p&gt;Pretty interesting to see what might be going on inside your CNN.&lt;/p&gt;

&lt;p&gt;If any one is interested it can be found here - &lt;a href=""https://pypi.org/project/mapextrackt/""&gt;MapExtrackt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/vblfnchyh3y41.jpg?width=1500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=85f2e3e7a5694114fc8141f2ccdb91d6223698be""&gt;https://preview.redd.it/vblfnchyh3y41.jpg?width=1500&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=85f2e3e7a5694114fc8141f2ccdb91d6223698be&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/5cf4bihyh3y41.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=535f00a019c87f72cc56c455b480a24ec748698c""&gt;https://preview.redd.it/5cf4bihyh3y41.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=535f00a019c87f72cc56c455b480a24ec748698c&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/7q6r4fhyh3y41.jpg?width=400&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ddc5d07d07dbd788773c34b8c9210e4068f950dc""&gt;https://preview.redd.it/7q6r4fhyh3y41.jpg?width=400&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ddc5d07d07dbd788773c34b8c9210e4068f950dc&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/g2vx0ohyh3y41.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=67e8042c027ec5f09dd7c518bf7c96c2bf13da2d""&gt;https://preview.redd.it/g2vx0ohyh3y41.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=67e8042c027ec5f09dd7c518bf7c96c2bf13da2d&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/dmga3ohyh3y41.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=78e6a312702c965ce6ff58e7be517f65fbaae2db""&gt;https://preview.redd.it/dmga3ohyh3y41.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=78e6a312702c965ce6ff58e7be517f65fbaae2db&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/x4ledghyh3y41.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b1e326d7d95f1f8a1f6ecefb40bae754180b1de1""&gt;https://preview.redd.it/x4ledghyh3y41.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b1e326d7d95f1f8a1f6ecefb40bae754180b1de1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/p3HQXQzdkmyXyJ89enL6PkgmSdstFY5z1QkzOzRNUaU.jpg?auto=webp&amp;s=01b29ed2d2e90d072e1fc7295da2c1cb3797f686', 'width': 300, 'height': 300}, 'resolutions': [{'url': 'https://external-preview.redd.it/p3HQXQzdkmyXyJ89enL6PkgmSdstFY5z1QkzOzRNUaU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=54fdfef1cb192ed04e4ba25828970287fc1ecde9', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/p3HQXQzdkmyXyJ89enL6PkgmSdstFY5z1QkzOzRNUaU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=22ff535b36cf2b9412be48a21108ba34479e2ba5', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'POVR4AtJHvry29k6WQQwgYhMSdLrOeYwBodMqA6lPGk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ggmn21,True,,LewisgMorris,,3,True,all_ads,False,[],False,,/r/pytorch/comments/ggmn21/pytorch_feature_extraction_visualization_p/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ggmn21/pytorch_feature_extraction_visualization_p/,7135,1589054905.0,0,,False,,,,"{'dmga3ohyh3y41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 81, 'x': 108, 'u': 'https://preview.redd.it/dmga3ohyh3y41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d10e6c03778162784af309ac9aeeb43647d483af'}, {'y': 162, 'x': 216, 'u': 'https://preview.redd.it/dmga3ohyh3y41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2322afcd3ad3e9a9f2926f856536790b6ae87c6b'}, {'y': 240, 'x': 320, 'u': 'https://preview.redd.it/dmga3ohyh3y41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ca2bcc2811348e94e21779766b70e7406158d1ad'}, {'y': 480, 'x': 640, 'u': 'https://preview.redd.it/dmga3ohyh3y41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7b747e7ceaac11cdeef2abb75ae8319cdc8a8981'}], 's': {'y': 600, 'x': 800, 'u': 'https://preview.redd.it/dmga3ohyh3y41.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp;s=78e6a312702c965ce6ff58e7be517f65fbaae2db'}, 'id': 'dmga3ohyh3y41'}, 'vblfnchyh3y41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 21, 'x': 108, 'u': 'https://preview.redd.it/vblfnchyh3y41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=61bdd942a64724a8b2e0fdbd71045d2ba8a375d3'}, {'y': 43, 'x': 216, 'u': 'https://preview.redd.it/vblfnchyh3y41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8956b892822e906130abadb0fde7314ec192d8d7'}, {'y': 64, 'x': 320, 'u': 'https://preview.redd.it/vblfnchyh3y41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e4f22b9ebbe34bcbdb3dc7c3962ea9731a32eb1b'}, {'y': 128, 'x': 640, 'u': 'https://preview.redd.it/vblfnchyh3y41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d0272be767a3f0867208241f8778814decf8e1d'}, {'y': 192, 'x': 960, 'u': 'https://preview.redd.it/vblfnchyh3y41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=47b0b10756b7c01d048d836a8c3dbe09fdf8e6fa'}, {'y': 216, 'x': 1080, 'u': 'https://preview.redd.it/vblfnchyh3y41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bd85a72dde05ead32228f7c30ac3ee3b2ee0557d'}], 's': {'y': 300, 'x': 1500, 'u': 'https://preview.redd.it/vblfnchyh3y41.jpg?width=1500&amp;format=pjpg&amp;auto=webp&amp;s=85f2e3e7a5694114fc8141f2ccdb91d6223698be'}, 'id': 'vblfnchyh3y41'}, 'g2vx0ohyh3y41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/g2vx0ohyh3y41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=75a8b1aef41664149899f89db762d774a3d06eb9'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/g2vx0ohyh3y41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f352e84c85de6243530d65e1f719d67d4cbf61d6'}, {'y': 180, 'x': 320, 'u': 'https://preview.redd.it/g2vx0ohyh3y41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9e3e0653b0a3e5caefffd582ab3b436a0f1df65'}, {'y': 360, 'x': 640, 'u': 'https://preview.redd.it/g2vx0ohyh3y41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2b250011cc5a2d3f6f771aa13f2b2fa1bcee0e7f'}, {'y': 540, 'x': 960, 'u': 'https://preview.redd.it/g2vx0ohyh3y41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9ecabf4f2c3d44a24975fef64a4075195d7eac3b'}, {'y': 607, 'x': 1080, 'u': 'https://preview.redd.it/g2vx0ohyh3y41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ea157098c67fa8c5bcbd3d50d85745675237b310'}], 's': {'y': 1080, 'x': 1920, 'u': 'https://preview.redd.it/g2vx0ohyh3y41.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=67e8042c027ec5f09dd7c518bf7c96c2bf13da2d'}, 'id': 'g2vx0ohyh3y41'}, 'x4ledghyh3y41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 81, 'x': 108, 'u': 'https://preview.redd.it/x4ledghyh3y41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ca04811f4e111b6236169076ea733f1678322b67'}, {'y': 162, 'x': 216, 'u': 'https://preview.redd.it/x4ledghyh3y41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8ef98703dd5e9fa5897e10972e814445be242c6c'}, {'y': 240, 'x': 320, 'u': 'https://preview.redd.it/x4ledghyh3y41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ccc80b11ba00e387fe45d8712d69eac93b0cc33'}, {'y': 480, 'x': 640, 'u': 'https://preview.redd.it/x4ledghyh3y41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a17c978639965e2f9921fe5255d5fe961648e1f'}], 's': {'y': 600, 'x': 800, 'u': 'https://preview.redd.it/x4ledghyh3y41.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp;s=b1e326d7d95f1f8a1f6ecefb40bae754180b1de1'}, 'id': 'x4ledghyh3y41'}, '5cf4bihyh3y41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 108, 'x': 108, 'u': 'https://preview.redd.it/5cf4bihyh3y41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a662aa78dd2f9260ee1167eed4e1b9e2c3819317'}, {'y': 216, 'x': 216, 'u': 'https://preview.redd.it/5cf4bihyh3y41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=efd010c875351367df936db105fc38dedbfde1ec'}, {'y': 320, 'x': 320, 'u': 'https://preview.redd.it/5cf4bihyh3y41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0b3fac6b014e94692873147cead8ed3cfdd097be'}, {'y': 640, 'x': 640, 'u': 'https://preview.redd.it/5cf4bihyh3y41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=062a7753e2c99e2583ff843b30387393f1eca34f'}], 's': {'y': 800, 'x': 800, 'u': 'https://preview.redd.it/5cf4bihyh3y41.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp;s=535f00a019c87f72cc56c455b480a24ec748698c'}, 'id': '5cf4bihyh3y41'}, '7q6r4fhyh3y41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 108, 'x': 108, 'u': 'https://preview.redd.it/7q6r4fhyh3y41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f3c7f47bf5897345ed30c5641cc61b4cb13e3df9'}, {'y': 216, 'x': 216, 'u': 'https://preview.redd.it/7q6r4fhyh3y41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d23098e87363ae48733e784a0e86c1784eebc3c3'}, {'y': 320, 'x': 320, 'u': 'https://preview.redd.it/7q6r4fhyh3y41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=af8de9a105821ac13c305e938426e794725f8d61'}], 's': {'y': 400, 'x': 400, 'u': 'https://preview.redd.it/7q6r4fhyh3y41.jpg?width=400&amp;format=pjpg&amp;auto=webp&amp;s=ddc5d07d07dbd788773c34b8c9210e4068f950dc'}, 'id': '7q6r4fhyh3y41'}}",,,,
572,,pytorch,"Hi..

I want to convert a Restricted boltzman machine model (code in python)  from TensorFlow to PyTorch. Is there any tool or software that can be  helpful?",t2_1qfa83l3,False,,0,False,How do I convert code written in TensorFlow to PyTorch?,[],r/pytorch,False,6,,0,,,False,t3_ggmcdk,False,dark,0.89,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1589082772.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi..&lt;/p&gt;

&lt;p&gt;I want to convert a Restricted boltzman machine model (code in python)  from TensorFlow to PyTorch. Is there any tool or software that can be  helpful?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ggmcdk,True,,sherlock_flash,,7,True,all_ads,False,[],False,,/r/pytorch/comments/ggmcdk/how_do_i_convert_code_written_in_tensorflow_to/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ggmcdk/how_do_i_convert_code_written_in_tensorflow_to/,7135,1589053972.0,0,,False,,,,,,,,
573,,pytorch,"I think I know the answer to this (""no"") but I wanted to see if someone would confirm this, as I have no experience with these kinds of tools. I'm on macOS Mojave with Intel GPU.

I was interested in setting up OpenAI's Jukebox ([github](https://github.com/openai/jukebox/)) but I hit a roadblock here:

    conda install pytorch=1.4 torchvision=0.5 cudatoolkit=10.0 -c pytorch

It looks like, one, you need to build pytorch from source on mac for CUDA support, and two, I would need an Nvidia GPU.

Am I out of luck? Maybe I should be building a pc anyways for this kind of thing...",t2_1o7i0529,False,,0,False,Pytorch with CUDA on mac,[],r/pytorch,False,6,,0,,,False,t3_ggg6mf,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1589062749.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I think I know the answer to this (&amp;quot;no&amp;quot;) but I wanted to see if someone would confirm this, as I have no experience with these kinds of tools. I&amp;#39;m on macOS Mojave with Intel GPU.&lt;/p&gt;

&lt;p&gt;I was interested in setting up OpenAI&amp;#39;s Jukebox (&lt;a href=""https://github.com/openai/jukebox/""&gt;github&lt;/a&gt;) but I hit a roadblock here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda install pytorch=1.4 torchvision=0.5 cudatoolkit=10.0 -c pytorch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It looks like, one, you need to build pytorch from source on mac for CUDA support, and two, I would need an Nvidia GPU.&lt;/p&gt;

&lt;p&gt;Am I out of luck? Maybe I should be building a pc anyways for this kind of thing...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DZ7Hqtu8pacE0RH3j35lvbGP5nfugXvx4BZYutIgkUQ.jpg?auto=webp&amp;s=e790dfd22ddf29a4faf92d7bd2985bac28e32e28', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/DZ7Hqtu8pacE0RH3j35lvbGP5nfugXvx4BZYutIgkUQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cddfafccceb971dceea8e590068a6b05daf3cd6a', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/DZ7Hqtu8pacE0RH3j35lvbGP5nfugXvx4BZYutIgkUQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=820112006878142f35d7fdf09ed6aadb0f609aa6', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/DZ7Hqtu8pacE0RH3j35lvbGP5nfugXvx4BZYutIgkUQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6a819b7dfd1139f626c559dddadd25b9aedc5953', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'rmkuBneBGTGepgZx-zZwrobffFPKaUKQxumwWo3FGHQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ggg6mf,True,,tv_room,,3,True,all_ads,False,[],False,,/r/pytorch/comments/ggg6mf/pytorch_with_cuda_on_mac/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ggg6mf/pytorch_with_cuda_on_mac/,7135,1589033949.0,0,,False,,,,,,,,
574,,pytorch,,t2_44mbtmjy,False,,0,False,From CVPR '20: High-Fidelity 3D Face Reconstruction,[],r/pytorch,False,6,,0,55.0,,False,t3_gg8qxy,False,dark,0.84,,public,4,0,{},140.0,,False,[],,False,False,,{},,False,4,,False,https://a.thumbs.redditmedia.com/X812G8Juom6hky5SWddMRxxU2OXoaa7oOmn42ar7Ky4.jpg,False,,[],{},link,,False,,1589027214.0,text,6,,,text,self.LatestInML,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?auto=webp&amp;s=6225767bb348a765a9bf7d2773be244a42a8fc78', 'width': 594, 'height': 338}, 'resolutions': [{'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0dd3ce83c20ba2090d9b6f7d70601f5dd8b077b', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5228bdf69e06e26b78ceae9944e24b6a372e2cad', 'width': 216, 'height': 122}, {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=60e25d55d7e1dd48a99d40907ff417d0c0f8a7f8', 'width': 320, 'height': 182}], 'variants': {}, 'id': 'KVXM33i4nWnl2TFfCz8N6JSXlCNimw6gLao5I5QC0gc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gg8qxy,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gg8qxy/from_cvpr_20_highfidelity_3d_face_reconstruction/,all_ads,False,/r/LatestInML/comments/gg8qay/from_cvpr_20_highfidelity_3d_face_reconstruction/,7135,1588998414.0,0,,False,/r/LatestInML/comments/gg8qay/from_cvpr_20_highfidelity_3d_face_reconstruction/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2003.05653?fbclid=IwAR1JquJ4bBWX7VfUGm78NJqOPKAyF1O7XOkU_l0k1xT7rg6nm06VueJVoGI)\n\nhttps://preview.redd.it/81ta0o3c2ox41.png?width=1898&amp;format=png&amp;auto=webp&amp;s=35dbcac798fe7ec62fd8f82e13b826f0081c7d2e\n\nmain idea is to refine the initial texture generated by a 3DMM based method with facial details from the input image', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""From CVPR '20: High-Fidelity 3D Face Reconstruction"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 55, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'81ta0o3c2ox41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 43, 'x': 108, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=55c32367cf95f7e6fa654cbf786e32d01ecd2dcc'}, {'y': 86, 'x': 216, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f885fc37a7b8df2c81d03bc56b51edb2a591a7ab'}, {'y': 127, 'x': 320, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f7b2695c338c17666bf413cccda5684a106c7efa'}, {'y': 255, 'x': 640, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=46fba7b645462a6a2fcbd3bd624b5df9e9aefdec'}, {'y': 383, 'x': 960, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b8d855cfc2c0ccc5d012b462f5e9d1d7fe03c2c'}, {'y': 431, 'x': 1080, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=21026136b7f8cca8faf9058e0a868ef6c6a1143d'}], 's': {'y': 758, 'x': 1898, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=1898&amp;format=png&amp;auto=webp&amp;s=35dbcac798fe7ec62fd8f82e13b826f0081c7d2e'}, 'id': '81ta0o3c2ox41'}}, 'name': 't3_gg8qay', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 10, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/X812G8Juom6hky5SWddMRxxU2OXoaa7oOmn42ar7Ky4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589027133.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2003.05653?fbclid=IwAR1JquJ4bBWX7VfUGm78NJqOPKAyF1O7XOkU_l0k1xT7rg6nm06VueJVoGI""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/81ta0o3c2ox41.png?width=1898&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35dbcac798fe7ec62fd8f82e13b826f0081c7d2e""&gt;https://preview.redd.it/81ta0o3c2ox41.png?width=1898&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35dbcac798fe7ec62fd8f82e13b826f0081c7d2e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;main idea is to refine the initial texture generated by a 3DMM based method with facial details from the input image&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?auto=webp&amp;s=6225767bb348a765a9bf7d2773be244a42a8fc78', 'width': 594, 'height': 338}, 'resolutions': [{'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0dd3ce83c20ba2090d9b6f7d70601f5dd8b077b', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5228bdf69e06e26b78ceae9944e24b6a372e2cad', 'width': 216, 'height': 122}, {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=60e25d55d7e1dd48a99d40907ff417d0c0f8a7f8', 'width': 320, 'height': 182}], 'variants': {}, 'id': 'KVXM33i4nWnl2TFfCz8N6JSXlCNimw6gLao5I5QC0gc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gg8qay', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gg8qay/from_cvpr_20_highfidelity_3d_face_reconstruction/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gg8qay/from_cvpr_20_highfidelity_3d_face_reconstruction/', 'subreddit_subscribers': 6676, 'created_utc': 1588998333.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_gg8qay,,,,,
575,,pytorch,,t2_44mbtmjy,False,,0,False,Bring Old Photos Back to Life,[],r/pytorch,False,6,,0,57.0,,False,t3_gg3x4j,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/l_xQrQGuMxwpN5G0aUWM9-OGZoi0zRHs6y5KQn9qXps.jpg,False,,[],{},link,,False,,1589008279.0,text,6,,,text,self.LatestInML,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?auto=webp&amp;s=3ee6961d88445c2d066121eaff6b23559a952474', 'width': 654, 'height': 348}, 'resolutions': [{'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f3851843805287ba92edaff8f11f3ce2b26ffa4e', 'width': 108, 'height': 57}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=568754b724e494b26903388e154799a13b2fae1c', 'width': 216, 'height': 114}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4829d78277bf3af52eafcf8db8ccab736992f513', 'width': 320, 'height': 170}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db342fc2adc7956d97654fcfe9fb6945789d5447', 'width': 640, 'height': 340}], 'variants': {}, 'id': 'JLDhPQFAIizsdivC3iTOBXsQt1gQDs0ndz-oTKqUcI0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gg3x4j,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gg3x4j/bring_old_photos_back_to_life/,all_ads,False,/r/LatestInML/comments/gg3rac/bring_old_photos_back_to_life/,7135,1588979479.0,0,,False,/r/LatestInML/comments/gg3rac/bring_old_photos_back_to_life/,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'From researchers at Microsoft, City University of Hong Kong, University of Science and Technology of China:\n\n**Bring Old Photos Back to Life**       \nFor project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2004.09484)\n\nhttps://preview.redd.it/q9cz19sjgmx41.png?width=1352&amp;format=png&amp;auto=webp&amp;s=4c9b8f4cac19e2c8a424927e566ccbd942ed6a4b\n\nThis new method can handle the complex degradation mixed by both unstructured and structured defects in real old photos', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Bring Old Photos Back to Life', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 57, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'q9cz19sjgmx41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 44, 'x': 108, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5a0301ab1954fe92d14162ef010d5594c2163ac'}, {'y': 88, 'x': 216, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b15b420753d4d84a5c4adede4fabd0ca0ee41f8b'}, {'y': 130, 'x': 320, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=571432baf4e3158e45084882f8b87bc44d31b982'}, {'y': 261, 'x': 640, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=56dce9805fac99c032f0a6dcd382a6575a08ee5b'}, {'y': 391, 'x': 960, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c3f3cd35f64b66f1c381fe8a349e4ba144e069f'}, {'y': 440, 'x': 1080, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f9e19d9f978923cc491fa3295836356d1888206'}], 's': {'y': 552, 'x': 1352, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=1352&amp;format=png&amp;auto=webp&amp;s=4c9b8f4cac19e2c8a424927e566ccbd942ed6a4b'}, 'id': 'q9cz19sjgmx41'}}, 'name': 't3_gg3rac', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 45, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 45, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/l_xQrQGuMxwpN5G0aUWM9-OGZoi0zRHs6y5KQn9qXps.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589007729.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;From researchers at Microsoft, City University of Hong Kong, University of Science and Technology of China:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Bring Old Photos Back to Life&lt;/strong&gt;&lt;br/&gt;\nFor project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2004.09484""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/q9cz19sjgmx41.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c9b8f4cac19e2c8a424927e566ccbd942ed6a4b""&gt;https://preview.redd.it/q9cz19sjgmx41.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c9b8f4cac19e2c8a424927e566ccbd942ed6a4b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This new method can handle the complex degradation mixed by both unstructured and structured defects in real old photos&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?auto=webp&amp;s=3ee6961d88445c2d066121eaff6b23559a952474', 'width': 654, 'height': 348}, 'resolutions': [{'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f3851843805287ba92edaff8f11f3ce2b26ffa4e', 'width': 108, 'height': 57}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=568754b724e494b26903388e154799a13b2fae1c', 'width': 216, 'height': 114}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4829d78277bf3af52eafcf8db8ccab736992f513', 'width': 320, 'height': 170}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db342fc2adc7956d97654fcfe9fb6945789d5447', 'width': 640, 'height': 340}], 'variants': {}, 'id': 'JLDhPQFAIizsdivC3iTOBXsQt1gQDs0ndz-oTKqUcI0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gg3rac', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gg3rac/bring_old_photos_back_to_life/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gg3rac/bring_old_photos_back_to_life/', 'subreddit_subscribers': 6676, 'created_utc': 1588978929.0, 'num_crossposts': 16, 'media': None, 'is_video': False}]",t3_gg3rac,,,,,
576,,pytorch,,t2_2o7eaff,False,,0,False,This Week in AI - Issue #17 | Rubik's Code,[],r/pytorch,False,6,,0,78.0,,False,t3_gfotac,False,dark,0.25,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/q40TYZVgvDuVIhiCMY2LGrrwPV64Jh7cItGv36aVymY.jpg,False,,[],{},link,,False,,1588953131.0,text,6,,,text,rubikscode.net,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?auto=webp&amp;s=aba15a8e08da24b0f366ff7e94f7f49c652884e3', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19f794df176d742445e70a834f02073618a034a8', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bbc2caf7263a43928c2f96facf6709c61c560994', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba4603038973138eb62c7d4c3e483b55662d27a9', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c5118555c9f824b549676e01773b5ef1341a3b0c', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4bdb1652f7804251c04d9290f584d7a00cd93d38', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=238aef893d35150be90c75468911d2d706a4e494', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'A1f3svXfhPHnYTvqZdu1PaAFyL18pTPA-Iq6T1_JU48'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gfotac,True,,RubiksCodeNMZ,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gfotac/this_week_in_ai_issue_17_rubiks_code/,all_ads,False,https://rubikscode.net/2020/05/08/this-week-in-ai-issue-17/,7135,1588924331.0,0,,False,https://rubikscode.net/2020/05/08/this-week-in-ai-issue-17/,,,,,,,
577,,pytorch,,t2_2o7eaff,False,,0,False,Top 15 AI Articles You Should Read This Month - April 2020,[],r/pytorch,False,6,,0,78.0,,False,t3_gdt204,False,dark,0.82,,public,7,0,{},140.0,,False,[],,False,False,,{},,False,7,,False,https://b.thumbs.redditmedia.com/1_teSgzEAE5waskdxrtZPZ02ZA4wHpa_hByOUiiL61s.jpg,False,,[],{},link,,False,,1588691641.0,text,6,,,text,rubikscode.net,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/bM0Vm_Ci6tVmJ1zpR-tssHgxdQ-jcs7glPsqaO2FApk.jpg?auto=webp&amp;s=f5997d281c56b90a5a72c5ad3dbf7034c6f4a6c0', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/bM0Vm_Ci6tVmJ1zpR-tssHgxdQ-jcs7glPsqaO2FApk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2742b4b7bfd5a8cd33349217fd6364e3abcbb607', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/bM0Vm_Ci6tVmJ1zpR-tssHgxdQ-jcs7glPsqaO2FApk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=12b7f30c0e6fee5d83be55500f6f6dc7be461ed5', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/bM0Vm_Ci6tVmJ1zpR-tssHgxdQ-jcs7glPsqaO2FApk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1bd8cdd7939b8ecfdb887523e98ed0ee6300d8ec', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/bM0Vm_Ci6tVmJ1zpR-tssHgxdQ-jcs7glPsqaO2FApk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=43243088e33eb87712c3d919f1c59c73c74da86a', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/bM0Vm_Ci6tVmJ1zpR-tssHgxdQ-jcs7glPsqaO2FApk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9eba99bfa4ae447b10c8d6cc16595ffa8502cb63', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/bM0Vm_Ci6tVmJ1zpR-tssHgxdQ-jcs7glPsqaO2FApk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=59038531ba91bfa2706079e1cc72019e508267f7', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'geLripYatV7zGGZmga9Dut91-FINjUsMdKiFB53T-oI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gdt204,True,,RubiksCodeNMZ,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gdt204/top_15_ai_articles_you_should_read_this_month/,all_ads,False,https://rubikscode.net/2020/05/05/top-15-ai-articles-you-should-read-this-month-april-2020/,7135,1588662841.0,0,,False,https://rubikscode.net/2020/05/05/top-15-ai-articles-you-should-read-this-month-april-2020/,,,,,,,
578,,pytorch,Is there any github repo for face detection pytorch using simple CNN?,t2_5q7yjzhc,False,,0,False,Face Detection Pytorch,[],r/pytorch,False,6,,0,,,False,t3_gdta8w,False,dark,0.99,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588692842.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there any github repo for face detection pytorch using simple CNN?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gdta8w,True,,Alakhator,,2,True,all_ads,False,[],False,,/r/pytorch/comments/gdta8w/face_detection_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gdta8w/face_detection_pytorch/,7135,1588664042.0,0,,False,,,,,,,,
579,,pytorch,,t2_2o7eaff,False,,0,False,Top 3 Artificial Intelligence Research Papers – April 2020,[],r/pytorch,False,6,,0,78.0,,False,t3_gd6rih,False,dark,0.85,,public,10,0,{},140.0,,False,[],,False,False,,{},,False,10,,False,https://b.thumbs.redditmedia.com/JFB1oUJj4IvRHG7t4JCzS2dkNumRFOJ7YqGCapGP54E.jpg,False,,[],{},link,,False,,1588604668.0,text,6,,,text,rubikscode.net,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/uOkNXGre62VhQZELf8PlFKocZyHbD3pCYZH2wCdleX0.jpg?auto=webp&amp;s=09749d9a7b1d66dfe0cdd39ea59eced328f3befa', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/uOkNXGre62VhQZELf8PlFKocZyHbD3pCYZH2wCdleX0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3ec41c3962c646c0e3926b2d07ca09620bc6ecf7', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/uOkNXGre62VhQZELf8PlFKocZyHbD3pCYZH2wCdleX0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=64a3b0ebbea48e15a41f50c04c65b7e5e55875ff', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/uOkNXGre62VhQZELf8PlFKocZyHbD3pCYZH2wCdleX0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b0c965d6174cba09fbba6ca15b2a51591126894', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/uOkNXGre62VhQZELf8PlFKocZyHbD3pCYZH2wCdleX0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cb8c9cf6862a079ded94de89ae11c670f3b39c70', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/uOkNXGre62VhQZELf8PlFKocZyHbD3pCYZH2wCdleX0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5b6c5f91afb4350536ec21ebbe00c9bdc7b3af93', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/uOkNXGre62VhQZELf8PlFKocZyHbD3pCYZH2wCdleX0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cf172fd709e910281cf7b610c20d8c35ead8405a', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'AHDNEJRkyd83a4EEQf3oHiGwH8zikjd4XlMIVSpyYvk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gd6rih,True,,RubiksCodeNMZ,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gd6rih/top_3_artificial_intelligence_research_papers/,all_ads,False,https://rubikscode.net/2020/05/04/top-3-artificial-intelligence-research-papers-april-2020/,7135,1588575868.0,0,,False,https://rubikscode.net/2020/05/04/top-3-artificial-intelligence-research-papers-april-2020/,,,,,,,
580,,pytorch,"I am implementing [SCAE](https://www.sciencedirect.com/science/article/abs/pii/S0169260719300690). Which is a type of Convolutional Autoencoder

One of the operations in the decoder network is a Max Up-Pool layer (technically it is not a layer as it does not have wrights). Max Up Pool adds zeros at alternate positions in the upscaled tensor. This can be illustrated below:  


https://preview.redd.it/mj9lrhd44iw41.png?width=278&amp;format=png&amp;auto=webp&amp;s=bd6488ba806de675e743c8556c8d46f87a53f116

&amp;#x200B;

My input tensor’s shape is 1\* 4 \* 64  and I need to upsample it to 1\* 4 \* 128  shape using the Max Up-pooing. I made the below function to perform max up pooling:

    def maxUpPool(self, inpTensor):     
        up_sampled = torch.zeros(inpTensor.shape[0], inpTensor.shape[1], inpTensor.shape[2] * 2)     
        up_sampled[:, :, ::2] = inpTensor
        return up_sampled 

Is this the right approach to perform this operation ? I doubt if the back propagation will work since I have made a new tensor and it might not have the history of the operations performed on the inpTensor.",t2_3roejhk5,False,,0,False,How to perform Max-up Pooling on tensor?,[],r/pytorch,False,6,,0,135.0,,False,t3_gcmhon,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/Sk6yhQg7X6z32EOkeoz8i-eRGCxBToMTzG7hI4KtMOM.jpg,False,,[],{},,,True,,1588519302.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am implementing &lt;a href=""https://www.sciencedirect.com/science/article/abs/pii/S0169260719300690""&gt;SCAE&lt;/a&gt;. Which is a type of Convolutional Autoencoder&lt;/p&gt;

&lt;p&gt;One of the operations in the decoder network is a Max Up-Pool layer (technically it is not a layer as it does not have wrights). Max Up Pool adds zeros at alternate positions in the upscaled tensor. This can be illustrated below:  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/mj9lrhd44iw41.png?width=278&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd6488ba806de675e743c8556c8d46f87a53f116""&gt;https://preview.redd.it/mj9lrhd44iw41.png?width=278&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd6488ba806de675e743c8556c8d46f87a53f116&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;My input tensor’s shape is 1* 4 * 64  and I need to upsample it to 1* 4 * 128  shape using the Max Up-pooing. I made the below function to perform max up pooling:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def maxUpPool(self, inpTensor):     
    up_sampled = torch.zeros(inpTensor.shape[0], inpTensor.shape[1], inpTensor.shape[2] * 2)     
    up_sampled[:, :, ::2] = inpTensor
    return up_sampled 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Is this the right approach to perform this operation ? I doubt if the back propagation will work since I have made a new tensor and it might not have the history of the operations performed on the inpTensor.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gcmhon,True,,kkziga,,4,True,all_ads,False,[],False,,/r/pytorch/comments/gcmhon/how_to_perform_maxup_pooling_on_tensor/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gcmhon/how_to_perform_maxup_pooling_on_tensor/,7135,1588490502.0,0,,False,,,,"{'mj9lrhd44iw41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 104, 'x': 108, 'u': 'https://preview.redd.it/mj9lrhd44iw41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=274b538c147618e8e454f66f41505d07ddb87c94'}, {'y': 209, 'x': 216, 'u': 'https://preview.redd.it/mj9lrhd44iw41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=213cda4403f2d173f8d4e32d05d224aa1e484917'}], 's': {'y': 270, 'x': 278, 'u': 'https://preview.redd.it/mj9lrhd44iw41.png?width=278&amp;format=png&amp;auto=webp&amp;s=bd6488ba806de675e743c8556c8d46f87a53f116'}, 'id': 'mj9lrhd44iw41'}}",,,,
581,,pytorch,"I had developed an estimator in Scikit-learn but because of performance issues (both speed and memory usage) I am thinking of making the estimator to run using GPU.

One way I can think of to do this is to write the estimator in PyTorch (so I can use GPU processing) and then use Google Colab to leverage on their cloud GPUs and memory capacity.

What would be the best way to write an estimator which is already scikit-learn compatible in PyTorch?

Any pointers or hints pointing to the right direction would really be appreciated. Many thanks in advance.",t2_zmqho4m,False,,0,False,How to write a scikit-learn estimator in PyTorch,[],r/pytorch,False,6,,0,,,False,t3_gc2yj0,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1588436368.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I had developed an estimator in Scikit-learn but because of performance issues (both speed and memory usage) I am thinking of making the estimator to run using GPU.&lt;/p&gt;

&lt;p&gt;One way I can think of to do this is to write the estimator in PyTorch (so I can use GPU processing) and then use Google Colab to leverage on their cloud GPUs and memory capacity.&lt;/p&gt;

&lt;p&gt;What would be the best way to write an estimator which is already scikit-learn compatible in PyTorch?&lt;/p&gt;

&lt;p&gt;Any pointers or hints pointing to the right direction would really be appreciated. Many thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gc2yj0,True,,leockl,,8,True,all_ads,False,[],False,,/r/pytorch/comments/gc2yj0/how_to_write_a_scikitlearn_estimator_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/gc2yj0/how_to_write_a_scikitlearn_estimator_in_pytorch/,7135,1588407568.0,0,,False,,,,,,,,
582,,pytorch,,t2_2o7eaff,False,,0,False,This Week in AI - Issue #16 | Rubik's Code,[],r/pytorch,False,6,,0,78.0,,False,t3_gbeffa,False,dark,0.67,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/ngk7y3icFzj_oCSl-PpaqNKy_pdavZrzbOqPULk6TYA.jpg,False,,[],{},link,,False,,1588350935.0,text,6,,,text,rubikscode.net,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/-JrRfwF5Ck94BEao_jkvPrWvsRrqUibiZxwLEeJoTHE.jpg?auto=webp&amp;s=4fc2388485a5af518b7b13c619d6265d8915a227', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/-JrRfwF5Ck94BEao_jkvPrWvsRrqUibiZxwLEeJoTHE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49fb44c09ea1d75c80742ad6bc54ebb741b8d4ab', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/-JrRfwF5Ck94BEao_jkvPrWvsRrqUibiZxwLEeJoTHE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=95e50a45ca2321a0b7cccbb4cf731ad0895f6ed4', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/-JrRfwF5Ck94BEao_jkvPrWvsRrqUibiZxwLEeJoTHE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c2d03ca6ed8f5856de5e6729cd1459ab8519f17', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/-JrRfwF5Ck94BEao_jkvPrWvsRrqUibiZxwLEeJoTHE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1d316de8feec9554b9ec14fe94a909d77ec3fef8', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/-JrRfwF5Ck94BEao_jkvPrWvsRrqUibiZxwLEeJoTHE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=37c5479177c9da05a2d4ced8870362cd74d9749d', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/-JrRfwF5Ck94BEao_jkvPrWvsRrqUibiZxwLEeJoTHE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6cff518e8405de9615f19176b9480a79870ab1ec', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'wFrM5mQRhJrE5CJ6g9DmlDHLfiukllE9wG7IP7t6JdU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gbeffa,True,,RubiksCodeNMZ,,0,True,all_ads,False,[],False,,/r/pytorch/comments/gbeffa/this_week_in_ai_issue_16_rubiks_code/,all_ads,False,https://rubikscode.net/2020/05/01/this-week-in-ai-issue-16-2/,7135,1588322135.0,0,,False,https://rubikscode.net/2020/05/01/this-week-in-ai-issue-16-2/,,,,,,,
583,,pytorch,,t2_k83mik4,False,,0,False,Fastest way to work with a dataset of sparse features in PyTorch?,[],r/pytorch,False,6,,0,140.0,,False,t3_gazjoz,False,dark,1.0,,public,9,0,{},140.0,,False,[],,False,False,,{},,False,9,,False,https://a.thumbs.redditmedia.com/Up-JJHNuTztvM_dZV0fM3MJ-BKlOmcjCo2XFWu8ZHb8.jpg,False,,[],{},link,,False,,1588294744.0,text,6,,,text,discuss.pytorch.org,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gazjoz,True,,whiteboy2471,,13,True,all_ads,False,[],False,,/r/pytorch/comments/gazjoz/fastest_way_to_work_with_a_dataset_of_sparse/,all_ads,False,https://discuss.pytorch.org/t/best-way-to-load-sparse-data-in-pytorch/79134,7135,1588265944.0,0,,False,https://discuss.pytorch.org/t/best-way-to-load-sparse-data-in-pytorch/79134,,,,,,,
584,,pytorch,,t2_15n8fb,False,,0,False,How does one pickle arbitrary pytorch models that use lambda functions?,[],r/pytorch,False,6,,0,70.0,,False,t3_gagpjg,False,dark,1.0,,public,6,0,{},70.0,,False,[],,False,False,,{},,False,6,,False,https://b.thumbs.redditmedia.com/sb7j2nH4e2cLQeTz-xt3WP_j8m6_2qdOiIfOm487HjA.jpg,False,,[],{},link,,False,,1588219983.0,text,6,,,text,discuss.pytorch.org,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/JKT6vgeYaKMiKgd7X1EW55G3JQM8y4nr2T96QqYgK6c.jpg?auto=webp&amp;s=fbb0fb7e5d18f847497656dbcb71c09c65f65b49', 'width': 128, 'height': 128}, 'resolutions': [{'url': 'https://external-preview.redd.it/JKT6vgeYaKMiKgd7X1EW55G3JQM8y4nr2T96QqYgK6c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=56d388b1b9556cc3daad8aecd39b307af760ec0d', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'AuvUxEeP-sCTbnEj5b1KUxnPpXpRQ0yKwQ9XkXeC0kI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,gagpjg,True,,real_pinocchio,,2,True,all_ads,False,[],False,,/r/pytorch/comments/gagpjg/how_does_one_pickle_arbitrary_pytorch_models_that/,all_ads,False,https://discuss.pytorch.org/t/how-does-one-pickle-arbitrary-pytorch-models-that-use-lambda-functions/79026,7135,1588191183.0,0,,False,https://discuss.pytorch.org/t/how-does-one-pickle-arbitrary-pytorch-models-that-use-lambda-functions/79026,,,,,,,
585,,pytorch,Can anyone help out on with [this](https://stackoverflow.com/questions/61451339/pytorch-lstm-crashing-on-colab-gpu-works-fine-on-cpu) issue. Really weird. No error logs and runs fine in CPU mode. Don't know if it is a Colab or more general bug.,t2_353elmq7,False,,0,False,LSTM Crashing on Colab GPU,[],r/pytorch,False,6,,0,,,False,t3_g95z31,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},self,,True,,1588039719.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can anyone help out on with &lt;a href=""https://stackoverflow.com/questions/61451339/pytorch-lstm-crashing-on-colab-gpu-works-fine-on-cpu""&gt;this&lt;/a&gt; issue. Really weird. No error logs and runs fine in CPU mode. Don&amp;#39;t know if it is a Colab or more general bug.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g95z31,True,,svpadd3,,2,True,all_ads,False,[],False,,/r/pytorch/comments/g95z31/lstm_crashing_on_colab_gpu/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g95z31/lstm_crashing_on_colab_gpu/,7135,1588010919.0,0,,False,,,,,,,,
586,,pytorch,"# Kornia 0.3.0 release

Today we released 0.3.0 which aligns with PyTorch releases cycle and includes:

- Full support to PyTorch v1.5.
- Semi-automated GPU tests coverage.
- Documentation has been reorganized [[docs]](https://kornia.readthedocs.io/en/latest/)
- Data augmentation API compatible with torchvision v0.6.0.
- Well integration with ecosystem e.g. Pytorch-Lightning.

For more detailed changes check out [v0.2.1](https://github.com/kornia/kornia/releases/tag/v0.2.1) and [v0.2.2](https://github.com/kornia/kornia/releases/tag/v0.2.2).

# Highlights

## Data Augmentation

We provide [`kornia.augmentation`](https://kornia.readthedocs.io/en/latest/augmentation.html#) a high-level framework that implements `kornia-core` functionalities and is fully compatible with torchvision supporting batched mode, multi device cpu, gpu, and xla/tpu (comming), auto differentiable and able to retrieve (and chain) applied geometric transforms. To check how to reproduce torchvision in kornia refer to this [Colab: Kornia vs. Torchvision](https://colab.research.google.com/drive/1T20UNAG4SdlE2n2wstuhiewve5Q81VpS#revisionId=0B4unZG1uMc-WdzZqaStjVzZ1U0hHOHphQkgvcGFCZ1RlUzJvPQ/) @shijianjian 


```python
import kornia as K
import torchvision as T

# kornia

transform_fcn = torch.nn.Sequential(
  K.augmentation.RandomAffine(
    [-45., 45.], [0., 0.5], [0.5, 1.5], [0., 0.5], return_transform=True),
  K.color.Normalize(0.1307, 0.3081),
)

# torchvision

transform_fcn = T.transforms.Compose([
  T.transforms.RandomAffine(
    [-45., 45.], [0., 0.5], [0.5, 1.5], [0., 0.5]),
  T.transforms.ToTensor(),
  T.transforms.Normalize((0.1307,), (0.3081,)),
])
```
[panda]: https://drive.google.com/open?id=18K2nXu1xNLlG8iFckj9otKhOXt9wy-yV

## Ecosystem compatibility

Kornia has been designed to be very flexible in order to be integrated in other existing frameworks. See the example below about how easy you can define a custom data augmentation pipeline to later be integrated into any training framework such as [Pytorch-Lighting](https://pytorch-lightning.readthedocs.io/en/latest/). We provide examples in [[here]](https://github.com/kornia/kornia-examples/blob/master/kornia_lightning_mnist_gpu.ipynb) and [[here]](https://github.com/PyTorchLightning/lightning-Covid19/blob/master/lightning_covid19/model/densenet.py#L44).

```python
class DataAugmentatonPipeline(nn.Module):
    """"""Module to perform data augmentation using Kornia on torch tensors.""""""
    def __init__(self, apply_color_jitter: bool = False) -&gt; None:
        super().__init__()
        self._apply_color_jitter = apply_color_jitter

        self._max_val: float = 1024.

        self.transforms = nn.Sequential(
            K.augmentation.Normalize(0., self._max_val),
            K.augmentation.RandomHorizontalFlip(p=0.5)
        )

        self.jitter = K.augmentation.ColorJitter(0.5, 0.5, 0.5, 0.5)

    @torch.no_grad()  # disable gradients for effiency
    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        x_out = self.transforms(x)
        if self._apply_color_jitter:
            x_out = self.jitter(x_out)
        return x_out
```

## GPU tests

Now easy to run GPU tests with `pytest --typetest cuda`",t2_11dkcj,False,,0,False,"Kornia 0.3.0: support to PyTorch 1.5, accelerated Data Augmentation + stable GPU testing and easy ecosystem integration.",[],r/pytorch,False,6,,0,,,False,t3_g8yw4w,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1588015209.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h1&gt;Kornia 0.3.0 release&lt;/h1&gt;

&lt;p&gt;Today we released 0.3.0 which aligns with PyTorch releases cycle and includes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Full support to PyTorch v1.5.&lt;/li&gt;
&lt;li&gt;Semi-automated GPU tests coverage.&lt;/li&gt;
&lt;li&gt;Documentation has been reorganized &lt;a href=""https://kornia.readthedocs.io/en/latest/""&gt;[docs]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Data augmentation API compatible with torchvision v0.6.0.&lt;/li&gt;
&lt;li&gt;Well integration with ecosystem e.g. Pytorch-Lightning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more detailed changes check out &lt;a href=""https://github.com/kornia/kornia/releases/tag/v0.2.1""&gt;v0.2.1&lt;/a&gt; and &lt;a href=""https://github.com/kornia/kornia/releases/tag/v0.2.2""&gt;v0.2.2&lt;/a&gt;.&lt;/p&gt;

&lt;h1&gt;Highlights&lt;/h1&gt;

&lt;h2&gt;Data Augmentation&lt;/h2&gt;

&lt;p&gt;We provide &lt;a href=""https://kornia.readthedocs.io/en/latest/augmentation.html#""&gt;&lt;code&gt;kornia.augmentation&lt;/code&gt;&lt;/a&gt; a high-level framework that implements &lt;code&gt;kornia-core&lt;/code&gt; functionalities and is fully compatible with torchvision supporting batched mode, multi device cpu, gpu, and xla/tpu (comming), auto differentiable and able to retrieve (and chain) applied geometric transforms. To check how to reproduce torchvision in kornia refer to this &lt;a href=""https://colab.research.google.com/drive/1T20UNAG4SdlE2n2wstuhiewve5Q81VpS#revisionId=0B4unZG1uMc-WdzZqaStjVzZ1U0hHOHphQkgvcGFCZ1RlUzJvPQ/""&gt;Colab: Kornia vs. Torchvision&lt;/a&gt; @shijianjian &lt;/p&gt;

&lt;p&gt;```python
import kornia as K
import torchvision as T&lt;/p&gt;

&lt;h1&gt;kornia&lt;/h1&gt;

&lt;p&gt;transform_fcn = torch.nn.Sequential(
  K.augmentation.RandomAffine(
    [-45., 45.], [0., 0.5], [0.5, 1.5], [0., 0.5], return_transform=True),
  K.color.Normalize(0.1307, 0.3081),
)&lt;/p&gt;

&lt;h1&gt;torchvision&lt;/h1&gt;

&lt;p&gt;transform_fcn = T.transforms.Compose([
  T.transforms.RandomAffine(
    [-45., 45.], [0., 0.5], [0.5, 1.5], [0., 0.5]),
  T.transforms.ToTensor(),
  T.transforms.Normalize((0.1307,), (0.3081,)),
])
```&lt;/p&gt;

&lt;h2&gt;Ecosystem compatibility&lt;/h2&gt;

&lt;p&gt;Kornia has been designed to be very flexible in order to be integrated in other existing frameworks. See the example below about how easy you can define a custom data augmentation pipeline to later be integrated into any training framework such as &lt;a href=""https://pytorch-lightning.readthedocs.io/en/latest/""&gt;Pytorch-Lighting&lt;/a&gt;. We provide examples in &lt;a href=""https://github.com/kornia/kornia-examples/blob/master/kornia_lightning_mnist_gpu.ipynb""&gt;[here]&lt;/a&gt; and &lt;a href=""https://github.com/PyTorchLightning/lightning-Covid19/blob/master/lightning_covid19/model/densenet.py#L44""&gt;[here]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;```python
class DataAugmentatonPipeline(nn.Module):
    &amp;quot;&amp;quot;&amp;quot;Module to perform data augmentation using Kornia on torch tensors.&amp;quot;&amp;quot;&amp;quot;
    def &lt;strong&gt;init&lt;/strong&gt;(self, apply&lt;em&gt;color_jitter: bool = False) -&amp;gt; None:
        super().&lt;/em&gt;&lt;em&gt;init&lt;/em&gt;_()
        self._apply_color_jitter = apply_color_jitter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    self._max_val: float = 1024.

    self.transforms = nn.Sequential(
        K.augmentation.Normalize(0., self._max_val),
        K.augmentation.RandomHorizontalFlip(p=0.5)
    )

    self.jitter = K.augmentation.ColorJitter(0.5, 0.5, 0.5, 0.5)

@torch.no_grad()  # disable gradients for effiency
def forward(self, x: torch.Tensor) -&amp;gt; torch.Tensor:
    x_out = self.transforms(x)
    if self._apply_color_jitter:
        x_out = self.jitter(x_out)
    return x_out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;h2&gt;GPU tests&lt;/h2&gt;

&lt;p&gt;Now easy to run GPU tests with &lt;code&gt;pytest --typetest cuda&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g8yw4w,True,,edgarriba,,0,True,all_ads,False,[],False,,/r/pytorch/comments/g8yw4w/kornia_030_support_to_pytorch_15_accelerated_data/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g8yw4w/kornia_030_support_to_pytorch_15_accelerated_data/,7135,1587986409.0,0,,False,,,,,,,,
587,,pytorch,"I have been trying to build  a basic single object detection in pytorch from scratch, but I'm getting error , Can anyone  suggest some good tutorial other than the official documentation.

Thanks in advacne",t2_4ni3km8k,False,,0,False,Help with Object Localization,[],r/pytorch,False,6,,0,,,False,t3_g8yple,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588014318.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been trying to build  a basic single object detection in pytorch from scratch, but I&amp;#39;m getting error , Can anyone  suggest some good tutorial other than the official documentation.&lt;/p&gt;

&lt;p&gt;Thanks in advacne&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g8yple,True,,kunalkarda08,,1,True,all_ads,False,[],False,,/r/pytorch/comments/g8yple/help_with_object_localization/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g8yple/help_with_object_localization/,7135,1587985518.0,0,,False,,,,,,,,
588,,pytorch,"I'm trying to do some reinforcement learning, in particular an implementation of AlphaZero, and need to compare the probability distributions from a tree with a neural net. Is there an accepted way to do this? 

https://pytorch.org/docs/stable/nn.html#crossentropyloss

The main implementation assumes that you know the classification in advance, and so takes a label as opposed to a vector of probabilities.",t2_3z2dt,False,,0,False,Is there an inbuilt cross entropy loss for comparing two probability distributions in pytorch?,[],r/pytorch,False,6,,0,,,False,t3_g8park,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1587973517.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to do some reinforcement learning, in particular an implementation of AlphaZero, and need to compare the probability distributions from a tree with a neural net. Is there an accepted way to do this? &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://pytorch.org/docs/stable/nn.html#crossentropyloss""&gt;https://pytorch.org/docs/stable/nn.html#crossentropyloss&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The main implementation assumes that you know the classification in advance, and so takes a label as opposed to a vector of probabilities.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jYGjd9HNUsAXXNvMaRvzvi_5jJoBwaVOADjbz_QDVEU.jpg?auto=webp&amp;s=ed5ad7c35d75dbd9bca66c22aead098893e1b6b5', 'width': 640, 'height': 480}, 'resolutions': [{'url': 'https://external-preview.redd.it/jYGjd9HNUsAXXNvMaRvzvi_5jJoBwaVOADjbz_QDVEU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b04a6ebf2f01a6a2cc5dcdd50d21ea570a7ca70c', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/jYGjd9HNUsAXXNvMaRvzvi_5jJoBwaVOADjbz_QDVEU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=87e6025523a4ed73f236c7fd22afc37f1b260e3f', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/jYGjd9HNUsAXXNvMaRvzvi_5jJoBwaVOADjbz_QDVEU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a9ee111b029dd387a95260262c71d42c22783adf', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/jYGjd9HNUsAXXNvMaRvzvi_5jJoBwaVOADjbz_QDVEU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60dd0ab3df5660b239f9a64b77c7d5874965ffeb', 'width': 640, 'height': 480}], 'variants': {}, 'id': 'JyYWpgJb1xoJMe3whGKaTZ6fqF7Fcnm9nxsRfDcNzOU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g8park,True,,qazadex,,4,True,all_ads,False,[],False,,/r/pytorch/comments/g8park/is_there_an_inbuilt_cross_entropy_loss_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g8park/is_there_an_inbuilt_cross_entropy_loss_for/,7135,1587944717.0,0,,False,,,,,,,,
589,,pytorch,"I currently have a project for automotive. Basically, I have to make a neural net that is able to inpaint images in which for example the middle is [masked](https://i.imgur.com/rA7bVH2.png). 

The final idea is that the neural net will never be able to see the full images. So it trains on the broken images. However, in the end it should be able to ""hallucinate"" the correct pre-masked images. 

Before I'll get there, I will need a neural net type. I have been looking at u-net type neural nets, however, most examples on the internet are all classifiers, and I need to get an image-to-image neural net. 

I wonder if u-net is the way to go, or you guys recommend some other structure neural net.",t2_h7ymv,False,,0,False,Image Inpainting,[],r/pytorch,False,6,,0,,,False,t3_g6r45p,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1587692453.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I currently have a project for automotive. Basically, I have to make a neural net that is able to inpaint images in which for example the middle is &lt;a href=""https://i.imgur.com/rA7bVH2.png""&gt;masked&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;The final idea is that the neural net will never be able to see the full images. So it trains on the broken images. However, in the end it should be able to &amp;quot;hallucinate&amp;quot; the correct pre-masked images. &lt;/p&gt;

&lt;p&gt;Before I&amp;#39;ll get there, I will need a neural net type. I have been looking at u-net type neural nets, however, most examples on the internet are all classifiers, and I need to get an image-to-image neural net. &lt;/p&gt;

&lt;p&gt;I wonder if u-net is the way to go, or you guys recommend some other structure neural net.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/CUzWmA4odpp4N0pMFCBKvhrX8423CfY_DAPv7v3N1-A.png?auto=webp&amp;s=abfa28599f15bfc59890097f483248a0ecfc49f2', 'width': 251, 'height': 248}, 'resolutions': [{'url': 'https://external-preview.redd.it/CUzWmA4odpp4N0pMFCBKvhrX8423CfY_DAPv7v3N1-A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2e15ee9c1b00ab0683433740509a485f4b1b9dbf', 'width': 108, 'height': 106}, {'url': 'https://external-preview.redd.it/CUzWmA4odpp4N0pMFCBKvhrX8423CfY_DAPv7v3N1-A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0350e144bcd7b2e282b75fd88f9791399839c1ad', 'width': 216, 'height': 213}], 'variants': {}, 'id': 'xXCN3ILHs1dZQG9yJrm5GJZ7bBGG0rsSW81lMJSNocc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g6r45p,True,,r3ktIKevin,,2,True,all_ads,False,[],False,,/r/pytorch/comments/g6r45p/image_inpainting/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g6r45p/image_inpainting/,7135,1587663653.0,0,,False,,,,,,,,
590,,pytorch,"The PyTorch Team yesterday announced the release of PyTorch 1.5, along with new and updated libraries. The release features several major new API additions and improvements, including a significant update to the C++ frontend, Channel Last memory format for computer vision models, and a stable release of the distributed RPC framework used for model-parallel training.

Read more: [PyTorch 1.5 Released: New APIs, Updated C++ Frontend and More](https://medium.com/syncedreview/pytorch-1-5-released-new-apis-updated-c-frontend-and-more-d59cb1dcf325)

Check PyTorch 1.5 on [GitHub](https://github.com/pytorch/pytorch/releases).",t2_2fv4yodo,False,,0,False,"PyTorch 1.5 Released: New APIs, Updated C++ Frontend and More",[],r/pytorch,False,6,,0,,,False,t3_g66ba2,False,dark,1.0,,public,28,0,{},,,False,[],,False,False,,{},,False,28,,False,self,False,,[],{},self,,True,,1587608302.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The PyTorch Team yesterday announced the release of PyTorch 1.5, along with new and updated libraries. The release features several major new API additions and improvements, including a significant update to the C++ frontend, Channel Last memory format for computer vision models, and a stable release of the distributed RPC framework used for model-parallel training.&lt;/p&gt;

&lt;p&gt;Read more: &lt;a href=""https://medium.com/syncedreview/pytorch-1-5-released-new-apis-updated-c-frontend-and-more-d59cb1dcf325""&gt;PyTorch 1.5 Released: New APIs, Updated C++ Frontend and More&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Check PyTorch 1.5 on &lt;a href=""https://github.com/pytorch/pytorch/releases""&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/NGZB1G9vKDhpuKq7uUwDKE6FJmZN4ozcEgqfwGQ7d-E.jpg?auto=webp&amp;s=28db4148fd33f95bbbe4dedef7afb32afd158741', 'width': 601, 'height': 314}, 'resolutions': [{'url': 'https://external-preview.redd.it/NGZB1G9vKDhpuKq7uUwDKE6FJmZN4ozcEgqfwGQ7d-E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=757d5b88bf2b5788accd59186234161484f1d4cf', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/NGZB1G9vKDhpuKq7uUwDKE6FJmZN4ozcEgqfwGQ7d-E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0fd0ef4e25361d472367769c65f1cc99910cbb92', 'width': 216, 'height': 112}, {'url': 'https://external-preview.redd.it/NGZB1G9vKDhpuKq7uUwDKE6FJmZN4ozcEgqfwGQ7d-E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ccea159873c6808f6674bf71c4e77c269595ac3', 'width': 320, 'height': 167}], 'variants': {}, 'id': 'QoJ6Bm8tjONMGo__tDsfac3JiX6M9pzniQAqiDDUTxc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g66ba2,True,,Yuqing7,,2,True,all_ads,False,[],False,,/r/pytorch/comments/g66ba2/pytorch_15_released_new_apis_updated_c_frontend/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g66ba2/pytorch_15_released_new_apis_updated_c_frontend/,7135,1587579502.0,0,,False,,,,,,,,
591,,pytorch,"I want to start learning PyTorch from basics like how one start learning any programming language. I have a very good understanding of python and have been searching on the internet for PyTorch but could not get the one that I like. The one on the pytorch.org website itself is not from the basic. 

  
I will be very grateful if you could point me in such tutorials or videos that teach PyTorch from ground zero level. Thank you.",t2_4r0l3pnp,False,,0,False,I need help to learn PyTorch from level 0. !!! HELP !!!,[],r/pytorch,False,6,,0,,,False,t3_g6hnxh,False,dark,0.43,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1587652095.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to start learning PyTorch from basics like how one start learning any programming language. I have a very good understanding of python and have been searching on the internet for PyTorch but could not get the one that I like. The one on the pytorch.org website itself is not from the basic. &lt;/p&gt;

&lt;p&gt;I will be very grateful if you could point me in such tutorials or videos that teach PyTorch from ground zero level. Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g6hnxh,True,,ryan_computer_geek,,10,True,all_ads,False,[],False,,/r/pytorch/comments/g6hnxh/i_need_help_to_learn_pytorch_from_level_0_help/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g6hnxh/i_need_help_to_learn_pytorch_from_level_0_help/,7135,1587623295.0,0,,False,,,,,,,,
592,,pytorch,,t2_5xswlya4,False,,0,False,Torchserve released for serving pytorch models,[],r/pytorch,False,6,,0,140.0,,False,t3_g5ndj6,False,dark,0.89,,public,16,0,{},140.0,,False,[],,False,False,,{},,False,16,,False,https://a.thumbs.redditmedia.com/tsTdO8cMG-tFv0BcKLkhbFq8EmqenlrcCRRFypo3ia8.jpg,False,,[],{},link,,False,,1587530922.0,text,6,,,text,github.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?auto=webp&amp;s=08c43430ff8793a25b767aa172468acb35be611a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4adad794ed3c0e2f76e2da992ae9eef8c074cae4', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5b3cb400fea60e0dfe87ebc5314f7db8016f8ab', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f149723b636ef3087022ddc1efa6ab53e3d4bc69', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_YPUxJ8AyYwgpO9r-tD7JUoQ-Nx8IlATLoEfmjW48iQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g5ndj6,True,,killcorona,,4,True,all_ads,False,[],False,,/r/pytorch/comments/g5ndj6/torchserve_released_for_serving_pytorch_models/,all_ads,False,https://github.com/pytorch/serve,7135,1587502122.0,0,,False,https://github.com/pytorch/serve,,,,,,,
593,,pytorch,,t2_3pqe5,False,,0,False,A shinier memory profiler for PyTorch,[],r/pytorch,False,6,,0,111.0,,False,t3_g3knpy,False,dark,1.0,,public,17,0,"{'content': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;Today&amp;#39;s fun lil yak shaving: a shinier Torch memory profiler&lt;a href=""https://t.co/wCpNCRP1in""&gt;https://t.co/wCpNCRP1in&lt;/a&gt; &lt;a href=""https://t.co/vuqHa59wTJ""&gt;pic.twitter.com/vuqHa59wTJ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andy Jones (@andy_l_jones) &lt;a href=""https://twitter.com/andy_l_jones/status/1251436467890634752?ref_src=twsrc%5Etfw""&gt;April 18, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'width': 350, 'scrolling': False, 'height': 476}",140.0,,False,[],"{'type': 'twitter.com', 'oembed': {'provider_url': 'https://twitter.com', 'version': '1.0', 'url': 'https://twitter.com/andy_l_jones/status/1251436467890634752', 'author_name': 'Andy Jones', 'height': 476, 'width': 350, 'html': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;Today&amp;#39;s fun lil yak shaving: a shinier Torch memory profiler&lt;a href=""https://t.co/wCpNCRP1in""&gt;https://t.co/wCpNCRP1in&lt;/a&gt; &lt;a href=""https://t.co/vuqHa59wTJ""&gt;pic.twitter.com/vuqHa59wTJ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andy Jones (@andy_l_jones) &lt;a href=""https://twitter.com/andy_l_jones/status/1251436467890634752?ref_src=twsrc%5Etfw""&gt;April 18, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'author_url': 'https://twitter.com/andy_l_jones', 'provider_name': 'Twitter', 'cache_age': 3153600000, 'type': 'rich'}}",False,False,,"{'content': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;Today&amp;#39;s fun lil yak shaving: a shinier Torch memory profiler&lt;a href=""https://t.co/wCpNCRP1in""&gt;https://t.co/wCpNCRP1in&lt;/a&gt; &lt;a href=""https://t.co/vuqHa59wTJ""&gt;pic.twitter.com/vuqHa59wTJ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andy Jones (@andy_l_jones) &lt;a href=""https://twitter.com/andy_l_jones/status/1251436467890634752?ref_src=twsrc%5Etfw""&gt;April 18, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'width': 350, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g3knpy', 'height': 476}",,False,17,,False,https://b.thumbs.redditmedia.com/zpO2fguxQz3qwm639bt-0JmunAsVQYajVIyrOhqJebk.jpg,False,,[],{},link,,False,,1587229974.0,text,6,,,text,twitter.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/WVwwIpiPCYKvO-jsNDD6AAfEgNQ-skIVyYGlxXokaZk.jpg?auto=webp&amp;s=4044bc62d94a3d9f82e3792b1ad57aaa7c0e324f', 'width': 140, 'height': 111}, 'resolutions': [{'url': 'https://external-preview.redd.it/WVwwIpiPCYKvO-jsNDD6AAfEgNQ-skIVyYGlxXokaZk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1a18ea4880bbc050c9399a58c4410594c804eb53', 'width': 108, 'height': 85}], 'variants': {}, 'id': '_1-4B8ZHTUYd2J3a4BvH0z3Tj3bpWSWGa2N-ameg6z0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g3knpy,True,,bluecoffee,,0,True,all_ads,False,[],False,,/r/pytorch/comments/g3knpy/a_shinier_memory_profiler_for_pytorch/,all_ads,False,https://twitter.com/andy_l_jones/status/1251436467890634752,7135,1587201174.0,0,"{'type': 'twitter.com', 'oembed': {'provider_url': 'https://twitter.com', 'version': '1.0', 'url': 'https://twitter.com/andy_l_jones/status/1251436467890634752', 'author_name': 'Andy Jones', 'height': 476, 'width': 350, 'html': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;Today&amp;#39;s fun lil yak shaving: a shinier Torch memory profiler&lt;a href=""https://t.co/wCpNCRP1in""&gt;https://t.co/wCpNCRP1in&lt;/a&gt; &lt;a href=""https://t.co/vuqHa59wTJ""&gt;pic.twitter.com/vuqHa59wTJ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andy Jones (@andy_l_jones) &lt;a href=""https://twitter.com/andy_l_jones/status/1251436467890634752?ref_src=twsrc%5Etfw""&gt;April 18, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'author_url': 'https://twitter.com/andy_l_jones', 'provider_name': 'Twitter', 'cache_age': 3153600000, 'type': 'rich'}}",False,https://twitter.com/andy_l_jones/status/1251436467890634752,,,,,,,
594,,pytorch," What is the purpose of the [following code snippet](https://github.com/huawei-noah/AdderNet/blob/master/resnet20.py#L65-L79) in [AdderNet](https://arxiv.org/pdf/1912.13200.pdf#page=3) ?

        def _make_layer(self, block, planes, blocks, stride=1):
            downsample = None
            if stride != 1 or self.inplanes != planes * block.expansion:
                downsample = nn.Sequential(
                    adder.adder2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),
                    nn.BatchNorm2d(planes * block.expansion)
                )
    
            layers = []
            layers.append(block(inplanes = self.inplanes, planes = planes, stride = stride, downsample = downsample))
            self.inplanes = planes * block.expansion
            for _ in range(1, blocks):
                layers.append(block(inplanes = self.inplanes, planes = planes))
    
            return nn.Sequential(*layers)",t2_bpftl,False,,0,False,Understanding AdderNet model construction coding,[],r/pytorch,False,6,,0,,,False,t3_g3ns5k,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1587246526.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What is the purpose of the &lt;a href=""https://github.com/huawei-noah/AdderNet/blob/master/resnet20.py#L65-L79""&gt;following code snippet&lt;/a&gt; in &lt;a href=""https://arxiv.org/pdf/1912.13200.pdf#page=3""&gt;AdderNet&lt;/a&gt; ?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                adder.adder2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion)
            )

        layers = []
        layers.append(block(inplanes = self.inplanes, planes = planes, stride = stride, downsample = downsample))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(inplanes = self.inplanes, planes = planes))

        return nn.Sequential(*layers)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?auto=webp&amp;s=b134e75c125f5c05086645695c0f7dd3390375e9', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0220f379ccc6f8cf5817f79238538ba2ffe05c70', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70cc6291695445faa1333d0a07c41b31fd9dbfcb', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=45992e6fd013e395412db216c83ded62bc740c38', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'bAKE-mgRYAKYVYhfiJp-sr0yWP8hFmq8xaav2zgIqNI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g3ns5k,True,,promach,,1,True,all_ads,False,[],False,,/r/pytorch/comments/g3ns5k/understanding_addernet_model_construction_coding/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g3ns5k/understanding_addernet_model_construction_coding/,7135,1587217726.0,2,,False,,,,,,,,
595,,pytorch,,t2_3ipkvvpg,False,,0,False,Detecto - An object detection library for PyTorch,[],r/pytorch,False,6,,0,87.0,,False,t3_g3a642,False,dark,1.0,,public,9,0,{},140.0,,False,[],,False,False,,{},,False,9,,False,https://b.thumbs.redditmedia.com/8Ne0VEZ6LJCDItbGDwJqrrv0KlOeCO5IBX3Sf5UYNfg.jpg,False,,[],{},link,,False,,1587185243.0,text,6,,,text,medium.com,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/iQcIWaeg2p8S9h9YF6x3ig-G-HczttAPiBAwKlyPs98.jpg?auto=webp&amp;s=34b1b8b135cf6ae7830685737b61435d260dda83', 'width': 600, 'height': 373}, 'resolutions': [{'url': 'https://external-preview.redd.it/iQcIWaeg2p8S9h9YF6x3ig-G-HczttAPiBAwKlyPs98.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d159411a7e4538c7b6880594ec730fb8d83e782', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/iQcIWaeg2p8S9h9YF6x3ig-G-HczttAPiBAwKlyPs98.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f9ad6e1e8cdb5227f8845a2ff5c4eba7924502bc', 'width': 216, 'height': 134}, {'url': 'https://external-preview.redd.it/iQcIWaeg2p8S9h9YF6x3ig-G-HczttAPiBAwKlyPs98.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fcf3c7843886665d59e039b904173d2eda0e1cb6', 'width': 320, 'height': 198}], 'variants': {}, 'id': 'Y5c6OJ2o0D_Le6hpLRr90VTSB6nx7Ks7M-aHuNNhMXk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g3a642,True,,alanbi,,1,True,all_ads,False,[],False,,/r/pytorch/comments/g3a642/detecto_an_object_detection_library_for_pytorch/,all_ads,False,https://medium.com/pytorch/detecto-build-and-train-object-detection-models-with-pytorch-5f31b68a8109,7135,1587156443.0,0,,False,https://medium.com/pytorch/detecto-build-and-train-object-detection-models-with-pytorch-5f31b68a8109,,,,,,,
596,,pytorch,"How shall I modify this [new\_cdist()](https://github.com/huawei-noah/AdderNet/issues/6#issuecomment-594212162) function to eliminate [GPU out-of-memory runtime error](https://github.com/huawei-noah/AdderNet/issues/6#issuecomment-613362421) ?

More specifically replacing `torch.cdist()` with something lighter in memory footprint.

Note: The following code snippet is related to a [new type of back-propagation equation and adaptive learning rate](https://github.com/huawei-noah/AdderNet/issues/5).

See [here](https://stackoverflow.com/questions/61154470/understanding-cdist-function) for more information on how the existing code works

I saw there are two  cdist() implementations online ([code 1](https://github.com/pytorch/pytorch/pull/25799#issuecomment-529021810) , [code 2](https://github.com/pytorch/pytorch/issues/15253#issuecomment-491467128))

    def new_cdist(p, eta):
        class cdist(torch.autograd.Function):
            @staticmethod
            def forward(ctx, W, X):
                ctx.save_for_backward(W, X)
                out = -torch.cdist(W, X, p)
                return out
    
            @staticmethod
            def backward(ctx, grad_output):
                W, X = ctx.saved_tensors
                grad_W = grad_X = None
                if ctx.needs_input_grad[0]:
                    _temp1 = torch.unsqueeze(X, 2).expand(X.shape[0], X.shape[1], W.shape[0]).permute(1, 0, 2)
                    _temp2 = torch.unsqueeze(W.transpose(0, 1), 1)
                    _temp = torch.cdist(_temp1, _temp2, p).squeeze().transpose(0, 1)
                    grad_W = torch.matmul(grad_output, _temp)
                    # print('before norm: ', torch.norm(grad_W))
                    grad_W = eta * np.sqrt(grad_W.numel()) / torch.norm(grad_W) * grad_W
                    print('after norm: ', torch.norm(grad_W))
                if ctx.needs_input_grad[1]:
                    _temp1 = torch.unsqueeze(W, 2).expand(W.shape[0], W.shape[1], X.shape[0]).permute(1, 0, 2)
                    _temp2 = torch.unsqueeze(X.transpose(0, 1), 1)
                    _temp = torch.cdist(_temp1, _temp2, p).squeeze().transpose(0, 1)
                    _temp = torch.nn.functional.hardtanh(_temp, min_val=-1., max_val=1.)
                    grad_X = torch.matmul(grad_output.transpose(0, 1), _temp)
                return grad_W, grad_X
        return cdist().apply

Note:

I was told that `_temp1 = torch.unsqueeze(X, 2).expand(X.shape[0], X.shape[1], W.shape[0]).permute(1, 0, 2)` seems to use up a lot of GPU memory during [intermediate calculation](https://github.com/promach/PyTorch-YOLOv3/blob/addernet/adder.py#L29) .But how to modify this line to use less GPU memory ?

&amp;#x200B;

I was told to either

&amp;#x200B;

1. **decrease the tensors dimension, then increase the tensors dimension again** or
2. **increase the tensors dimensions and then use depth-wise convolution** or
3. **use mixed-precision training**

I know I can use [https://github.com/NVIDIA/apex](https://github.com/NVIDIA/apex) for the mixed precision training.  
 However, I am not sure how to modify the code for suggestions 1 and 2 above.",t2_bpftl,False,,0,False,Replacing torch.cdist() function to eliminate GPU out-of-memory runtime error,[],r/pytorch,False,6,,0,,,False,t3_g31hmd,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,1587713189.0,,[],{},self,,True,,1587157026.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How shall I modify this &lt;a href=""https://github.com/huawei-noah/AdderNet/issues/6#issuecomment-594212162""&gt;new_cdist()&lt;/a&gt; function to eliminate &lt;a href=""https://github.com/huawei-noah/AdderNet/issues/6#issuecomment-613362421""&gt;GPU out-of-memory runtime error&lt;/a&gt; ?&lt;/p&gt;

&lt;p&gt;More specifically replacing &lt;code&gt;torch.cdist()&lt;/code&gt; with something lighter in memory footprint.&lt;/p&gt;

&lt;p&gt;Note: The following code snippet is related to a &lt;a href=""https://github.com/huawei-noah/AdderNet/issues/5""&gt;new type of back-propagation equation and adaptive learning rate&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;See &lt;a href=""https://stackoverflow.com/questions/61154470/understanding-cdist-function""&gt;here&lt;/a&gt; for more information on how the existing code works&lt;/p&gt;

&lt;p&gt;I saw there are two  cdist() implementations online (&lt;a href=""https://github.com/pytorch/pytorch/pull/25799#issuecomment-529021810""&gt;code 1&lt;/a&gt; , &lt;a href=""https://github.com/pytorch/pytorch/issues/15253#issuecomment-491467128""&gt;code 2&lt;/a&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def new_cdist(p, eta):
    class cdist(torch.autograd.Function):
        @staticmethod
        def forward(ctx, W, X):
            ctx.save_for_backward(W, X)
            out = -torch.cdist(W, X, p)
            return out

        @staticmethod
        def backward(ctx, grad_output):
            W, X = ctx.saved_tensors
            grad_W = grad_X = None
            if ctx.needs_input_grad[0]:
                _temp1 = torch.unsqueeze(X, 2).expand(X.shape[0], X.shape[1], W.shape[0]).permute(1, 0, 2)
                _temp2 = torch.unsqueeze(W.transpose(0, 1), 1)
                _temp = torch.cdist(_temp1, _temp2, p).squeeze().transpose(0, 1)
                grad_W = torch.matmul(grad_output, _temp)
                # print(&amp;#39;before norm: &amp;#39;, torch.norm(grad_W))
                grad_W = eta * np.sqrt(grad_W.numel()) / torch.norm(grad_W) * grad_W
                print(&amp;#39;after norm: &amp;#39;, torch.norm(grad_W))
            if ctx.needs_input_grad[1]:
                _temp1 = torch.unsqueeze(W, 2).expand(W.shape[0], W.shape[1], X.shape[0]).permute(1, 0, 2)
                _temp2 = torch.unsqueeze(X.transpose(0, 1), 1)
                _temp = torch.cdist(_temp1, _temp2, p).squeeze().transpose(0, 1)
                _temp = torch.nn.functional.hardtanh(_temp, min_val=-1., max_val=1.)
                grad_X = torch.matmul(grad_output.transpose(0, 1), _temp)
            return grad_W, grad_X
    return cdist().apply
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note:&lt;/p&gt;

&lt;p&gt;I was told that &lt;code&gt;_temp1 = torch.unsqueeze(X, 2).expand(X.shape[0], X.shape[1], W.shape[0]).permute(1, 0, 2)&lt;/code&gt; seems to use up a lot of GPU memory during &lt;a href=""https://github.com/promach/PyTorch-YOLOv3/blob/addernet/adder.py#L29""&gt;intermediate calculation&lt;/a&gt; .But how to modify this line to use less GPU memory ?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I was told to either&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;decrease the tensors dimension, then increase the tensors dimension again&lt;/strong&gt; or&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;increase the tensors dimensions and then use depth-wise convolution&lt;/strong&gt; or&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;use mixed-precision training&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I know I can use &lt;a href=""https://github.com/NVIDIA/apex""&gt;https://github.com/NVIDIA/apex&lt;/a&gt; for the mixed precision training.&lt;br/&gt;
 However, I am not sure how to modify the code for suggestions 1 and 2 above.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?auto=webp&amp;s=b134e75c125f5c05086645695c0f7dd3390375e9', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0220f379ccc6f8cf5817f79238538ba2ffe05c70', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70cc6291695445faa1333d0a07c41b31fd9dbfcb', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=45992e6fd013e395412db216c83ded62bc740c38', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'bAKE-mgRYAKYVYhfiJp-sr0yWP8hFmq8xaav2zgIqNI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g31hmd,True,,promach,,0,True,all_ads,False,[],False,,/r/pytorch/comments/g31hmd/replacing_torchcdist_function_to_eliminate_gpu/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g31hmd/replacing_torchcdist_function_to_eliminate_gpu/,7135,1587128226.0,0,,False,,,,,,,,
597,,pytorch," So basically I'm trying to vectorize the game snake so it can run faster. Here is my code till now:

    import torch
    import torch.nn.functional as F
    device = torch.device(""cpu"")
    
    class SnakeBoard:
      def __init__(self, board=None):
        if board != None:
          self.channels = board
        else:
          # 0 - Food, 1 - Head, 2 - Body
          self.channels = torch.zeros(1, 3, 15, 17, device=device)
          # Initialize game channels
          self.channels[:, 0, 7, 12] = 1
          self.channels[:, 1, 7, 5] = 1
          self.channels[:, 2, 7, 2:6] = torch.arange(1, 5)
         self.move()
      def move(self):
        self.channels[:, 2] -= 1
        F.relu(self.channels[:, 2], inplace=True)
        # Up movement test
        F.conv2d(self.channels[:, 1], torch.tensor([[[0,1,0],[0,0,0],[0,0,0]]]), padding=1)
    
    SnakeBoard() 

The first dimension in channels represents batch size, second dimension represent the 3 channels of the snake game: food, head, and body, and finally the third and fourth dimensions represent the height and width of the board.

Unfortunately when running the code I get error: Expected stride to be a single integer value or a list of 1 values to match the convolution dimensions, but got stride=\[1, 1\]

How can I fix that?",t2_1rvizv1g,False,,0,False,"Runtime Error: Expected stride to be a single integer value or a list of 1 values to match the convolution dimensions, but got stride=[1, 1]",[],r/pytorch,False,6,,0,,,False,t3_g30c4q,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587152307.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So basically I&amp;#39;m trying to vectorize the game snake so it can run faster. Here is my code till now:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import torch
import torch.nn.functional as F
device = torch.device(&amp;quot;cpu&amp;quot;)

class SnakeBoard:
  def __init__(self, board=None):
    if board != None:
      self.channels = board
    else:
      # 0 - Food, 1 - Head, 2 - Body
      self.channels = torch.zeros(1, 3, 15, 17, device=device)
      # Initialize game channels
      self.channels[:, 0, 7, 12] = 1
      self.channels[:, 1, 7, 5] = 1
      self.channels[:, 2, 7, 2:6] = torch.arange(1, 5)
     self.move()
  def move(self):
    self.channels[:, 2] -= 1
    F.relu(self.channels[:, 2], inplace=True)
    # Up movement test
    F.conv2d(self.channels[:, 1], torch.tensor([[[0,1,0],[0,0,0],[0,0,0]]]), padding=1)

SnakeBoard() 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first dimension in channels represents batch size, second dimension represent the 3 channels of the snake game: food, head, and body, and finally the third and fourth dimensions represent the height and width of the board.&lt;/p&gt;

&lt;p&gt;Unfortunately when running the code I get error: Expected stride to be a single integer value or a list of 1 values to match the convolution dimensions, but got stride=[1, 1]&lt;/p&gt;

&lt;p&gt;How can I fix that?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g30c4q,True,,Itwist101,,2,True,all_ads,False,[],False,,/r/pytorch/comments/g30c4q/runtime_error_expected_stride_to_be_a_single/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g30c4q/runtime_error_expected_stride_to_be_a_single/,7135,1587123507.0,0,,False,,,,,,,,
598,,pytorch,"Hello all, I need examples/resources on multi task learning especially training loops (preferably NLP tasks). I have no clarity on how to setup the tasks;
1.  if I should just add the loss functions in cases where the inputs are the same for both the tasks but labels are different
2. How to train if the losses are separate? Should I alternate between tasks for each epoch?
3. What if the tasks are hierarchical i.e one task outputs feed into the other task as inputs.

Anything would help.",t2_5x9ajge5,False,,0,False,[HELP] Need resources on multi-task learning,[],r/pytorch,False,6,,0,,,False,t3_g2rtze,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1587113834.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all, I need examples/resources on multi task learning especially training loops (preferably NLP tasks). I have no clarity on how to setup the tasks;
1.  if I should just add the loss functions in cases where the inputs are the same for both the tasks but labels are different
2. How to train if the losses are separate? Should I alternate between tasks for each epoch?
3. What if the tasks are hierarchical i.e one task outputs feed into the other task as inputs.&lt;/p&gt;

&lt;p&gt;Anything would help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g2rtze,True,,divrgnce,,0,True,all_ads,False,[],False,,/r/pytorch/comments/g2rtze/help_need_resources_on_multitask_learning/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g2rtze/help_need_resources_on_multitask_learning/,7135,1587085034.0,0,,False,,,,,,,,
599,,pytorch,,t2_1f95rs3v,False,,0,False,Deep Spatial Autoencoders in PyTorch,[],r/pytorch,False,6,,0,140.0,,False,t3_g2ngug,False,dark,0.84,,public,4,0,{},140.0,,False,[],,False,False,,{},,False,4,,False,https://b.thumbs.redditmedia.com/5T4FApNAmSdxs1KA7KjMD0Evn9TJTtL-nda28vdBUmo.jpg,False,,[],{},link,,False,,1587099410.0,text,6,,,text,github.com,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/6cuIlaRSLpzpvXXW_c1k5UcJcwseAgKTSn-bljoSFIw.jpg?auto=webp&amp;s=a0779f216b40ce01e0af4120ba8a111c7bd20ee7', 'width': 378, 'height': 378}, 'resolutions': [{'url': 'https://external-preview.redd.it/6cuIlaRSLpzpvXXW_c1k5UcJcwseAgKTSn-bljoSFIw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ee3d00cbae059b85fc979afc1036fcc9d8d40078', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/6cuIlaRSLpzpvXXW_c1k5UcJcwseAgKTSn-bljoSFIw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=92e910ce11f00759097c6761b9d553666961c6e6', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/6cuIlaRSLpzpvXXW_c1k5UcJcwseAgKTSn-bljoSFIw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=454163922d142885d24fce0a0ad47f9988e8e5f3', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'gRoZwVq8uTEkN4JsYb6scvsB-RMLDxtxz44_3LeafMU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g2ngug,True,,gorosgobe,,1,True,all_ads,False,[],False,,/r/pytorch/comments/g2ngug/deep_spatial_autoencoders_in_pytorch/,all_ads,False,https://github.com/gorosgobe/dsae-torch,7135,1587070610.0,0,,False,https://github.com/gorosgobe/dsae-torch,,,,,,,
600,,pytorch,"Hello. I am trying to understand how the ""grid\_sample"" function works in Pytorch.  
For example, for an input matrix of size (2,2) and a flow field of shape (4,4,2), how does the function work mathematically? Does it repeat the input matrix to size (4,4) and then multiply with the flow fields? There are two flow fields, one for the movements on x and one for the movements on y. I am trying to replicate it using more basic operations and I don't know how to emulate the result.",t2_28a57i1o,False,,0,False,Understanding grid sample,[],r/pytorch,False,6,,0,,,False,t3_g2b4s6,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587055014.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello. I am trying to understand how the &amp;quot;grid_sample&amp;quot; function works in Pytorch.&lt;br/&gt;
For example, for an input matrix of size (2,2) and a flow field of shape (4,4,2), how does the function work mathematically? Does it repeat the input matrix to size (4,4) and then multiply with the flow fields? There are two flow fields, one for the movements on x and one for the movements on y. I am trying to replicate it using more basic operations and I don&amp;#39;t know how to emulate the result.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g2b4s6,True,,Alex-S-S,,1,True,all_ads,False,[],False,,/r/pytorch/comments/g2b4s6/understanding_grid_sample/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g2b4s6/understanding_grid_sample/,7135,1587026214.0,0,,False,,,,,,,,
601,,pytorch,"Hi, I am a beginner, learning pytorch these days. I am having difficulty with a reshaping question. If someone can help me it would be a great favour.
Thanks.",t2_4l82bmgx,False,,0,False,Beginner: Help in Reshaping,[],r/pytorch,False,6,,0,,,False,t3_g255lr,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587028381.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am a beginner, learning pytorch these days. I am having difficulty with a reshaping question. If someone can help me it would be a great favour.
Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g255lr,True,,Ranafida2017,,3,True,all_ads,False,[],False,,/r/pytorch/comments/g255lr/beginner_help_in_reshaping/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g255lr/beginner_help_in_reshaping/,7135,1586999581.0,0,,False,,,,,,,,
602,,pytorch,"Hi all,

I just built and trained a neural network. The neural network consists of an imput image, that is linearly scaled to a hidden layer with N hidden units. The results of the hidden layer are then once again linearly saled to the output layer. In both the hidden and output layer i’'m using ReLu activation function. However now i want to create a second network, which has a similar form as the previous but this time the hidden layer needs to consist of N+1 hidden units. Furthermore i would like to reuse the weights found from the previous network, and initialize the extra needed weight using a normal distribution. The net i have so far looks like this. Note, the idea is to extend this to a larger network, for the first initialization i want to use the xavier numbers. I hope someone can give some general advice.


class Neural_Network(nn.Module):

    def __init__(self, inpsize, outsize, hidsize, N, n, Weights1,Weights2):

        super(Neural_Network, self).__init__()
        self.inputSize = inpsize
        self.outputSize = outsize
        self.hiddenSize = hidsize

        if N == 0:     
            self.W1 = nn.Linear(self.inputSize,self.hiddenSize, bias=False)
            self.W2 = nn.Linear(self.hiddenSize,self.inputSize, bias=False)
        
            torch.nn.init.xavier_uniform_(self.W1.weight, gain=1.0)
            torch.nn.init.xavier_uniform_(self.W2.weight, gain=1.0)
        else:
            self.W1 = nn.Linear(self.inputSize,self.hiddenSize, bias=False)
            self.W2 = nn.Linear(self.hiddenSize,self.inputSize, bias=False)
            
    def forward(self, X):
        x = F.relu(self.W1(X))
        x = F.relu(self.W2(x))
        return x, self.W1.weight, self.W2.weight",t2_396jjp6a,False,,0,False,Reusing weights from a previous neural network using pytorch,[],r/pytorch,False,6,,0,,,False,t3_g1r9x5,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1586983736.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I just built and trained a neural network. The neural network consists of an imput image, that is linearly scaled to a hidden layer with N hidden units. The results of the hidden layer are then once again linearly saled to the output layer. In both the hidden and output layer i’&amp;#39;m using ReLu activation function. However now i want to create a second network, which has a similar form as the previous but this time the hidden layer needs to consist of N+1 hidden units. Furthermore i would like to reuse the weights found from the previous network, and initialize the extra needed weight using a normal distribution. The net i have so far looks like this. Note, the idea is to extend this to a larger network, for the first initialization i want to use the xavier numbers. I hope someone can give some general advice.&lt;/p&gt;

&lt;p&gt;class Neural_Network(nn.Module):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def __init__(self, inpsize, outsize, hidsize, N, n, Weights1,Weights2):

    super(Neural_Network, self).__init__()
    self.inputSize = inpsize
    self.outputSize = outsize
    self.hiddenSize = hidsize

    if N == 0:     
        self.W1 = nn.Linear(self.inputSize,self.hiddenSize, bias=False)
        self.W2 = nn.Linear(self.hiddenSize,self.inputSize, bias=False)

        torch.nn.init.xavier_uniform_(self.W1.weight, gain=1.0)
        torch.nn.init.xavier_uniform_(self.W2.weight, gain=1.0)
    else:
        self.W1 = nn.Linear(self.inputSize,self.hiddenSize, bias=False)
        self.W2 = nn.Linear(self.hiddenSize,self.inputSize, bias=False)

def forward(self, X):
    x = F.relu(self.W1(X))
    x = F.relu(self.W2(x))
    return x, self.W1.weight, self.W2.weight
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g1r9x5,True,,bvdm96,,2,True,all_ads,False,[],False,,/r/pytorch/comments/g1r9x5/reusing_weights_from_a_previous_neural_network/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g1r9x5/reusing_weights_from_a_previous_neural_network/,7135,1586954936.0,0,,False,,,,,,,,
603,,pytorch,"I found this [tutorial](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb) for a binary classifier using LSTM architecture. I tried to manipulate this code for a multiclass application, but some tricky errors arose (one with multiple PyTorch issues opened with very different code, so this doesn't help much.)

I'm curious does anyone have boilerplate multiclass LSTM code they could share?",t2_201d2h31,False,,0,False,Multiclass classification with LSTM architecture?,[],r/pytorch,False,6,,0,,,False,t3_g19x8u,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1586914352.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I found this &lt;a href=""https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb""&gt;tutorial&lt;/a&gt; for a binary classifier using LSTM architecture. I tried to manipulate this code for a multiclass application, but some tricky errors arose (one with multiple PyTorch issues opened with very different code, so this doesn&amp;#39;t help much.)&lt;/p&gt;

&lt;p&gt;I&amp;#39;m curious does anyone have boilerplate multiclass LSTM code they could share?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/2ZPKYFvDW2W3GEn_b66ZPEzPAgLpaXOv1lWOUHD9hzw.jpg?auto=webp&amp;s=d16e412e3c0f52817b343c1a412970bd345c067e', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/2ZPKYFvDW2W3GEn_b66ZPEzPAgLpaXOv1lWOUHD9hzw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9d85e7fde8e1c4f9655f470ac4dad1ddb69abbb5', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/2ZPKYFvDW2W3GEn_b66ZPEzPAgLpaXOv1lWOUHD9hzw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a332c4d9d0de28c5943d6bf1e36a7ecd83fbf8dc', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/2ZPKYFvDW2W3GEn_b66ZPEzPAgLpaXOv1lWOUHD9hzw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=276f55091258a014b01b9dbcbe091a0d313eb731', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'nk8Qam-1ORyEq3xWx12hZ_tkC-f80roORbE-Kj9vfEk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g19x8u,True,,jbuddy_13,,1,True,all_ads,False,[],False,,/r/pytorch/comments/g19x8u/multiclass_classification_with_lstm_architecture/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g19x8u/multiclass_classification_with_lstm_architecture/,7135,1586885552.0,0,,False,,,,,,,,
604,,pytorch,"Let me preface this by saying I'm totally a beginner when it comes to Pytorch ... That being out of the way, I am trying to implement policy based reinforcement learning for a Transformer Seq2Seq model \[abstractive summarization\].

For that, I am using a policy and target network, greedy sampling a summary from the target network and using the Categorical distribution to sample a summary \[token by token\] from the policy network \[functions attached below\].

However, tokens sampled from Categorical are random, and I would like to somehow prioritize the tokens I am sampling \[say with a threshold\] and not simply pick a random sample.

Is there anyway to do so in Pytorch, I am thinking I could either mask the tensor I am passing to categorical? 

Please let me know if you have any ideas ... 

&amp;#x200B;

    def get_categorical_distribution(model, batch, max_length):
    
        src = batch.src
        segs = batch.segs
        mask_src = batch.mask_src
        batch_size = batch.batch_size
    
        # tensor to store decoded sequence
        alive_seq = torch.full([batch_size, 1], symbols[""BOS""], dtype=torch.long, device=device)
        
        # encoding src
        src_features = model.bert(src, segs, mask_src)
            
        # initializing decoder states
        dec_states = model.decoder.init_decoder_state(src, src_features, with_cache=False)
    
        samples_log_probs = []
    
        for i in range(max_length):
    
            decoder_input = alive_seq[:, -1].view(1, -1)
            decoder_input = decoder_input.transpose(0, 1)
    
            dec_out, dec_states = model.decoder(decoder_input, src_features, dec_states, step=i)
            log_probs = model.generator(dec_out.transpose(0, 1).squeeze(0))
       
            indexes = torch.ones(batch_size, 30522).type(torch.LongTensor).to(device)
            indexes.fill_(i)
    
            token_1 = torch.gather(log_probs, 1, indexes)
            # this is where I would like to be able to prioritize tokens as the Transformer I am using has a huge vocabulary &gt; 30000 words
            multi_dist = Categorical(token_1)
            x_t = multi_dist.sample()
    
            # log_prob keeps grad which will be used for .backward operation
            log_prob = multi_dist.log_prob(x_t)
            samples_log_probs.append(log_prob)
            alive_seq = torch.cat([alive_seq, x_t.view(-1, 1)], -1)
    
        return alive_seq, samples_log_probs

&amp;#x200B;

    def get_greedy_distribution(model, batch, max_length):
    
        src = batch.src
        segs = batch.segs
        mask_src = batch.mask_src
        batch_size = batch.batch_size
    
        # tensor to store decoded sequence
        alive_seq = torch.full([batch_size, 1], symbols[""BOS""], dtype=torch.long, device=device)
    
        with torch.no_grad():
            # encoding src
            src_features = model.bert(src, segs, mask_src)
            
            # initializing decoder states
            dec_states = model.decoder.init_decoder_state(src, src_features, with_cache=False)
    
            # greedy decoding loop
            for i in range(max_length):
                # slicing because previous layer shapes must match
                decoder_input = alive_seq[:, -1].view(1, -1)
                decoder_input = decoder_input.transpose(0, 1)
                    
                # decode one step
                dec_out, dec_states = model.decoder(decoder_input, src_features, dec_states, step=i)
    
                # softmax and linear layer
                log_probs = model.generator(dec_out.transpose(0, 1).squeeze(0))
                val, ix = log_probs.topk(1)
    
                # appending decoded to sequence
                alive_seq = torch.cat([alive_seq, ix.view(-1, 1)], -1)
    
        return alive_seq",t2_56buto1b,False,,0,False,Priority sampling from Pytorch Categorical?,[],r/pytorch,False,6,,0,,,False,t3_g0ksw7,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1586819549.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let me preface this by saying I&amp;#39;m totally a beginner when it comes to Pytorch ... That being out of the way, I am trying to implement policy based reinforcement learning for a Transformer Seq2Seq model [abstractive summarization].&lt;/p&gt;

&lt;p&gt;For that, I am using a policy and target network, greedy sampling a summary from the target network and using the Categorical distribution to sample a summary [token by token] from the policy network [functions attached below].&lt;/p&gt;

&lt;p&gt;However, tokens sampled from Categorical are random, and I would like to somehow prioritize the tokens I am sampling [say with a threshold] and not simply pick a random sample.&lt;/p&gt;

&lt;p&gt;Is there anyway to do so in Pytorch, I am thinking I could either mask the tensor I am passing to categorical? &lt;/p&gt;

&lt;p&gt;Please let me know if you have any ideas ... &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_categorical_distribution(model, batch, max_length):

    src = batch.src
    segs = batch.segs
    mask_src = batch.mask_src
    batch_size = batch.batch_size

    # tensor to store decoded sequence
    alive_seq = torch.full([batch_size, 1], symbols[&amp;quot;BOS&amp;quot;], dtype=torch.long, device=device)

    # encoding src
    src_features = model.bert(src, segs, mask_src)

    # initializing decoder states
    dec_states = model.decoder.init_decoder_state(src, src_features, with_cache=False)

    samples_log_probs = []

    for i in range(max_length):

        decoder_input = alive_seq[:, -1].view(1, -1)
        decoder_input = decoder_input.transpose(0, 1)

        dec_out, dec_states = model.decoder(decoder_input, src_features, dec_states, step=i)
        log_probs = model.generator(dec_out.transpose(0, 1).squeeze(0))

        indexes = torch.ones(batch_size, 30522).type(torch.LongTensor).to(device)
        indexes.fill_(i)

        token_1 = torch.gather(log_probs, 1, indexes)
        # this is where I would like to be able to prioritize tokens as the Transformer I am using has a huge vocabulary &amp;gt; 30000 words
        multi_dist = Categorical(token_1)
        x_t = multi_dist.sample()

        # log_prob keeps grad which will be used for .backward operation
        log_prob = multi_dist.log_prob(x_t)
        samples_log_probs.append(log_prob)
        alive_seq = torch.cat([alive_seq, x_t.view(-1, 1)], -1)

    return alive_seq, samples_log_probs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_greedy_distribution(model, batch, max_length):

    src = batch.src
    segs = batch.segs
    mask_src = batch.mask_src
    batch_size = batch.batch_size

    # tensor to store decoded sequence
    alive_seq = torch.full([batch_size, 1], symbols[&amp;quot;BOS&amp;quot;], dtype=torch.long, device=device)

    with torch.no_grad():
        # encoding src
        src_features = model.bert(src, segs, mask_src)

        # initializing decoder states
        dec_states = model.decoder.init_decoder_state(src, src_features, with_cache=False)

        # greedy decoding loop
        for i in range(max_length):
            # slicing because previous layer shapes must match
            decoder_input = alive_seq[:, -1].view(1, -1)
            decoder_input = decoder_input.transpose(0, 1)

            # decode one step
            dec_out, dec_states = model.decoder(decoder_input, src_features, dec_states, step=i)

            # softmax and linear layer
            log_probs = model.generator(dec_out.transpose(0, 1).squeeze(0))
            val, ix = log_probs.topk(1)

            # appending decoded to sequence
            alive_seq = torch.cat([alive_seq, ix.view(-1, 1)], -1)

    return alive_seq
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g0ksw7,True,,andelie97,,4,True,all_ads,False,[],False,,/r/pytorch/comments/g0ksw7/priority_sampling_from_pytorch_categorical/,all_ads,False,https://www.reddit.com/r/pytorch/comments/g0ksw7/priority_sampling_from_pytorch_categorical/,7135,1586790749.0,0,,False,,,,,,,,
605,,pytorch,,t2_cvc9f,False,,0,False,t-SNE for Feature Visualization,[],r/pytorch,False,6,,0,93.0,,False,t3_g0nlmo,False,dark,0.44,,public,0,0,{},140.0,,False,[],,True,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/3-PpmkgW5_kSFXNeW5_9RkAdBqiFT3Vv_TeeQeIie9Y.jpg,False,,[],{},image,,False,,1586828137.0,text,6,,,text,i.redd.it,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/o8cjcvzwems41.jpg?auto=webp&amp;s=22e8726701fc2d72a5b37e18333514da3e55b89e', 'width': 600, 'height': 400}, 'resolutions': [{'url': 'https://preview.redd.it/o8cjcvzwems41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=af6dfe02377d2b82c451ce4c2cce80ef012b7808', 'width': 108, 'height': 72}, {'url': 'https://preview.redd.it/o8cjcvzwems41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=90bffb627b105ba07cc03bdb215cc9c153ba592a', 'width': 216, 'height': 144}, {'url': 'https://preview.redd.it/o8cjcvzwems41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3e1c1c36fa56977a2fcf55cc896438cfb2cbd567', 'width': 320, 'height': 213}], 'variants': {}, 'id': 'Q1bMM8NSv0fIyATN8xCpdb9OiQekBcvymwTMRJSL8P0'}], 'enabled': True}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,g0nlmo,True,,spmallick,,2,True,all_ads,False,[],False,,/r/pytorch/comments/g0nlmo/tsne_for_feature_visualization/,all_ads,False,https://i.redd.it/o8cjcvzwems41.jpg,7135,1586799337.0,0,,False,https://i.redd.it/o8cjcvzwems41.jpg,,,,,,,
606,,pytorch,"hello everyone, I hope you're doing good,

I'm working on my first project in deep learning and as the title says it's classification of ECG signals into multiple classes (17 precisely).

the dataset is 1000 records of patients divided into 17 folders. Each record is a 10 seconds reading of the ECG (1D array of 3600 value).

I want to know how to shape the input since I read that ""LSTMs don’t like sequences of more than 200-400 time steps"" and where to put the class label? should it be a tuple of (x, y) x being the record and y the class label?

most of what I found is about prediction and not classification except some papers but they don't explain how they shaped the data.

So if you have any advise or links or tutorials I'll be really thankful. And I apologize for such newbie question but I just wanna do things the right way. Thanks again.",t2_1zjoa41g,False,,0,False,LSTM multi-class classification of ECG,[],r/pytorch,False,6,,0,,,False,t3_fz8i6w,False,dark,1.0,,public,8,0,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},,,True,,1586650427.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;hello everyone, I hope you&amp;#39;re doing good,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on my first project in deep learning and as the title says it&amp;#39;s classification of ECG signals into multiple classes (17 precisely).&lt;/p&gt;

&lt;p&gt;the dataset is 1000 records of patients divided into 17 folders. Each record is a 10 seconds reading of the ECG (1D array of 3600 value).&lt;/p&gt;

&lt;p&gt;I want to know how to shape the input since I read that &amp;quot;LSTMs don’t like sequences of more than 200-400 time steps&amp;quot; and where to put the class label? should it be a tuple of (x, y) x being the record and y the class label?&lt;/p&gt;

&lt;p&gt;most of what I found is about prediction and not classification except some papers but they don&amp;#39;t explain how they shaped the data.&lt;/p&gt;

&lt;p&gt;So if you have any advise or links or tutorials I&amp;#39;ll be really thankful. And I apologize for such newbie question but I just wanna do things the right way. Thanks again.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fz8i6w,True,,walid_idk,,11,True,all_ads,False,[],False,,/r/pytorch/comments/fz8i6w/lstm_multiclass_classification_of_ecg/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fz8i6w/lstm_multiclass_classification_of_ecg/,7135,1586621627.0,0,,False,,,,,,,,
607,,pytorch,"What does this [new\_cdist()](https://github.com/huawei-noah/AdderNet/issues/6#issuecomment-594212162) function actually do ?

I mean that it seems to be related to a [new type of back-propagation equation and adaptive learning rate](https://github.com/huawei-noah/AdderNet/issues/5).

&amp;#x200B;

What does `_temp = torch.cdist(_temp1, _temp2, p).squeeze().transpose(0, 1)` do ?

&amp;#x200B;

    def new_cdist(p, eta):
        class cdist(torch.autograd.Function):
            @staticmethod
            def forward(ctx, W, X):
                ctx.save_for_backward(W, X)
                out = -torch.cdist(W, X, p)
                return out
    
            @staticmethod
            def backward(ctx, grad_output):
                W, X = ctx.saved_tensors
                grad_W = grad_X = None
                if ctx.needs_input_grad[0]:
                    _temp1 = torch.unsqueeze(X, 2).expand(X.shape[0], X.shape[1], W.shape[0]).permute(1, 0, 2)
                    _temp2 = torch.unsqueeze(W.transpose(0, 1), 1)
                    _temp = torch.cdist(_temp1, _temp2, p).squeeze().transpose(0, 1)
                    grad_W = torch.matmul(grad_output, _temp)
                    # print('before norm: ', torch.norm(grad_W))
                    grad_W = eta * np.sqrt(grad_W.numel()) / torch.norm(grad_W) * grad_W
                    print('after norm: ', torch.norm(grad_W))
                if ctx.needs_input_grad[1]:
                    _temp1 = torch.unsqueeze(W, 2).expand(W.shape[0], W.shape[1], X.shape[0]).permute(1, 0, 2)
                    _temp2 = torch.unsqueeze(X.transpose(0, 1), 1)
                    _temp = torch.cdist(_temp1, _temp2, p).squeeze().transpose(0, 1)
                    _temp = torch.nn.functional.hardtanh(_temp, min_val=-1., max_val=1.)
                    grad_X = torch.matmul(grad_output.transpose(0, 1), _temp)
                return grad_W, grad_X
        return cdist().apply",t2_bpftl,False,,0,False,Understanding cdist() function,[],r/pytorch,False,6,,0,,,False,t3_fz0p9m,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1586862719.0,,[],{},self,,True,,1586624562.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What does this &lt;a href=""https://github.com/huawei-noah/AdderNet/issues/6#issuecomment-594212162""&gt;new_cdist()&lt;/a&gt; function actually do ?&lt;/p&gt;

&lt;p&gt;I mean that it seems to be related to a &lt;a href=""https://github.com/huawei-noah/AdderNet/issues/5""&gt;new type of back-propagation equation and adaptive learning rate&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What does &lt;code&gt;_temp = torch.cdist(_temp1, _temp2, p).squeeze().transpose(0, 1)&lt;/code&gt; do ?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def new_cdist(p, eta):
    class cdist(torch.autograd.Function):
        @staticmethod
        def forward(ctx, W, X):
            ctx.save_for_backward(W, X)
            out = -torch.cdist(W, X, p)
            return out

        @staticmethod
        def backward(ctx, grad_output):
            W, X = ctx.saved_tensors
            grad_W = grad_X = None
            if ctx.needs_input_grad[0]:
                _temp1 = torch.unsqueeze(X, 2).expand(X.shape[0], X.shape[1], W.shape[0]).permute(1, 0, 2)
                _temp2 = torch.unsqueeze(W.transpose(0, 1), 1)
                _temp = torch.cdist(_temp1, _temp2, p).squeeze().transpose(0, 1)
                grad_W = torch.matmul(grad_output, _temp)
                # print(&amp;#39;before norm: &amp;#39;, torch.norm(grad_W))
                grad_W = eta * np.sqrt(grad_W.numel()) / torch.norm(grad_W) * grad_W
                print(&amp;#39;after norm: &amp;#39;, torch.norm(grad_W))
            if ctx.needs_input_grad[1]:
                _temp1 = torch.unsqueeze(W, 2).expand(W.shape[0], W.shape[1], X.shape[0]).permute(1, 0, 2)
                _temp2 = torch.unsqueeze(X.transpose(0, 1), 1)
                _temp = torch.cdist(_temp1, _temp2, p).squeeze().transpose(0, 1)
                _temp = torch.nn.functional.hardtanh(_temp, min_val=-1., max_val=1.)
                grad_X = torch.matmul(grad_output.transpose(0, 1), _temp)
            return grad_W, grad_X
    return cdist().apply
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?auto=webp&amp;s=b134e75c125f5c05086645695c0f7dd3390375e9', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0220f379ccc6f8cf5817f79238538ba2ffe05c70', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70cc6291695445faa1333d0a07c41b31fd9dbfcb', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/3HdwO1pjr_MTNW7DZPHcP2yBXteD2sw48Mv0KfHAU20.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=45992e6fd013e395412db216c83ded62bc740c38', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'bAKE-mgRYAKYVYhfiJp-sr0yWP8hFmq8xaav2zgIqNI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fz0p9m,True,,promach,,0,True,all_ads,False,[],False,,/r/pytorch/comments/fz0p9m/understanding_cdist_function/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fz0p9m/understanding_cdist_function/,7135,1586595762.0,2,,False,,,,,,,,
608,,pytorch,"Edit:  

Turns out it was a memory issue. I added more system memory to my docker instance and it works fine.

I have no idea why it exits without throwing any errors though.

&amp;#x200B;

Hello everyone. I'm fairly new to both pytorch and docker. I've been trying to troubleshoot this as much as possible however I'm completely stuck. My code works perfectly in Jupyter Notebook however it crashes without any error messages in docker when I try to run the model.

&amp;#x200B;

I have two scripts. The first script will take a sample from a database of text that I have, will run the bert tokenizer on it, then will split the resulting tokens into a series of batches which it then saves to disk. I then have a second script that runs inside of docker that will take each saved batch, run it through the bert model and retrieve word vectors which are then saved to disk. When I try and run the line `output = model(tokens_tensor)[0][0]`while within docker the container simply exits without any error codes. If I run this exact same code within Jupyter Notebook however it works perfectly.

&amp;#x200B;

I've tried searching Google for issues related to running pytorch in docker containers and although I can find plenty of information regarding issues with GPU's I can't find anything on this.

&amp;#x200B;

**Does anyone have any ideas on where I should start searching? It simply crashes with no errors so I'm unsure how to even start troubleshooting this.**

&amp;#x200B;

Note: I'm new to both python and docker. I'm running the container with the -it flag so that my print commands are displayed within powershell on my host

&amp;#x200B;

Thanks

&amp;#x200B;

Small version of my code with FOR loops, sanity checks, etc removed:

    try:
        with torch.no_grad():
        	print('loading:', file)
        	tokens_tensor = torch.load(inputPath + '/' + file)
        	
        	#run network
        	print('running network', i, 'of', len(files))
        	output = model(tokens_tensor)[0][0]
        	
        	#save results to disk
        	print('saving network')
        	torch.save(output, outputPath+'/'+file)
        	
        	#log reached end of loop
        	print('closing torch')
    except Exception as e:
    	#log errors
    	print('crashed on exception', e)

Here is my dockerfile:

    FROM python:3
    
    RUN pip install pandas
    RUN pip install numpy
    RUN pip install torch
    RUN pip install pytorch-pretrained-bert
    RUN pip install xlrd
    RUN pip install openpyxl
    RUN pip install tqdm
    
    ADD bert_writer.py /
    
    ENV BatchSize 16
    ENV BatchNumber 0
    ENV SentenceTokenPath bertfiles/tokens
    ENV SentenceVectorPath bertfiles/vectors
    ENV WorkerCount 1
    ENV LeadingZeros 9
    # SOLO WORKER ONLY ENV WorkerNumber 0
    
    CMD [ ""python"", ""./bert_writer.py"" ]

Full code base:

    print('started')
    import torch
    from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM
    import pandas as pd
    from tqdm import tqdm
    import numpy as np
    import re
    import time
    import os
    
    BatchSize = 16
    BatchNumber = 0
    if 'BatchSize' in os.environ: BatchSize = int(os.environ['BatchSize'])
    if 'BatchNumber' in os.environ: BatchNumber = int(os.environ['BatchNumber'])
    SentenceTokenPath = os.environ['SentenceTokenPath']
    SentenceVectorPath = os.environ['SentenceVectorPath']
    WorkerCount = 1
    if 'WorkerCount' in os.environ: WorkerCount = int(os.environ['WorkerCount'])
    WorkerNumber = 0
    if 'WorkerNumber' in os.environ: WorkerNumber = int(os.environ['WorkerNumber'])
    LeadingZeros = 9
    if 'LeadingZeros' in os.environ: LeadingZeros = int(os.environ['LeadingZeros'])
    
    assert WorkerCount &gt; WorkerNumber, 'workercount must be greater then workernumber'
    
    print('batchSize = ', BatchSize)
    print('BatchNumber = ', BatchNumber)
    print('SentenceTokenPath = ', SentenceTokenPath)
    print('SentenceVectorPath = ', SentenceVectorPath)
    
    def ProcessTensorsFolder(model, inputPath, outputPath, WorkerCount = 1, WorkerNumber = 0, LeadingZeros = 9):
    	assert WorkerCount &gt; WorkerNumber, 'workercount must be greater then workernumber'
    	
    	
    	#assert True==False, 'I NEED TO FIX THE INPUT OUTPUTS TO USE VARIABLES'
    	#create output folder if it doesn't already exist
    	if not os.path.isdir(outputPath):
    		os.makedirs(outputPath)
    	# get list of files already loaded
    	skipList = os.listdir(outputPath)
    	
    	# get file paths
    	files = os.listdir(inputPath)
    	
    	# reduce list to only tensor files and sort
    	tmp = []
    	for file in files:
    		if 'tensor' in file:
    			tmp.append(file)
    	files = tmp
    	files.sort()
    	print(len(files),'files found:')
    	for i, file in enumerate(files):
    		print(i, file)
    	
    	#run model on all batches
    	try:
    		with torch.no_grad():
    			#iterate through files on disk
    			for i, file in enumerate(tqdm(files)):
    				
    				#get batch number from file name
    				fileNumber = int(file[-LeadingZeros:])
    				
    				#if using multiple docker instances see if I need to skip file based on file name
    				if fileNumber % WorkerCount != WorkerNumber:
    					print('skipping', file,'as modulo equals', )
    				else:
    					#if file has already been processed skip it
    					if file in skipList: 
    						print('skipping', file)
    						continue
    					#validate file name matches convention in case wrong file appears in folder
    					if 'tensor' in file:
    						# load file to memory
    						print('loading:', file)
    						tokens_tensor = torch.load(inputPath + '/' + file)
    						
    						#run network
    						print('running network', i, 'of', len(files))
    						output = model(tokens_tensor)[0][0]
    						
    						#save results to disk
    						print('saving network')
    						torch.save(output, outputPath+'/'+file)
    			print('closing torch')
    	except Exception as e:
    		print('crashed on exception', e)
    	
    print('loading bert model')
    model = BertModel.from_pretrained('bert-base-uncased')
    print('loaded model')
    ProcessTensorsFolder(model, SentenceTokenPath, SentenceVectorPath)
    print('exiting')

&amp;#x200B;

TLDR: My code works perfectly in Jupyter notebook. It crashes without any error messages while inside docker on the line `output = model(tokens_tensor)[0][0]` My Google'fu has failed and I have no idea why this is failing, does anyone have any suggestions on how I proceed with troubleshooting?",t2_bkq88,False,,0,False,Script works in jupyter notebook but not docker container,[],r/pytorch,False,6,,0,,,False,t3_fyo757,False,dark,0.4,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,1586584388.0,,[],{},,,True,,1586579784.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Edit:  &lt;/p&gt;

&lt;p&gt;Turns out it was a memory issue. I added more system memory to my docker instance and it works fine.&lt;/p&gt;

&lt;p&gt;I have no idea why it exits without throwing any errors though.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Hello everyone. I&amp;#39;m fairly new to both pytorch and docker. I&amp;#39;ve been trying to troubleshoot this as much as possible however I&amp;#39;m completely stuck. My code works perfectly in Jupyter Notebook however it crashes without any error messages in docker when I try to run the model.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I have two scripts. The first script will take a sample from a database of text that I have, will run the bert tokenizer on it, then will split the resulting tokens into a series of batches which it then saves to disk. I then have a second script that runs inside of docker that will take each saved batch, run it through the bert model and retrieve word vectors which are then saved to disk. When I try and run the line &lt;code&gt;output = model(tokens_tensor)[0][0]&lt;/code&gt;while within docker the container simply exits without any error codes. If I run this exact same code within Jupyter Notebook however it works perfectly.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve tried searching Google for issues related to running pytorch in docker containers and although I can find plenty of information regarding issues with GPU&amp;#39;s I can&amp;#39;t find anything on this.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Does anyone have any ideas on where I should start searching? It simply crashes with no errors so I&amp;#39;m unsure how to even start troubleshooting this.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Note: I&amp;#39;m new to both python and docker. I&amp;#39;m running the container with the -it flag so that my print commands are displayed within powershell on my host&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Small version of my code with FOR loops, sanity checks, etc removed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;try:
    with torch.no_grad():
        print(&amp;#39;loading:&amp;#39;, file)
        tokens_tensor = torch.load(inputPath + &amp;#39;/&amp;#39; + file)

        #run network
        print(&amp;#39;running network&amp;#39;, i, &amp;#39;of&amp;#39;, len(files))
        output = model(tokens_tensor)[0][0]

        #save results to disk
        print(&amp;#39;saving network&amp;#39;)
        torch.save(output, outputPath+&amp;#39;/&amp;#39;+file)

        #log reached end of loop
        print(&amp;#39;closing torch&amp;#39;)
except Exception as e:
    #log errors
    print(&amp;#39;crashed on exception&amp;#39;, e)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is my dockerfile:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM python:3

RUN pip install pandas
RUN pip install numpy
RUN pip install torch
RUN pip install pytorch-pretrained-bert
RUN pip install xlrd
RUN pip install openpyxl
RUN pip install tqdm

ADD bert_writer.py /

ENV BatchSize 16
ENV BatchNumber 0
ENV SentenceTokenPath bertfiles/tokens
ENV SentenceVectorPath bertfiles/vectors
ENV WorkerCount 1
ENV LeadingZeros 9
# SOLO WORKER ONLY ENV WorkerNumber 0

CMD [ &amp;quot;python&amp;quot;, &amp;quot;./bert_writer.py&amp;quot; ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Full code base:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print(&amp;#39;started&amp;#39;)
import torch
from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM
import pandas as pd
from tqdm import tqdm
import numpy as np
import re
import time
import os

BatchSize = 16
BatchNumber = 0
if &amp;#39;BatchSize&amp;#39; in os.environ: BatchSize = int(os.environ[&amp;#39;BatchSize&amp;#39;])
if &amp;#39;BatchNumber&amp;#39; in os.environ: BatchNumber = int(os.environ[&amp;#39;BatchNumber&amp;#39;])
SentenceTokenPath = os.environ[&amp;#39;SentenceTokenPath&amp;#39;]
SentenceVectorPath = os.environ[&amp;#39;SentenceVectorPath&amp;#39;]
WorkerCount = 1
if &amp;#39;WorkerCount&amp;#39; in os.environ: WorkerCount = int(os.environ[&amp;#39;WorkerCount&amp;#39;])
WorkerNumber = 0
if &amp;#39;WorkerNumber&amp;#39; in os.environ: WorkerNumber = int(os.environ[&amp;#39;WorkerNumber&amp;#39;])
LeadingZeros = 9
if &amp;#39;LeadingZeros&amp;#39; in os.environ: LeadingZeros = int(os.environ[&amp;#39;LeadingZeros&amp;#39;])

assert WorkerCount &amp;gt; WorkerNumber, &amp;#39;workercount must be greater then workernumber&amp;#39;

print(&amp;#39;batchSize = &amp;#39;, BatchSize)
print(&amp;#39;BatchNumber = &amp;#39;, BatchNumber)
print(&amp;#39;SentenceTokenPath = &amp;#39;, SentenceTokenPath)
print(&amp;#39;SentenceVectorPath = &amp;#39;, SentenceVectorPath)

def ProcessTensorsFolder(model, inputPath, outputPath, WorkerCount = 1, WorkerNumber = 0, LeadingZeros = 9):
    assert WorkerCount &amp;gt; WorkerNumber, &amp;#39;workercount must be greater then workernumber&amp;#39;


    #assert True==False, &amp;#39;I NEED TO FIX THE INPUT OUTPUTS TO USE VARIABLES&amp;#39;
    #create output folder if it doesn&amp;#39;t already exist
    if not os.path.isdir(outputPath):
        os.makedirs(outputPath)
    # get list of files already loaded
    skipList = os.listdir(outputPath)

    # get file paths
    files = os.listdir(inputPath)

    # reduce list to only tensor files and sort
    tmp = []
    for file in files:
        if &amp;#39;tensor&amp;#39; in file:
            tmp.append(file)
    files = tmp
    files.sort()
    print(len(files),&amp;#39;files found:&amp;#39;)
    for i, file in enumerate(files):
        print(i, file)

    #run model on all batches
    try:
        with torch.no_grad():
            #iterate through files on disk
            for i, file in enumerate(tqdm(files)):

                #get batch number from file name
                fileNumber = int(file[-LeadingZeros:])

                #if using multiple docker instances see if I need to skip file based on file name
                if fileNumber % WorkerCount != WorkerNumber:
                    print(&amp;#39;skipping&amp;#39;, file,&amp;#39;as modulo equals&amp;#39;, )
                else:
                    #if file has already been processed skip it
                    if file in skipList: 
                        print(&amp;#39;skipping&amp;#39;, file)
                        continue
                    #validate file name matches convention in case wrong file appears in folder
                    if &amp;#39;tensor&amp;#39; in file:
                        # load file to memory
                        print(&amp;#39;loading:&amp;#39;, file)
                        tokens_tensor = torch.load(inputPath + &amp;#39;/&amp;#39; + file)

                        #run network
                        print(&amp;#39;running network&amp;#39;, i, &amp;#39;of&amp;#39;, len(files))
                        output = model(tokens_tensor)[0][0]

                        #save results to disk
                        print(&amp;#39;saving network&amp;#39;)
                        torch.save(output, outputPath+&amp;#39;/&amp;#39;+file)
            print(&amp;#39;closing torch&amp;#39;)
    except Exception as e:
        print(&amp;#39;crashed on exception&amp;#39;, e)

print(&amp;#39;loading bert model&amp;#39;)
model = BertModel.from_pretrained(&amp;#39;bert-base-uncased&amp;#39;)
print(&amp;#39;loaded model&amp;#39;)
ProcessTensorsFolder(model, SentenceTokenPath, SentenceVectorPath)
print(&amp;#39;exiting&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;TLDR: My code works perfectly in Jupyter notebook. It crashes without any error messages while inside docker on the line &lt;code&gt;output = model(tokens_tensor)[0][0]&lt;/code&gt; My Google&amp;#39;fu has failed and I have no idea why this is failing, does anyone have any suggestions on how I proceed with troubleshooting?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fyo757,True,,Celebrinborn,,2,True,all_ads,False,[],False,,/r/pytorch/comments/fyo757/script_works_in_jupyter_notebook_but_not_docker/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fyo757/script_works_in_jupyter_notebook_but_not_docker/,7135,1586550984.0,0,,False,,,,,,,,
609,,pytorch,"How to correctly modify [https://github.com/eriklindernoren/PyTorch-YOLOv3](https://github.com/eriklindernoren/PyTorch-YOLOv3) to use [https://github.com/huawei-noah/AdderNet](https://github.com/huawei-noah/AdderNet) ?

The following colab is what I have so far with the helps of others:  
 [https://colab.research.google.com/drive/1VCafwykgNKAO6144LssBFFy0TmruDNSE#scrollTo=W3e-WcVxnKfs](https://colab.research.google.com/drive/1VCafwykgNKAO6144LssBFFy0TmruDNSE#scrollTo=W3e-WcVxnKfs)

How to solve [the error on this models.py file](https://gist.github.com/promach/8ddd9794f242c24ffdaa612bcb0bfa33#file-models-py-L321) ?  


https://preview.redd.it/s35hc89ya0s41.png?width=1875&amp;format=png&amp;auto=webp&amp;s=2c84e0ca0bc0bec04cc67becd120dbf441f1c9f1",t2_bpftl,False,,0,False,Modify YOLOv3 backbone from DarkNet to AdderNet,[],r/pytorch,False,6,,0,18.0,,False,t3_fyhlcn,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/xAzvTu9lLNLvjAj6jdcFdOXqonarPnqEZJ7LehA2KdE.jpg,False,,[],{},self,,True,,1586560285.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How to correctly modify &lt;a href=""https://github.com/eriklindernoren/PyTorch-YOLOv3""&gt;https://github.com/eriklindernoren/PyTorch-YOLOv3&lt;/a&gt; to use &lt;a href=""https://github.com/huawei-noah/AdderNet""&gt;https://github.com/huawei-noah/AdderNet&lt;/a&gt; ?&lt;/p&gt;

&lt;p&gt;The following colab is what I have so far with the helps of others:&lt;br/&gt;
 &lt;a href=""https://colab.research.google.com/drive/1VCafwykgNKAO6144LssBFFy0TmruDNSE#scrollTo=W3e-WcVxnKfs""&gt;https://colab.research.google.com/drive/1VCafwykgNKAO6144LssBFFy0TmruDNSE#scrollTo=W3e-WcVxnKfs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;How to solve &lt;a href=""https://gist.github.com/promach/8ddd9794f242c24ffdaa612bcb0bfa33#file-models-py-L321""&gt;the error on this models.py file&lt;/a&gt; ?  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/s35hc89ya0s41.png?width=1875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c84e0ca0bc0bec04cc67becd120dbf441f1c9f1""&gt;https://preview.redd.it/s35hc89ya0s41.png?width=1875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c84e0ca0bc0bec04cc67becd120dbf441f1c9f1&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/CZuLZgTJxfSAyyGfc7A9fwrPL8MkXLYjwU0dZ26emqM.jpg?auto=webp&amp;s=3796c10fd3f9668ba136552d94ffa6f9a6b1363a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/CZuLZgTJxfSAyyGfc7A9fwrPL8MkXLYjwU0dZ26emqM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=607e360c3252f7e5d0fa655b4b6d5beb7d8923ae', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/CZuLZgTJxfSAyyGfc7A9fwrPL8MkXLYjwU0dZ26emqM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a29fc0bfc2070111851c3381257360d851dbee54', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/CZuLZgTJxfSAyyGfc7A9fwrPL8MkXLYjwU0dZ26emqM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6253ccc05af2b91eea6531525bf674d995eea6ec', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'HES80j8UhHmgrcusl1k4mYDV3njCeEd-2rt7xes6Cnk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fyhlcn,True,,promach,,1,True,all_ads,False,[],False,,/r/pytorch/comments/fyhlcn/modify_yolov3_backbone_from_darknet_to_addernet/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fyhlcn/modify_yolov3_backbone_from_darknet_to_addernet/,7135,1586531485.0,1,,False,,,,"{'s35hc89ya0s41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 14, 'x': 108, 'u': 'https://preview.redd.it/s35hc89ya0s41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=833def3ad36de0852ab199c295ab21a1eff0e6d1'}, {'y': 28, 'x': 216, 'u': 'https://preview.redd.it/s35hc89ya0s41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=200e031077cd095675e46fd59faf3b1af4007529'}, {'y': 42, 'x': 320, 'u': 'https://preview.redd.it/s35hc89ya0s41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e5b2ba4be6924d59cddf817ca4fc89e36ea4fae'}, {'y': 84, 'x': 640, 'u': 'https://preview.redd.it/s35hc89ya0s41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b69b863a83950dbe90a2e349a95cc35e45c667d'}, {'y': 126, 'x': 960, 'u': 'https://preview.redd.it/s35hc89ya0s41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2784ae9c4ba55826d25564e7a3046b1cd3d2c346'}, {'y': 142, 'x': 1080, 'u': 'https://preview.redd.it/s35hc89ya0s41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=64e069a0b2c22368e1ec4dccca3175a08a71143f'}], 's': {'y': 247, 'x': 1875, 'u': 'https://preview.redd.it/s35hc89ya0s41.png?width=1875&amp;format=png&amp;auto=webp&amp;s=2c84e0ca0bc0bec04cc67becd120dbf441f1c9f1'}, 'id': 's35hc89ya0s41'}}",,,,
610,,pytorch,"\[When code GAN architecture, \]

I think ones used ConvTransposed2D originally, however most of code use upsampling and Conv2D nowadays. 

Is there any special reason ones use upsampling and Conv2D instead?",t2_2t7ib9lz,False,,0,False,Question about code in GANs,[],r/pytorch,False,6,,0,,,False,t3_fyap9g,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1586531478.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[When code GAN architecture, ]&lt;/p&gt;

&lt;p&gt;I think ones used ConvTransposed2D originally, however most of code use upsampling and Conv2D nowadays. &lt;/p&gt;

&lt;p&gt;Is there any special reason ones use upsampling and Conv2D instead?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fyap9g,True,,lepoeme20,,3,True,all_ads,False,[],False,,/r/pytorch/comments/fyap9g/question_about_code_in_gans/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fyap9g/question_about_code_in_gans/,7135,1586502678.0,0,,False,,,,,,,,
611,,pytorch,"i'm working on [facenet-pytorch](https://github.com/timesler/facenet-pytorch/) library now i want to take faces using webcam after detected using mtcnnthen recognize it depend on the model which trained with evaluation mode?

thanks for responding any idea i will appreciate , this is a apart of the code

    cap = cv.VideoCapture(0) 
    while True:
        ret, frame = cap.read()
        frame = cv.resize(frame, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)     
    image = predict_draw_bounding_box(frame)
        cv.imshow('Output', image)
        c = cv.waitKey(1) if c == 27: break
    cap.release()
    cv.destroyAllWindows()",t2_2yr3b1dy,False,,0,False,how to make (predict draw bounding box) function for face recognition in real time?,[],r/pytorch,False,6,,0,,,False,t3_fx20np,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},self,,True,,1586359812.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;i&amp;#39;m working on &lt;a href=""https://github.com/timesler/facenet-pytorch/""&gt;facenet-pytorch&lt;/a&gt; library now i want to take faces using webcam after detected using mtcnnthen recognize it depend on the model which trained with evaluation mode?&lt;/p&gt;

&lt;p&gt;thanks for responding any idea i will appreciate , this is a apart of the code&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cap = cv.VideoCapture(0) 
while True:
    ret, frame = cap.read()
    frame = cv.resize(frame, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)     
image = predict_draw_bounding_box(frame)
    cv.imshow(&amp;#39;Output&amp;#39;, image)
    c = cv.waitKey(1) if c == 27: break
cap.release()
cv.destroyAllWindows()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/WZM6NYJFKpMidWZJG3OmyNsRc6YRu-H5tZHW6R7evjw.jpg?auto=webp&amp;s=1947b2b7f331b3da9a0ab8a9b11b2cd7394df6bc', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/WZM6NYJFKpMidWZJG3OmyNsRc6YRu-H5tZHW6R7evjw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bbd6ff6b05ccf3a432734514b1938eae8385bddb', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/WZM6NYJFKpMidWZJG3OmyNsRc6YRu-H5tZHW6R7evjw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe965562448dd5bf240f00cd3da44045cff7421f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/WZM6NYJFKpMidWZJG3OmyNsRc6YRu-H5tZHW6R7evjw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=938afe42496e7ae7f053b70372b88bf85a6153b5', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'SkS3aBjQkH1L5RGs_7hi_ct-eXwANI1fKqvmaNz-Wls'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fx20np,True,,artiestArt,,6,True,all_ads,False,[],False,,/r/pytorch/comments/fx20np/how_to_make_predict_draw_bounding_box_function/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fx20np/how_to_make_predict_draw_bounding_box_function/,7135,1586331012.0,0,,False,,,,,,,,
612,,pytorch,"I have tried to change nn.ReLU(True) or nn.LeakyReLU(True) from ‘Ture’ to ‘False’, but it didn’t work.

I am using Pytorch Version 1.4.0

Code is somewhat similar to this :

    model_1 = my_model()
    model_2 = my_model()
    model_3 = my_model()
    
    criterion = Loss()
    optimizer = torch.optim.Adam(list(model_1.parameters())+list(model_2.parameters())+list(model_3.parameters()), lr=learning_rate)
    
    for epoch in range(num_epochs):
    
    	out_1 = model_1(a)
    	out_2 = model_2(b)
    	out_3 = model_3(out_1+out_2)
    
    	loss = criterion(out_3,truth)
    	loss.backward()
    	optimizer.step()

Can't I optimize models where the output of 2 models is input to the third?",t2_5fglor6r,False,,0,False,Getting RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation while optimizing a linear set of models,[],r/pytorch,False,6,,0,,,False,t3_fvsoxa,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1586178729.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have tried to change nn.ReLU(True) or nn.LeakyReLU(True) from ‘Ture’ to ‘False’, but it didn’t work.&lt;/p&gt;

&lt;p&gt;I am using Pytorch Version 1.4.0&lt;/p&gt;

&lt;p&gt;Code is somewhat similar to this :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model_1 = my_model()
model_2 = my_model()
model_3 = my_model()

criterion = Loss()
optimizer = torch.optim.Adam(list(model_1.parameters())+list(model_2.parameters())+list(model_3.parameters()), lr=learning_rate)

for epoch in range(num_epochs):

    out_1 = model_1(a)
    out_2 = model_2(b)
    out_3 = model_3(out_1+out_2)

    loss = criterion(out_3,truth)
    loss.backward()
    optimizer.step()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Can&amp;#39;t I optimize models where the output of 2 models is input to the third?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fvsoxa,True,,awesomeness_infinity,,0,True,all_ads,False,[],False,,/r/pytorch/comments/fvsoxa/getting_runtimeerror_one_of_the_variables_needed/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fvsoxa/getting_runtimeerror_one_of_the_variables_needed/,7135,1586149929.0,0,,False,,,,,,,,
613,,pytorch,"I've just started to learn about RNN and I am trying to use it solve a  problem where I make 16 measurements that happen sequentially in time  and use these as input for my model to predict the 17th measurement  value (between -1 and +1). All my data is in a txt file where each row  is one experiment (so 16 values in sequence separated by TAB), the file  has a total of 2000 rows, so 2000 experiments. The 17th measurement for  each experiment is in another file where each measurement is in the  same row as its respective experiment in the first file.

So, I believe I need to create a model where each input has dimension  one and it has 16 steps before returning me the predicted result for  the 17th value. Also, I want to train my model with batches of 64  experiments. It will be a 1 layer network with about 10 neurons. I've tried many ways, but I always have some problems with the matrix  dimensions, probably because I still couldn't figure out how to use RNN  or RNNCell in pytorch.

Can anyone please guide me here?

Here is how I've defined my model:

&amp;#x200B;

    class BasicRNN(nn.Module):
    def __init__(self, batch_size, n_inputs, n_neurons):
        super(BasicRNN, self).__init__()
    
        self.rnn = nn.RNNCell(n_inputs, n_neurons)
        self.hx = torch.zeros(batch_size, n_neurons)
    
        self.fc1 = nn.Linear(n_neurons, 1)
    
        self.batch_size = batch_size
        self.n_neurons = n_neurons
    
    def init_hidden(self, ):
        return torch.zeros(self.batch_size, self.n_neurons)
    
    def forward(self, X):
    
        self.hx = self.init_hidden()
    
        for i in range(16):
            self.hx = self.rnn(X[i], self.hx)
    
        return torch.tanh(self.fc1(self.hx))

Thanks",t2_mwu7o,False,,0,False,Preparing data for RNN,[],r/pytorch,False,6,,0,,,False,t3_fvkcg5,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1586117498.0,,[],{},,,True,,1586146045.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve just started to learn about RNN and I am trying to use it solve a  problem where I make 16 measurements that happen sequentially in time  and use these as input for my model to predict the 17th measurement  value (between -1 and +1). All my data is in a txt file where each row  is one experiment (so 16 values in sequence separated by TAB), the file  has a total of 2000 rows, so 2000 experiments. The 17th measurement for  each experiment is in another file where each measurement is in the  same row as its respective experiment in the first file.&lt;/p&gt;

&lt;p&gt;So, I believe I need to create a model where each input has dimension  one and it has 16 steps before returning me the predicted result for  the 17th value. Also, I want to train my model with batches of 64  experiments. It will be a 1 layer network with about 10 neurons. I&amp;#39;ve tried many ways, but I always have some problems with the matrix  dimensions, probably because I still couldn&amp;#39;t figure out how to use RNN  or RNNCell in pytorch.&lt;/p&gt;

&lt;p&gt;Can anyone please guide me here?&lt;/p&gt;

&lt;p&gt;Here is how I&amp;#39;ve defined my model:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class BasicRNN(nn.Module):
def __init__(self, batch_size, n_inputs, n_neurons):
    super(BasicRNN, self).__init__()

    self.rnn = nn.RNNCell(n_inputs, n_neurons)
    self.hx = torch.zeros(batch_size, n_neurons)

    self.fc1 = nn.Linear(n_neurons, 1)

    self.batch_size = batch_size
    self.n_neurons = n_neurons

def init_hidden(self, ):
    return torch.zeros(self.batch_size, self.n_neurons)

def forward(self, X):

    self.hx = self.init_hidden()

    for i in range(16):
        self.hx = self.rnn(X[i], self.hx)

    return torch.tanh(self.fc1(self.hx))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fvkcg5,True,,arivar,,3,True,all_ads,False,[],False,,/r/pytorch/comments/fvkcg5/preparing_data_for_rnn/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fvkcg5/preparing_data_for_rnn/,7135,1586117245.0,0,,False,,,,,,,,
614,,pytorch,"Is there any difference between h\_n and output\[-1\] in nn.LSTM outputs? Both are the last time step output from a LSTM, right?",t2_2ixta5gv,False,,0,False,nn.LSTM output differences,[],r/pytorch,False,6,,0,,,False,t3_fvbgtm,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1586109677.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there any difference between h_n and output[-1] in nn.LSTM outputs? Both are the last time step output from a LSTM, right?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fvbgtm,True,,Grubzer,,3,True,all_ads,False,[],False,,/r/pytorch/comments/fvbgtm/nnlstm_output_differences/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fvbgtm/nnlstm_output_differences/,7135,1586080877.0,0,,False,,,,,,,,
615,,pytorch,"Hi guys,

I am a newbie to PyTorch and wanted to reach out for some help with a project I am currently working on. Any help or advice on how to implement this project would be greatly appreciated.

The project:
I am trying to implement a CNN simulation of a synaptic transistor. The main challenge I am facing is in replicating how a weight works in hardware. Without going into excessive detail about the hardware, synaptic transistors are used to store the weights connecting neurons, and these weights have discrete weight states as opposed to being a continuous variable.

To give a brief example, let’s say I normalize all weights such that they are in the range [0,1]. In software, a weight can have any value in this range and is only limited by your bit precision. However if we use synaptic transistor hardware, the only allowed weight values might be {0, 0.25, 0.63, 0.76, 0.9, 1}. This means that if my weight started at 1, and I needed to update the weight to 0.856739 during backprop, the weight would actually update to 0.9 since that’s the closest state. Similarly if the weight needed to update to 0.13, it would update to 0.25 instead (since 0.25 is the closest state, not 0). These discrete states would be a property of a layer in the network and would need to work as stated during training (as opposed to something that would be done once post-training, like post-training quantization).

How would one go about implementing the simple example above?

From an initial search, it seems the best way to approach implementing the discrete weight states above might be to write a custom optimizer. However, I am just learning PyTorch and would really appreciate any advice and feedback. 

Thanks again!

Tl;dr: how can I implement a CNN simulation with discrete weight states?",t2_2ioikgki,False,,0,False,Help with custom optimizer,[],r/pytorch,False,6,,0,,,False,t3_fu7575,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1585944000.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys,&lt;/p&gt;

&lt;p&gt;I am a newbie to PyTorch and wanted to reach out for some help with a project I am currently working on. Any help or advice on how to implement this project would be greatly appreciated.&lt;/p&gt;

&lt;p&gt;The project:
I am trying to implement a CNN simulation of a synaptic transistor. The main challenge I am facing is in replicating how a weight works in hardware. Without going into excessive detail about the hardware, synaptic transistors are used to store the weights connecting neurons, and these weights have discrete weight states as opposed to being a continuous variable.&lt;/p&gt;

&lt;p&gt;To give a brief example, let’s say I normalize all weights such that they are in the range [0,1]. In software, a weight can have any value in this range and is only limited by your bit precision. However if we use synaptic transistor hardware, the only allowed weight values might be {0, 0.25, 0.63, 0.76, 0.9, 1}. This means that if my weight started at 1, and I needed to update the weight to 0.856739 during backprop, the weight would actually update to 0.9 since that’s the closest state. Similarly if the weight needed to update to 0.13, it would update to 0.25 instead (since 0.25 is the closest state, not 0). These discrete states would be a property of a layer in the network and would need to work as stated during training (as opposed to something that would be done once post-training, like post-training quantization).&lt;/p&gt;

&lt;p&gt;How would one go about implementing the simple example above?&lt;/p&gt;

&lt;p&gt;From an initial search, it seems the best way to approach implementing the discrete weight states above might be to write a custom optimizer. However, I am just learning PyTorch and would really appreciate any advice and feedback. &lt;/p&gt;

&lt;p&gt;Thanks again!&lt;/p&gt;

&lt;p&gt;Tl;dr: how can I implement a CNN simulation with discrete weight states?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fu7575,True,,lolcopter01,,5,True,all_ads,False,[],False,,/r/pytorch/comments/fu7575/help_with_custom_optimizer/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fu7575/help_with_custom_optimizer/,7135,1585915200.0,0,,False,,,,,,,,
616,,pytorch,"Am i missing something? You should download it with python console? How can you tell?

Main page says: ""This repository contains an official pytorch implementation.""

For example: [https://github.com/shunsukesaito/PIFuHD](https://github.com/shunsukesaito/PIFuHD)",t2_11qalp,False,,0,False,I see these repositories of some researches in Pytorch on github but there is no code.,[],r/pytorch,False,6,,0,,,False,t3_ftp1tp,False,dark,0.77,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},self,,True,,1585872605.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Am i missing something? You should download it with python console? How can you tell?&lt;/p&gt;

&lt;p&gt;Main page says: &amp;quot;This repository contains an official pytorch implementation.&amp;quot;&lt;/p&gt;

&lt;p&gt;For example: &lt;a href=""https://github.com/shunsukesaito/PIFuHD""&gt;https://github.com/shunsukesaito/PIFuHD&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/usDvAHHkQEwRkVtYGaPhqKfoyeQONLn91pO2c4bbZ6I.jpg?auto=webp&amp;s=c392a0834de4eb7896daedba5673f71dce655baf', 'width': 640, 'height': 299}, 'resolutions': [{'url': 'https://external-preview.redd.it/usDvAHHkQEwRkVtYGaPhqKfoyeQONLn91pO2c4bbZ6I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0ff25343ae5721884a967c6a158a44011cd6dd6b', 'width': 108, 'height': 50}, {'url': 'https://external-preview.redd.it/usDvAHHkQEwRkVtYGaPhqKfoyeQONLn91pO2c4bbZ6I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6095da59fabbe84f4c39f1994caf15d775d6d61', 'width': 216, 'height': 100}, {'url': 'https://external-preview.redd.it/usDvAHHkQEwRkVtYGaPhqKfoyeQONLn91pO2c4bbZ6I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=72d43460d9d3d3faa8e2ed692dce397d9ad2ad21', 'width': 320, 'height': 149}, {'url': 'https://external-preview.redd.it/usDvAHHkQEwRkVtYGaPhqKfoyeQONLn91pO2c4bbZ6I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0963e7dc892f02ceefefd55d83e6a75bc72816d6', 'width': 640, 'height': 299}], 'variants': {}, 'id': 'f3hG6XTBe9RHntgjYsodgqfIpugGJUHliuLgdQ2D7J0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ftp1tp,True,,YanDaik,,4,True,all_ads,False,[],False,,/r/pytorch/comments/ftp1tp/i_see_these_repositories_of_some_researches_in/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ftp1tp/i_see_these_repositories_of_some_researches_in/,7135,1585843805.0,0,,False,,,,,,,,
617,,pytorch,"I was working on this  [https://www.kaggle.com/gti-upm/leapgestrecog](https://www.kaggle.com/gti-upm/leapgestrecog)  data set lately. Its a hand gesture dataset and I was trying to make a classifier. Due to images available in different types of folder I made my on data loader. Here it is

    class DatasetLoader(Dataset):
        def __init__(self,path):
            self.path_list = path
            self.labels = []
            self.to_tensor = transforms.ToTensor()
            self.resize = transforms.Resize((120,320))
            self.gray = transforms.Grayscale(num_output_channels = 1)
            self._init_dataset()
        def _init_dataset(self):
            labels = set()
            for diro in os.listdir(""/kaggle/input/leapgestrecog/leapGestRecog""):
                for d in os.listdir(os.path.join(""/kaggle/input/leapgestrecog/leapGestRecog"",diro)):
                    if len(d.split('_'))&gt;2:
                        labels.add(""_"".join(d.split(""_"")[-2:]))
                    else:
                        labels.add(d.split(""_"")[-1])
            labels = list(labels)
            ## help me on this line with some codes
        def __getitem__(self,idx):
            if torch.is_tensor(idx):
                idx = idx.tolist()
            img_name = self.path_list[idx]
            img = Image.open(img_name)
            img = self.resize(img)
            img = self.gray(img)
            img = self.to_tensor(img)
            if len(img_name.split('/')[-2].split('_')) &gt; 2:
                label = ""_"".join(img_name.split('/')[-2].split('_')[-2:])
            else:
                label = img_name.split('/')[-2].split('_')[-1]
            label = ## Here also
            return img,label
        def __len__(self):
            return len(self.path_list)

I have problem with label which I am getting from this dataset loader. As I have created a model which takes n batches of data with 10 classes so during loss calculation I need my labels to of size(n,10). I dont know what to do. Here is my network design:

    class Net(nn.Module):
        def __init__(self):
            super(Net,self).__init__()
            self.conv1 = nn.Conv2d(1,32,5)
            self.pool = nn.MaxPool2d(2,2)
            self.conv2 = nn.Conv2d(32,64,3)
            self.conv3 = nn.Conv2d(64,64,3)
            self.fc1 = nn.Linear(64*38*13,128)
            self.fc2 = nn.Linear(128,10)
        def forward(self,x):
            x = self.pool(F.relu(self.conv1(x)))
            x = self.pool(F.relu(self.conv2(x)))
            x = self.pool(F.relu(self.conv3(x)))
            x = x.view(64,64*38*13)
            x = F.relu(self.fc1(x))
            return F.log_softmax(self.fc2(x),dim = 1)

&amp;#x200B;",t2_5ij540b2,False,,0,False,Need some help in Pytorch Image Classifier with custom data loader,[],r/pytorch,False,6,,0,,,False,t3_fsyl9c,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1585847914.0,,[],{},self,,True,,1585771887.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was working on this  &lt;a href=""https://www.kaggle.com/gti-upm/leapgestrecog""&gt;https://www.kaggle.com/gti-upm/leapgestrecog&lt;/a&gt;  data set lately. Its a hand gesture dataset and I was trying to make a classifier. Due to images available in different types of folder I made my on data loader. Here it is&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class DatasetLoader(Dataset):
    def __init__(self,path):
        self.path_list = path
        self.labels = []
        self.to_tensor = transforms.ToTensor()
        self.resize = transforms.Resize((120,320))
        self.gray = transforms.Grayscale(num_output_channels = 1)
        self._init_dataset()
    def _init_dataset(self):
        labels = set()
        for diro in os.listdir(&amp;quot;/kaggle/input/leapgestrecog/leapGestRecog&amp;quot;):
            for d in os.listdir(os.path.join(&amp;quot;/kaggle/input/leapgestrecog/leapGestRecog&amp;quot;,diro)):
                if len(d.split(&amp;#39;_&amp;#39;))&amp;gt;2:
                    labels.add(&amp;quot;_&amp;quot;.join(d.split(&amp;quot;_&amp;quot;)[-2:]))
                else:
                    labels.add(d.split(&amp;quot;_&amp;quot;)[-1])
        labels = list(labels)
        ## help me on this line with some codes
    def __getitem__(self,idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        img_name = self.path_list[idx]
        img = Image.open(img_name)
        img = self.resize(img)
        img = self.gray(img)
        img = self.to_tensor(img)
        if len(img_name.split(&amp;#39;/&amp;#39;)[-2].split(&amp;#39;_&amp;#39;)) &amp;gt; 2:
            label = &amp;quot;_&amp;quot;.join(img_name.split(&amp;#39;/&amp;#39;)[-2].split(&amp;#39;_&amp;#39;)[-2:])
        else:
            label = img_name.split(&amp;#39;/&amp;#39;)[-2].split(&amp;#39;_&amp;#39;)[-1]
        label = ## Here also
        return img,label
    def __len__(self):
        return len(self.path_list)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have problem with label which I am getting from this dataset loader. As I have created a model which takes n batches of data with 10 classes so during loss calculation I need my labels to of size(n,10). I dont know what to do. Here is my network design:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Net(nn.Module):
    def __init__(self):
        super(Net,self).__init__()
        self.conv1 = nn.Conv2d(1,32,5)
        self.pool = nn.MaxPool2d(2,2)
        self.conv2 = nn.Conv2d(32,64,3)
        self.conv3 = nn.Conv2d(64,64,3)
        self.fc1 = nn.Linear(64*38*13,128)
        self.fc2 = nn.Linear(128,10)
    def forward(self,x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = x.view(64,64*38*13)
        x = F.relu(self.fc1(x))
        return F.log_softmax(self.fc2(x),dim = 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/8fhGXBgFab8J90fmrOfTpC9YX_-JhyrQRynqoJdKmg4.jpg?auto=webp&amp;s=4fb61b4549c2bbcb98b90b25c7dbcb7d64a06bc9', 'width': 1200, 'height': 1200}, 'resolutions': [{'url': 'https://external-preview.redd.it/8fhGXBgFab8J90fmrOfTpC9YX_-JhyrQRynqoJdKmg4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4e6c4d635f715a1c2d1c41a7e95369e83541ada1', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/8fhGXBgFab8J90fmrOfTpC9YX_-JhyrQRynqoJdKmg4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=52d9c5d22ded4488487acc7b3ae7f74aeeaff7dd', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/8fhGXBgFab8J90fmrOfTpC9YX_-JhyrQRynqoJdKmg4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=162c2e1502bf32ddd5664a6bf9b11a7a68575590', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/8fhGXBgFab8J90fmrOfTpC9YX_-JhyrQRynqoJdKmg4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=44961e8cbf5fbdfa15623560af80942dbfbf652e', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/8fhGXBgFab8J90fmrOfTpC9YX_-JhyrQRynqoJdKmg4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=194cd9004c8cd5f575b65801ba6925c89be3644d', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/8fhGXBgFab8J90fmrOfTpC9YX_-JhyrQRynqoJdKmg4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=04b52120ef86857e5ce0f1103b57b5225426db2d', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'Kgd75pFBtm1kpKolf01GkCl4qJYp4-Dpo9ajuBJXYZg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fsyl9c,True,,Ambesh_sk,,4,True,all_ads,False,[],False,,/r/pytorch/comments/fsyl9c/need_some_help_in_pytorch_image_classifier_with/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fsyl9c/need_some_help_in_pytorch_image_classifier_with/,7135,1585743087.0,0,,False,,,,,,,,
618,,pytorch,,t2_3g275pc4,False,,0,False,Wrote a Blog on Graph based SLAM,[],r/pytorch,False,6,,0,,,False,t3_fssj88,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,default,False,,[],{},,,False,,1585743715.0,text,6,,,text,link.medium.com,False,,,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fssj88,True,,Nailer_Owl,,0,True,all_ads,False,[],False,,/r/pytorch/comments/fssj88/wrote_a_blog_on_graph_based_slam/,all_ads,False,https://link.medium.com/5xvEDNJVj5,7135,1585714915.0,0,,False,https://link.medium.com/5xvEDNJVj5,,,,,,,
619,,pytorch,,t2_1bq357tt,False,,0,False,[P] torchlayers: Shape inference for PyTorch (like in Keras) + new SoTA layers!,[],r/pytorch,False,6,,0,,,False,t3_fro86j,False,dark,0.9,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,default,False,,[],{},link,,False,,1585592463.0,text,6,,,text,self.MachineLearning,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jzGWvkTTccuSQzDdQb60oxImw17s14f940b7f9LcfpY.jpg?auto=webp&amp;s=33ac43da1bd14d49a3b920ade7b6b5f62c83e44a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/jzGWvkTTccuSQzDdQb60oxImw17s14f940b7f9LcfpY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=217f9806047602b4dd0ffd922ead2bf9f0cb1476', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/jzGWvkTTccuSQzDdQb60oxImw17s14f940b7f9LcfpY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1bd535e4506e607faed0a3726a99b43eeb94c98e', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/jzGWvkTTccuSQzDdQb60oxImw17s14f940b7f9LcfpY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b96130017586140a80c75a33a92ddff4af3eb298', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'S4UiFajST-sn1ptkHDBThjToZTcyghHRSajwYoGsv4k'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fro86j,True,,szymonmaszke,,2,True,all_ads,False,[],False,,/r/pytorch/comments/fro86j/p_torchlayers_shape_inference_for_pytorch_like_in/,all_ads,False,/r/MachineLearning/comments/fqj60m/p_torchlayers_shape_inference_for_pytorch_like_in/,7135,1585563663.0,0,,False,/r/MachineLearning/comments/fqj60m/p_torchlayers_shape_inference_for_pytorch_like_in/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'See [GitHub](https://github.com/szymonmaszke/torchlayers) for full project description. \n\n## Fast features overview\n \n* Shape inference in PyTorch known from Keras (during first pass of data `in_features` will be automatically added) \n* Support **for all provided PyTorch layers** (including transformers, convolutions etc.)\n*  Dimension inference (`torchlayers.Conv` during inference pass can switch to `1D`, `2D` or `3D`, similarly for other layers with ""D"")\n*  Additional layers (mostly convolution layers known from ImageNet like `InvertedResidualBottleneck` from EfficientNet, `Poly` layers, `Fire` etc.)\n*  Padding `same` for any convolution for odd kernel/dilation/stride values and other useful defaults \n* No performance penalty (model will be ""compiled to"" PyTorch) \n* JIT support because above \n\n## Mini-Example \n\nSimple example usage (see comments, harder example with autoencoder-like architecture inside [README](https://github.com/szymonmaszke/torchlayers))\n\n\n    import torch\n    import torchlayers\n\n    # torch.nn and torchlayers can be mixed easily\n    model = torch.nn.Sequential(\n        torchlayers.Conv(64, kernel_size=5),  # specify ONLY out_channels\n        torch.nn.ReLU(),  # use torch.nn wherever you wish\n        torchlayers.BatchNorm(),  # BatchNorm2d inferred from input\n        torchlayers.Conv(128),  # Default kernel_size equal to 3\n        torchlayers.ReLU(),\n        torchlayers.Conv(256, kernel_size=11),  # ""same"" padding as default\n        torchlayers.SqueezeExcitation() # new layer with shape inference\n        torchlayers.GlobalMaxPool(),  # Known from Keras\n        torchlayers.Linear(10),\n    )\n\n    # Build your model into original PyTorch  using example input\n    model = torchlayers.build(model, torch.randn(1, 3, 32, 32))\n\n\n## Links/Installation \n\n* Star on [GitHub](https://github.com/szymonmaszke/torchlayers) if you find it useful/interesting, thanks! \n* Check out [documentation](https://szymonmaszke.github.io/torchlayers/) directly\n* Install via `pip` (see the package [here](https://pypi.org/project/torchlayers/)) (Python `3.7`)', 'author_fullname': 't2_1bq357tt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[P] torchlayers: Shape inference for PyTorch (like in Keras) + new SoTA layers!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'four', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_fqj60m', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 181, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 181, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': 1585420214.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1585426776.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': True, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;See &lt;a href=""https://github.com/szymonmaszke/torchlayers""&gt;GitHub&lt;/a&gt; for full project description. &lt;/p&gt;\n\n&lt;h2&gt;Fast features overview&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Shape inference in PyTorch known from Keras (during first pass of data &lt;code&gt;in_features&lt;/code&gt; will be automatically added) &lt;/li&gt;\n&lt;li&gt;Support &lt;strong&gt;for all provided PyTorch layers&lt;/strong&gt; (including transformers, convolutions etc.)&lt;/li&gt;\n&lt;li&gt; Dimension inference (&lt;code&gt;torchlayers.Conv&lt;/code&gt; during inference pass can switch to &lt;code&gt;1D&lt;/code&gt;, &lt;code&gt;2D&lt;/code&gt; or &lt;code&gt;3D&lt;/code&gt;, similarly for other layers with &amp;quot;D&amp;quot;)&lt;/li&gt;\n&lt;li&gt; Additional layers (mostly convolution layers known from ImageNet like &lt;code&gt;InvertedResidualBottleneck&lt;/code&gt; from EfficientNet, &lt;code&gt;Poly&lt;/code&gt; layers, &lt;code&gt;Fire&lt;/code&gt; etc.)&lt;/li&gt;\n&lt;li&gt; Padding &lt;code&gt;same&lt;/code&gt; for any convolution for odd kernel/dilation/stride values and other useful defaults &lt;/li&gt;\n&lt;li&gt;No performance penalty (model will be &amp;quot;compiled to&amp;quot; PyTorch) &lt;/li&gt;\n&lt;li&gt;JIT support because above &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Mini-Example&lt;/h2&gt;\n\n&lt;p&gt;Simple example usage (see comments, harder example with autoencoder-like architecture inside &lt;a href=""https://github.com/szymonmaszke/torchlayers""&gt;README&lt;/a&gt;)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import torch\nimport torchlayers\n\n# torch.nn and torchlayers can be mixed easily\nmodel = torch.nn.Sequential(\n    torchlayers.Conv(64, kernel_size=5),  # specify ONLY out_channels\n    torch.nn.ReLU(),  # use torch.nn wherever you wish\n    torchlayers.BatchNorm(),  # BatchNorm2d inferred from input\n    torchlayers.Conv(128),  # Default kernel_size equal to 3\n    torchlayers.ReLU(),\n    torchlayers.Conv(256, kernel_size=11),  # &amp;quot;same&amp;quot; padding as default\n    torchlayers.SqueezeExcitation() # new layer with shape inference\n    torchlayers.GlobalMaxPool(),  # Known from Keras\n    torchlayers.Linear(10),\n)\n\n# Build your model into original PyTorch  using example input\nmodel = torchlayers.build(model, torch.randn(1, 3, 32, 32))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h2&gt;Links/Installation&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Star on &lt;a href=""https://github.com/szymonmaszke/torchlayers""&gt;GitHub&lt;/a&gt; if you find it useful/interesting, thanks! &lt;/li&gt;\n&lt;li&gt;Check out &lt;a href=""https://szymonmaszke.github.io/torchlayers/""&gt;documentation&lt;/a&gt; directly&lt;/li&gt;\n&lt;li&gt;Install via &lt;code&gt;pip&lt;/code&gt; (see the package &lt;a href=""https://pypi.org/project/torchlayers/""&gt;here&lt;/a&gt;) (Python &lt;code&gt;3.7&lt;/code&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/jzGWvkTTccuSQzDdQb60oxImw17s14f940b7f9LcfpY.jpg?auto=webp&amp;s=33ac43da1bd14d49a3b920ade7b6b5f62c83e44a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/jzGWvkTTccuSQzDdQb60oxImw17s14f940b7f9LcfpY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=217f9806047602b4dd0ffd922ead2bf9f0cb1476', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/jzGWvkTTccuSQzDdQb60oxImw17s14f940b7f9LcfpY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1bd535e4506e607faed0a3726a99b43eeb94c98e', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/jzGWvkTTccuSQzDdQb60oxImw17s14f940b7f9LcfpY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b96130017586140a80c75a33a92ddff4af3eb298', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'S4UiFajST-sn1ptkHDBThjToZTcyghHRSajwYoGsv4k'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'fqj60m', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'szymonmaszke', 'discussion_type': None, 'num_comments': 25, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/fqj60m/p_torchlayers_shape_inference_for_pytorch_like_in/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/fqj60m/p_torchlayers_shape_inference_for_pytorch_like_in/', 'subreddit_subscribers': 1740777, 'created_utc': 1585397976.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_fqj60m,,,,,
620,,pytorch,,t2_1ffz9tjt,False,,0,False,[Source code with demo] Here is my python implementation of Deep Q-learning for playing Tetris,[],r/pytorch,False,6,,0,140.0,,False,t3_fr717n,False,dark,0.9,,public,26,0,{},140.0,,False,[],,True,False,,{},,False,26,,False,https://b.thumbs.redditmedia.com/mtEFApsI58B1lPgVNwtMOAp__lHZaa9uir72CCAihaU.jpg,False,,[],{},image,,False,,1585522328.0,text,6,,,text,i.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/zp2qcrookmp41.gif?format=png8&amp;s=79e2fa8471fcb9d62fdc51bb5edcdceb2c0dfb47', 'width': 450, 'height': 600}, 'resolutions': [{'url': 'https://preview.redd.it/zp2qcrookmp41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=331564e73018cc595569f70817d756d0215a6a5f', 'width': 108, 'height': 144}, {'url': 'https://preview.redd.it/zp2qcrookmp41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=e89cd3df2cfb6871ea649495cc89fc069a2a33b0', 'width': 216, 'height': 288}, {'url': 'https://preview.redd.it/zp2qcrookmp41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=8b706ea86c02a01dd30bcd8c02f7d46e1ee6578e', 'width': 320, 'height': 426}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/zp2qcrookmp41.gif?s=21bb426c4fa8f2272e8d0d1bbe5929a2393ef0cf', 'width': 450, 'height': 600}, 'resolutions': [{'url': 'https://preview.redd.it/zp2qcrookmp41.gif?width=108&amp;crop=smart&amp;s=9bb5efc4b489eef08a6a3012d628c6962e6fcd1f', 'width': 108, 'height': 144}, {'url': 'https://preview.redd.it/zp2qcrookmp41.gif?width=216&amp;crop=smart&amp;s=2eb1eab3e332e7dcdf1d8c963a0213d8ff50a52b', 'width': 216, 'height': 288}, {'url': 'https://preview.redd.it/zp2qcrookmp41.gif?width=320&amp;crop=smart&amp;s=a9afb4f87982494d559d618110301f2a93161c27', 'width': 320, 'height': 426}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/zp2qcrookmp41.gif?format=mp4&amp;s=0d3548e1480d8757c8260860a6e3258e63b43b08', 'width': 450, 'height': 600}, 'resolutions': [{'url': 'https://preview.redd.it/zp2qcrookmp41.gif?width=108&amp;format=mp4&amp;s=50027c5ed46b091107a6a9e743825afcb68bae77', 'width': 108, 'height': 144}, {'url': 'https://preview.redd.it/zp2qcrookmp41.gif?width=216&amp;format=mp4&amp;s=c55f0673278cb5646b42af36a8c0fd4f2bb063a5', 'width': 216, 'height': 288}, {'url': 'https://preview.redd.it/zp2qcrookmp41.gif?width=320&amp;format=mp4&amp;s=3d822e22e281ef1ea4f6304a00a8142c9b69f1ae', 'width': 320, 'height': 426}]}}, 'id': '8Khb60WrPUTI7fSOdSqyogArY-MSzUhTUNQrII5W_tc'}], 'enabled': True}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fr717n,True,,1991viet,,1,True,all_ads,False,[],False,,/r/pytorch/comments/fr717n/source_code_with_demo_here_is_my_python/,all_ads,False,https://i.redd.it/zp2qcrookmp41.gif,7135,1585493528.0,0,,False,https://i.redd.it/zp2qcrookmp41.gif,,,,,,,
621,,pytorch,"Hello,

I am trying to train my network with provided cifar dataset. 
The shape of the tensor is (32, 32, 3) per image. However, my CNN requires the input dimension to be 3. (3, 32, 32).
Is it okay to make the tensors fit by doing tensor.view(-1, 3, 32, 32)?
Ive tried to do so and ran some epochs, the model didn’t seem to lear anything. 
Thanks in advance for your help!",t2_162rbg,False,,0,False,Tensor conversion(noob),[],r/pytorch,False,6,,0,,,False,t3_fr46g6,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1585508315.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am trying to train my network with provided cifar dataset. 
The shape of the tensor is (32, 32, 3) per image. However, my CNN requires the input dimension to be 3. (3, 32, 32).
Is it okay to make the tensors fit by doing tensor.view(-1, 3, 32, 32)?
Ive tried to do so and ran some epochs, the model didn’t seem to lear anything. 
Thanks in advance for your help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fr46g6,True,,Mexican_TronaldDump,,6,True,all_ads,False,[],False,,/r/pytorch/comments/fr46g6/tensor_conversionnoob/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fr46g6/tensor_conversionnoob/,7135,1585479515.0,0,,False,,,,,,,,True
622,,pytorch,"Hey

&amp;#x200B;

I was trying to export my python programm which uses the pytorch library. I tried to do it with pyinstaller but I got the following error when I executed the .exe file:

Traceback (most recent call last):

File ""data\_evaluation.py"", line 20, in &lt;module&gt;

ModuleNotFoundError: No module named 'torch'

\[8496\] Failed to execute script data\_evaluation

On further research I discovered that Pyinstaller apparently doesn't support pytorch (found it here:  [https://github.com/pyinstaller/pyinstaller/wiki/Supported-Packages](https://github.com/pyinstaller/pyinstaller/wiki/Supported-Packages) )

Now to my Questions:

\- Is there a way to solve this problem and still export it with pyinstaller?

\- If not, is there another way to export any PyTorch project?

&amp;#x200B;

Thanks for your time and efforts!",t2_yenq3,False,,0,False,Executable PyTorch application using Pyinstaller?,[],r/pytorch,False,6,,0,,,False,t3_fpynxo,False,dark,0.8,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,1585330582.0,,[],{},self,,True,,1585351050.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I was trying to export my python programm which uses the pytorch library. I tried to do it with pyinstaller but I got the following error when I executed the .exe file:&lt;/p&gt;

&lt;p&gt;Traceback (most recent call last):&lt;/p&gt;

&lt;p&gt;File &amp;quot;data_evaluation.py&amp;quot;, line 20, in &amp;lt;module&amp;gt;&lt;/p&gt;

&lt;p&gt;ModuleNotFoundError: No module named &amp;#39;torch&amp;#39;&lt;/p&gt;

&lt;p&gt;[8496] Failed to execute script data_evaluation&lt;/p&gt;

&lt;p&gt;On further research I discovered that Pyinstaller apparently doesn&amp;#39;t support pytorch (found it here:  &lt;a href=""https://github.com/pyinstaller/pyinstaller/wiki/Supported-Packages""&gt;https://github.com/pyinstaller/pyinstaller/wiki/Supported-Packages&lt;/a&gt; )&lt;/p&gt;

&lt;p&gt;Now to my Questions:&lt;/p&gt;

&lt;p&gt;- Is there a way to solve this problem and still export it with pyinstaller?&lt;/p&gt;

&lt;p&gt;- If not, is there another way to export any PyTorch project?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks for your time and efforts!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/PS64Dl_OlIm8HwQc2FD1QfMq24IDOA0Bawrhm1kq0Ms.jpg?auto=webp&amp;s=2356695b5ce88d5b8d41deec9ac280c40d641464', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/PS64Dl_OlIm8HwQc2FD1QfMq24IDOA0Bawrhm1kq0Ms.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d01142aef601c9f1dd82bf390ab4322eb5c972f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/PS64Dl_OlIm8HwQc2FD1QfMq24IDOA0Bawrhm1kq0Ms.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=21f9a303a31248ed6a897c8a017cbecd215a2d22', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/PS64Dl_OlIm8HwQc2FD1QfMq24IDOA0Bawrhm1kq0Ms.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f4b1e61521174946ea81b861589100d1f555d09f', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'M2wMqVFUT97_joqmj4UxyGr9VAlvspAAzC6GGoNofqQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fpynxo,True,,EricDerIV,,2,True,all_ads,False,[],False,,/r/pytorch/comments/fpynxo/executable_pytorch_application_using_pyinstaller/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fpynxo/executable_pytorch_application_using_pyinstaller/,7135,1585322250.0,0,,False,,,,,,,,
623,,pytorch,"Hi everyone,

I was wondering why in Deep Learning a lot of times the batch size is considered not the first dimension but the second one. LSTM also has the parameters *batch\_size* to choose if the batch is the first or the second dimension of the tensor. I personally prefer the \[Batch, Seq\_len, Hidden\_dim\] instead of \[Seq\_len, Batch, Hidden\_dim\], so I do not understand why there are two ways to reshape the input. 

I have also noticed that in a lot of computer vision stuffs that the channel dimension is considered as the second one \[Batch, Channels, Height, Width\] while usually is more comfortable for us to consider the image \[Height, Width, Channels\].

I don't know if the two things I have just mentioned are related, but my question is: Does the ordering of the input tensor dimensions affects the computational time?",t2_34dbezd6,False,,0,False,Why batch_first in LSTM?,[],r/pytorch,False,6,,0,,,False,t3_fpxjm4,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1585346922.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I was wondering why in Deep Learning a lot of times the batch size is considered not the first dimension but the second one. LSTM also has the parameters &lt;em&gt;batch_size&lt;/em&gt; to choose if the batch is the first or the second dimension of the tensor. I personally prefer the [Batch, Seq_len, Hidden_dim] instead of [Seq_len, Batch, Hidden_dim], so I do not understand why there are two ways to reshape the input. &lt;/p&gt;

&lt;p&gt;I have also noticed that in a lot of computer vision stuffs that the channel dimension is considered as the second one [Batch, Channels, Height, Width] while usually is more comfortable for us to consider the image [Height, Width, Channels].&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t know if the two things I have just mentioned are related, but my question is: Does the ordering of the input tensor dimensions affects the computational time?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fpxjm4,True,,_Seoo,,2,True,all_ads,False,[],False,,/r/pytorch/comments/fpxjm4/why_batch_first_in_lstm/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fpxjm4/why_batch_first_in_lstm/,7135,1585318122.0,0,,False,,,,,,,,
624,,pytorch,"Hi,

Am I correct that when this error is thrown, I can basically reload python? Is there no way to recover from \`RuntimeError: CUDA error: device-side assert triggered\` within exception-handling, like reloading a module or anything?

&amp;#x200B;

The thing is:

I have huge datasets and I need to create vectors out of them. The problem is that a tiny fraction of some input is malformed. Searching, validating and cleaning input data before trying to create vectors from it would be a tremendous task, so I want to try to create vectors and handle failed inputs afterwards.

However, once the RuntimeError was thrown, the GPU is basically useless. Can I only overcome it by reloading the script? 

&amp;#x200B;

Is the best practice in this situation to dump the current state of your program to disk and restart it? can I do this from within the script? going through all data takes about half a day and I don't want to check the machine during this. Can I restart a python script from within the same script during error handling?RuntimeError: CUDA error: device-side assert triggered",t2_613eh,False,,0,False,Is RuntimeError: CUDA error: device-side assert triggered the end of my script?,[],r/pytorch,False,6,,0,,,False,t3_fpv4jd,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1585336301.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Am I correct that when this error is thrown, I can basically reload python? Is there no way to recover from `RuntimeError: CUDA error: device-side assert triggered` within exception-handling, like reloading a module or anything?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The thing is:&lt;/p&gt;

&lt;p&gt;I have huge datasets and I need to create vectors out of them. The problem is that a tiny fraction of some input is malformed. Searching, validating and cleaning input data before trying to create vectors from it would be a tremendous task, so I want to try to create vectors and handle failed inputs afterwards.&lt;/p&gt;

&lt;p&gt;However, once the RuntimeError was thrown, the GPU is basically useless. Can I only overcome it by reloading the script? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is the best practice in this situation to dump the current state of your program to disk and restart it? can I do this from within the script? going through all data takes about half a day and I don&amp;#39;t want to check the machine during this. Can I restart a python script from within the same script during error handling?RuntimeError: CUDA error: device-side assert triggered&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fpv4jd,True,,Coffeebender,,0,True,all_ads,False,[],False,,/r/pytorch/comments/fpv4jd/is_runtimeerror_cuda_error_deviceside_assert/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fpv4jd/is_runtimeerror_cuda_error_deviceside_assert/,7135,1585307501.0,0,,False,,,,,,,,
625,,pytorch,"Have you ever dreamt of fitting a Gaussian mixture model on hundreds of millions of high-dimensional data points? Probably not too often, but if you are keen on training GMMs on a GPU to gain speedups by some factor in the hundreds, this might be for you...

In the context of some research work, I recently wrote a library, PyCave, which provides the possibility to fit GMMs and Markov Models quickly by building directly on PyTorch and enabling training on a GPU.

Check it out and I'm happy to get some feedback!

[https://github.com/borchero/pycave](https://github.com/borchero/pycave)",t2_22mmlgir,False,,0,False,Mini-Batch Training of Gaussian Mixture Models on a GPU,[],r/pytorch,False,6,,0,,,False,t3_fpculf,False,dark,1.0,,public,13,0,{},,,False,[],,False,False,,{},,False,13,,False,self,False,,[],{},self,,True,,1585263303.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Have you ever dreamt of fitting a Gaussian mixture model on hundreds of millions of high-dimensional data points? Probably not too often, but if you are keen on training GMMs on a GPU to gain speedups by some factor in the hundreds, this might be for you...&lt;/p&gt;

&lt;p&gt;In the context of some research work, I recently wrote a library, PyCave, which provides the possibility to fit GMMs and Markov Models quickly by building directly on PyTorch and enabling training on a GPU.&lt;/p&gt;

&lt;p&gt;Check it out and I&amp;#39;m happy to get some feedback!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/borchero/pycave""&gt;https://github.com/borchero/pycave&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/oIsqnqmVWIgJS4V5TPoI9Jw1DetPTG2dZRkDZ0tvuIQ.jpg?auto=webp&amp;s=fd5e3715961d6e9effaf57c0b0cf5eacaf9cf11b', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/oIsqnqmVWIgJS4V5TPoI9Jw1DetPTG2dZRkDZ0tvuIQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=81527106bfb0b76c67ff73839c54fed0b77362b7', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/oIsqnqmVWIgJS4V5TPoI9Jw1DetPTG2dZRkDZ0tvuIQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=82cf1a30a1492cb8f6f28e6004ba33fe3521cbc3', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/oIsqnqmVWIgJS4V5TPoI9Jw1DetPTG2dZRkDZ0tvuIQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9aaa7a291924da3784119e6f940eb16d4fa2bf9e', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'EUnquBhItSlykbHdYCdN151xtSjjdWeL6zScFjYcEmc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fpculf,True,,borchero,,1,True,all_ads,False,[],False,,/r/pytorch/comments/fpculf/minibatch_training_of_gaussian_mixture_models_on/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fpculf/minibatch_training_of_gaussian_mixture_models_on/,7135,1585234503.0,0,,False,,,,,,,,
626,,pytorch,"&amp;#x200B;

Hello, I've been working on PyTorch and wanted to use Cuda tensors but I've been having trouble getting it to work.

When I use the line torch.cuda.is\_available(), it returns false.

But when I use the same line on the anaconda command prompt, it returns true. What gives? Do I need to set the device somehow? Or maybe have the interpreter include my GPU? All I want is my GPU to be recognized as CUDA usable and can use in code.

I use:

python 3.7

CUDA 10.1

GPU: GTX 1070",t2_hi7ht,False,,0,False,How to enable Cuda within pyCharm,[],r/pytorch,False,6,,0,,,False,t3_fpls62,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1585292840.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Hello, I&amp;#39;ve been working on PyTorch and wanted to use Cuda tensors but I&amp;#39;ve been having trouble getting it to work.&lt;/p&gt;

&lt;p&gt;When I use the line torch.cuda.is_available(), it returns false.&lt;/p&gt;

&lt;p&gt;But when I use the same line on the anaconda command prompt, it returns true. What gives? Do I need to set the device somehow? Or maybe have the interpreter include my GPU? All I want is my GPU to be recognized as CUDA usable and can use in code.&lt;/p&gt;

&lt;p&gt;I use:&lt;/p&gt;

&lt;p&gt;python 3.7&lt;/p&gt;

&lt;p&gt;CUDA 10.1&lt;/p&gt;

&lt;p&gt;GPU: GTX 1070&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fpls62,True,,blackfangs,,5,True,all_ads,False,[],False,,/r/pytorch/comments/fpls62/how_to_enable_cuda_within_pycharm/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fpls62/how_to_enable_cuda_within_pycharm/,7135,1585264040.0,0,,False,,,,,,,,
627,,pytorch,"Hi! One of the best things you can do during quarantine is learning a new framework, programming language or something entirely different.

But doing these courses feels kinda lonely and often you just stop doing them so I thought I’d create a site where you can find buddies to do the same course with (frankly this quarantine is becoming really boring).

The idea is that you talk regularly about your progress and problems you're facing so you can learn and grow together.

If you’re interested, take a look at [Cuddy](https://cuddy.app/?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=find_course_buddy) and sign up for the newsletter!

If enough people sign up I’ll be happy to implement the whole thing this week.

Also if you've got questions or feature ideas please do let me know in the comments! :)

**Let's destroy this virus together and take it as an opportunity to get better at what we love!**",t2_4qmi0iq4,False,,0,False,Find a course buddy during quarantine!,[],r/pytorch,False,6,,0,,,False,t3_foo7nt,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,1585133547.0,,[],{},,,True,,1585161886.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi! One of the best things you can do during quarantine is learning a new framework, programming language or something entirely different.&lt;/p&gt;

&lt;p&gt;But doing these courses feels kinda lonely and often you just stop doing them so I thought I’d create a site where you can find buddies to do the same course with (frankly this quarantine is becoming really boring).&lt;/p&gt;

&lt;p&gt;The idea is that you talk regularly about your progress and problems you&amp;#39;re facing so you can learn and grow together.&lt;/p&gt;

&lt;p&gt;If you’re interested, take a look at &lt;a href=""https://cuddy.app/?utm_source=reddit&amp;amp;utm_medium=post&amp;amp;utm_campaign=find_course_buddy""&gt;Cuddy&lt;/a&gt; and sign up for the newsletter!&lt;/p&gt;

&lt;p&gt;If enough people sign up I’ll be happy to implement the whole thing this week.&lt;/p&gt;

&lt;p&gt;Also if you&amp;#39;ve got questions or feature ideas please do let me know in the comments! :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Let&amp;#39;s destroy this virus together and take it as an opportunity to get better at what we love!&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,foo7nt,True,,agcty,,0,True,all_ads,False,[],False,,/r/pytorch/comments/foo7nt/find_a_course_buddy_during_quarantine/,all_ads,False,https://www.reddit.com/r/pytorch/comments/foo7nt/find_a_course_buddy_during_quarantine/,7135,1585133086.0,0,,False,,,,,,,,
628,,pytorch,"I am implementing a active machine learning object detection pipeline with pytorch inside a jupyter notebook. I am using fasterRCNN, COCO annotations, SGD optimizer and GPU training.

To ensure determinism i try to run one epoch of training two times and receive different losses by the end of both. The loss after the first step is always the same, so initialization is not the problem.

What i already tried:

* i made sure the order of images fed are in the same order
* jupyter kernel restartet between training runs
* batch\_size = 1, num\_workers = 1, disabled augmentation
* CPU training is deterministic(!)
* the following seeds are set:
   * seed\_number = 2
   * torch.backends.cudnn.deterministic = True
   * torch.backends.cudnn.benchmark = False
   * random.seed(seed\_number)
   * torch.manual\_seed(seed\_number)
   * torch.cuda.manual\_seed(seed\_number)
   * np.random.seed(seed\_number)
   * os.environ\['PYTHONHASHSEED'\]=str(seed\_number)

Here is a link to the primary code for the training: [https://colab.research.google.com/drive/1fPvTBt\_fXE7-vXKumh1tOURT8x\_RDgak](https://colab.research.google.com/drive/1fPvTBt_fXE7-vXKumh1tOURT8x_RDgak)(its not functional, since i just copied it out of my local jupyter but shows what i am trying to do)

The training log for two identical runs look like this:

[it starts with the same loss but then changes](https://preview.redd.it/4nvhhb198lo41.jpg?width=1578&amp;format=pjpg&amp;auto=webp&amp;s=7e3e6abf0d1a668fcecf3128979f7314a172f73a)

Please let me know if there is any additional information needed :)",t2_3p4d4hht,False,,0,False,Can't achive reproducability / determinism in pytorch training,[],r/pytorch,False,6,,0,140.0,,False,t3_fo23lt,False,dark,1.0,,public,8,0,{},140.0,,False,[],,False,False,,{},,False,8,,False,https://b.thumbs.redditmedia.com/vT1PUgbqo-BkoEEh-tmbNjEu64_XhpcL6f27zbE7ByM.jpg,1585042859.0,,[],{},self,,True,,1585070225.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am implementing a active machine learning object detection pipeline with pytorch inside a jupyter notebook. I am using fasterRCNN, COCO annotations, SGD optimizer and GPU training.&lt;/p&gt;

&lt;p&gt;To ensure determinism i try to run one epoch of training two times and receive different losses by the end of both. The loss after the first step is always the same, so initialization is not the problem.&lt;/p&gt;

&lt;p&gt;What i already tried:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;i made sure the order of images fed are in the same order&lt;/li&gt;
&lt;li&gt;jupyter kernel restartet between training runs&lt;/li&gt;
&lt;li&gt;batch_size = 1, num_workers = 1, disabled augmentation&lt;/li&gt;
&lt;li&gt;CPU training is deterministic(!)&lt;/li&gt;
&lt;li&gt;the following seeds are set:

&lt;ul&gt;
&lt;li&gt;seed_number = 2&lt;/li&gt;
&lt;li&gt;torch.backends.cudnn.deterministic = True&lt;/li&gt;
&lt;li&gt;torch.backends.cudnn.benchmark = False&lt;/li&gt;
&lt;li&gt;random.seed(seed_number)&lt;/li&gt;
&lt;li&gt;torch.manual_seed(seed_number)&lt;/li&gt;
&lt;li&gt;torch.cuda.manual_seed(seed_number)&lt;/li&gt;
&lt;li&gt;np.random.seed(seed_number)&lt;/li&gt;
&lt;li&gt;os.environ[&amp;#39;PYTHONHASHSEED&amp;#39;]=str(seed_number)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is a link to the primary code for the training: &lt;a href=""https://colab.research.google.com/drive/1fPvTBt_fXE7-vXKumh1tOURT8x_RDgak""&gt;https://colab.research.google.com/drive/1fPvTBt_fXE7-vXKumh1tOURT8x_RDgak&lt;/a&gt;(its not functional, since i just copied it out of my local jupyter but shows what i am trying to do)&lt;/p&gt;

&lt;p&gt;The training log for two identical runs look like this:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/4nvhhb198lo41.jpg?width=1578&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7e3e6abf0d1a668fcecf3128979f7314a172f73a""&gt;it starts with the same loss but then changes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Please let me know if there is any additional information needed :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/kfDh9FNsuWs5VThFyc16bdwBtJ99FmMWgOZYUyq7LPg.jpg?auto=webp&amp;s=ffc13b63083ce9180bdf2cf28705ee407b659975', 'width': 256, 'height': 256}, 'resolutions': [{'url': 'https://external-preview.redd.it/kfDh9FNsuWs5VThFyc16bdwBtJ99FmMWgOZYUyq7LPg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f53340195fae0f84ec3871625087bf02fd428ab', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/kfDh9FNsuWs5VThFyc16bdwBtJ99FmMWgOZYUyq7LPg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6722a649351a89204dede7d65ad009eea43566c', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'mAwt5FN14h93icEvsjkWqFiukRT9FqTVVq54ZHDgaos'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fo23lt,True,,sh0rt_boy,,15,True,all_ads,False,[],False,,/r/pytorch/comments/fo23lt/cant_achive_reproducability_determinism_in/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fo23lt/cant_achive_reproducability_determinism_in/,7135,1585041425.0,0,,False,,,,"{'4nvhhb198lo41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 15, 'x': 108, 'u': 'https://preview.redd.it/4nvhhb198lo41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1a2ff68dddd7bc26dd67e093a0d9a3c7c229928'}, {'y': 31, 'x': 216, 'u': 'https://preview.redd.it/4nvhhb198lo41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=532d99e26e68d8c9e8d34f33050c27e83397f3ed'}, {'y': 46, 'x': 320, 'u': 'https://preview.redd.it/4nvhhb198lo41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=413456da2207d15b77a0b1bcd520bc101f0b4da6'}, {'y': 93, 'x': 640, 'u': 'https://preview.redd.it/4nvhhb198lo41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f1b6b60247c4f10ea9a53a42b5ab6bd4924e342'}, {'y': 140, 'x': 960, 'u': 'https://preview.redd.it/4nvhhb198lo41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e509119cbf11837c6c2041844a78592f7b848f70'}, {'y': 158, 'x': 1080, 'u': 'https://preview.redd.it/4nvhhb198lo41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=16521a9e2f6eaf9e0bc813e725b8e49922f528dc'}], 's': {'y': 231, 'x': 1578, 'u': 'https://preview.redd.it/4nvhhb198lo41.jpg?width=1578&amp;format=pjpg&amp;auto=webp&amp;s=7e3e6abf0d1a668fcecf3128979f7314a172f73a'}, 'id': '4nvhhb198lo41'}}",,,,
629,,pytorch,"Hello. I want to use pytorch c++ api with Unreal Engine. I want to make a visualization tool to make training and testing much more understandable. Also training should continue while data collecting. 

Not: I considered tensorflow but c api documentation is looks bad to me.

3 questions;

1) Can I use Pytorch c++ api for mobile?

2) How is it stable? Will change in future?

3) Is there any GPU support(or plan) for mobile gpu? (not mandatory.)",t2_28kn8h29,False,,0,False,Pytorch c++,[],r/pytorch,False,6,,0,,,False,t3_fnp8fo,False,dark,0.91,,public,8,0,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},,,True,,1585017377.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello. I want to use pytorch c++ api with Unreal Engine. I want to make a visualization tool to make training and testing much more understandable. Also training should continue while data collecting. &lt;/p&gt;

&lt;p&gt;Not: I considered tensorflow but c api documentation is looks bad to me.&lt;/p&gt;

&lt;p&gt;3 questions;&lt;/p&gt;

&lt;p&gt;1) Can I use Pytorch c++ api for mobile?&lt;/p&gt;

&lt;p&gt;2) How is it stable? Will change in future?&lt;/p&gt;

&lt;p&gt;3) Is there any GPU support(or plan) for mobile gpu? (not mandatory.)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fnp8fo,True,,a1z2s3x4,,5,True,all_ads,False,[],False,,/r/pytorch/comments/fnp8fo/pytorch_c/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fnp8fo/pytorch_c/,7135,1584988577.0,0,,False,,,,,,,,
630,,pytorch,"So I've been trying to get into deep learning for a while now. I started out following pytorch tutorials, but it was a bit python heavy for me, and it felt like it takes a lot of code to accomplish some feats. I'm still very much interested in learning pytorch as that is what a research lab I work at in school uses. I'm currently watching some [fast.ai](https://fast.ai) videos, and I wanted to know if there is a pytorch equivalent of *fit\_one\_cycle* because as far as I know fast ai was built on pytorch, so I am assuming some stuff carries over - kindly correct me if I am wrong",t2_1u764t1o,False,,0,False,Fit One Cycle,[],r/pytorch,False,6,,0,,,False,t3_fnwid8,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1585042855.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I&amp;#39;ve been trying to get into deep learning for a while now. I started out following pytorch tutorials, but it was a bit python heavy for me, and it felt like it takes a lot of code to accomplish some feats. I&amp;#39;m still very much interested in learning pytorch as that is what a research lab I work at in school uses. I&amp;#39;m currently watching some &lt;a href=""https://fast.ai""&gt;fast.ai&lt;/a&gt; videos, and I wanted to know if there is a pytorch equivalent of &lt;em&gt;fit_one_cycle&lt;/em&gt; because as far as I know fast ai was built on pytorch, so I am assuming some stuff carries over - kindly correct me if I am wrong&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fnwid8,True,,BlackFreud,,2,True,all_ads,False,[],False,,/r/pytorch/comments/fnwid8/fit_one_cycle/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fnwid8/fit_one_cycle/,7135,1585014055.0,0,,False,,,,,,,,
631,,pytorch,,t2_16diqth,False,,0,False,The Python Magic Behind PyTorch,[],r/pytorch,False,6,,0,140.0,,False,t3_fnlj60,False,dark,0.6,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/rcl8TuwPSKkMnQvx4Ep4-MoxQPeefazhs27inZx9TiQ.jpg,False,,[],{},link,,False,,1585005074.0,text,6,,,text,amitness.com,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BeYiid4-09JzbAQYxEeqQCJ_gtuVL_wXwQAGDkT8E1k.jpg?auto=webp&amp;s=e0ff3ad53b64fad30ba52b07cc24df663c6af730', 'width': 222, 'height': 222}, 'resolutions': [{'url': 'https://external-preview.redd.it/BeYiid4-09JzbAQYxEeqQCJ_gtuVL_wXwQAGDkT8E1k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6e81ea67882e6e93942288b7a81884fce6b5e79a', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/BeYiid4-09JzbAQYxEeqQCJ_gtuVL_wXwQAGDkT8E1k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0308ccaa5e8a418a91739ccba5c88212e81b8838', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'kqe_S5pOG1icDl1WNQekpW_c5qdEyWfVuMWP6JXpys8'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fnlj60,True,,amitness,,3,True,all_ads,False,[],False,,/r/pytorch/comments/fnlj60/the_python_magic_behind_pytorch/,all_ads,False,https://amitness.com/2020/03/python-magic-behind-pytorch/,7135,1584976274.0,0,,False,https://amitness.com/2020/03/python-magic-behind-pytorch/,,,,,,,
632,,pytorch,I want to make contributions to pytorch but i cant find a good issue and simple issue( either someone is already working on it or its too advanced) .  I have worked on pytorch computer vision projects and i can also code in c++ . Can someone pls help me,t2_3r2qxcvl,False,,0,False,Help Wanted!!,[],r/pytorch,False,6,,0,,,False,t3_fndtsw,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1584966085.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to make contributions to pytorch but i cant find a good issue and simple issue( either someone is already working on it or its too advanced) .  I have worked on pytorch computer vision projects and i can also code in c++ . Can someone pls help me&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fndtsw,True,,salinger_vignesh,,2,True,all_ads,False,[],False,,/r/pytorch/comments/fndtsw/help_wanted/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fndtsw/help_wanted/,7135,1584937285.0,0,,False,,,,,,,,
633,,pytorch,,t2_81yj9s9,False,,0,False,PyTorch based Probabilistic Time Series forecasting framework based on GluonTS backend,[],r/pytorch,False,6,,0,140.0,,False,t3_fmz2r7,False,dark,0.94,,public,16,0,{},140.0,,False,[],,False,False,,{},,False,16,,False,https://b.thumbs.redditmedia.com/SlH5bfiWVwXzneWSW0AogWM75ayuWh4bRntSDkXy4EE.jpg,False,,[],{},link,,False,,1584908346.0,text,6,,,text,github.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fQBltkCZGLI3zM1e9BnVRWCqvARJp49IIy04jjctkxk.jpg?auto=webp&amp;s=f33cc66527d1bbc4c1bbb88a992c7cabce68a968', 'width': 240, 'height': 240}, 'resolutions': [{'url': 'https://external-preview.redd.it/fQBltkCZGLI3zM1e9BnVRWCqvARJp49IIy04jjctkxk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3fc02f266851ed0d0996dd742ef0479afc970030', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/fQBltkCZGLI3zM1e9BnVRWCqvARJp49IIy04jjctkxk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1802b05c20d386310214cb3f5f1e756691adce1d', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'IuNI_634M6a5NSCM75Tf1zDSmkFtM0jDeWbMuBi_j1M'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fmz2r7,True,,donutloop,,0,True,all_ads,False,[],False,,/r/pytorch/comments/fmz2r7/pytorch_based_probabilistic_time_series/,all_ads,False,https://github.com/zalandoresearch/pytorch-ts,7135,1584879546.0,0,,False,https://github.com/zalandoresearch/pytorch-ts,,,,,,,
634,,pytorch,"anyone know free/commercial models to detect nuts, bolts, pipes, machines etc from photos?",t2_3t5cvaqs,False,,0,False,"Models for Nuts, bolts,pipe, machine",[],r/pytorch,False,6,,0,,,False,t3_fmco9n,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1584813207.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;anyone know free/commercial models to detect nuts, bolts, pipes, machines etc from photos?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fmco9n,True,,portoal,,1,True,all_ads,False,[],False,,/r/pytorch/comments/fmco9n/models_for_nuts_boltspipe_machine/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fmco9n/models_for_nuts_boltspipe_machine/,7135,1584784407.0,0,,False,,,,,,,,
635,,pytorch,"Data augmentation library for Deep Learning, which supports images, segmentation masks, labels and keypoints. Furthermore, SOLT is fast and has OpenCV in its backend. Full auto-generated docs and examples are available here: [https://mipt-oulu.github.io/solt/](https://mipt-oulu.github.io/solt/).

&amp;#x200B;

||albumentations (0.4.3)|torchvision (Pillow-SIMD backend) (0.5.0)|augmentor (0.2.8)|solt (0.1.9)|
|:-|:-|:-|:-|:-|
|HorizontalFlip|2253|2549|2561|**3530**|
|VerticalFlip|2380|2557|2572|**3740**|
|RotateAny|1479|1389|670|**2070**|
|Crop224|2566|1966|1981|**4281**|
|Crop128|5467|5738|5720|**7186**|
|Crop64|9285|9112|9049|**10345**|
|Crop32|11979|10550|10607|**12348**|
|Pad300|1642|109|\-|**2631**|
|VHFlipRotateCrop|1574|1334|616|**1889**|
|HFlipCrop|2391|1943|1917|**3572**|

Github: [https://github.com/MIPT-Oulu/solt](https://github.com/MIPT-Oulu/solt)",t2_1hs4u0yj,False,,0,False,"SOLT, a Pytorch-friendly augmentation library. Quite impressive in speed benchmark compared to Torchvision and others.",[],r/pytorch,False,6,,0,,,False,t3_flunaf,False,dark,0.71,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1584738146.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Data augmentation library for Deep Learning, which supports images, segmentation masks, labels and keypoints. Furthermore, SOLT is fast and has OpenCV in its backend. Full auto-generated docs and examples are available here: &lt;a href=""https://mipt-oulu.github.io/solt/""&gt;https://mipt-oulu.github.io/solt/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;&lt;/th&gt;
&lt;th align=""left""&gt;albumentations (0.4.3)&lt;/th&gt;
&lt;th align=""left""&gt;torchvision (Pillow-SIMD backend) (0.5.0)&lt;/th&gt;
&lt;th align=""left""&gt;augmentor (0.2.8)&lt;/th&gt;
&lt;th align=""left""&gt;solt (0.1.9)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;HorizontalFlip&lt;/td&gt;
&lt;td align=""left""&gt;2253&lt;/td&gt;
&lt;td align=""left""&gt;2549&lt;/td&gt;
&lt;td align=""left""&gt;2561&lt;/td&gt;
&lt;td align=""left""&gt;&lt;strong&gt;3530&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;VerticalFlip&lt;/td&gt;
&lt;td align=""left""&gt;2380&lt;/td&gt;
&lt;td align=""left""&gt;2557&lt;/td&gt;
&lt;td align=""left""&gt;2572&lt;/td&gt;
&lt;td align=""left""&gt;&lt;strong&gt;3740&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;RotateAny&lt;/td&gt;
&lt;td align=""left""&gt;1479&lt;/td&gt;
&lt;td align=""left""&gt;1389&lt;/td&gt;
&lt;td align=""left""&gt;670&lt;/td&gt;
&lt;td align=""left""&gt;&lt;strong&gt;2070&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;Crop224&lt;/td&gt;
&lt;td align=""left""&gt;2566&lt;/td&gt;
&lt;td align=""left""&gt;1966&lt;/td&gt;
&lt;td align=""left""&gt;1981&lt;/td&gt;
&lt;td align=""left""&gt;&lt;strong&gt;4281&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;Crop128&lt;/td&gt;
&lt;td align=""left""&gt;5467&lt;/td&gt;
&lt;td align=""left""&gt;5738&lt;/td&gt;
&lt;td align=""left""&gt;5720&lt;/td&gt;
&lt;td align=""left""&gt;&lt;strong&gt;7186&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;Crop64&lt;/td&gt;
&lt;td align=""left""&gt;9285&lt;/td&gt;
&lt;td align=""left""&gt;9112&lt;/td&gt;
&lt;td align=""left""&gt;9049&lt;/td&gt;
&lt;td align=""left""&gt;&lt;strong&gt;10345&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;Crop32&lt;/td&gt;
&lt;td align=""left""&gt;11979&lt;/td&gt;
&lt;td align=""left""&gt;10550&lt;/td&gt;
&lt;td align=""left""&gt;10607&lt;/td&gt;
&lt;td align=""left""&gt;&lt;strong&gt;12348&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;Pad300&lt;/td&gt;
&lt;td align=""left""&gt;1642&lt;/td&gt;
&lt;td align=""left""&gt;109&lt;/td&gt;
&lt;td align=""left""&gt;-&lt;/td&gt;
&lt;td align=""left""&gt;&lt;strong&gt;2631&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;VHFlipRotateCrop&lt;/td&gt;
&lt;td align=""left""&gt;1574&lt;/td&gt;
&lt;td align=""left""&gt;1334&lt;/td&gt;
&lt;td align=""left""&gt;616&lt;/td&gt;
&lt;td align=""left""&gt;&lt;strong&gt;1889&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;HFlipCrop&lt;/td&gt;
&lt;td align=""left""&gt;2391&lt;/td&gt;
&lt;td align=""left""&gt;1943&lt;/td&gt;
&lt;td align=""left""&gt;1917&lt;/td&gt;
&lt;td align=""left""&gt;&lt;strong&gt;3572&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/MIPT-Oulu/solt""&gt;https://github.com/MIPT-Oulu/solt&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,flunaf,True,,nqd14,,3,True,all_ads,False,[],False,,/r/pytorch/comments/flunaf/solt_a_pytorchfriendly_augmentation_library_quite/,all_ads,False,https://www.reddit.com/r/pytorch/comments/flunaf/solt_a_pytorchfriendly_augmentation_library_quite/,7135,1584709346.0,0,,False,,,,,,,,
636,,pytorch,"I am trying to run a neural network on msr action 3d dataset (this data set contain 60 feature with 20 class)  here is my [notebook](https://gist.github.com/mirsahib/adcf81ba6986a2ffc1f449e71fe417fe) 

as you can see my model can take 60 input feature and output a tensor of size 20

but when i try to run the following code

 `y_pred = net(X_train)` 

the y\_pred shape is  \[18237, 20\] can you tell me why is that happening (it should be of shape 20) and each value should be between 1-20 not any floating point value",t2_z7gxp,False,,0,False,Pytorch model returning a matrix instead of a column vector,[],r/pytorch,False,6,,0,,,False,t3_fl6zzx,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1584637609.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to run a neural network on msr action 3d dataset (this data set contain 60 feature with 20 class)  here is my &lt;a href=""https://gist.github.com/mirsahib/adcf81ba6986a2ffc1f449e71fe417fe""&gt;notebook&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;as you can see my model can take 60 input feature and output a tensor of size 20&lt;/p&gt;

&lt;p&gt;but when i try to run the following code&lt;/p&gt;

&lt;p&gt;&lt;code&gt;y_pred = net(X_train)&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;the y_pred shape is  [18237, 20] can you tell me why is that happening (it should be of shape 20) and each value should be between 1-20 not any floating point value&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?auto=webp&amp;s=079a7260ec149880c73263d64811698adb22760a', 'width': 1280, 'height': 640}, 'resolutions': [{'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5811c5bda5fece1040636a6af8702ba790f0fd4', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eee576fd4da7535eb53ceb88dd8b52f073048441', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=72872d880460efa723918c000adca0ed259cf775', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3545b9335d763c9da9c16bf7bf9a3f907dbd6f6', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d241ace0f1c07088fac3f8469dbad3b05d2d419', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9055f11bdc00beb0b3589e1cae5817d6070d83bc', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fl6zzx,True,,mirsahib,,4,True,all_ads,False,[],False,,/r/pytorch/comments/fl6zzx/pytorch_model_returning_a_matrix_instead_of_a/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fl6zzx/pytorch_model_returning_a_matrix_instead_of_a/,7135,1584608809.0,0,,False,,,,,,,,
637,,pytorch,"Just as the title. Can I use a 1080 GTX in conjunction with a 2080 RTX for training models? Will one be a bottleneck for the other? Does anyone has a setup like this?

I would really appreciate any feedback, thank you!",t2_5sxnverv,False,,0,False,Is it possible to mix GPU card models?,[],r/pytorch,False,6,,0,,,False,t3_fkq858,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1584569951.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just as the title. Can I use a 1080 GTX in conjunction with a 2080 RTX for training models? Will one be a bottleneck for the other? Does anyone has a setup like this?&lt;/p&gt;

&lt;p&gt;I would really appreciate any feedback, thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fkq858,True,,small-kosmos,,6,True,all_ads,False,[],False,,/r/pytorch/comments/fkq858/is_it_possible_to_mix_gpu_card_models/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fkq858/is_it_possible_to_mix_gpu_card_models/,7135,1584541151.0,0,,False,,,,,,,,
638,,pytorch,"noob alert 

I am a noob at deep learning and pytorch, recently in a course there was a challenge to build a densenet 121. I dont know ho to connect every convolution  layer to each other using forward function in pytorch 

please help",t2_8xpdglv,False,,0,False,how to build a dense block in densenet-121 architecture,[],r/pytorch,False,6,,0,,,False,t3_fkk8pq,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1584537207.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;noob alert &lt;/p&gt;

&lt;p&gt;I am a noob at deep learning and pytorch, recently in a course there was a challenge to build a densenet 121. I dont know ho to connect every convolution  layer to each other using forward function in pytorch &lt;/p&gt;

&lt;p&gt;please help&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fkk8pq,True,,tusharkulkarni95,,4,True,all_ads,False,[],False,,/r/pytorch/comments/fkk8pq/how_to_build_a_dense_block_in_densenet121/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fkk8pq/how_to_build_a_dense_block_in_densenet121/,7135,1584508407.0,0,,False,,,,,,,,
639,,pytorch,"Is there a way to compile PyTorch from the source code for the SSE4.2 set of instructions? I need to compile it on PC A (possibly any set of instructions) to work on PC B (only SSE4.2) in production (both Linux buster); also, I can’t build it in the production PC: for security reasons I can’t ""pip install"" or ""apt-get"" anything on the PC B.

Is there any way? Or I don’t even need to worry about building it for SSE4.2?",t2_ygs0o,False,,0,False,Compile PyTorch from source with SEE4.2 instructions,[],r/pytorch,False,6,,0,,,False,t3_fkbugt,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1584475344.0,,[],{},,,True,,1584503668.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there a way to compile PyTorch from the source code for the SSE4.2 set of instructions? I need to compile it on PC A (possibly any set of instructions) to work on PC B (only SSE4.2) in production (both Linux buster); also, I can’t build it in the production PC: for security reasons I can’t &amp;quot;pip install&amp;quot; or &amp;quot;apt-get&amp;quot; anything on the PC B.&lt;/p&gt;

&lt;p&gt;Is there any way? Or I don’t even need to worry about building it for SSE4.2?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fkbugt,True,,paulomann,,0,True,all_ads,False,[],False,,/r/pytorch/comments/fkbugt/compile_pytorch_from_source_with_see42/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fkbugt/compile_pytorch_from_source_with_see42/,7135,1584474868.0,0,,False,,,,,,,,
640,,pytorch,"I found these two books on O'reilly. Which one should I start with ?

1. [Hands-on Natural Language Processing with PyTorch](https://www.amazon.com/Hands-Natural-Language-Processing-Python-ebook/dp/B07D6KYQYP/ref=sr_1_fkmr2_1?keywords=Hands-On+Natural+Language+Processing+with+Pytorch&amp;qid=1584363518&amp;sr=8-1-fkmr2)
2. [Natural Language Processing with PyTorch](https://www.amazon.com/Natural-Language-Processing-PyTorch-Applications/dp/1491978236/ref=sr_1_1?keywords=Natural+Language+Processing+with+PyTorch&amp;qid=1584363697&amp;sr=8-1)",t2_5n6xtijf,False,,0,False,Which of the books is better for NLP ?,[],r/pytorch,False,6,,0,,,False,t3_fjklhl,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1584392531.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I found these two books on O&amp;#39;reilly. Which one should I start with ?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=""https://www.amazon.com/Hands-Natural-Language-Processing-Python-ebook/dp/B07D6KYQYP/ref=sr_1_fkmr2_1?keywords=Hands-On+Natural+Language+Processing+with+Pytorch&amp;amp;qid=1584363518&amp;amp;sr=8-1-fkmr2""&gt;Hands-on Natural Language Processing with PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://www.amazon.com/Natural-Language-Processing-PyTorch-Applications/dp/1491978236/ref=sr_1_1?keywords=Natural+Language+Processing+with+PyTorch&amp;amp;qid=1584363697&amp;amp;sr=8-1""&gt;Natural Language Processing with PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fjklhl,True,,the-machine-learner,,2,True,all_ads,False,[],False,,/r/pytorch/comments/fjklhl/which_of_the_books_is_better_for_nlp/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fjklhl/which_of_the_books_is_better_for_nlp/,7135,1584363731.0,0,,False,,,,,,,,
641,,pytorch,"Hello guys *noob alert*,
So I have been using keras for months now and I mainly learner from a book (Intro to deep learning with Keats) and I have a basic sense of Machine learning and ANNs but I would like to expand my capabilities by moving to Pytorch. 

Gist: I would like to shift to Pytorch. 
Any suggestions of source from which I should start? 

Sorry guys if I offended someone.",t2_4yj3qq2w,False,,0,False,Guide me please...,[],r/pytorch,False,6,,0,,,False,t3_fj8m0y,False,dark,0.66,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1584335314.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello guys &lt;em&gt;noob alert&lt;/em&gt;,
So I have been using keras for months now and I mainly learner from a book (Intro to deep learning with Keats) and I have a basic sense of Machine learning and ANNs but I would like to expand my capabilities by moving to Pytorch. &lt;/p&gt;

&lt;p&gt;Gist: I would like to shift to Pytorch. 
Any suggestions of source from which I should start? &lt;/p&gt;

&lt;p&gt;Sorry guys if I offended someone.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fj8m0y,True,,crazy_sax_guy,,4,True,all_ads,False,[],False,,/r/pytorch/comments/fj8m0y/guide_me_please/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fj8m0y/guide_me_please/,7135,1584306514.0,0,,False,,,,,,,,
642,,pytorch,,t2_x54ow,False,,0,False,Make your models smaller (Part 2),[],r/pytorch,False,6,,0,140.0,,False,t3_fhudvw,False,dark,0.84,,public,4,0,{},140.0,,False,[],,False,False,,{},,False,4,,False,https://a.thumbs.redditmedia.com/EV0GI2G4qEBGooa2VvozIDIeOAIlgYgA6kWn3-CtIR0.jpg,False,,[],{},link,,False,,1584104306.0,text,6,,,text,amandeepsp.github.io,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/EWcJB42wYYZgo9ruUyjRSLCJTG6UcVtcbESj91txZJc.jpg?auto=webp&amp;s=8142c4b87acc60c616eea56fafdadeb0d9284ce5', 'width': 339, 'height': 519}, 'resolutions': [{'url': 'https://external-preview.redd.it/EWcJB42wYYZgo9ruUyjRSLCJTG6UcVtcbESj91txZJc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=585fef586d9c258d7a58d4d05c6ea0ea2b1dddaa', 'width': 108, 'height': 165}, {'url': 'https://external-preview.redd.it/EWcJB42wYYZgo9ruUyjRSLCJTG6UcVtcbESj91txZJc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a64ec1c5c6a30a8ceed081d33b58a286741b83e', 'width': 216, 'height': 330}, {'url': 'https://external-preview.redd.it/EWcJB42wYYZgo9ruUyjRSLCJTG6UcVtcbESj91txZJc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=95421bc88f7c598bdd110efaed641a5637cfb9b9', 'width': 320, 'height': 489}], 'variants': {}, 'id': 'xUKo-XumheVW8LNfEKNSnS7voIrVuSWjwY2AYpqyULw'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fhudvw,True,,amandeepspdhr,,0,True,all_ads,False,[],False,,/r/pytorch/comments/fhudvw/make_your_models_smaller_part_2/,all_ads,False,https://amandeepsp.github.io/ml-model-compression-part2/,7135,1584075506.0,0,,False,https://amandeepsp.github.io/ml-model-compression-part2/,,,,,,,
643,,pytorch,"Hi,

Did anybody get a segfault inside the [libtorch.so](https://libtorch.so)? What's the protocol in this situation?

Heavily unexpected experience :-)",t2_gqxax,False,,0,False,SegFault,[],r/pytorch,False,6,,0,,,False,t3_fhysms,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1584130928.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Did anybody get a segfault inside the &lt;a href=""https://libtorch.so""&gt;libtorch.so&lt;/a&gt;? What&amp;#39;s the protocol in this situation?&lt;/p&gt;

&lt;p&gt;Heavily unexpected experience :-)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fhysms,True,,zbroyar,,0,True,all_ads,False,[],False,,/r/pytorch/comments/fhysms/segfault/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fhysms/segfault/,7135,1584102128.0,0,,False,,,,,,,,
644,,pytorch,"For a research project, I'm trying to build a model that takes a textual job descriptions as inputs and predicts an hourly wage in USD as the output. 

I've trained a Random Forest Regressor on TF-IDF tokenized word vectors, which worked decently well. Most wages vary between $10/hr to $50/hr with a few outliers  &gt;$80+. The mean absolute error is $6. Which means that on average, predictions are \~$6.00 off from what the actual hourly wage was. 

As noted at the begging, I'd like to make a better model using LSTM (or some RNN variant) which can take text as inputs and predict the hourly salary. Most many-to-one architectures I've seen use softmax as the final layer for classification purposes. But because my goal is to predict a continuous variable, this is not ideal.

A few questions:

\- Can anyone link a many-2-one LSTM architecture with a regressor output?

\- Is LSTM a good choice? Job descriptions are often &gt;100 words, which can test the limits of LSTM's ability to ""remember"" the first x words. I want to improve upon TF-IDF vectorization, but I'm not certain that this approach will be fruitful.

\- Any alternative recommendations you'd like to offer? (Preferably in PyTorch .)",t2_201d2h31,False,,0,False,LSTM many-to-one regression architecture?,[],r/pytorch,False,6,,0,,,False,t3_fhj8ub,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1584060094.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For a research project, I&amp;#39;m trying to build a model that takes a textual job descriptions as inputs and predicts an hourly wage in USD as the output. &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve trained a Random Forest Regressor on TF-IDF tokenized word vectors, which worked decently well. Most wages vary between $10/hr to $50/hr with a few outliers  &amp;gt;$80+. The mean absolute error is $6. Which means that on average, predictions are ~$6.00 off from what the actual hourly wage was. &lt;/p&gt;

&lt;p&gt;As noted at the begging, I&amp;#39;d like to make a better model using LSTM (or some RNN variant) which can take text as inputs and predict the hourly salary. Most many-to-one architectures I&amp;#39;ve seen use softmax as the final layer for classification purposes. But because my goal is to predict a continuous variable, this is not ideal.&lt;/p&gt;

&lt;p&gt;A few questions:&lt;/p&gt;

&lt;p&gt;- Can anyone link a many-2-one LSTM architecture with a regressor output?&lt;/p&gt;

&lt;p&gt;- Is LSTM a good choice? Job descriptions are often &amp;gt;100 words, which can test the limits of LSTM&amp;#39;s ability to &amp;quot;remember&amp;quot; the first x words. I want to improve upon TF-IDF vectorization, but I&amp;#39;m not certain that this approach will be fruitful.&lt;/p&gt;

&lt;p&gt;- Any alternative recommendations you&amp;#39;d like to offer? (Preferably in PyTorch .)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fhj8ub,True,,jbuddy_13,,1,True,all_ads,False,[],False,,/r/pytorch/comments/fhj8ub/lstm_manytoone_regression_architecture/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fhj8ub/lstm_manytoone_regression_architecture/,7135,1584031294.0,0,,False,,,,,,,,
645,,pytorch,"For text classification, I want to add multiple Convolution and pooling layers with different kernel sizes using pytorch. Can any one suggest related  code or material for that?",t2_4xpoa8ie,False,,0,False,[D] How to add multiple Convolution and pooling layers with different kernel sizes using pytorch?,[],r/pytorch,False,6,,0,,,False,t3_ff91ub,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1583679791.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For text classification, I want to add multiple Convolution and pooling layers with different kernel sizes using pytorch. Can any one suggest related  code or material for that?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ff91ub,True,,sreenuroyal568,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ff91ub/d_how_to_add_multiple_convolution_and_pooling/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ff91ub/d_how_to_add_multiple_convolution_and_pooling/,7135,1583650991.0,0,,False,,,,,,,,
646,,pytorch,"I'm wanting to implement the NEAT algorithm. It evolves the topology of an ANN's nodes and connection weights, that connect the input layer to the output layer. Normally this is done with a set of nodes in one hidden layer, feeding into the next, and so on. NEAT isn't confined to a rigid layer by layer structure. For example, an input node can connect through 5 hidden nodes, before reaching the output layer. But that same input node (or a different one) can connect through 1, or even 0, hidden nodes, to reach the output layer. The typical back propagation algorithm no longer works on such a topology.

What PyTorch data structures or API might I use to make a modified back propagation algorithm?",t2_10vyes,False,,0,False,Tips/Suggestions for designing ANN w/o structured hidden layer,[],r/pytorch,False,6,,0,,,False,t3_ff0u72,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1583641123.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m wanting to implement the NEAT algorithm. It evolves the topology of an ANN&amp;#39;s nodes and connection weights, that connect the input layer to the output layer. Normally this is done with a set of nodes in one hidden layer, feeding into the next, and so on. NEAT isn&amp;#39;t confined to a rigid layer by layer structure. For example, an input node can connect through 5 hidden nodes, before reaching the output layer. But that same input node (or a different one) can connect through 1, or even 0, hidden nodes, to reach the output layer. The typical back propagation algorithm no longer works on such a topology.&lt;/p&gt;

&lt;p&gt;What PyTorch data structures or API might I use to make a modified back propagation algorithm?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ff0u72,True,,Cupofcalculus,,5,True,all_ads,False,[],False,,/r/pytorch/comments/ff0u72/tipssuggestions_for_designing_ann_wo_structured/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ff0u72/tipssuggestions_for_designing_ann_wo_structured/,7135,1583612323.0,0,,False,,,,,,,,
647,,pytorch,"Hi guys, I've been trying to learn the basics in pytorch  and DL for a while now but an hectic schedule makes it hard to spend more than a couple hours a week on it and I'm starting to be depressed just thinking about it.

To help me, and because I work in the field of Earth observation, I would like to find a ultra basic model working with aerial or satellite imagery to do some object recognition or classification when I have a downtime at work. All I have found yet are very advanced models from papers and while I can run them, I quite often cannot see why a change works or doesn't. 

I know this is kind of a long shot, and that it is not a very good way to learn, but I kind of need a small ""oh I get it"" to keep me going. 

Thanks guys",t2_435v5,False,,0,False,Looking for a basic model using satellite imagery or aerial imagery to help me understand some basics I'm missing.,[],r/pytorch,False,6,,0,,,False,t3_fdors0,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1583406169.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys, I&amp;#39;ve been trying to learn the basics in pytorch  and DL for a while now but an hectic schedule makes it hard to spend more than a couple hours a week on it and I&amp;#39;m starting to be depressed just thinking about it.&lt;/p&gt;

&lt;p&gt;To help me, and because I work in the field of Earth observation, I would like to find a ultra basic model working with aerial or satellite imagery to do some object recognition or classification when I have a downtime at work. All I have found yet are very advanced models from papers and while I can run them, I quite often cannot see why a change works or doesn&amp;#39;t. &lt;/p&gt;

&lt;p&gt;I know this is kind of a long shot, and that it is not a very good way to learn, but I kind of need a small &amp;quot;oh I get it&amp;quot; to keep me going. &lt;/p&gt;

&lt;p&gt;Thanks guys&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fdors0,True,,jimbuz,,1,True,all_ads,False,[],False,,/r/pytorch/comments/fdors0/looking_for_a_basic_model_using_satellite_imagery/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fdors0/looking_for_a_basic_model_using_satellite_imagery/,7135,1583377369.0,0,,False,,,,,,,,
648,,pytorch,"hi please can someone help me with this question , to complete my final year project [face recognition and detection](https://stackoverflow.com/questions/60526738/real-time-face-recognition-problems-looking-for-recommendation) 

thanks  ..",t2_2yr3b1dy,False,,0,False,"face recognition problem in real time , looking for recommendation",[],r/pytorch,False,6,,0,,,False,t3_fddtmx,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1583361171.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;hi please can someone help me with this question , to complete my final year project &lt;a href=""https://stackoverflow.com/questions/60526738/real-time-face-recognition-problems-looking-for-recommendation""&gt;face recognition and detection&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;thanks  ..&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fddtmx,True,,artiestArt,,7,True,all_ads,False,[],False,,/r/pytorch/comments/fddtmx/face_recognition_problem_in_real_time_looking_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fddtmx/face_recognition_problem_in_real_time_looking_for/,7135,1583332371.0,0,,False,,,,,,,,
649,,pytorch,"I'm a beginner with PyTorch and ML and I would like to know the techniques and strategies used to improve the network performance on the test dataset.

Currently, I have two network architecture:

1 - ConvNet1

    # experiment 1
    # 3 convolutional layers and 2 linear layers
    class ConvNet1(nn.Module):
        
        def __init__(self, num_classes=10):
            super(ConvNet1, self).__init__()
    
            self.layer1 = nn.Sequential(
                nn.Conv2d(3, 16, kernel_size=3),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2),
                nn.Dropout2d(p=0.3))
            
            self.layer2 = nn.Sequential(
                nn.Conv2d(16, 24, kernel_size=4),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2),
                nn.Dropout2d(p=0.3))
    
            self.layer3 = nn.Sequential(
                nn.Conv2d(24, 32, kernel_size=4),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2),
                nn.Dropout2d(p=0.3))
    
            self.dropout = nn.Dropout2d(p=0.3)
            
            self.fc1 = nn.Linear(32*29*29, 120)
    
            self.relu = nn.ReLU()
    
            self.fc2 = nn.Linear(120, 10)
    
            
        def forward(self, x):
    
            x = self.layer1(x)
    
            x = self.layer2(x)
    
            x = self.layer3(x)
    
            # print(out.shape)
            
            x = x.view(-1, 32*29*29)
    
            x = self.fc1(x)
    
            x = self.relu(x)
    
            x = self.fc2(x)
    
            return x
    
 and

2 - ConvNet2

    # experiment 2
    # 1 convolutional layer and 1 linear layer
    class ConvNet2(nn.Module):
    
        def __init__(self, num_classes=10):
            super(ConvNet2, self).__init__()
    
            self.layer1 = nn.Sequential(
                nn.Conv2d(3, 16, kernel_size=3),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2),
                nn.Dropout2d(p=0.3))
    
            self.fc1 = nn.Linear(258064, 120)
    
        def forward(self, x):
            x = self.layer1(x)
    
            x = x.view(-1, 16 * 127 * 127)
    
            x = self.fc1(x)
    
            return x

Surprisingly, the ConvNet2 network performs much better than ConvNet1 even if its architecture is simpler. When I train for 10 epochs, ConvNet1 has 41% accuracy and ConvNet2 has 78%. Not really sure why, though.

What would you do to ConvNet2 (or ConvNet1?) to improve its accuracy?",t2_13vhak,False,,0,False,How can I improve the test accuracy of my CNN in PyTorch?,[],r/pytorch,False,6,,0,,,False,t3_fbgkxn,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1583029056.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a beginner with PyTorch and ML and I would like to know the techniques and strategies used to improve the network performance on the test dataset.&lt;/p&gt;

&lt;p&gt;Currently, I have two network architecture:&lt;/p&gt;

&lt;p&gt;1 - ConvNet1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# experiment 1
# 3 convolutional layers and 2 linear layers
class ConvNet1(nn.Module):

    def __init__(self, num_classes=10):
        super(ConvNet1, self).__init__()

        self.layer1 = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
            nn.Dropout2d(p=0.3))

        self.layer2 = nn.Sequential(
            nn.Conv2d(16, 24, kernel_size=4),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
            nn.Dropout2d(p=0.3))

        self.layer3 = nn.Sequential(
            nn.Conv2d(24, 32, kernel_size=4),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
            nn.Dropout2d(p=0.3))

        self.dropout = nn.Dropout2d(p=0.3)

        self.fc1 = nn.Linear(32*29*29, 120)

        self.relu = nn.ReLU()

        self.fc2 = nn.Linear(120, 10)


    def forward(self, x):

        x = self.layer1(x)

        x = self.layer2(x)

        x = self.layer3(x)

        # print(out.shape)

        x = x.view(-1, 32*29*29)

        x = self.fc1(x)

        x = self.relu(x)

        x = self.fc2(x)

        return x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;2 - ConvNet2&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# experiment 2
# 1 convolutional layer and 1 linear layer
class ConvNet2(nn.Module):

    def __init__(self, num_classes=10):
        super(ConvNet2, self).__init__()

        self.layer1 = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
            nn.Dropout2d(p=0.3))

        self.fc1 = nn.Linear(258064, 120)

    def forward(self, x):
        x = self.layer1(x)

        x = x.view(-1, 16 * 127 * 127)

        x = self.fc1(x)

        return x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprisingly, the ConvNet2 network performs much better than ConvNet1 even if its architecture is simpler. When I train for 10 epochs, ConvNet1 has 41% accuracy and ConvNet2 has 78%. Not really sure why, though.&lt;/p&gt;

&lt;p&gt;What would you do to ConvNet2 (or ConvNet1?) to improve its accuracy?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fbgkxn,True,,pythonistaaaaaaa,,3,True,all_ads,False,[],False,,/r/pytorch/comments/fbgkxn/how_can_i_improve_the_test_accuracy_of_my_cnn_in/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fbgkxn/how_can_i_improve_the_test_accuracy_of_my_cnn_in/,7135,1583000256.0,0,,False,,,,,,,,
650,,pytorch,"I've been following along with [this tutorial](https://www.youtube.com/watch?v=9aYuQmMJvjA&amp;list=PLQVvvaa0QuDdeMyHEYc0gxFpYwHY2Qfdh&amp;index=5) on deep learning and neural networks. I think the explanations have been really good and have enjoyed it so far. However, I just got to part 5/8 where he asks for us to install a few packages. Unfortunately, my system seems to be stuck with the opencv. I first attempted this one:  conda install -c conda-forge opencv followed by a few others. However, they all fail in the same place.

    Collecting package metadata (current_repodata.json): done
    Solving environment: failed with initial frozen solve. Retrying with flexible solve.
    Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
    Collecting package metadata (repodata.json): done
    Solving environment: failed with initial frozen solve. Retrying with flexible solve.
    Solving environment: \
    Found conflicts! Looking for incompatible packages.

So that's fun and everything I have tried from the internet has not gotten me around it. If anyone has suggestions on how to get this package installed I would be very appreciative! (I also installed tqdm in between fail attempts for opencv... so it really does seem to just be this one and not necessarily a problem with my system).

&amp;#x200B;

However, since I am currently stalled, I thought I would ask: is this the best tutorial for me to be pursuing right now? I would like to build a learning-AI that can play a board game I've built in Python. Right now I think the best way to do this is to have it generate random 'strategies' and then play a series of games employing these strategies against each other in order to determine which is the best. As such, I don't know how directly applicable all of the image recognition would be!

&amp;#x200B;

Thank you for any input!

&amp;#x200B;

TL;DR

1) opencv won't install--any tips on how to get it working?

2) I want to build a learning AI (it can be simple)  to play a game... is there a decent tutorial to start me on this?

(Unfortunately, it is 'won' by victory points so I need the AI to achieve the end goal of highest possible VPs rather than a simpler game such as getting 4- in a row or something)",t2_138cs3,False,,0,False,can't install opencv with conda for a tutorial I'm doing? And a good follow up since I'm stalled.. is this the best tutorial moving forwards?,[],r/pytorch,False,6,,0,,,False,t3_fb04j3,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1582946906.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been following along with &lt;a href=""https://www.youtube.com/watch?v=9aYuQmMJvjA&amp;amp;list=PLQVvvaa0QuDdeMyHEYc0gxFpYwHY2Qfdh&amp;amp;index=5""&gt;this tutorial&lt;/a&gt; on deep learning and neural networks. I think the explanations have been really good and have enjoyed it so far. However, I just got to part 5/8 where he asks for us to install a few packages. Unfortunately, my system seems to be stuck with the opencv. I first attempted this one:  conda install -c conda-forge opencv followed by a few others. However, they all fail in the same place.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: \
Found conflicts! Looking for incompatible packages.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So that&amp;#39;s fun and everything I have tried from the internet has not gotten me around it. If anyone has suggestions on how to get this package installed I would be very appreciative! (I also installed tqdm in between fail attempts for opencv... so it really does seem to just be this one and not necessarily a problem with my system).&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;However, since I am currently stalled, I thought I would ask: is this the best tutorial for me to be pursuing right now? I would like to build a learning-AI that can play a board game I&amp;#39;ve built in Python. Right now I think the best way to do this is to have it generate random &amp;#39;strategies&amp;#39; and then play a series of games employing these strategies against each other in order to determine which is the best. As such, I don&amp;#39;t know how directly applicable all of the image recognition would be!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you for any input!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;TL;DR&lt;/p&gt;

&lt;p&gt;1) opencv won&amp;#39;t install--any tips on how to get it working?&lt;/p&gt;

&lt;p&gt;2) I want to build a learning AI (it can be simple)  to play a game... is there a decent tutorial to start me on this?&lt;/p&gt;

&lt;p&gt;(Unfortunately, it is &amp;#39;won&amp;#39; by victory points so I need the AI to achieve the end goal of highest possible VPs rather than a simpler game such as getting 4- in a row or something)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/MJJ_KUZdSWHT1wcCYvuzO04SX-gdiAsGhk7zzcPfq4U.jpg?auto=webp&amp;s=cfbd988a14e9f68c97560038547273892bcdcc14', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/MJJ_KUZdSWHT1wcCYvuzO04SX-gdiAsGhk7zzcPfq4U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=749fcc38f7207ff89ffea1fb463163e73ff69a7e', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/MJJ_KUZdSWHT1wcCYvuzO04SX-gdiAsGhk7zzcPfq4U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ea28e21ca73bc5aee507892acc158a944bea5e3', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/MJJ_KUZdSWHT1wcCYvuzO04SX-gdiAsGhk7zzcPfq4U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=26304d5f03294a806f358b6d813fe387ec7820a5', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'QYLeEDQ780YckhJT6PSA1fsyJ2ptAssJcpvgrD1XfiU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fb04j3,True,,FleetOfFeet,,5,True,all_ads,False,[],False,,/r/pytorch/comments/fb04j3/cant_install_opencv_with_conda_for_a_tutorial_im/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fb04j3/cant_install_opencv_with_conda_for_a_tutorial_im/,7135,1582918106.0,0,,False,,,,,,,,
651,,pytorch,"I'm trying to plot a confusion matrix and it doesn't work. I'm getting a weird result and I'm not sure how to interpret it (see below). I think my problem comes from just having the last confusion matrix and plotting it, but I'm not even sure because it should still plot something that looks like the 2nd picture, I think?

If someone can take a look at this and help that'd be amazing. 

[my current confusion matrix][1]


[what I would like to have][2]


Here's my code generating this:

    model = torch.load('model-5-layers.pt')
    
    correct = 0
    total = 0
    
    # Why don't we need gradients? What happens if we do include gradients?
    with torch.no_grad():
        
        # Iterate over the test set
        for data in test_loader:
            images, labels = data
    
            images = images.to(device)
            labels = labels.to(device)
            
            outputs = model(images)
            
            # torch.max is an argmax operation
            _, predicted = torch.max(outputs.data, 1)
            
            total += labels.size(0)
            correct += (predicted == labels).sum().item()


    print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))

**which prints an accuracy of 48%.**

and my plotting function:


    from sklearn.metrics import confusion_matrix
    import matplotlib.pyplot as plt
    
    
    cm = confusion_matrix(labels, predicted)
    
    import itertools
    
    
    def plot_confusion_matrix(cm,
                              classes,
                              normalize=False,
                              title='Confusion matrix',
                              cmap=plt.cm.Blues):
        """"""
        This function prints and plots the confusion matrix very prettily.
        Normalization can be applied by setting `normalize=True`.
        """"""
        if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            print(""Normalized confusion matrix"")
        else:
            print('Confusion matrix, without normalization')
    
        plt.imshow(cm, interpolation='nearest', cmap=cmap)
        plt.title(title)
    
        # Specify the tick marks and axis text
        tick_marks = np.arange(len(classes))
        plt.xticks(tick_marks, classes, rotation=90)
        plt.yticks(tick_marks, classes)
    
        # The data formatting
        fmt = '.2f' if normalize else 'd'
        thresh = cm.max() / 2.
    
        # Print the text of the matrix, adjusting text colour for display
        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
            plt.text(j, i, format(cm[i, j], fmt),
                     horizontalalignment=""center"",
                     color=""white"" if cm[i, j] &gt; thresh else ""black"")
    
        plt.ylabel('True label')
        plt.xlabel('Predicted label')
        plt.tight_layout()
        plt.show()
    
    plot_confusion_matrix(cm, classes)

  [1]: https://i.stack.imgur.com/B4Ez3.png
  [2]: https://i.stack.imgur.com/DVVSn.png",t2_13vhak,False,,0,False,Beginner PyTorch - trying to plot a confusion matrix,[],r/pytorch,False,6,,0,,,False,t3_fazhvs,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1582944458.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to plot a confusion matrix and it doesn&amp;#39;t work. I&amp;#39;m getting a weird result and I&amp;#39;m not sure how to interpret it (see below). I think my problem comes from just having the last confusion matrix and plotting it, but I&amp;#39;m not even sure because it should still plot something that looks like the 2nd picture, I think?&lt;/p&gt;

&lt;p&gt;If someone can take a look at this and help that&amp;#39;d be amazing. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.stack.imgur.com/B4Ez3.png""&gt;my current confusion matrix&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.stack.imgur.com/DVVSn.png""&gt;what I would like to have&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s my code generating this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = torch.load(&amp;#39;model-5-layers.pt&amp;#39;)

correct = 0
total = 0

# Why don&amp;#39;t we need gradients? What happens if we do include gradients?
with torch.no_grad():

    # Iterate over the test set
    for data in test_loader:
        images, labels = data

        images = images.to(device)
        labels = labels.to(device)

        outputs = model(images)

        # torch.max is an argmax operation
        _, predicted = torch.max(outputs.data, 1)

        total += labels.size(0)
        correct += (predicted == labels).sum().item()


print(&amp;#39;Accuracy of the network on the test images: %d %%&amp;#39; % (100 * correct / total))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;which prints an accuracy of 48%.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;and my plotting function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt


cm = confusion_matrix(labels, predicted)

import itertools


def plot_confusion_matrix(cm,
                          classes,
                          normalize=False,
                          title=&amp;#39;Confusion matrix&amp;#39;,
                          cmap=plt.cm.Blues):
    &amp;quot;&amp;quot;&amp;quot;
    This function prints and plots the confusion matrix very prettily.
    Normalization can be applied by setting `normalize=True`.
    &amp;quot;&amp;quot;&amp;quot;
    if normalize:
        cm = cm.astype(&amp;#39;float&amp;#39;) / cm.sum(axis=1)[:, np.newaxis]
        print(&amp;quot;Normalized confusion matrix&amp;quot;)
    else:
        print(&amp;#39;Confusion matrix, without normalization&amp;#39;)

    plt.imshow(cm, interpolation=&amp;#39;nearest&amp;#39;, cmap=cmap)
    plt.title(title)

    # Specify the tick marks and axis text
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90)
    plt.yticks(tick_marks, classes)

    # The data formatting
    fmt = &amp;#39;.2f&amp;#39; if normalize else &amp;#39;d&amp;#39;
    thresh = cm.max() / 2.

    # Print the text of the matrix, adjusting text colour for display
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment=&amp;quot;center&amp;quot;,
                 color=&amp;quot;white&amp;quot; if cm[i, j] &amp;gt; thresh else &amp;quot;black&amp;quot;)

    plt.ylabel(&amp;#39;True label&amp;#39;)
    plt.xlabel(&amp;#39;Predicted label&amp;#39;)
    plt.tight_layout()
    plt.show()

plot_confusion_matrix(cm, classes)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/1_gtnlJNo0w0qPjGC27aU9Il6AxAsVMTpuT-gojZBHQ.png?auto=webp&amp;s=94ca86495c00d614beb4f846991d45df404b336a', 'width': 1258, 'height': 950}, 'resolutions': [{'url': 'https://external-preview.redd.it/1_gtnlJNo0w0qPjGC27aU9Il6AxAsVMTpuT-gojZBHQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0ff65b1d4e0cfb3e591502013019dbfd1416a268', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/1_gtnlJNo0w0qPjGC27aU9Il6AxAsVMTpuT-gojZBHQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=898355bec7712accc47884769665b80cbee91cc0', 'width': 216, 'height': 163}, {'url': 'https://external-preview.redd.it/1_gtnlJNo0w0qPjGC27aU9Il6AxAsVMTpuT-gojZBHQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=916c64c18d2edb81d86d61f54f0b65840b8d9f02', 'width': 320, 'height': 241}, {'url': 'https://external-preview.redd.it/1_gtnlJNo0w0qPjGC27aU9Il6AxAsVMTpuT-gojZBHQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=40cd4b5d62f22bff8acfccea2f35d0fb8175df2a', 'width': 640, 'height': 483}, {'url': 'https://external-preview.redd.it/1_gtnlJNo0w0qPjGC27aU9Il6AxAsVMTpuT-gojZBHQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e62a293dfc839dbf628168b71b55406b68462c7', 'width': 960, 'height': 724}, {'url': 'https://external-preview.redd.it/1_gtnlJNo0w0qPjGC27aU9Il6AxAsVMTpuT-gojZBHQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ff78485f383470222ae8cc31c51cfdf9c16f3125', 'width': 1080, 'height': 815}], 'variants': {}, 'id': 'MJImVxfqntzHXgAvcW73iJgccE71cwWHjnTic0rjZnE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fazhvs,True,,pythonistaaaaaaa,,0,True,all_ads,False,[],False,,/r/pytorch/comments/fazhvs/beginner_pytorch_trying_to_plot_a_confusion_matrix/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fazhvs/beginner_pytorch_trying_to_plot_a_confusion_matrix/,7135,1582915658.0,0,,False,,,,,,,,
652,,pytorch,I am a beginner in pytorch.I learnt linear regression basic and trying to play with different activation functions and how it affects the optimization. I tried input values x as values from 1 to 6 and y values as a f(x) = x\^4 + 1 . I tried using relu and tanh but the loss was to high.Can anybody help me with this.And can anybody tell me where i can learn about activation functions.Thanks in advance,t2_1q0e70u1,False,,0,False,What layers and activation function should i use for this data?,[],r/pytorch,False,6,,0,,,False,t3_fawewn,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1582932773.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a beginner in pytorch.I learnt linear regression basic and trying to play with different activation functions and how it affects the optimization. I tried input values x as values from 1 to 6 and y values as a f(x) = x^4 + 1 . I tried using relu and tanh but the loss was to high.Can anybody help me with this.And can anybody tell me where i can learn about activation functions.Thanks in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fawewn,True,,gokulPRO,,5,True,all_ads,False,[],False,,/r/pytorch/comments/fawewn/what_layers_and_activation_function_should_i_use/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fawewn/what_layers_and_activation_function_should_i_use/,7135,1582903973.0,0,,False,,,,,,,,
653,,pytorch,"When I use ""pip install torch torchvision"" I get this error:

Collecting torch

  Downloading [https://files.pythonhosted.org/packages/46/ca/306bb933a68b888ab1c20ede0342506b85857635f04fb55a56e53065579b/torch-1.4.0-cp27-cp27mu-manylinux1\_x86\_64.whl](https://files.pythonhosted.org/packages/46/ca/306bb933a68b888ab1c20ede0342506b85857635f04fb55a56e53065579b/torch-1.4.0-cp27-cp27mu-manylinux1_x86_64.whl) (753.4MB)

99% |████████████████████████████████| 753.4MB 9.5MB/s eta 0:00:01Exception:

Traceback (most recent call last):

  File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 215, in main

status = [self.run](https://self.run)(options, args)

  File ""/usr/lib/python2.7/dist-packages/pip/commands/install.py"", line 342, in run

requirement\_set.prepare\_files(finder)

  File ""/usr/lib/python2.7/dist-packages/pip/req/req\_set.py"", line 380, in prepare\_files

ignore\_dependencies=self.ignore\_dependencies))

  File ""/usr/lib/python2.7/dist-packages/pip/req/req\_set.py"", line 620, in \_prepare\_file

session=self.session, hashes=hashes)

  File ""/usr/lib/python2.7/dist-packages/pip/download.py"", line 821, in unpack\_url

hashes=hashes

  File ""/usr/lib/python2.7/dist-packages/pip/download.py"", line 659, in unpack\_http\_url

hashes)

  File ""/usr/lib/python2.7/dist-packages/pip/download.py"", line 882, in \_download\_http\_url

\_download\_url(resp, link, content\_file, hashes)

  File ""/usr/lib/python2.7/dist-packages/pip/download.py"", line 603, in \_download\_url

hashes.check\_against\_chunks(downloaded\_chunks)

  File ""/usr/lib/python2.7/dist-packages/pip/utils/hashes.py"", line 46, in check\_against\_chunks

for chunk in chunks:

  File ""/usr/lib/python2.7/dist-packages/pip/download.py"", line 571, in written\_chunks

for chunk in chunks:

  File ""/usr/lib/python2.7/dist-packages/pip/utils/ui.py"", line 139, in iter

for x in it:

  File ""/usr/lib/python2.7/dist-packages/pip/download.py"", line 560, in resp\_read

decode\_content=False):

  File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream

data = [self.read](https://self.read)(amt=amt, decode\_content=decode\_content)

  File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 384, in read

data = self.\_fp.read(amt)

  File ""/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/filewrapper.py"", line 63, in read

self.\_close()

  File ""/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/filewrapper.py"", line 50, in \_close

self.\_\_callback(self.\_\_buf.getvalue())

  File ""/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/controller.py"", line 275, in cache\_response

self.serializer.dumps(request, response, body=body),

  File ""/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/serialize.py"", line 87, in dumps

).encode(""utf8""),

MemoryError",t2_1hjos5n4,False,,0,False,[HELP] Can't install PyTorch with pip,[],r/pytorch,False,6,,0,,,False,t3_farok2,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1582907530.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When I use &amp;quot;pip install torch torchvision&amp;quot; I get this error:&lt;/p&gt;

&lt;p&gt;Collecting torch&lt;/p&gt;

&lt;p&gt;Downloading &lt;a href=""https://files.pythonhosted.org/packages/46/ca/306bb933a68b888ab1c20ede0342506b85857635f04fb55a56e53065579b/torch-1.4.0-cp27-cp27mu-manylinux1_x86_64.whl""&gt;https://files.pythonhosted.org/packages/46/ca/306bb933a68b888ab1c20ede0342506b85857635f04fb55a56e53065579b/torch-1.4.0-cp27-cp27mu-manylinux1_x86_64.whl&lt;/a&gt; (753.4MB)&lt;/p&gt;

&lt;p&gt;99% |████████████████████████████████| 753.4MB 9.5MB/s eta 0:00:01Exception:&lt;/p&gt;

&lt;p&gt;Traceback (most recent call last):&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/lib/python2.7/dist-packages/pip/basecommand.py&amp;quot;, line 215, in main&lt;/p&gt;

&lt;p&gt;status = &lt;a href=""https://self.run""&gt;self.run&lt;/a&gt;(options, args)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/lib/python2.7/dist-packages/pip/commands/install.py&amp;quot;, line 342, in run&lt;/p&gt;

&lt;p&gt;requirement_set.prepare_files(finder)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/lib/python2.7/dist-packages/pip/req/req_set.py&amp;quot;, line 380, in prepare_files&lt;/p&gt;

&lt;p&gt;ignore_dependencies=self.ignore_dependencies))&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/lib/python2.7/dist-packages/pip/req/req_set.py&amp;quot;, line 620, in _prepare_file&lt;/p&gt;

&lt;p&gt;session=self.session, hashes=hashes)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/lib/python2.7/dist-packages/pip/download.py&amp;quot;, line 821, in unpack_url&lt;/p&gt;

&lt;p&gt;hashes=hashes&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/lib/python2.7/dist-packages/pip/download.py&amp;quot;, line 659, in unpack_http_url&lt;/p&gt;

&lt;p&gt;hashes)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/lib/python2.7/dist-packages/pip/download.py&amp;quot;, line 882, in _download_http_url&lt;/p&gt;

&lt;p&gt;_download_url(resp, link, content_file, hashes)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/lib/python2.7/dist-packages/pip/download.py&amp;quot;, line 603, in _download_url&lt;/p&gt;

&lt;p&gt;hashes.check_against_chunks(downloaded_chunks)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/lib/python2.7/dist-packages/pip/utils/hashes.py&amp;quot;, line 46, in check_against_chunks&lt;/p&gt;

&lt;p&gt;for chunk in chunks:&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/lib/python2.7/dist-packages/pip/download.py&amp;quot;, line 571, in written_chunks&lt;/p&gt;

&lt;p&gt;for chunk in chunks:&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/lib/python2.7/dist-packages/pip/utils/ui.py&amp;quot;, line 139, in iter&lt;/p&gt;

&lt;p&gt;for x in it:&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/lib/python2.7/dist-packages/pip/download.py&amp;quot;, line 560, in resp_read&lt;/p&gt;

&lt;p&gt;decode_content=False):&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py&amp;quot;, line 436, in stream&lt;/p&gt;

&lt;p&gt;data = &lt;a href=""https://self.read""&gt;self.read&lt;/a&gt;(amt=amt, decode_content=decode_content)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py&amp;quot;, line 384, in read&lt;/p&gt;

&lt;p&gt;data = self._fp.read(amt)&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/filewrapper.py&amp;quot;, line 63, in read&lt;/p&gt;

&lt;p&gt;self._close()&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/filewrapper.py&amp;quot;, line 50, in _close&lt;/p&gt;

&lt;p&gt;self.__callback(self.__buf.getvalue())&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/controller.py&amp;quot;, line 275, in cache_response&lt;/p&gt;

&lt;p&gt;self.serializer.dumps(request, response, body=body),&lt;/p&gt;

&lt;p&gt;File &amp;quot;/usr/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/serialize.py&amp;quot;, line 87, in dumps&lt;/p&gt;

&lt;p&gt;).encode(&amp;quot;utf8&amp;quot;),&lt;/p&gt;

&lt;p&gt;MemoryError&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,farok2,True,,NerdyPug123,,3,True,all_ads,False,[],False,,/r/pytorch/comments/farok2/help_cant_install_pytorch_with_pip/,all_ads,False,https://www.reddit.com/r/pytorch/comments/farok2/help_cant_install_pytorch_with_pip/,7135,1582878730.0,0,,False,,,,,,,,
654,,pytorch,,t2_17b1li,False,,0,False,"Torch.cat throws error for tensor list when compiling with torchscript, any help would be appreciated",[],r/pytorch,False,6,,0,140.0,,False,t3_fafbe6,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/OmtWPjbEDtFdyDGKPAC8C5nKnJcHyqTkPESMklMnDLk.jpg,False,,[],{},link,,False,,1582852452.0,text,6,,,text,discuss.pytorch.org,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fafbe6,True,,IluvitarTheAinur,,0,True,all_ads,False,[],False,,/r/pytorch/comments/fafbe6/torchcat_throws_error_for_tensor_list_when/,all_ads,False,https://discuss.pytorch.org/t/torch-cat-throws-error-for-tensor-list-when-compiling-with-torchscript/71317?u=particularlypythonic,7135,1582823652.0,0,,False,https://discuss.pytorch.org/t/torch-cat-throws-error-for-tensor-list-when-compiling-with-torchscript/71317?u=particularlypythonic,,,,,,,
655,,pytorch,"Where is the .backward() method documented for pytorch modules?  I've scoured the torch.nn docs page and searched the source code as best I can, and can't find the method documented/defined anywhere.  I'm assuming this is because it's implemented in c++, but where is that defined and documented?  It seems quite weird that such a prominent method would have literally _no_ documentation, am I just an idiot that missed it somewhere obvious?",t2_kpz51,False,,0,False,Where is .backward() documented?,[],r/pytorch,False,6,,0,,,False,t3_fa84xj,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1582815100.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Where is the .backward() method documented for pytorch modules?  I&amp;#39;ve scoured the torch.nn docs page and searched the source code as best I can, and can&amp;#39;t find the method documented/defined anywhere.  I&amp;#39;m assuming this is because it&amp;#39;s implemented in c++, but where is that defined and documented?  It seems quite weird that such a prominent method would have literally &lt;em&gt;no&lt;/em&gt; documentation, am I just an idiot that missed it somewhere obvious?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,fa84xj,True,,cderwin15,,1,True,all_ads,False,[],False,,/r/pytorch/comments/fa84xj/where_is_backward_documented/,all_ads,False,https://www.reddit.com/r/pytorch/comments/fa84xj/where_is_backward_documented/,7135,1582786300.0,0,,False,,,,,,,,
656,,pytorch,,t2_5qsmj233,False,,0,False,[R] Learning Certified Individually Fair Representations,[],r/pytorch,False,6,,0,,,False,t3_f9ql3n,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,default,False,,[],{},,,False,,1582737081.0,text,6,,,text,self.MachineLearning,False,,,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f9ql3n,True,,anianruoss,,0,True,all_ads,False,[],False,,/r/pytorch/comments/f9ql3n/r_learning_certified_individually_fair/,all_ads,False,/r/MachineLearning/comments/f99sow/r_learning_certified_individually_fair/,7135,1582708281.0,0,,False,/r/MachineLearning/comments/f99sow/r_learning_certified_individually_fair/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'We present a modular framework that: (i) enables the definition of individual fairness constraints via interpretable logical formulas, (ii) enforces these constraints by mapping similar individuals close to each other in the latent space, (iii) and leverages this proximity in the latent space to compute certificates of equal outcome for all similar individuals.\n\nPaper: [https://arxiv.org/abs/2002.10312](https://arxiv.org/abs/2002.10312)\n\nCode: [https://github.com/eth-sri/lcifr](https://github.com/eth-sri/lcifr)\n\n**Abstract.** To effectively enforce fairness constraints one needs to define an appropriate notion of fairness and employ representation learning in order to impose this notion without compromising downstream utility for the data consumer. A desirable notion is individual fairness as it guarantees similar treatment for similar individuals. In this work, we introduce the first method which generalizes individual fairness to rich similarity notions via logical constraints while also enabling data consumers to obtain fairness certificates for their models. The key idea is to learn a representation that provably maps similar individuals to latent representations at most epsilon apart in l-infinity distance, enabling data consumers to certify individual fairness by proving epsilon-robustness of their classifier. Our experimental evaluation on six real-world datasets and a wide range of fairness constraints demonstrates that our approach is expressive enough to capture similarity notions beyond existing distance metrics while scaling to realistic use cases.', 'author_fullname': 't2_5qsmj233', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[R] Learning Certified Individually Fair Representations', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'three', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_f99sow', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 22, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 22, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1582663867.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We present a modular framework that: (i) enables the definition of individual fairness constraints via interpretable logical formulas, (ii) enforces these constraints by mapping similar individuals close to each other in the latent space, (iii) and leverages this proximity in the latent space to compute certificates of equal outcome for all similar individuals.&lt;/p&gt;\n\n&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/abs/2002.10312""&gt;https://arxiv.org/abs/2002.10312&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=""https://github.com/eth-sri/lcifr""&gt;https://github.com/eth-sri/lcifr&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Abstract.&lt;/strong&gt; To effectively enforce fairness constraints one needs to define an appropriate notion of fairness and employ representation learning in order to impose this notion without compromising downstream utility for the data consumer. A desirable notion is individual fairness as it guarantees similar treatment for similar individuals. In this work, we introduce the first method which generalizes individual fairness to rich similarity notions via logical constraints while also enabling data consumers to obtain fairness certificates for their models. The key idea is to learn a representation that provably maps similar individuals to latent representations at most epsilon apart in l-infinity distance, enabling data consumers to certify individual fairness by proving epsilon-robustness of their classifier. Our experimental evaluation on six real-world datasets and a wide range of fairness constraints demonstrates that our approach is expressive enough to capture similarity notions beyond existing distance metrics while scaling to realistic use cases.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'f99sow', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'anianruoss', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/f99sow/r_learning_certified_individually_fair/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/f99sow/r_learning_certified_individually_fair/', 'subreddit_subscribers': 1740778, 'created_utc': 1582635067.0, 'num_crossposts': 4, 'media': None, 'is_video': False}]",t3_f99sow,,,,,
657,,pytorch,"Hi all .

I have been asked to check if CNTK can work along with Pytorch (Meaning both installed on the system and don't interfere each other). I'm aware to the fact CNTK will no longer get updated . This is the reason we are moving to Pytorch . Before we are going fully with Pytorch we need to make sure we still can work with both 

Please note I'm not familiar with CNTK nor Pytorch but I do have the technical knowledge to install it etc .  I also don't have a machine with GPU so I have installed the CPU version of the packages .

I have installed everything on Windows 10 64bit system

First I have installed Anaconda 3 ( 5.2.0 64bit)

Next I have created  virtual conda environment and installed all the suggested packages ( However the only packages that relevant for me are CNTK , Pytorch and msgpack)

I walked trough all the packages installed on the virtual conda environment and couldn't find CNTK and Pytorch  so I have installed them manually (I have WHL files) . I have installed the following :

torch-1.2.0+cpu-cp36-cp36m-win\_amd64.whl

cntk-2.7.post1-cp36-cp36m-win\_amd64.whl

&amp;#x200B;

I did the following to check that pytorch is working

`import torch`

`print(torch.1.2.0-cpu)`

`print(torch.randn(1, device='cpu'))`

I got the following :

`tensor([0.4544])`

Now I want to test CNTK to check if it works . The thing is I'm not sure how to test it . I googled it and found some info (See below) , however didn't really understand what to do :

[https://docs.microsoft.com/en-us/cognitive-toolkit/how-to-test](https://docs.microsoft.com/en-us/cognitive-toolkit/how-to-test)

&amp;#x200B;

Please let me know if you need any further information",t2_95bfv3j,False,,0,False,Using pytorch with CNTK,[],r/pytorch,False,6,,0,,,False,t3_f8um14,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1582593922.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all .&lt;/p&gt;

&lt;p&gt;I have been asked to check if CNTK can work along with Pytorch (Meaning both installed on the system and don&amp;#39;t interfere each other). I&amp;#39;m aware to the fact CNTK will no longer get updated . This is the reason we are moving to Pytorch . Before we are going fully with Pytorch we need to make sure we still can work with both &lt;/p&gt;

&lt;p&gt;Please note I&amp;#39;m not familiar with CNTK nor Pytorch but I do have the technical knowledge to install it etc .  I also don&amp;#39;t have a machine with GPU so I have installed the CPU version of the packages .&lt;/p&gt;

&lt;p&gt;I have installed everything on Windows 10 64bit system&lt;/p&gt;

&lt;p&gt;First I have installed Anaconda 3 ( 5.2.0 64bit)&lt;/p&gt;

&lt;p&gt;Next I have created  virtual conda environment and installed all the suggested packages ( However the only packages that relevant for me are CNTK , Pytorch and msgpack)&lt;/p&gt;

&lt;p&gt;I walked trough all the packages installed on the virtual conda environment and couldn&amp;#39;t find CNTK and Pytorch  so I have installed them manually (I have WHL files) . I have installed the following :&lt;/p&gt;

&lt;p&gt;torch-1.2.0+cpu-cp36-cp36m-win_amd64.whl&lt;/p&gt;

&lt;p&gt;cntk-2.7.post1-cp36-cp36m-win_amd64.whl&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I did the following to check that pytorch is working&lt;/p&gt;

&lt;p&gt;&lt;code&gt;import torch&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(torch.1.2.0-cpu)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(torch.randn(1, device=&amp;#39;cpu&amp;#39;))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I got the following :&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tensor([0.4544])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now I want to test CNTK to check if it works . The thing is I&amp;#39;m not sure how to test it . I googled it and found some info (See below) , however didn&amp;#39;t really understand what to do :&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://docs.microsoft.com/en-us/cognitive-toolkit/how-to-test""&gt;https://docs.microsoft.com/en-us/cognitive-toolkit/how-to-test&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Please let me know if you need any further information&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f8um14,True,,MusicIsLife1122,,5,True,all_ads,False,[],False,,/r/pytorch/comments/f8um14/using_pytorch_with_cntk/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f8um14/using_pytorch_with_cntk/,7135,1582565122.0,0,,False,,,,,,,,
658,,pytorch,"I am trying to create a visualization tool for Pytorch models. I need to send the complete model along with architecture to my web server and run it there. Currently Pytorch's [model.save](https://model.save) just saves the model object and states, not the model architecture. Can this be achieved or is there any other better way to save pytorch models? Thanks.",t2_3kkdjj7e,False,,0,False,Saving Pytorch model to a different computer,[],r/pytorch,False,6,,0,,,False,t3_f726q9,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1582271371.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to create a visualization tool for Pytorch models. I need to send the complete model along with architecture to my web server and run it there. Currently Pytorch&amp;#39;s &lt;a href=""https://model.save""&gt;model.save&lt;/a&gt; just saves the model object and states, not the model architecture. Can this be achieved or is there any other better way to save pytorch models? Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f726q9,True,,nikhltyagi,,9,True,all_ads,False,[],False,,/r/pytorch/comments/f726q9/saving_pytorch_model_to_a_different_computer/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f726q9/saving_pytorch_model_to_a_different_computer/,7135,1582242571.0,0,,False,,,,,,,,
659,,pytorch,[https://drive.google.com/open?id=1K8gw3l-0Is8ASB6YyF2seRvmt1o4pRba](https://drive.google.com/open?id=1K8gw3l-0Is8ASB6YyF2seRvmt1o4pRba),,False,,0,False,My model trains super slow. How can i speed it up?,[],r/pytorch,False,6,,0,,,False,t3_f6n26a,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,,self,False,,,{},,,True,,1582199257.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://drive.google.com/open?id=1K8gw3l-0Is8ASB6YyF2seRvmt1o4pRba""&gt;https://drive.google.com/open?id=1K8gw3l-0Is8ASB6YyF2seRvmt1o4pRba&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f6n26a,True,,[deleted],,7,True,all_ads,False,[],,dark,/r/pytorch/comments/f6n26a/my_model_trains_super_slow_how_can_i_speed_it_up/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f6n26a/my_model_trains_super_slow_how_can_i_speed_it_up/,7135,1582170457.0,0,,False,,,,,,,,
660,,pytorch,"does anyone know if there's a prewritten way to load data into pytorch with a file structure of train/class\_name/folder\_name/image? i don't want to spend a ton of time overwriting datasetfolder if there's something easier i can do instead lol

for context, each folder under the class name represents a video fo images, and i only want to work with one video's worth of images at a time when passing it through my model",t2_3udyy0e1,False,,0,False,pytorch dataloading,[],r/pytorch,False,6,,0,,,False,t3_f6iim6,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1582179682.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;does anyone know if there&amp;#39;s a prewritten way to load data into pytorch with a file structure of train/class_name/folder_name/image? i don&amp;#39;t want to spend a ton of time overwriting datasetfolder if there&amp;#39;s something easier i can do instead lol&lt;/p&gt;

&lt;p&gt;for context, each folder under the class name represents a video fo images, and i only want to work with one video&amp;#39;s worth of images at a time when passing it through my model&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f6iim6,True,,anghst_,,3,True,all_ads,False,[],False,,/r/pytorch/comments/f6iim6/pytorch_dataloading/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f6iim6/pytorch_dataloading/,7135,1582150882.0,0,,False,,,,,,,,
661,,pytorch,"Hi all,

I'm  about to start a ML project (time-series forecasting) using PyTorch. So far I only  did relatively small ML projects (working with 20k images for example).  This one will be larger (over 2 million time-series). Therefore I want  to set up a solid pipeline.

I would like to build the model in PyTorch, do some visualization with e.g. Flask and containerize the application/model (not the data).

The  raw data consists of 2 million time series (JSON-structured). Most likely I  will add meta-data to these time-series. Also I would like to experiment  with different types of models (DL, traditional ones &amp; hybrid)

What  set up would you recommend for such project? Are there some recommendations regarding project/data-pipeline set up for time-series forecasting? (so far only experience with computer vision) 

Many thanks in advance,

S",t2_fk28j,False,,0,False,Data pipeline/project structure for PyTorch-based time-series forecasting,[],r/pytorch,False,6,,0,,,False,t3_f6dsqx,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1582161149.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m  about to start a ML project (time-series forecasting) using PyTorch. So far I only  did relatively small ML projects (working with 20k images for example).  This one will be larger (over 2 million time-series). Therefore I want  to set up a solid pipeline.&lt;/p&gt;

&lt;p&gt;I would like to build the model in PyTorch, do some visualization with e.g. Flask and containerize the application/model (not the data).&lt;/p&gt;

&lt;p&gt;The  raw data consists of 2 million time series (JSON-structured). Most likely I  will add meta-data to these time-series. Also I would like to experiment  with different types of models (DL, traditional ones &amp;amp; hybrid)&lt;/p&gt;

&lt;p&gt;What  set up would you recommend for such project? Are there some recommendations regarding project/data-pipeline set up for time-series forecasting? (so far only experience with computer vision) &lt;/p&gt;

&lt;p&gt;Many thanks in advance,&lt;/p&gt;

&lt;p&gt;S&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f6dsqx,True,,Florisc,,3,True,all_ads,False,[],False,,/r/pytorch/comments/f6dsqx/data_pipelineproject_structure_for_pytorchbased/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f6dsqx/data_pipelineproject_structure_for_pytorchbased/,7135,1582132349.0,0,,False,,,,,,,,
662,,pytorch,,t2_x54ow,False,,0,False,Make your models smaller! (Part 1),[],r/pytorch,False,6,,0,79.0,,False,t3_f64huq,False,dark,0.95,,public,20,0,{},140.0,,False,[],,False,False,,{},,False,20,,False,https://b.thumbs.redditmedia.com/phquucr8qB5g7cD1EHPzUg0gPtnseI9lz3PHsDt3Xuc.jpg,False,,[],{},,,False,,1582111708.0,text,6,,,text,amandeepsp.github.io,False,,,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f64huq,True,,amandeepspdhr,,0,True,all_ads,False,[],False,,/r/pytorch/comments/f64huq/make_your_models_smaller_part_1/,all_ads,False,https://amandeepsp.github.io/ml-model-compression-part1/,7135,1582082908.0,0,,False,https://amandeepsp.github.io/ml-model-compression-part1/,,,,,,,
663,,pytorch," Hi, guys

I believe this question was asked before. If so, I'd appreciate if someone could redirect me to such post. Regardless, I will ask it here too. My question is ""Why does my NN keep converging to ""mean"" values even though I vary network architecture from very simple to ""deep"", learning rates, use/not use Dropouts and etc.

my model is defined as follows:

def CNN\_model():  
    model = keras.Sequential(\[layers.Conv1D(30, 10, padding='valid', activation='relu', input\_shape=(train\_dataset.shape\[1\], 1)),  
                              layers.MaxPooling1D(),  
\#   layers.Conv1D(30, 5, activation='relu'),  
\#   layers.MaxPooling1D(),  
\#   layers.Dropout(0.25),  
                              layers.Flatten(),  
\#   layers.Dropout(0.25),  
\#   layers.Dense(120, activation='linear'),  
\#   layers.Dense(60, activation='linear'),  
                              layers.Dense(30)\])

    optimizer = tf.keras.optimizers.Adam(0.00001)  
    model.compile(optimizer=optimizer, loss = 'mse', metrics = \['mse'\])  
return model

I am using a CNN for 1D signals in this case. I also used a Fully-connected NN for the same problem. Regardless, the issue remains.

I'll be really grateful if you can share your thoughts on this issue.

Thank you.

Briefly about my data:

Both X and y are continuous, which are more or less within the same range. However, it's fair to note that they are not a normalized in my case. The thing is that my original X and y are dimensions of 500 and 10000, respectively. But I'm only capturing 30 PCA features in each. Therefore, my X training data is 500x30 matrix (30 features, 500 number of examples), and y training data is 500x30.

\#neuralnetworks #CNN #regression",t2_4ecq42y4,False,,0,False,"NN converges to ""mean"" values for any test input data",[],r/pytorch,False,6,,0,,,False,t3_f6g1ja,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1582169818.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, guys&lt;/p&gt;

&lt;p&gt;I believe this question was asked before. If so, I&amp;#39;d appreciate if someone could redirect me to such post. Regardless, I will ask it here too. My question is &amp;quot;Why does my NN keep converging to &amp;quot;mean&amp;quot; values even though I vary network architecture from very simple to &amp;quot;deep&amp;quot;, learning rates, use/not use Dropouts and etc.&lt;/p&gt;

&lt;p&gt;my model is defined as follows:&lt;/p&gt;

&lt;p&gt;def CNN_model():&lt;br/&gt;
    model = keras.Sequential([layers.Conv1D(30, 10, padding=&amp;#39;valid&amp;#39;, activation=&amp;#39;relu&amp;#39;, input_shape=(train_dataset.shape[1], 1)),&lt;br/&gt;
                              layers.MaxPooling1D(),&lt;br/&gt;
#   layers.Conv1D(30, 5, activation=&amp;#39;relu&amp;#39;),&lt;br/&gt;
#   layers.MaxPooling1D(),&lt;br/&gt;
#   layers.Dropout(0.25),&lt;br/&gt;
                              layers.Flatten(),&lt;br/&gt;
#   layers.Dropout(0.25),&lt;br/&gt;
#   layers.Dense(120, activation=&amp;#39;linear&amp;#39;),&lt;br/&gt;
#   layers.Dense(60, activation=&amp;#39;linear&amp;#39;),&lt;br/&gt;
                              layers.Dense(30)])&lt;/p&gt;

&lt;p&gt;    optimizer = tf.keras.optimizers.Adam(0.00001)&lt;br/&gt;
    model.compile(optimizer=optimizer, loss = &amp;#39;mse&amp;#39;, metrics = [&amp;#39;mse&amp;#39;])&lt;br/&gt;
return model&lt;/p&gt;

&lt;p&gt;I am using a CNN for 1D signals in this case. I also used a Fully-connected NN for the same problem. Regardless, the issue remains.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ll be really grateful if you can share your thoughts on this issue.&lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;

&lt;p&gt;Briefly about my data:&lt;/p&gt;

&lt;p&gt;Both X and y are continuous, which are more or less within the same range. However, it&amp;#39;s fair to note that they are not a normalized in my case. The thing is that my original X and y are dimensions of 500 and 10000, respectively. But I&amp;#39;m only capturing 30 PCA features in each. Therefore, my X training data is 500x30 matrix (30 features, 500 number of examples), and y training data is 500x30.&lt;/p&gt;

&lt;p&gt;#neuralnetworks #CNN #regression&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f6g1ja,True,,ncuxomun,,7,True,all_ads,False,[],False,,/r/pytorch/comments/f6g1ja/nn_converges_to_mean_values_for_any_test_input/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f6g1ja/nn_converges_to_mean_values_for_any_test_input/,7135,1582141018.0,0,,False,,,,,,,,
664,,pytorch,"I am making my own linear regression model from scratch

Here is the code

```python
w = torch.tensor(-10.0, requires_grad=True)

X = torch.arange(-3, 3.1, 0.1, requires_grad=True).view(-1, 1)
F = -3*X

Y = f + 0.1 * torch.randn(X.size())

def forward(x):
    global w
    y = w*x  
    return y

def mse(yhat, y):
    return torch.mean((y-yhat)**2)

learning_rate = 0.1

for epoch in range(1, 11):
    yhat = forward(x)
    loss = mse(yhat, y)
    loss.backward(retain_graph=True) # error occurs here
    pass
```

The following exception is thrown after a successful iteration of the dataset

```
RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.
```",t2_3xzz10an,False,,0,False,Getting buffers have already been freed even though retain_graph is True,[],r/pytorch,False,6,,0,,,False,t3_f5qj5a,False,dark,0.43,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1582054922.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am making my own linear regression model from scratch&lt;/p&gt;

&lt;p&gt;Here is the code&lt;/p&gt;

&lt;p&gt;```python
w = torch.tensor(-10.0, requires_grad=True)&lt;/p&gt;

&lt;p&gt;X = torch.arange(-3, 3.1, 0.1, requires_grad=True).view(-1, 1)
F = -3*X&lt;/p&gt;

&lt;p&gt;Y = f + 0.1 * torch.randn(X.size())&lt;/p&gt;

&lt;p&gt;def forward(x):
    global w
    y = w*x&lt;br/&gt;
    return y&lt;/p&gt;

&lt;p&gt;def mse(yhat, y):
    return torch.mean((y-yhat)**2)&lt;/p&gt;

&lt;p&gt;learning_rate = 0.1&lt;/p&gt;

&lt;p&gt;for epoch in range(1, 11):
    yhat = forward(x)
    loss = mse(yhat, y)
    loss.backward(retain_graph=True) # error occurs here
    pass
```&lt;/p&gt;

&lt;p&gt;The following exception is thrown after a successful iteration of the dataset&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.
&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f5qj5a,True,,tbhaxor,,2,True,all_ads,False,[],False,,/r/pytorch/comments/f5qj5a/getting_buffers_have_already_been_freed_even/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f5qj5a/getting_buffers_have_already_been_freed_even/,7135,1582026122.0,0,,False,,,,,,,,
665,,pytorch,"Here is an example what I am trying to do. Here's what my dataset may look like


    2   | 34, 64, 2243, 55678, 323, 778, 4454, 23, 4433, 3445, 455, 32
    23  | 343 ,56, 2, 5, 675, 34, 232, 677, 7, 54, 436, 77, 85, 33
    592 | 343, 54, 4, 6
    23  | 34
    123 | 2, 4, 54, 38, 6643, 67, 3
    ...


Each of these numbers are indexes (not the actual model inputs/target data) which point to the actual data that will be fed into the model. The actual model data is in a separate dataset, organized by indexes (same number used in the first dataset), so it would look something like this


    1 | data1
    2 | data2
    3 | data3



For the first set, on the left side are indexes to the targets, so 2, 23, 592, 23, 123. On the right side are indexes to the potential inputs, and for each target, 4 inputs are picked at random. For targets which have a number of inputs less than 4, inputs are repeated. 

So taking the first training example in the first set, the model input would look like this

    target = torch.tensor([data2])
    input = torch.mean( torch.tensor([data34]), torch.tensor([data2243]), torch.tensor([data23]), torch.tensor([data32]) )

Both sets are too big to hold in memory at the sametime, and I don't have enough diskspace to save targets with every permutation of inputs. 

I think the best strategy is to have X number of batches set up ahead of time, using multithreading to setup future batches, so that the data pipeline does't become the bottle neck in the training, but I am wondering how to go about doing that.

Edit: 

I think my description is a bit hazy, so I made an analogy of the type of data pipeline I'm trying to develop. 

An analogy would be some types of word embedding trainings, where the number of input words is smaller than the context window. So for a particular target word, the input words need to be chosen at random within the context window of that target word. 

For example, take this sentence:

`There once was a very fast dog who could out run any other animal in the city. `

Say the target word was dog, and that the context window size was 6, but the input size was 2. So we have a choice of 'a', 'very', 'fast', 'who', 'could', 'out', of which we need to pick two inputs randomly. So an example training example would be the word embedding for 'dog' as the target, and the word embeddings of 'fast' and 'out' for the inputs. 

So in word embedding trainings, many different inputs will be used in many different targets. But with word embedding trainings, all the word embeddings are able to be stored in live memory since the vocab is 6 figures. For my case, the inputs can't all be in live memory.

So an idea of what I'm looking to do is word2vec training but most of the word embeddings have to stay in disk space at any particular instance.",t2_10efjmjx,False,,0,False,Best way to handle a data pipeline where training inputs/targets are randomly sampled with TORCH.UTILS.DATA,[],r/pytorch,False,6,,0,,,False,t3_f5mnw9,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1582141833.0,,[],{},,,True,,1582031951.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Here is an example what I am trying to do. Here&amp;#39;s what my dataset may look like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2   | 34, 64, 2243, 55678, 323, 778, 4454, 23, 4433, 3445, 455, 32
23  | 343 ,56, 2, 5, 675, 34, 232, 677, 7, 54, 436, 77, 85, 33
592 | 343, 54, 4, 6
23  | 34
123 | 2, 4, 54, 38, 6643, 67, 3
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each of these numbers are indexes (not the actual model inputs/target data) which point to the actual data that will be fed into the model. The actual model data is in a separate dataset, organized by indexes (same number used in the first dataset), so it would look something like this&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1 | data1
2 | data2
3 | data3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the first set, on the left side are indexes to the targets, so 2, 23, 592, 23, 123. On the right side are indexes to the potential inputs, and for each target, 4 inputs are picked at random. For targets which have a number of inputs less than 4, inputs are repeated. &lt;/p&gt;

&lt;p&gt;So taking the first training example in the first set, the model input would look like this&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;target = torch.tensor([data2])
input = torch.mean( torch.tensor([data34]), torch.tensor([data2243]), torch.tensor([data23]), torch.tensor([data32]) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both sets are too big to hold in memory at the sametime, and I don&amp;#39;t have enough diskspace to save targets with every permutation of inputs. &lt;/p&gt;

&lt;p&gt;I think the best strategy is to have X number of batches set up ahead of time, using multithreading to setup future batches, so that the data pipeline does&amp;#39;t become the bottle neck in the training, but I am wondering how to go about doing that.&lt;/p&gt;

&lt;p&gt;Edit: &lt;/p&gt;

&lt;p&gt;I think my description is a bit hazy, so I made an analogy of the type of data pipeline I&amp;#39;m trying to develop. &lt;/p&gt;

&lt;p&gt;An analogy would be some types of word embedding trainings, where the number of input words is smaller than the context window. So for a particular target word, the input words need to be chosen at random within the context window of that target word. &lt;/p&gt;

&lt;p&gt;For example, take this sentence:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;There once was a very fast dog who could out run any other animal in the city.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Say the target word was dog, and that the context window size was 6, but the input size was 2. So we have a choice of &amp;#39;a&amp;#39;, &amp;#39;very&amp;#39;, &amp;#39;fast&amp;#39;, &amp;#39;who&amp;#39;, &amp;#39;could&amp;#39;, &amp;#39;out&amp;#39;, of which we need to pick two inputs randomly. So an example training example would be the word embedding for &amp;#39;dog&amp;#39; as the target, and the word embeddings of &amp;#39;fast&amp;#39; and &amp;#39;out&amp;#39; for the inputs. &lt;/p&gt;

&lt;p&gt;So in word embedding trainings, many different inputs will be used in many different targets. But with word embedding trainings, all the word embeddings are able to be stored in live memory since the vocab is 6 figures. For my case, the inputs can&amp;#39;t all be in live memory.&lt;/p&gt;

&lt;p&gt;So an idea of what I&amp;#39;m looking to do is word2vec training but most of the word embeddings have to stay in disk space at any particular instance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f5mnw9,True,,BatmantoshReturns,,4,True,all_ads,False,[],False,,/r/pytorch/comments/f5mnw9/best_way_to_handle_a_data_pipeline_where_training/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f5mnw9/best_way_to_handle_a_data_pipeline_where_training/,7135,1582003151.0,0,,False,,,,,,,,
666,,pytorch,,t2_xt6j8xa,False,,0,False,Does pytorch have something similar to callbacks where I can perform certain operations on epoch beginning and epoch end?,[],r/pytorch,False,6,,0,,,False,t3_f5q8m3,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1582053174.0,text,6,,,text,self.pytorch,False,,,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f5q8m3,True,,clean_pegasus,,3,True,all_ads,False,[],False,,/r/pytorch/comments/f5q8m3/does_pytorch_have_something_similar_to_callbacks/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f5q8m3/does_pytorch_have_something_similar_to_callbacks/,7135,1582024374.0,0,,False,,,,,,,,
667,,pytorch,,t2_15n8fb,False,,0,False,How does one have parameters in a pytorch model not be leafs and be in the computation graph?,[],r/pytorch,False,6,,0,140.0,,False,t3_f5gu3g,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/xu4W9dIGPtDYIW8j_BN851YcEt_H-wd5YFgrp9ILoTQ.jpg,False,,[],{},link,,False,,1582006732.0,text,6,,,text,stackoverflow.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f5gu3g,True,,real_pinocchio,,0,True,all_ads,False,[],False,,/r/pytorch/comments/f5gu3g/how_does_one_have_parameters_in_a_pytorch_model/,all_ads,False,https://stackoverflow.com/questions/60271131/how-does-one-have-parameters-in-a-pytorch-model-not-be-leafs-and-be-in-the-compu,7135,1581977932.0,0,,False,https://stackoverflow.com/questions/60271131/how-does-one-have-parameters-in-a-pytorch-model-not-be-leafs-and-be-in-the-compu,,,,,,,
668,,pytorch,"Hi guys, I am looking for some tutorials on ways to use Pytorch that are not related to image classification. Predictions made from stats, mainly. Sorry if this is not the place for this, I lost a good chunk of my week-end looking for this and my Google-Fu seems to weak for this...

To be similar to what I would like to be able to try in the future, a tutorial on the way to predict the final score of a game or classify a row in a table according to the values of different fields would be appropriate, I think. I do not really care about the subject, I just want to see what can be done with a table instead of a picture. 

Side note, I am mainly tring to understand how the model works, and how you build one. Most tutorial I have found yet consists on telling you how to prepare the data for a basic model they copy-pasted from another tutorial, usually the one about dogs from the pytorch website.",t2_435v5,False,,0,False,Tutorials not image related,[],r/pytorch,False,6,,0,,,False,t3_f58q87,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1581974879.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys, I am looking for some tutorials on ways to use Pytorch that are not related to image classification. Predictions made from stats, mainly. Sorry if this is not the place for this, I lost a good chunk of my week-end looking for this and my Google-Fu seems to weak for this...&lt;/p&gt;

&lt;p&gt;To be similar to what I would like to be able to try in the future, a tutorial on the way to predict the final score of a game or classify a row in a table according to the values of different fields would be appropriate, I think. I do not really care about the subject, I just want to see what can be done with a table instead of a picture. &lt;/p&gt;

&lt;p&gt;Side note, I am mainly tring to understand how the model works, and how you build one. Most tutorial I have found yet consists on telling you how to prepare the data for a basic model they copy-pasted from another tutorial, usually the one about dogs from the pytorch website.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f58q87,True,,jimbuz,,5,True,all_ads,False,[],False,,/r/pytorch/comments/f58q87/tutorials_not_image_related/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f58q87/tutorials_not_image_related/,7135,1581946079.0,0,,False,,,,,,,,
669,,pytorch,,t2_15ttxm,False,,0,False,[PyTorch] Output and target tensors dimensions,[],r/pytorch,False,6,,0,99.0,,False,t3_f4j8qg,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/7CImRBjmUqwpJPcWYZSMGPw192Z0eS10_dyMGeDK1no.jpg,False,,[],{},,,False,,1581844970.0,text,6,,,text,self.learnmachinelearning,False,,,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f4j8qg,True,,abbaahmad,,0,True,all_ads,False,[],False,,/r/pytorch/comments/f4j8qg/pytorch_output_and_target_tensors_dimensions/,all_ads,False,/r/learnmachinelearning/comments/f4fkgg/pytorch_output_and_target_tensors_dimensions/,7135,1581816170.0,0,,False,/r/learnmachinelearning/comments/f4fkgg/pytorch_output_and_target_tensors_dimensions/,"[{'approved_at_utc': None, 'subreddit': 'learnmachinelearning', 'selftext': 'I need help figuring out what the target dimension should be for a 4-class model. I\'m trying to retrain Alexnet for bounding box prediction, part of the dataloader class is shown below:\n\ndef \\_\\_getitem\\_\\_(self,idx):\n\n    \t\\#load image\n    \n    \timg\\_path = os.path.join(self.root,\'images\',self.images\\[idx\\])\n    \n    \tlabel\\_path = os.path.join(self.root,\'labels\',self.labels\\[idx\\])\n    \n    \timg\\_array = [Image.open](https://Image.open)(img\\_path).convert(\'RGB\')\n    \n    \tpreprocess = transforms.Compose(\\[transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225]),\\])\n    img = preprocess(img\\_array)\n    \n    \t\\#label\n    \n    \twith open(label\\_path,\'r\') as f:\n    \n    \t\tlabel = [f.read](https://f.read)().split()\n    \n    \t\tlabel = \\[float(x) for x in label if x != \'0\'\\]\n    \n    \tlabel\\_tensor = torch.FloatTensor(label).flatten()\n    \n    \treturn img, label\\_tensor\n    \n\nI keep getting this error. The other suggestion i got was to use "".squeeze(1)"" and it still throws up the same error. Any help is appreciated.\n\nhttps://preview.redd.it/fyortpxai5h41.png?width=793&amp;format=png&amp;auto=webp&amp;s=a800d100d29aac344f3a1b230280b2d800144647', 'author_fullname': 't2_15ttxm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[PyTorch] Output and target tensors dimensions', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/learnmachinelearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 99, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'fyortpxai5h41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 76, 'x': 108, 'u': 'https://preview.redd.it/fyortpxai5h41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=509f0bb92a0aa14b2017b1e382906f3c6c09a64d'}, {'y': 152, 'x': 216, 'u': 'https://preview.redd.it/fyortpxai5h41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d9cfdbc692f4e8bfa7fa910c6a7fe409832d5006'}, {'y': 226, 'x': 320, 'u': 'https://preview.redd.it/fyortpxai5h41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3b094e37f8bdd693e8ed2a64bf8957e23507414d'}, {'y': 452, 'x': 640, 'u': 'https://preview.redd.it/fyortpxai5h41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c379f19b0048696d456f4774c66c4eb411e39cad'}], 's': {'y': 561, 'x': 793, 'u': 'https://preview.redd.it/fyortpxai5h41.png?width=793&amp;format=png&amp;auto=webp&amp;s=a800d100d29aac344f3a1b230280b2d800144647'}, 'id': 'fyortpxai5h41'}}, 'name': 't3_f4fkgg', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 1, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/7CImRBjmUqwpJPcWYZSMGPw192Z0eS10_dyMGeDK1no.jpg', 'edited': 1581803592.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1581828943.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.learnmachinelearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need help figuring out what the target dimension should be for a 4-class model. I&amp;#39;m trying to retrain Alexnet for bounding box prediction, part of the dataloader class is shown below:&lt;/p&gt;\n\n&lt;p&gt;def __getitem__(self,idx):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    \\#load image\n\n    img\\_path = os.path.join(self.root,&amp;#39;images&amp;#39;,self.images\\[idx\\])\n\n    label\\_path = os.path.join(self.root,&amp;#39;labels&amp;#39;,self.labels\\[idx\\])\n\n    img\\_array = [Image.open](https://Image.open)(img\\_path).convert(&amp;#39;RGB&amp;#39;)\n\n    preprocess = transforms.Compose(\\[transforms.Resize(256),\ntransforms.CenterCrop(224),\ntransforms.ToTensor(),\ntransforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225]),\\])\nimg = preprocess(img\\_array)\n\n    \\#label\n\n    with open(label\\_path,&amp;#39;r&amp;#39;) as f:\n\n        label = [f.read](https://f.read)().split()\n\n        label = \\[float(x) for x in label if x != &amp;#39;0&amp;#39;\\]\n\n    label\\_tensor = torch.FloatTensor(label).flatten()\n\n    return img, label\\_tensor\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I keep getting this error. The other suggestion i got was to use &amp;quot;.squeeze(1)&amp;quot; and it still throws up the same error. Any help is appreciated.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/fyortpxai5h41.png?width=793&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a800d100d29aac344f3a1b230280b2d800144647""&gt;https://preview.redd.it/fyortpxai5h41.png?width=793&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a800d100d29aac344f3a1b230280b2d800144647&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3cqa1', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'f4fkgg', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'abbaahmad', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/learnmachinelearning/comments/f4fkgg/pytorch_output_and_target_tensors_dimensions/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/learnmachinelearning/comments/f4fkgg/pytorch_output_and_target_tensors_dimensions/', 'subreddit_subscribers': 217921, 'created_utc': 1581800143.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_f4fkgg,,,,,
670,,pytorch,"I realize there is packed_padded_sequence and so on for batch training LSTMs, but that takes an entire sequence and embeds it then forwards it through the LSTM. My LSTM is built so that it just takes an input character then forward just outputs the categorical at each sequence. So I built it so that I pad the sequences before hand so they’re equal length then each index is fed in sequentially. Unfortunately this also means the first characters are pad characters (Because I use prepadding). Is there a way to get the LSTM not to backprop on inputs that are just pads using this LSTM setup?",t2_14d0xp,False,,0,False,Ignore padding for LSTM batch training,[],r/pytorch,False,6,,0,,,False,t3_f413tw,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1581753022.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I realize there is packed_padded_sequence and so on for batch training LSTMs, but that takes an entire sequence and embeds it then forwards it through the LSTM. My LSTM is built so that it just takes an input character then forward just outputs the categorical at each sequence. So I built it so that I pad the sequences before hand so they’re equal length then each index is fed in sequentially. Unfortunately this also means the first characters are pad characters (Because I use prepadding). Is there a way to get the LSTM not to backprop on inputs that are just pads using this LSTM setup?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f413tw,True,,DolantheMFWizard,,1,True,all_ads,False,[],False,,/r/pytorch/comments/f413tw/ignore_padding_for_lstm_batch_training/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f413tw/ignore_padding_for_lstm_batch_training/,7135,1581724222.0,0,,False,,,,,,,,
671,,pytorch,Any useful material? Thanks,t2_jsn8x,False,,0,False,"PyTorch coding best practices, optimization hints, etc.",[],r/pytorch,False,6,,0,,,False,t3_f3a5xx,False,dark,1.0,,public,22,0,{},,,False,[],,False,False,,{},,False,22,,False,self,False,,[],{},,,True,,1581630230.0,text,6,,,text,self.pytorch,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Any useful material? Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f3a5xx,True,,zepmck,,8,True,all_ads,False,[],False,,/r/pytorch/comments/f3a5xx/pytorch_coding_best_practices_optimization_hints/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f3a5xx/pytorch_coding_best_practices_optimization_hints/,7135,1581601430.0,0,,False,,,,,,,,
672,,pytorch,"When running the prompt  

conda install pytorch torchvision cpuonly -c pytorch

&amp;#x200B;

I get the error : CondaHTTPError: HTTP 000 CONNECTION FAILED for url &lt;[https://conda.anaconda.org/pytorch/win-64/current\_repodata.json](https://conda.anaconda.org/pytorch/win-64/current_repodata.json)\&gt;

&amp;#x200B;

When I go to that website it says the website it says the page I am looking for does not exist. How can I solve this?",,False,,0,False,Downloaded Anaconda but can't install PyTorch,[],r/pytorch,False,6,,0,,,False,t3_f3iali,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,,self,False,,,{},,,True,,1581662355.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When running the prompt  &lt;/p&gt;

&lt;p&gt;conda install pytorch torchvision cpuonly -c pytorch&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I get the error : CondaHTTPError: HTTP 000 CONNECTION FAILED for url &amp;lt;&lt;a href=""https://conda.anaconda.org/pytorch/win-64/current_repodata.json""&gt;https://conda.anaconda.org/pytorch/win-64/current_repodata.json&lt;/a&gt;&amp;gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;When I go to that website it says the website it says the page I am looking for does not exist. How can I solve this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f3iali,True,,[deleted],,2,True,all_ads,False,[],,dark,/r/pytorch/comments/f3iali/downloaded_anaconda_but_cant_install_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f3iali/downloaded_anaconda_but_cant_install_pytorch/,7135,1581633555.0,0,,False,,,,,,,,
673,,pytorch,"Hi, I am new to Pytorch. I have some experience with TensorFlow and I really used to enjoy how visualising the model with Tensorboard makes it easier to understand. Since, Pytorch also offers support for Tensorboard I was expecting a similar experience, but unfortunately it hasn't been very pleasant for me.

The main issue is that, Tensorboard creates a node for every single operation (even for slicing and squeezing) (I understand that this is the default behaviour) and there is no way of understanding what's happening by looking at the ""messy"" model shown by Tensorboard. TF allows you to organize your model using name_scope, but I couldn't find any such function/methods for Pytorch. Defining a Class creates something like name_scope during visualisation, but that is not a feasible solution for organizing the model. So, I was wondering if there is any way of visualising the Pytorch models in Tensorboard in a way that it makes sense. Or is there any other package that can help?? I have tried torchviz but it wasn't very helpful either. Any help will be appreciated.


TL;DR: How to visualise a Pytorch model with a lot of operations? Tensorboard is not helping since it creates nodes for every operation and Pytorch doesn't have name_scope for organising them.",t2_10bt2qhy,False,,0,False,Visualising Models with Tensorboard,[],r/pytorch,False,6,,0,,,False,t3_f38nqy,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1581622416.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am new to Pytorch. I have some experience with TensorFlow and I really used to enjoy how visualising the model with Tensorboard makes it easier to understand. Since, Pytorch also offers support for Tensorboard I was expecting a similar experience, but unfortunately it hasn&amp;#39;t been very pleasant for me.&lt;/p&gt;

&lt;p&gt;The main issue is that, Tensorboard creates a node for every single operation (even for slicing and squeezing) (I understand that this is the default behaviour) and there is no way of understanding what&amp;#39;s happening by looking at the &amp;quot;messy&amp;quot; model shown by Tensorboard. TF allows you to organize your model using name_scope, but I couldn&amp;#39;t find any such function/methods for Pytorch. Defining a Class creates something like name_scope during visualisation, but that is not a feasible solution for organizing the model. So, I was wondering if there is any way of visualising the Pytorch models in Tensorboard in a way that it makes sense. Or is there any other package that can help?? I have tried torchviz but it wasn&amp;#39;t very helpful either. Any help will be appreciated.&lt;/p&gt;

&lt;p&gt;TL;DR: How to visualise a Pytorch model with a lot of operations? Tensorboard is not helping since it creates nodes for every operation and Pytorch doesn&amp;#39;t have name_scope for organising them.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f38nqy,True,,Nilotpal09,,0,True,all_ads,False,[],False,,/r/pytorch/comments/f38nqy/visualising_models_with_tensorboard/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f38nqy/visualising_models_with_tensorboard/,7135,1581593616.0,0,,False,,,,,,,,
674,,pytorch,"Hi everyone,  


I've decided to learn pytorch and ML, and I have completed andrew ng's course on coursera and after that I read the [book](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf) about pytorch which they have on their website. After that I tried a couple of tutorials for image recognition/classification.   


Now, I would like to make a nn that can ""restore"" old images, so i figured I could get a set of RGB images, turn them to greyscale, add some noise or whatever to make them look like old photos and use that as my train set. 

My question is, can I make a NN with 3x more outputs than inputs (rgb having 3 channels vs greyscale having 1), do any of you have any experience with that, and can someone provide me with guidance as to what types of layers/activation functions etc should be used in this case?  


Thank you all in advance. Big love

Milorad",t2_dgerp,False,,0,False,Help with choice of architecture,[],r/pytorch,False,6,,0,,,False,t3_f38vwi,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1581623760.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,  &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve decided to learn pytorch and ML, and I have completed andrew ng&amp;#39;s course on coursera and after that I read the &lt;a href=""https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf""&gt;book&lt;/a&gt; about pytorch which they have on their website. After that I tried a couple of tutorials for image recognition/classification.   &lt;/p&gt;

&lt;p&gt;Now, I would like to make a nn that can &amp;quot;restore&amp;quot; old images, so i figured I could get a set of RGB images, turn them to greyscale, add some noise or whatever to make them look like old photos and use that as my train set. &lt;/p&gt;

&lt;p&gt;My question is, can I make a NN with 3x more outputs than inputs (rgb having 3 channels vs greyscale having 1), do any of you have any experience with that, and can someone provide me with guidance as to what types of layers/activation functions etc should be used in this case?  &lt;/p&gt;

&lt;p&gt;Thank you all in advance. Big love&lt;/p&gt;

&lt;p&gt;Milorad&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f38vwi,True,,Glog97,,1,True,all_ads,False,[],False,,/r/pytorch/comments/f38vwi/help_with_choice_of_architecture/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f38vwi/help_with_choice_of_architecture/,7135,1581594960.0,0,,False,,,,,,,,
675,,pytorch,,t2_nbro8od,False,,0,False,Managing Pipeline Metadata,[],r/pytorch,False,6,,0,70.0,,False,t3_f2usrw,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/3zKn-vEXfeftttC52jeyQEH88FlywpnXtzHSO5BrouA.jpg,False,,[],{},link,,False,,1581557673.0,text,6,,,text,arangodb.com,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/zPS6Lrwu3kq6JkYM-Ub--eWzV3947iFMgaAfhjXf4ZU.jpg?auto=webp&amp;s=b3bee1e7e9c23da0044b913799f7706d1cf324b6', 'width': 800, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/zPS6Lrwu3kq6JkYM-Ub--eWzV3947iFMgaAfhjXf4ZU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=147d5620201dd0e44e0c0aa638bfab1831e01b6d', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/zPS6Lrwu3kq6JkYM-Ub--eWzV3947iFMgaAfhjXf4ZU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=10eb6f4222be20de8356fe0116d017f328d0cb53', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/zPS6Lrwu3kq6JkYM-Ub--eWzV3947iFMgaAfhjXf4ZU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b4358c9b40f5f7516530392da544d906720ff14', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/zPS6Lrwu3kq6JkYM-Ub--eWzV3947iFMgaAfhjXf4ZU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f3551e20508806dd7386961c8671c2e6fe7e641', 'width': 640, 'height': 320}], 'variants': {}, 'id': '4bAYJOE7cb_DS0tKJepoypHGIzkjnE0XluJFVeXikaE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f2usrw,True,,grmpf101,,0,True,all_ads,False,[],False,,/r/pytorch/comments/f2usrw/managing_pipeline_metadata/,all_ads,False,https://www.arangodb.com/arangodb-events/arangoml-pipeline-cloud/,7135,1581528873.0,0,,False,https://www.arangodb.com/arangodb-events/arangoml-pipeline-cloud/,,,,,,,
676,,pytorch,"So I am completely new to Pytorch/ machine learning. I have  downloaded a pretrained model based on GPT-2, and the folder contains  this files:

model.pt

optim.pt

params.json

sp.model

My aim is to use this pretrained model with my own data. First to see  how it performs, and then to finetune the model.  I have absolutely 0 idea where to start with this. It would be really helpful if someone could give me pointers. I am not  looking for a code, but more of a general approach on how someone would  use go to use this pretrained model. 

Thank you!",t2_57ryg4nt,False,,0,False,Where to start with a pretrained Pytorch model?,[],r/pytorch,False,6,,0,,,False,t3_f2rzyu,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1581546516.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I am completely new to Pytorch/ machine learning. I have  downloaded a pretrained model based on GPT-2, and the folder contains  this files:&lt;/p&gt;

&lt;p&gt;model.pt&lt;/p&gt;

&lt;p&gt;optim.pt&lt;/p&gt;

&lt;p&gt;params.json&lt;/p&gt;

&lt;p&gt;sp.model&lt;/p&gt;

&lt;p&gt;My aim is to use this pretrained model with my own data. First to see  how it performs, and then to finetune the model.  I have absolutely 0 idea where to start with this. It would be really helpful if someone could give me pointers. I am not  looking for a code, but more of a general approach on how someone would  use go to use this pretrained model. &lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f2rzyu,True,,temp333346,,1,True,all_ads,False,[],False,,/r/pytorch/comments/f2rzyu/where_to_start_with_a_pretrained_pytorch_model/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f2rzyu/where_to_start_with_a_pretrained_pytorch_model/,7135,1581517716.0,0,,False,,,,,,,,
677,,pytorch,,t2_5lxugag6,False,,0,False,Tutorial on Training Generative Adverserial Networks (GANs) from Scratch in PyTorch,[],r/pytorch,False,6,,0,140.0,,False,t3_f26cg5,False,dark,0.84,,public,11,0,{},140.0,,False,[],,False,False,,{},,False,11,,False,https://a.thumbs.redditmedia.com/doKv1lUZ_QvX5o54UBY98GmYCflQe4ytb60KV15mNy0.jpg,False,,[],{},link,,False,,1581444298.0,text,6,,,text,medium.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/aq20DCquxp-0UBoK98bTnlsLgZgq2bdw6qRHmcZYA2s.jpg?auto=webp&amp;s=c26254dcadb9fef228655562e819408fd6e2d990', 'width': 302, 'height': 302}, 'resolutions': [{'url': 'https://external-preview.redd.it/aq20DCquxp-0UBoK98bTnlsLgZgq2bdw6qRHmcZYA2s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=05933466375533afdd9b8c70f7b9bb5716dd34f8', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/aq20DCquxp-0UBoK98bTnlsLgZgq2bdw6qRHmcZYA2s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec7af224456249f144fbd0969073b3111f0f87b0', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'EdG4-iA9zATCRETXISu39jA9cP67Flg3_DC3CS3kmVA'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f26cg5,True,,jovianml,,0,True,all_ads,False,[],False,,/r/pytorch/comments/f26cg5/tutorial_on_training_generative_adverserial/,all_ads,False,https://medium.com/jovianml/generative-adverserial-networks-gans-from-scratch-in-pytorch-ad48256458a7,7135,1581415498.0,0,,False,https://medium.com/jovianml/generative-adverserial-networks-gans-from-scratch-in-pytorch-ad48256458a7,,,,,,,
678,,pytorch,I have a CSV file 'data.csv' . It has 20k samples and 26 columns out of which 20 input columns and 6 output columns. How do I proceed to make a Data loader from this? Sorry if it's too basic.,,False,,0,False,How to create a data loader from CSV file,[],r/pytorch,False,6,,0,,,False,t3_f2c39s,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,,self,False,,,{},,,True,,1581472730.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a CSV file &amp;#39;data.csv&amp;#39; . It has 20k samples and 26 columns out of which 20 input columns and 6 output columns. How do I proceed to make a Data loader from this? Sorry if it&amp;#39;s too basic.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f2c39s,True,,[deleted],,9,True,all_ads,False,[],,dark,/r/pytorch/comments/f2c39s/how_to_create_a_data_loader_from_csv_file/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f2c39s/how_to_create_a_data_loader_from_csv_file/,7135,1581443930.0,0,,False,,,,,,,,
679,,pytorch,,t2_7xdcbi,False,,0,False,Multiplying different part of tensor is counted as in place operation? Is this a bug or am I dumb?,[],r/pytorch,False,6,,0,61.0,,False,t3_f1yei8,False,dark,0.86,,public,5,0,{},140.0,,False,[],,True,False,,{},,False,5,,False,https://a.thumbs.redditmedia.com/PVFJLR2Cn8zJGKQrwioSER6yflLpvR-tXgiI04hGX_0.jpg,False,,[],{},image,,False,,1581403316.0,text,6,,,text,i.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/i8qtmb3fc6g41.jpg?auto=webp&amp;s=06e270ea5eeb456d9de806f488d82f0924751f18', 'width': 1096, 'height': 482}, 'resolutions': [{'url': 'https://preview.redd.it/i8qtmb3fc6g41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f7403631244aaf97865381776017d3c1d1b87de9', 'width': 108, 'height': 47}, {'url': 'https://preview.redd.it/i8qtmb3fc6g41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4507795b307636690f5ad9c0bbda2a13607b68df', 'width': 216, 'height': 94}, {'url': 'https://preview.redd.it/i8qtmb3fc6g41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=487c8fd62afbabda653d15e5c7b65eb481776df2', 'width': 320, 'height': 140}, {'url': 'https://preview.redd.it/i8qtmb3fc6g41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e3d2e4e03345dd5dac6631d50bb541e49bb2b8b0', 'width': 640, 'height': 281}, {'url': 'https://preview.redd.it/i8qtmb3fc6g41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c887ad8ad898d05a6d6b81048755e58f9d15346', 'width': 960, 'height': 422}, {'url': 'https://preview.redd.it/i8qtmb3fc6g41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=001f74f648278c1589d8230daace418c4c2c6e96', 'width': 1080, 'height': 474}], 'variants': {}, 'id': 'zc60IeKl0CFRxQqey7C6_dLFgnSVVJGCnNRzo5G1Mew'}], 'enabled': True}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f1yei8,True,,17daysatdennys,,16,True,all_ads,False,[],False,,/r/pytorch/comments/f1yei8/multiplying_different_part_of_tensor_is_counted/,all_ads,False,https://i.redd.it/i8qtmb3fc6g41.jpg,7135,1581374516.0,0,,False,https://i.redd.it/i8qtmb3fc6g41.jpg,,,,,,,
680,,pytorch,,,False,,0,False,"My loss is not improving , any help would be appreciated",[],r/pytorch,False,6,,0,78.0,,False,t3_f1n1cx,False,dark,1.0,,public,7,0,{},140.0,,False,[],,True,False,,{},,False,7,,,https://b.thumbs.redditmedia.com/4M2EU5WrccRmzJEBTfL6Pqf2gUlkdbg7thkzBF9jnbE.jpg,False,,,{},image,,False,,1581349064.0,text,6,,,,i.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/wb6yyykhv1g41.png?auto=webp&amp;s=010f70387ae396d7e1a4c928c6662dfcc9c58d1c', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://preview.redd.it/wb6yyykhv1g41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec9c7ed59e9a0f7fe4155c9c4ab9f153528a0ae0', 'width': 108, 'height': 60}, {'url': 'https://preview.redd.it/wb6yyykhv1g41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c0ebd0465b42805c18e232acae3c3c13ca6151cf', 'width': 216, 'height': 121}, {'url': 'https://preview.redd.it/wb6yyykhv1g41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c732432e243049e5f428d3b726ceed66d1d9f85b', 'width': 320, 'height': 180}, {'url': 'https://preview.redd.it/wb6yyykhv1g41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3cf997de83b0b086c119a22e8d8b3bdd8711840b', 'width': 640, 'height': 360}, {'url': 'https://preview.redd.it/wb6yyykhv1g41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e3d2a2c15b1b2989564f7f1d34720cc0415231f3', 'width': 960, 'height': 540}, {'url': 'https://preview.redd.it/wb6yyykhv1g41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1468e1bb641c6e1d1cf80a7d0f705c61e23a0892', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'Tm1b0k8_qIGdSTlGPLfBa-OtwywgnWyQkgdCpTzdsH4'}], 'enabled': True}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f1n1cx,True,,[deleted],,11,True,all_ads,False,[],,dark,/r/pytorch/comments/f1n1cx/my_loss_is_not_improving_any_help_would_be/,all_ads,False,https://i.redd.it/wb6yyykhv1g41.png,7135,1581320264.0,0,,False,https://i.redd.it/wb6yyykhv1g41.png,,,,,,,
681,,pytorch,"I posted this question on the pytroch form but had little luck, I thought I might try my chances here. If it doesn't fit, please let me know.

\----------------

Assume I think a reasonable shape for my output is n m x mmatrices, maybe I believe my loss would be best modeled in terms of  these matrices (e.g. I’d prefer to predict n/2 of them correctly, rather  than predict every 2nd element correctly but have 0 overall correct  matrices).

I would just have a linear layer of size: n \* m \* m as the output  from my network, and then construct a custom loss function I apply to  each m\*m chunk of my output, at least that was my first though.

However, I assume there might be a better way to approach such a  problem. So I’m curios if anyone has any ideas of experience doing this ?

If anyone knows how to do this for a more general case (e.g.  something that would apply to a list of arrays of arbitrary dimensions,  rather than to a list of 2-dimensional arrays as per my example) I’d be  even more curios to know that.

Are there any built-in loss functions that could be used for these  type of problem or would I be better off constructing one from scratch ?

Original post: [https://discuss.pytorch.org/t/way-of-modeling-multi-dimensional-output/69078](https://discuss.pytorch.org/t/way-of-modeling-multi-dimensional-output/69078)",t2_rgonfuk,False,,0,False,Way of modeling multi-dimensional output?,[],r/pytorch,False,6,,0,,,False,t3_f1dot1,False,dark,0.66,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1581305596.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I posted this question on the pytroch form but had little luck, I thought I might try my chances here. If it doesn&amp;#39;t fit, please let me know.&lt;/p&gt;

&lt;p&gt;----------------&lt;/p&gt;

&lt;p&gt;Assume I think a reasonable shape for my output is n m x mmatrices, maybe I believe my loss would be best modeled in terms of  these matrices (e.g. I’d prefer to predict n/2 of them correctly, rather  than predict every 2nd element correctly but have 0 overall correct  matrices).&lt;/p&gt;

&lt;p&gt;I would just have a linear layer of size: n * m * m as the output  from my network, and then construct a custom loss function I apply to  each m*m chunk of my output, at least that was my first though.&lt;/p&gt;

&lt;p&gt;However, I assume there might be a better way to approach such a  problem. So I’m curios if anyone has any ideas of experience doing this ?&lt;/p&gt;

&lt;p&gt;If anyone knows how to do this for a more general case (e.g.  something that would apply to a list of arrays of arbitrary dimensions,  rather than to a list of 2-dimensional arrays as per my example) I’d be  even more curios to know that.&lt;/p&gt;

&lt;p&gt;Are there any built-in loss functions that could be used for these  type of problem or would I be better off constructing one from scratch ?&lt;/p&gt;

&lt;p&gt;Original post: &lt;a href=""https://discuss.pytorch.org/t/way-of-modeling-multi-dimensional-output/69078""&gt;https://discuss.pytorch.org/t/way-of-modeling-multi-dimensional-output/69078&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f1dot1,True,,elcric_krej,,0,True,all_ads,False,[],False,,/r/pytorch/comments/f1dot1/way_of_modeling_multidimensional_output/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f1dot1/way_of_modeling_multidimensional_output/,7135,1581276796.0,0,,False,,,,,,,,
682,,pytorch,"I wrote a custom dataset using `torch.utils.data.Dataset` with size (20399, 5)

Why am I getting an index error when the program tries to get the 20399th index?

There are 20399 observations in the dataset, but the indexing starts at 0 so the last index should be 20398. Why does it even try to get the 20399th index?

Code for my dataset:

    from torch.utils.data import Dataset
    class MyDataset(Dataset):
        def __init__(self, root):
            self.df = pd.read_csv(root)
            self.data = self.df.to_numpy()
        def __getitem__(self, idx):
            return self.data[idx, :]
        def __len__(self):
            return len(self.df.index)

I'd really appreciate any help with this!",t2_ebu4m,False,,0,False,Question about Datasets,[],r/pytorch,False,6,,0,,,False,t3_f0krpt,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1581155647.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I wrote a custom dataset using &lt;code&gt;torch.utils.data.Dataset&lt;/code&gt; with size (20399, 5)&lt;/p&gt;

&lt;p&gt;Why am I getting an index error when the program tries to get the 20399th index?&lt;/p&gt;

&lt;p&gt;There are 20399 observations in the dataset, but the indexing starts at 0 so the last index should be 20398. Why does it even try to get the 20399th index?&lt;/p&gt;

&lt;p&gt;Code for my dataset:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from torch.utils.data import Dataset
class MyDataset(Dataset):
    def __init__(self, root):
        self.df = pd.read_csv(root)
        self.data = self.df.to_numpy()
    def __getitem__(self, idx):
        return self.data[idx, :]
    def __len__(self):
        return len(self.df.index)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;#39;d really appreciate any help with this!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f0krpt,True,,Pepipasta,,8,True,all_ads,False,[],False,,/r/pytorch/comments/f0krpt/question_about_datasets/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f0krpt/question_about_datasets/,7135,1581126847.0,0,,False,,,,,,,,
683,,pytorch,,t2_12i1bz,False,,0,False,Introducing PyTorch3D: An open-source library for 3D deep learning (Facebook),[],r/pytorch,False,6,,0,84.0,,False,t3_f00j7v,False,dark,1.0,,public,25,0,{},140.0,,False,[],,False,False,,{},,False,25,,False,https://b.thumbs.redditmedia.com/APpxSejt792GQuT286Wi6T7x2Ca6leDg2g5bznUajpk.jpg,False,,[],{},link,,False,,1581057565.0,text,6,,,text,ai.facebook.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/191qAtyKkETxkVsp79XrXd9T_khMRQ6kAoseM2dm8sU.jpg?auto=webp&amp;s=e7eb74444d3de77026394995bf5c07addd87d655', 'width': 1848, 'height': 1112}, 'resolutions': [{'url': 'https://external-preview.redd.it/191qAtyKkETxkVsp79XrXd9T_khMRQ6kAoseM2dm8sU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f7301bb7167ab0a17bb2638a200870aa2314c348', 'width': 108, 'height': 64}, {'url': 'https://external-preview.redd.it/191qAtyKkETxkVsp79XrXd9T_khMRQ6kAoseM2dm8sU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3bddfa86a5e43842aa368fb4d19b0c014538eba1', 'width': 216, 'height': 129}, {'url': 'https://external-preview.redd.it/191qAtyKkETxkVsp79XrXd9T_khMRQ6kAoseM2dm8sU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cb3328bd8eace62a7f075a390eb97275a281ab97', 'width': 320, 'height': 192}, {'url': 'https://external-preview.redd.it/191qAtyKkETxkVsp79XrXd9T_khMRQ6kAoseM2dm8sU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ccd7a8c58c51689938de2195e4b52f854bacae67', 'width': 640, 'height': 385}, {'url': 'https://external-preview.redd.it/191qAtyKkETxkVsp79XrXd9T_khMRQ6kAoseM2dm8sU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=09649ccb052290b98a9835a77b6b18e01ad4dcfa', 'width': 960, 'height': 577}, {'url': 'https://external-preview.redd.it/191qAtyKkETxkVsp79XrXd9T_khMRQ6kAoseM2dm8sU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6cebccf319bfe7b928cc15c36d00a39da32b4f7f', 'width': 1080, 'height': 649}], 'variants': {}, 'id': 'rSEdi0vXushaHR85zFEb8UwzbjBLaBL4SimNJnuTu04'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f00j7v,True,,101testing,,1,True,all_ads,False,[],False,,/r/pytorch/comments/f00j7v/introducing_pytorch3d_an_opensource_library_for/,all_ads,False,https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/,7135,1581028765.0,0,,False,https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/,,,,,,,
684,,pytorch,"Hello. I am experimenting with neural networks and music using dataset of mp3 files. Currently during training I load each file as numpy array using librosa, then I convert it to tensor and do some preprocessing. However, I noticed this whole process is very slow, so I want to do the loading and processing just once, save the resulting tensors and then load just those tensors during training. 

1. Is this approach reasonable at all? Are there any fancy ways of loading and saving pytorch tensors?
2. My tensors currently have shape around (2, 1.300.000). Would saving them as batches and loading whole batches at once save any considerable time?
3. Is using torch.Tensors for processing inefficient? I tried to use them so that I could eventually move processing to cuda.",t2_1323nw2p,False,,0,False,Saving and loading preprocessed tensors,[],r/pytorch,False,6,,0,,,False,t3_f0agrg,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1581110849.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello. I am experimenting with neural networks and music using dataset of mp3 files. Currently during training I load each file as numpy array using librosa, then I convert it to tensor and do some preprocessing. However, I noticed this whole process is very slow, so I want to do the loading and processing just once, save the resulting tensors and then load just those tensors during training. &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Is this approach reasonable at all? Are there any fancy ways of loading and saving pytorch tensors?&lt;/li&gt;
&lt;li&gt;My tensors currently have shape around (2, 1.300.000). Would saving them as batches and loading whole batches at once save any considerable time?&lt;/li&gt;
&lt;li&gt;Is using torch.Tensors for processing inefficient? I tried to use them so that I could eventually move processing to cuda.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,f0agrg,True,,fszatkowski,,3,True,all_ads,False,[],False,,/r/pytorch/comments/f0agrg/saving_and_loading_preprocessed_tensors/,all_ads,False,https://www.reddit.com/r/pytorch/comments/f0agrg/saving_and_loading_preprocessed_tensors/,7135,1581082049.0,0,,False,,,,,,,,
685,,pytorch,"I have recently built a machine to accelerate training with 4x RTX 2080 Ti cards and 2 NvLinks. Because NvLink for RTX can only pair 2 cards so actually they work as 2 pairs.

When using PyTorch DataParallel, it seems only one NvLink is used since there's no data flowing between slave cards. Is there any way I can use the other NvLink as well to boost the speed?",t2_fdsed2f,False,,0,False,Make the best of 4x RTX 2080Ti with PyTorch,[],r/pytorch,False,6,,0,,,False,t3_ezp92o,False,dark,0.89,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1581005560.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have recently built a machine to accelerate training with 4x RTX 2080 Ti cards and 2 NvLinks. Because NvLink for RTX can only pair 2 cards so actually they work as 2 pairs.&lt;/p&gt;

&lt;p&gt;When using PyTorch DataParallel, it seems only one NvLink is used since there&amp;#39;s no data flowing between slave cards. Is there any way I can use the other NvLink as well to boost the speed?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ezp92o,True,,Oystercreepglen,,4,True,all_ads,False,[],False,,/r/pytorch/comments/ezp92o/make_the_best_of_4x_rtx_2080ti_with_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ezp92o/make_the_best_of_4x_rtx_2080ti_with_pytorch/,7135,1580976760.0,0,,False,,,,,,,,
686,,pytorch,,t2_56buto1b,False,,0,False,Reinforcement learning using Transformer seq2seq model,[],r/pytorch,False,6,,0,,,False,t3_ey4oga,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,default,False,,[],{},,,False,,1580751316.0,text,6,,,text,self.reinforcementlearning,False,,,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ey4oga,True,,andelie97,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ey4oga/reinforcement_learning_using_transformer_seq2seq/,all_ads,False,/r/reinforcementlearning/comments/ey3ifr/reinforcement_learning_using_transformer_seq2seq/,7135,1580722516.0,0,,False,/r/reinforcementlearning/comments/ey3ifr/reinforcement_learning_using_transformer_seq2seq/,"[{'approved_at_utc': None, 'subreddit': 'reinforcementlearning', 'selftext': ""Hello everyone,\n\nI am trying to apply a policy gradient algorithm to a sequence to sequence transformer model for abstractive text summarization, in Pytorch.\n\nSo far I have been using RNN sequence to sequence models as examples, and the way they do this is by getting a baseline {greedy} summary and a sampled summary using the Categorical class in Pytorch {with probabilities of those samples} and then use the rouge metric as a reward.\n\nAnyway, my problem appears when I attempt to get a sampled output from my Transformer model. When using an RNN model, tokens are sampled one by one during inference \\[there is no gold label\\], which works because RNN's output one token at a time. However, Transformers output the entire summary at one step, even though there still is an inference loop.\n\nMy sampling function so far looks like:\n\n    def get_distribution(model, batch):\n    \n        src, (shift_tgt, lbl_tgt), segs, clss, mask_src, mask_tgt, mask_cls = batch\n    \n        # the mock tgt are just torch.zeros tensors that have the same shape as the tgt\n        mock_tgt = get_mock_tgt(shift_tgt)\n        mock_return = get_mock_tgt(shift_tgt)\n    \n        max_length = shift_tgt.shape[1]\n        \n        log_probs = []\n        \n        for i in range(0, max_length-1):\n            prediction = model(src, mock_tgt, segs, clss, mask_src, mask_tgt, mask_cls)\n            prediction = F.softmax(prediction, dim=2)\n            \n            multi_dist = Categorical(prediction[:, i])\n            x_t = multi_dist.sample()\n            mock_tgt[:, i+1] = x_t\n            mock_return[:, i] = x_t\n            log_prob = multi_dist.log_prob(x_t)\n            log_probs.append(log_prob)\n            \n        return mock_return, log_probs\n\nAt each timestep during inference I create a distribution using `multi_dist = Categorical(prediction[:, i])` which I believe might not be exactly correct.\n\nWhat is your opinion ? Gradient computes successfully but training sessions don't bring much of an improvement. Is there a better way to sample from the Transformer's output?"", 'author_fullname': 't2_56buto1b', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Reinforcement learning using Transformer seq2seq model', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/reinforcementlearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_ey3ifr', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 15, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1580744147.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.reinforcementlearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I am trying to apply a policy gradient algorithm to a sequence to sequence transformer model for abstractive text summarization, in Pytorch.&lt;/p&gt;\n\n&lt;p&gt;So far I have been using RNN sequence to sequence models as examples, and the way they do this is by getting a baseline {greedy} summary and a sampled summary using the Categorical class in Pytorch {with probabilities of those samples} and then use the rouge metric as a reward.&lt;/p&gt;\n\n&lt;p&gt;Anyway, my problem appears when I attempt to get a sampled output from my Transformer model. When using an RNN model, tokens are sampled one by one during inference [there is no gold label], which works because RNN&amp;#39;s output one token at a time. However, Transformers output the entire summary at one step, even though there still is an inference loop.&lt;/p&gt;\n\n&lt;p&gt;My sampling function so far looks like:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def get_distribution(model, batch):\n\n    src, (shift_tgt, lbl_tgt), segs, clss, mask_src, mask_tgt, mask_cls = batch\n\n    # the mock tgt are just torch.zeros tensors that have the same shape as the tgt\n    mock_tgt = get_mock_tgt(shift_tgt)\n    mock_return = get_mock_tgt(shift_tgt)\n\n    max_length = shift_tgt.shape[1]\n\n    log_probs = []\n\n    for i in range(0, max_length-1):\n        prediction = model(src, mock_tgt, segs, clss, mask_src, mask_tgt, mask_cls)\n        prediction = F.softmax(prediction, dim=2)\n\n        multi_dist = Categorical(prediction[:, i])\n        x_t = multi_dist.sample()\n        mock_tgt[:, i+1] = x_t\n        mock_return[:, i] = x_t\n        log_prob = multi_dist.log_prob(x_t)\n        log_probs.append(log_prob)\n\n    return mock_return, log_probs\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;At each timestep during inference I create a distribution using &lt;code&gt;multi_dist = Categorical(prediction[:, i])&lt;/code&gt; which I believe might not be exactly correct.&lt;/p&gt;\n\n&lt;p&gt;What is your opinion ? Gradient computes successfully but training sessions don&amp;#39;t bring much of an improvement. Is there a better way to sample from the Transformer&amp;#39;s output?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2tnvn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ey3ifr', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'andelie97', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/reinforcementlearning/comments/ey3ifr/reinforcement_learning_using_transformer_seq2seq/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/reinforcementlearning/comments/ey3ifr/reinforcement_learning_using_transformer_seq2seq/', 'subreddit_subscribers': 17691, 'created_utc': 1580715347.0, 'num_crossposts': 4, 'media': None, 'is_video': False}]",t3_ey3ifr,,,,,
687,,pytorch,"I have a CSV file with 26 columns. Out of which 20 are inputs and 6 are outputs. I wanna use CNN to make a model. I have 20k samples i.e rows in the CSV file. I'm getting an error saying Conv1D expects a 3d data and what I'm passing is not.
I don't understand what Im doing wrong.",,False,,0,False,How to use Conv1D?,[],r/pytorch,False,6,,0,,,False,t3_ey3tez,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,,self,False,,,{},,,True,,1580745964.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a CSV file with 26 columns. Out of which 20 are inputs and 6 are outputs. I wanna use CNN to make a model. I have 20k samples i.e rows in the CSV file. I&amp;#39;m getting an error saying Conv1D expects a 3d data and what I&amp;#39;m passing is not.
I don&amp;#39;t understand what Im doing wrong.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ey3tez,True,,[deleted],,5,True,all_ads,False,[],,dark,/r/pytorch/comments/ey3tez/how_to_use_conv1d/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ey3tez/how_to_use_conv1d/,7135,1580717164.0,0,,False,,,,,,,,
688,,pytorch,,t2_g3uaa,False,,0,False,"Making Backpropagation, Autograd, MNIST Classifier from scratch in Python",[],r/pytorch,False,6,,0,93.0,,False,t3_exd5f6,False,dark,1.0,,public,7,0,{},140.0,,False,[],,False,False,,{},,False,7,,False,https://b.thumbs.redditmedia.com/iZKovXIV4276CefYlR5NxNXfc-kQMYdmtSSqM6NRPOE.jpg,False,,[],{},link,,False,,1580622290.0,text,6,,,text,learnml.today,True,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ZmjF02Bhhy-IXdvTwG9pwKLKgABEeCOPFyCL_ktAZv0.jpg?auto=webp&amp;s=90b6e4437363342df39180083b1e73e3a61df4d6', 'width': 800, 'height': 533}, 'resolutions': [{'url': 'https://external-preview.redd.it/ZmjF02Bhhy-IXdvTwG9pwKLKgABEeCOPFyCL_ktAZv0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2973ffcd6e248ba733db6f9197b2b866d371de36', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/ZmjF02Bhhy-IXdvTwG9pwKLKgABEeCOPFyCL_ktAZv0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6e8239748194c1ba3e81e2474d500cdf0b57cd2', 'width': 216, 'height': 143}, {'url': 'https://external-preview.redd.it/ZmjF02Bhhy-IXdvTwG9pwKLKgABEeCOPFyCL_ktAZv0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2af75671e7b78716d6776c222e7429bc6e6bc5fb', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/ZmjF02Bhhy-IXdvTwG9pwKLKgABEeCOPFyCL_ktAZv0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6217aad3c9715c678853fe806d5564877451babd', 'width': 640, 'height': 426}], 'variants': {}, 'id': 'k8arjHYOOXpsrkg55TEvwwqiwhpLSXPdVwL8UrOLYj4'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,exd5f6,True,,creotiv,,0,True,all_ads,False,[],False,,/r/pytorch/comments/exd5f6/making_backpropagation_autograd_mnist_classifier/,all_ads,False,https://learnml.today/making-backpropagation-autograd-mnist-classifier-from-scratch-in-Python-5,7135,1580593490.0,0,,False,https://learnml.today/making-backpropagation-autograd-mnist-classifier-from-scratch-in-Python-5,,,,,,,
689,,pytorch,"Hi, can someone help me with this error?

[https://discuss.pytorch.org/t/error-in-simple-text-classification-model/68375](https://discuss.pytorch.org/t/error-in-simple-text-classification-model/68375)",t2_34n5pj9w,False,,0,False,Error in the simple model,[],r/pytorch,False,6,,0,,,False,t3_exfp0v,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1580632685.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, can someone help me with this error?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://discuss.pytorch.org/t/error-in-simple-text-classification-model/68375""&gt;https://discuss.pytorch.org/t/error-in-simple-text-classification-model/68375&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,exfp0v,True,,u314,,0,True,all_ads,False,[],False,,/r/pytorch/comments/exfp0v/error_in_the_simple_model/,all_ads,False,https://www.reddit.com/r/pytorch/comments/exfp0v/error_in_the_simple_model/,7135,1580603885.0,0,,False,,,,,,,,
690,,pytorch,,t2_nmlp,False,,0,False,OpenAI→PyTorch,[],r/pytorch,False,6,,0,78.0,,False,t3_ew8tff,False,dark,0.92,,public,30,0,{},140.0,,False,[],,False,False,,{},,False,30,,False,https://b.thumbs.redditmedia.com/1HdiyOJu1jbnP434bUkVUD_KLZl0dVdnZYE1JvJUHWA.jpg,False,,[],{},link,,False,,1580433602.0,text,6,,,text,openai.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/b8crJfsen9k-87y3e-ZghhMOuHhjmlLFEI1wbAVHoAk.jpg?auto=webp&amp;s=cc8ff7906d8f46e815436f136f06adc9ed67fa6d', 'width': 1280, 'height': 720}, 'resolutions': [{'url': 'https://external-preview.redd.it/b8crJfsen9k-87y3e-ZghhMOuHhjmlLFEI1wbAVHoAk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=67fc6cb9d8566968a15da906c6a55f99d0b021c5', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/b8crJfsen9k-87y3e-ZghhMOuHhjmlLFEI1wbAVHoAk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=757eb0bc9910444403d1ebb87e4ed7af81b95db4', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/b8crJfsen9k-87y3e-ZghhMOuHhjmlLFEI1wbAVHoAk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b0a5b36bfe149ba665dcc7957ae3f9dfc565da04', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/b8crJfsen9k-87y3e-ZghhMOuHhjmlLFEI1wbAVHoAk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=26d188f19d7481cab929be6833245455f09f91fb', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/b8crJfsen9k-87y3e-ZghhMOuHhjmlLFEI1wbAVHoAk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=62f6f9a2fb3f46317aa31af92bfc9ee67937ca66', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/b8crJfsen9k-87y3e-ZghhMOuHhjmlLFEI1wbAVHoAk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b1acc02cccfa7b06d39866b056427c9e418314ac', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'vNGJfvHypuM04J6-BcRBiDktKjxhKiexz7eOm6Q0l34'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ew8tff,True,,gwern,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ew8tff/openaipytorch/,all_ads,False,https://openai.com/blog/openai-pytorch/,7135,1580404802.0,0,,False,https://openai.com/blog/openai-pytorch/,,,,,,,
691,,pytorch,"Hey PyTorch community,

Hopefully this will be of interest to some people. I am building an online community (virtual institute) dedicated to ML/DL/AI in Life Sciences. The aim is to share code, expertise, datasets. Hop on and join our discussions!

[https://ails.discourse.group/](https://ails.discourse.group/)

[https://ails.institute/](https://ails.institute/)",t2_1kqut8u4,False,,0,False,A new community: Artificial Intelligence for Life Sciences,[],r/pytorch,False,6,,0,,,False,t3_evuxhr,False,dark,1.0,,public,9,0,{},,,False,[],,False,False,,{},,False,9,,False,self,False,,[],{},self,,True,,1580367195.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey PyTorch community,&lt;/p&gt;

&lt;p&gt;Hopefully this will be of interest to some people. I am building an online community (virtual institute) dedicated to ML/DL/AI in Life Sciences. The aim is to share code, expertise, datasets. Hop on and join our discussions!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://ails.discourse.group/""&gt;https://ails.discourse.group/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://ails.institute/""&gt;https://ails.institute/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Dy2MRk5qL0hR0LD-j_TJQfQ_DlkWaPxIXFPSYCdgkH4.jpg?auto=webp&amp;s=27602dfdc289b78de3285284f96de1d15c7acc51', 'width': 640, 'height': 640}, 'resolutions': [{'url': 'https://external-preview.redd.it/Dy2MRk5qL0hR0LD-j_TJQfQ_DlkWaPxIXFPSYCdgkH4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=57aed0d9e52fc0542a960eb1f041d3c93c8c3c69', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/Dy2MRk5qL0hR0LD-j_TJQfQ_DlkWaPxIXFPSYCdgkH4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=039ad0128c0f9c3ae7025de05b733ba48bd7c4a1', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/Dy2MRk5qL0hR0LD-j_TJQfQ_DlkWaPxIXFPSYCdgkH4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1793f3923f6155baea561deb564cc79f681d71ab', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/Dy2MRk5qL0hR0LD-j_TJQfQ_DlkWaPxIXFPSYCdgkH4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1ff444f711ad8665af7ce34cb7b6198474c0f677', 'width': 640, 'height': 640}], 'variants': {}, 'id': 'PYR5uMh9_gagMjJcH4luuWQJEVkineWTZaGm6wqoAbU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,evuxhr,True,,ayakimovich,,0,True,all_ads,False,[],False,,/r/pytorch/comments/evuxhr/a_new_community_artificial_intelligence_for_life/,all_ads,False,https://www.reddit.com/r/pytorch/comments/evuxhr/a_new_community_artificial_intelligence_for_life/,7135,1580338395.0,0,,False,,,,,,,,
692,,pytorch,"[https://stackoverflow.com/q/59955290/12652518](https://stackoverflow.com/q/59955290/12652518)

 i appreciate any helps",t2_2yr3b1dy,False,,0,False,"TypeError: stat: path should be string, bytes, os.PathLike or integer, not cv2.VideoCapture",[],r/pytorch,False,6,,0,,,False,t3_evmd3l,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1580330438.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://stackoverflow.com/q/59955290/12652518""&gt;https://stackoverflow.com/q/59955290/12652518&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;i appreciate any helps&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,evmd3l,True,,artiestArt,,0,True,all_ads,False,[],False,,/r/pytorch/comments/evmd3l/typeerror_stat_path_should_be_string_bytes/,all_ads,False,https://www.reddit.com/r/pytorch/comments/evmd3l/typeerror_stat_path_should_be_string_bytes/,7135,1580301638.0,0,,False,,,,,,,,
693,,pytorch,"[https://stackoverflow.com/q/59955290/12652518](https://stackoverflow.com/q/59955290/12652518)

thanks for helping",t2_2yr3b1dy,False,,0,False,"TypeError: stat: path should be string, bytes, os.PathLike or integer, not cv2.VideoCapture",[],r/pytorch,False,6,,0,,,False,t3_evaaor,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1580268936.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://stackoverflow.com/q/59955290/12652518""&gt;https://stackoverflow.com/q/59955290/12652518&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;thanks for helping&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,evaaor,True,,artiestArt,,0,True,all_ads,False,[],False,,/r/pytorch/comments/evaaor/typeerror_stat_path_should_be_string_bytes/,all_ads,False,https://www.reddit.com/r/pytorch/comments/evaaor/typeerror_stat_path_should_be_string_bytes/,7135,1580240136.0,0,,False,,,,,,,,
694,,pytorch,"Hi all,

I am trying to scatter a 2D point cloud i.e a list of 2-D points onto an image.

Given points (B \* 2 \* N ), scatter them onto an image of size (B \* H \* W). While scattering more than one point can fall on the same image pixel, and the value corresponding to those points should be added.

I tried creating N sparse tensors, then scattered points using advanced indexing, summed all sparse tensors and finally converted to\_dense(), but during backward pass, it goes crazily CUDA OOM.

&amp;#x200B;

It would be great if anyone could suggest a way to do so. Thanks !!",t2_1xt5bmz6,False,,0,False,torch.scatter_add() to multiple dimensions,[],r/pytorch,False,6,,0,,,False,t3_ev0ahj,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1580213260.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I am trying to scatter a 2D point cloud i.e a list of 2-D points onto an image.&lt;/p&gt;

&lt;p&gt;Given points (B * 2 * N ), scatter them onto an image of size (B * H * W). While scattering more than one point can fall on the same image pixel, and the value corresponding to those points should be added.&lt;/p&gt;

&lt;p&gt;I tried creating N sparse tensors, then scattered points using advanced indexing, summed all sparse tensors and finally converted to_dense(), but during backward pass, it goes crazily CUDA OOM.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;It would be great if anyone could suggest a way to do so. Thanks !!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ev0ahj,True,,WillingCucumber,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ev0ahj/torchscatter_add_to_multiple_dimensions/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ev0ahj/torchscatter_add_to_multiple_dimensions/,7135,1580184460.0,0,,False,,,,,,,,
695,,pytorch,"i'm trying to build face recognition in real time in neural network(facenet network) using pytorch and face detection using MTCNN  . i've tried this but doesnt work :

    import cv2  
    capture = cv2.VideoCapture(0)  
    while(True):      
        ret, frame = capture.read()     
        frames_tracked = []      
        print('\rTracking frame: {}'.format(i + 1), end='')     
        boxes,_ = mtcnn.detect(frame)     
        frame_draw = frame.copy()     
        draw = ImageDraw.Draw(frame_draw)     
        for box in boxes:
            draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)          
         frames_tracked.append(frame_draw.resize((640, 360), Image.BILINEAR))      
        d = display.display(frames_tracked[0], display_id=True)     
        i = 1     
        try:        
        while True:
            d.update(frames_tracked[i % len(frames_tracked)]) 
            i += 1     
        except KeyboardInterrupt:
            pass  
        if cv2.waitKey('q') == 27:     
             break  
    
    capture.release() 
    cv2.destroyAllWindows()
    
    AttributeError                            Traceback (most recent call last)
    &lt;ipython-input-16-a1b6e44286c1&gt; in &lt;module&gt;
          6     frames_tracked = []
          7     print('\rTracking frame: {}'.format(i + 1), end='')
    ----&gt; 8     boxes,_ = mtcnn.detect(frame)
          9     frame_draw = frame.copy()
         10     draw = ImageDraw.Draw(frame_draw)
    
    c:\users\username\appdata\local\programs\python\python37\lib\site-packages\facenet_pytorch\models\mtcnn.py in detect(self, img, landmarks)
        345                 self.pnet, self.rnet, self.onet,
        346                 self.thresholds, self.factor,
    --&gt; 347                 self.device
        348             )
        349 
    
    c:\users\username\appdata\local\programs\python\python37\lib\site-packages\facenet_pytorch\models\utils\detect_face.py in detect_face(imgs, minsize, pnet, rnet, onet, threshold, factor, device)
          9     if not isinstance(imgs, Iterable):
         10         imgs = [imgs]
    ---&gt; 11     if any(img.size != imgs[0].size for img in imgs):
         12         raise Exception(""MTCNN batch processing only compatible with equal-dimension images."")
         13 
    
    c:\users\username\appdata\local\programs\python\python37\lib\site-packages\facenet_pytorch\models\utils\detect_face.py in &lt;genexpr&gt;(.0)
          9     if not isinstance(imgs, Iterable):
         10         imgs = [imgs]
    ---&gt; 11     if any(img.size != imgs[0].size for img in imgs):
         12         raise Exception(""MTCNN batch processing only compatible with equal-dimension images."")
         13 
    
    AttributeError: 'NoneType' object has no attribute 'size'
    
    thanks for any advice",t2_2yr3b1dy,False,,0,False,face detection in real time with MTCNN ? 'NoneType' object has no attribute 'size',[],r/pytorch,False,6,,0,,,False,t3_eurifp,False,dark,0.99,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1580218366.0,,[],{},,,True,,1580176487.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;i&amp;#39;m trying to build face recognition in real time in neural network(facenet network) using pytorch and face detection using MTCNN  . i&amp;#39;ve tried this but doesnt work :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import cv2  
capture = cv2.VideoCapture(0)  
while(True):      
    ret, frame = capture.read()     
    frames_tracked = []      
    print(&amp;#39;\rTracking frame: {}&amp;#39;.format(i + 1), end=&amp;#39;&amp;#39;)     
    boxes,_ = mtcnn.detect(frame)     
    frame_draw = frame.copy()     
    draw = ImageDraw.Draw(frame_draw)     
    for box in boxes:
        draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)          
     frames_tracked.append(frame_draw.resize((640, 360), Image.BILINEAR))      
    d = display.display(frames_tracked[0], display_id=True)     
    i = 1     
    try:        
    while True:
        d.update(frames_tracked[i % len(frames_tracked)]) 
        i += 1     
    except KeyboardInterrupt:
        pass  
    if cv2.waitKey(&amp;#39;q&amp;#39;) == 27:     
         break  

capture.release() 
cv2.destroyAllWindows()

AttributeError                            Traceback (most recent call last)
&amp;lt;ipython-input-16-a1b6e44286c1&amp;gt; in &amp;lt;module&amp;gt;
      6     frames_tracked = []
      7     print(&amp;#39;\rTracking frame: {}&amp;#39;.format(i + 1), end=&amp;#39;&amp;#39;)
----&amp;gt; 8     boxes,_ = mtcnn.detect(frame)
      9     frame_draw = frame.copy()
     10     draw = ImageDraw.Draw(frame_draw)

c:\users\username\appdata\local\programs\python\python37\lib\site-packages\facenet_pytorch\models\mtcnn.py in detect(self, img, landmarks)
    345                 self.pnet, self.rnet, self.onet,
    346                 self.thresholds, self.factor,
--&amp;gt; 347                 self.device
    348             )
    349 

c:\users\username\appdata\local\programs\python\python37\lib\site-packages\facenet_pytorch\models\utils\detect_face.py in detect_face(imgs, minsize, pnet, rnet, onet, threshold, factor, device)
      9     if not isinstance(imgs, Iterable):
     10         imgs = [imgs]
---&amp;gt; 11     if any(img.size != imgs[0].size for img in imgs):
     12         raise Exception(&amp;quot;MTCNN batch processing only compatible with equal-dimension images.&amp;quot;)
     13 

c:\users\username\appdata\local\programs\python\python37\lib\site-packages\facenet_pytorch\models\utils\detect_face.py in &amp;lt;genexpr&amp;gt;(.0)
      9     if not isinstance(imgs, Iterable):
     10         imgs = [imgs]
---&amp;gt; 11     if any(img.size != imgs[0].size for img in imgs):
     12         raise Exception(&amp;quot;MTCNN batch processing only compatible with equal-dimension images.&amp;quot;)
     13 

AttributeError: &amp;#39;NoneType&amp;#39; object has no attribute &amp;#39;size&amp;#39;

thanks for any advice
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,eurifp,True,,artiestArt,,5,True,all_ads,False,[],False,,/r/pytorch/comments/eurifp/face_detection_in_real_time_with_mtcnn_nonetype/,all_ads,False,https://www.reddit.com/r/pytorch/comments/eurifp/face_detection_in_real_time_with_mtcnn_nonetype/,7135,1580147687.0,0,,False,,,,,,,,
696,,pytorch,,t2_377y4afa,False,,0,False,Would be grateful to anyone who could help with this! I’m still very new to pytorch 😅,[],r/pytorch,False,6,,0,140.0,,False,t3_eucmg1,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://a.thumbs.redditmedia.com/Om-qq8HQsbttj-ontvm5Ani9zigMiuHv2mOWzloaLQ4.jpg,False,,[],{},link,,False,,1580100381.0,text,6,,,text,discuss.pytorch.org,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,eucmg1,True,,giraffe_sloot,,8,True,all_ads,False,[],False,,/r/pytorch/comments/eucmg1/would_be_grateful_to_anyone_who_could_help_with/,all_ads,False,https://discuss.pytorch.org/t/intel-mkl-fatal-error-cannot-load-libmkl-core-dylib/67710,7135,1580071581.0,0,,False,https://discuss.pytorch.org/t/intel-mkl-fatal-error-cannot-load-libmkl-core-dylib/67710,,,,,,,
697,,pytorch,"Hello all,

I have been trying to package libtorch as debian, in an effort to make it easier to pull into my continuous integration process. However I have been having trouble getting libtorch to build when trying to enable features like Intel's MKL.

Has anyone been able to pull this off successfully?",t2_d3ne6,False,,0,False,Has anyone packaged libtorch as a debian?,[],r/pytorch,False,6,,0,,,False,t3_eudqpk,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1580104862.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all,&lt;/p&gt;

&lt;p&gt;I have been trying to package libtorch as debian, in an effort to make it easier to pull into my continuous integration process. However I have been having trouble getting libtorch to build when trying to enable features like Intel&amp;#39;s MKL.&lt;/p&gt;

&lt;p&gt;Has anyone been able to pull this off successfully?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,eudqpk,True,,spicy_indian,,0,True,all_ads,False,[],False,,/r/pytorch/comments/eudqpk/has_anyone_packaged_libtorch_as_a_debian/,all_ads,False,https://www.reddit.com/r/pytorch/comments/eudqpk/has_anyone_packaged_libtorch_as_a_debian/,7135,1580076062.0,0,,False,,,,,,,,
698,,pytorch,,t2_mjlci,False,,0,False,"Residual Networks (ResNets), Data Augmentation and Regularization in PyTorch",[],r/pytorch,False,6,,0,105.0,,False,t3_esqwfo,False,dark,0.89,,public,7,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/_frF97Ioz7c?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Residual Networks, Data Augmentation and Regularization | Deep Learning with PyTorch | Part 5 of 6', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/_frF97Ioz7c?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Jovian', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/_frF97Ioz7c/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCmKaoNn0OvxVAe7f_8sXYNQ'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/_frF97Ioz7c?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/esqwfo', 'height': 200}",,False,7,,False,https://b.thumbs.redditmedia.com/R9sENhxAmqD259Flk7Dms_zkhhtaqLu5Z-Wg7HcSLbw.jpg,False,,[],{},rich:video,,False,,1579801827.0,text,6,,,text,youtube.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/KfYw45027vTnQXXTvCeTAo0l5se0vjkXNmkN8KrSeKk.jpg?auto=webp&amp;s=c85322c51c32c2855a35c5a41aa62a571cc13bcc', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/KfYw45027vTnQXXTvCeTAo0l5se0vjkXNmkN8KrSeKk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=69aa37e95ba2e38a7f37df2d7b606e97ed8a1e2a', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/KfYw45027vTnQXXTvCeTAo0l5se0vjkXNmkN8KrSeKk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7792dbe622ba4cfdb8c694694a0d1e96626ff785', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/KfYw45027vTnQXXTvCeTAo0l5se0vjkXNmkN8KrSeKk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a1dc0faa67f6c0c217ac4baa0a14acd037046be', 'width': 320, 'height': 240}], 'variants': {}, 'id': '-EvR5m7e3e74newAQB0qQFe074AXYrz_DPwDeVh45qk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,esqwfo,True,,aakashns,,0,True,all_ads,False,[],False,,/r/pytorch/comments/esqwfo/residual_networks_resnets_data_augmentation_and/,all_ads,False,https://www.youtube.com/watch?v=_frF97Ioz7c,7135,1579773027.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Residual Networks, Data Augmentation and Regularization | Deep Learning with PyTorch | Part 5 of 6', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/_frF97Ioz7c?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Jovian', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/_frF97Ioz7c/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCmKaoNn0OvxVAe7f_8sXYNQ'}}",False,https://www.youtube.com/watch?v=_frF97Ioz7c,,,,,,,
699,,pytorch,"I am trying to train a model from AllenNLP and as soon as it starts training the console starts printing dozens of UserWarning messages `/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.`

I have changed all numpy arrays and torch tensors from uint8 to bool, but I still get this warning.

I am using:

python: 3.6.7

torch: 1.2.0

allennlp: 0.9.0

numpy: 1.18.1",t2_53oo1k06,False,,0,False,"What causes the UserWarning ""/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead."" ?",[],r/pytorch,False,6,,0,,,False,t3_estjtn,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1579817123.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to train a model from AllenNLP and as soon as it starts training the console starts printing dozens of UserWarning messages &lt;code&gt;/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I have changed all numpy arrays and torch tensors from uint8 to bool, but I still get this warning.&lt;/p&gt;

&lt;p&gt;I am using:&lt;/p&gt;

&lt;p&gt;python: 3.6.7&lt;/p&gt;

&lt;p&gt;torch: 1.2.0&lt;/p&gt;

&lt;p&gt;allennlp: 0.9.0&lt;/p&gt;

&lt;p&gt;numpy: 1.18.1&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,estjtn,True,,liv-rhea,,2,True,all_ads,False,[],False,,/r/pytorch/comments/estjtn/what_causes_the_userwarning/,all_ads,False,https://www.reddit.com/r/pytorch/comments/estjtn/what_causes_the_userwarning/,7135,1579788323.0,0,,False,,,,,,,,
700,,pytorch,"Hi, I am looking for a set of image processing tools (Canny edge, Watershed, Hough Transform type things)  that is not deep learning. I was wondering if PyTorch is appropriate for this sort of thing. For example the albumentations library.",t2_11jjtr,False,,0,False,Lower Level Image Processing,[],r/pytorch,False,6,,0,,,False,t3_es2lcz,False,dark,0.88,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1579675893.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am looking for a set of image processing tools (Canny edge, Watershed, Hough Transform type things)  that is not deep learning. I was wondering if PyTorch is appropriate for this sort of thing. For example the albumentations library.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,es2lcz,True,,xolopx,,4,True,all_ads,False,[],False,,/r/pytorch/comments/es2lcz/lower_level_image_processing/,all_ads,False,https://www.reddit.com/r/pytorch/comments/es2lcz/lower_level_image_processing/,7135,1579647093.0,0,,False,,,,,,,,
701,,pytorch,,t2_3g275pc4,False,,0,False,Wrote a Blog on Automatic Image Captioning with CNN &amp; RNN,[],r/pytorch,False,6,,0,62.0,,False,t3_erq0fm,False,dark,1.0,,public,7,0,{},140.0,,False,[],,False,False,,{},,False,7,,False,https://b.thumbs.redditmedia.com/ii84JnvDC4G9kbuOQ6aOk4dVhceOfhG13YgqOuqU3hM.jpg,False,,[],{},link,,False,,1579612778.0,text,6,,,text,medium.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/H8nTwDHZRATpyOAwBv1UhXIaOf6dKirBJYuskjKCeus.jpg?auto=webp&amp;s=7d3e325218e3c9a159923b7d50dc5d140e2e60fa', 'width': 1200, 'height': 532}, 'resolutions': [{'url': 'https://external-preview.redd.it/H8nTwDHZRATpyOAwBv1UhXIaOf6dKirBJYuskjKCeus.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0678387ae80e2da0fd15a27c6d7340008a937e77', 'width': 108, 'height': 47}, {'url': 'https://external-preview.redd.it/H8nTwDHZRATpyOAwBv1UhXIaOf6dKirBJYuskjKCeus.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=59c1c1c53b29af2096e9616b02c96c89e497b6a3', 'width': 216, 'height': 95}, {'url': 'https://external-preview.redd.it/H8nTwDHZRATpyOAwBv1UhXIaOf6dKirBJYuskjKCeus.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=798ca200d2ead1cf15b5a716110972a3d58b69fe', 'width': 320, 'height': 141}, {'url': 'https://external-preview.redd.it/H8nTwDHZRATpyOAwBv1UhXIaOf6dKirBJYuskjKCeus.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd96baebe057a004f6427a51574a358d5440dd28', 'width': 640, 'height': 283}, {'url': 'https://external-preview.redd.it/H8nTwDHZRATpyOAwBv1UhXIaOf6dKirBJYuskjKCeus.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=66e32f0e49869813847a5025b8e127403f51a53f', 'width': 960, 'height': 425}, {'url': 'https://external-preview.redd.it/H8nTwDHZRATpyOAwBv1UhXIaOf6dKirBJYuskjKCeus.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b96d1f0917b5f06ba055a4793c00bf913726cf9', 'width': 1080, 'height': 478}], 'variants': {}, 'id': '63gRCvxPN0KMaYnMMhK39VvuvGaQvyp7FSak8IpvNZs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,erq0fm,True,,Nailer_Owl,,2,True,all_ads,False,[],False,,/r/pytorch/comments/erq0fm/wrote_a_blog_on_automatic_image_captioning_with/,all_ads,False,https://medium.com/@krunalkshirsagar/automatic-image-captioning-with-cnn-rnn-aae3cd442d83,7135,1579583978.0,0,,False,https://medium.com/@krunalkshirsagar/automatic-image-captioning-with-cnn-rnn-aae3cd442d83,,,,,,,
702,,pytorch,"I would like to write a pytorch based program to make a choice about which option to take (out of 20 choices).

Based on a few variables such as color, type, size and name (integers and strings) it should make a choice from 20 options. Based on the ""points"" it gets it should compare its new choices to the previous choices to make a decision.  


i.e. ""I chose this red 5 meter cube named steve before and that got me 50 points"", ""I will thus choose this red 10 meter circle named bob based on the color"".

It should determine the importance of the input variables automatically.

Could I get a simple example similar to this? The pole/cart example on the pytorch site was too complex for me.",t2_a3hrsaj,False,,0,False,Could I get a simple example program comparing a few variables to previous choices to make a choice? (Reinforcement learning);,[],r/pytorch,False,6,,0,,,False,t3_er5eor,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1579510527.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would like to write a pytorch based program to make a choice about which option to take (out of 20 choices).&lt;/p&gt;

&lt;p&gt;Based on a few variables such as color, type, size and name (integers and strings) it should make a choice from 20 options. Based on the &amp;quot;points&amp;quot; it gets it should compare its new choices to the previous choices to make a decision.  &lt;/p&gt;

&lt;p&gt;i.e. &amp;quot;I chose this red 5 meter cube named steve before and that got me 50 points&amp;quot;, &amp;quot;I will thus choose this red 10 meter circle named bob based on the color&amp;quot;.&lt;/p&gt;

&lt;p&gt;It should determine the importance of the input variables automatically.&lt;/p&gt;

&lt;p&gt;Could I get a simple example similar to this? The pole/cart example on the pytorch site was too complex for me.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,er5eor,True,,I_Mod_Things,,2,True,all_ads,False,[],False,,/r/pytorch/comments/er5eor/could_i_get_a_simple_example_program_comparing_a/,all_ads,False,https://www.reddit.com/r/pytorch/comments/er5eor/could_i_get_a_simple_example_program_comparing_a/,7135,1579481727.0,0,,False,,,,,,,,
703,,pytorch,,t2_tiqto,False,,0,False,Unraveling Nvidia's Landscape Painting GANs,[],r/pytorch,False,6,,0,78.0,,False,t3_eqyfy0,False,dark,1.0,,public,4,0,{},140.0,,False,[],,False,False,,{},,False,4,,False,https://b.thumbs.redditmedia.com/3FP21s6EDhpGLczMo7aBweV4D0M9VtieIg5osuu8rms.jpg,False,,[],{},link,,False,,1579480320.0,text,6,,,text,blog.paperspace.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jEjOrWzl0BTyb3EalwSwE2z_Qqmeu-RAUlxFYXuG7_4.jpg?auto=webp&amp;s=c1f334fb20c03055fccdad5a112b772fdb9fe9dc', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://external-preview.redd.it/jEjOrWzl0BTyb3EalwSwE2z_Qqmeu-RAUlxFYXuG7_4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1711e1f3651c7d64be2015f73f74e9cf44c5f546', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/jEjOrWzl0BTyb3EalwSwE2z_Qqmeu-RAUlxFYXuG7_4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ccf4b9f279b613c1ea65eba45fa6bbccbf607d55', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/jEjOrWzl0BTyb3EalwSwE2z_Qqmeu-RAUlxFYXuG7_4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9b360add1c2bb59173831cd6a8cb7d983c10c18f', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/jEjOrWzl0BTyb3EalwSwE2z_Qqmeu-RAUlxFYXuG7_4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b75378f27a4ba33851aa08fe6f2fdfad36bca2f', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/jEjOrWzl0BTyb3EalwSwE2z_Qqmeu-RAUlxFYXuG7_4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fdc45b4d48c116d08e1d0473ef5f910edb973518', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/jEjOrWzl0BTyb3EalwSwE2z_Qqmeu-RAUlxFYXuG7_4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7a34c615982d157f448af273286b6cd88cc22b1a', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'V56HPaQ_4A7FXv1bwjTsIxq9DBsw6VOydESpfmUSBPs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,eqyfy0,True,,dkobran,,0,False,all_ads,False,[],False,,/r/pytorch/comments/eqyfy0/unraveling_nvidias_landscape_painting_gans/,all_ads,False,https://blog.paperspace.com/nvidia-gaugan-introduction/,7135,1579451520.0,0,,False,https://blog.paperspace.com/nvidia-gaugan-introduction/,,,,,,,
704,,pytorch,"Hey,

I have Googled everything Is there any solution (or plan in the future) to allow to use variable shape of input data in single batch without problems with distributed GPU training? Something like [https://pytorch.org/docs/stable/nn.html?highlight=pack\_padded\_sequence#torch.nn.utils.rnn.pack\_padded\_sequence](https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence) but for CNN (am I right that I am not able to use it with CNN?). My goal to feed CNN of variable input size with batch with variable size of input data. I am afraid that 0 padding or batches with same shape will make my model much worse. My input data are 2d matrix with size (None, None, 1) filled by 0 and 1.

Thanks.",t2_5lcc2,False,,0,False,Variable shapes in batch during CNN training,[],r/pytorch,False,6,,0,,,False,t3_eq1j6h,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1579302433.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey,&lt;/p&gt;

&lt;p&gt;I have Googled everything Is there any solution (or plan in the future) to allow to use variable shape of input data in single batch without problems with distributed GPU training? Something like &lt;a href=""https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence""&gt;https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&lt;/a&gt; but for CNN (am I right that I am not able to use it with CNN?). My goal to feed CNN of variable input size with batch with variable size of input data. I am afraid that 0 padding or batches with same shape will make my model much worse. My input data are 2d matrix with size (None, None, 1) filled by 0 and 1.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/mIEdxK1n4c1i1MameLP7lZT9nEUch20UkvB9DhKYKKU.jpg?auto=webp&amp;s=8ac003195b23712fd877ae57d2e26662aec6458b', 'width': 640, 'height': 480}, 'resolutions': [{'url': 'https://external-preview.redd.it/mIEdxK1n4c1i1MameLP7lZT9nEUch20UkvB9DhKYKKU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=777ae60463f9422b777460fc87a4a82feef117db', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/mIEdxK1n4c1i1MameLP7lZT9nEUch20UkvB9DhKYKKU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=122815c42b94c3d4179f286e050f45002f8da877', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/mIEdxK1n4c1i1MameLP7lZT9nEUch20UkvB9DhKYKKU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f1e7eb469af6dd64d9fd3f110ad5aaf8f9067fc', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/mIEdxK1n4c1i1MameLP7lZT9nEUch20UkvB9DhKYKKU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5974818619eaab8078087f4adeb343c08c2086ed', 'width': 640, 'height': 480}], 'variants': {}, 'id': '6u-UEsl00VpPoe4WC09egOicvvfUXiWoMwhk0rWXtAM'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,eq1j6h,True,,wilima,,2,True,all_ads,False,[],False,,/r/pytorch/comments/eq1j6h/variable_shapes_in_batch_during_cnn_training/,all_ads,False,https://www.reddit.com/r/pytorch/comments/eq1j6h/variable_shapes_in_batch_during_cnn_training/,7135,1579273633.0,0,,False,,,,,,,,
705,,pytorch,"[https://pytorch.org/blog/pytorch-1-dot-4-released-and-domain-libraries-updated/](https://pytorch.org/blog/pytorch-1-dot-4-released-and-domain-libraries-updated/)

&amp;#x200B;

The release contains over 1,500 commits and a significant amount of effort in areas spanning existing areas like JIT, ONNX, Distributed, Performance and Eager Frontend Improvements and improvements to experimental areas like mobile and quantization. It also contains new experimental features including rpc-based model parallel distributed training and language bindings for the Java language (inference only).

&amp;#x200B;

**PyTorch 1.4 is the last release that supports Python 2**. For the C++ API, it is the last release that supports C++11: you should start migrating to Python 3 and building with C++14 to make the future transition from 1.4 to 1.5 easier.",t2_4jk77,False,,0,False,Pytorch 1.4 released,[],r/pytorch,False,6,,0,,,False,t3_epkj13,False,dark,0.94,,public,38,0,{},,,False,[],,False,False,,{},,False,38,,False,self,False,,[],{},self,,True,,1579216057.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://pytorch.org/blog/pytorch-1-dot-4-released-and-domain-libraries-updated/""&gt;https://pytorch.org/blog/pytorch-1-dot-4-released-and-domain-libraries-updated/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The release contains over 1,500 commits and a significant amount of effort in areas spanning existing areas like JIT, ONNX, Distributed, Performance and Eager Frontend Improvements and improvements to experimental areas like mobile and quantization. It also contains new experimental features including rpc-based model parallel distributed training and language bindings for the Java language (inference only).&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PyTorch 1.4 is the last release that supports Python 2&lt;/strong&gt;. For the C++ API, it is the last release that supports C++11: you should start migrating to Python 3 and building with C++14 to make the future transition from 1.4 to 1.5 easier.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?auto=webp&amp;s=d0da5dbb7aff983a5173aba7d2bdd36fa28abe08', 'width': 2500, 'height': 2500}, 'resolutions': [{'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fef05ec37ddf39032979380cc7acc403e05834be', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e1de8e408f33ef3c93425e30e0bb5afffd5c631', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89562dde9cea0af5ebaaba50a9437e484d732513', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d6125bcd02c2fed8d84dac49cc8d73dbe205d81', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=40d7ca8d88566f27054acb632514012b5026acc9', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=757464a70e69532931dcbf0fbd397b9d433cf205', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'Mg0HAz_PmYkYWkaGTVyhCEzp15PpLW_TWWqmn9VNVFs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,epkj13,True,,briggers,,2,True,all_ads,False,[],False,,/r/pytorch/comments/epkj13/pytorch_14_released/,all_ads,False,https://www.reddit.com/r/pytorch/comments/epkj13/pytorch_14_released/,7135,1579187257.0,0,,False,,,,,,,,
706,,pytorch,,t2_3qvo75or,False,,0,False,AttributeError: 'Subset' object has no attribute 'targets' - please help,[],r/pytorch,False,6,,0,140.0,,False,t3_eoaqho,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/lx1idYuKOa8bS4MQKKDOvD3IstltOPvAVm0ndljoiUY.jpg,False,,[],{},link,,False,,1578979945.0,text,6,,,text,discuss.pytorch.org,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,eoaqho,True,,deep_learner_all,,0,True,all_ads,False,[],False,,/r/pytorch/comments/eoaqho/attributeerror_subset_object_has_no_attribute/,all_ads,False,https://discuss.pytorch.org/t/attributeerror-subset-object-has-no-attribute-targets/66564,7135,1578951145.0,0,,False,https://discuss.pytorch.org/t/attributeerror-subset-object-has-no-attribute-targets/66564,,,,,,,
707,,pytorch,,t2_5etjwfom,False,,0,False,Speed up your Pytorch training by 10%,[],r/pytorch,False,6,,0,,,False,t3_eo2g38,False,dark,0.31,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,default,False,,[],{},,,False,,1578940491.0,text,6,,,text,link.medium.com,False,,,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,eo2g38,True,,homo_sapiens_reddit,,2,True,all_ads,False,[],False,,/r/pytorch/comments/eo2g38/speed_up_your_pytorch_training_by_10/,all_ads,False,https://link.medium.com/f4KKZl76c3,7135,1578911691.0,0,,False,https://link.medium.com/f4KKZl76c3,"[{'approved_at_utc': None, 'subreddit': 'learnmachinelearning', 'selftext': '', 'author_fullname': 't2_5etjwfom', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Speed up Pytorch training by 10%', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/learnmachinelearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_eo1fy8', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 0, 'approved_by': None, 'author_premium': False, 'thumbnail': 'default', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1578932932.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'link.medium.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://link.medium.com/f4KKZl76c3', 'view_count': None, 'archived': True, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3cqa1', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'eo1fy8', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'homo_sapiens_reddit', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/learnmachinelearning/comments/eo1fy8/speed_up_pytorch_training_by_10/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://link.medium.com/f4KKZl76c3', 'subreddit_subscribers': 217921, 'created_utc': 1578904132.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_eo1fy8,,,,,
708,,pytorch,"I'm doing alpha zero with checkers, and I've successfully implemented (afaik) the code that trains the network from the data. This data consists of a one-hot representation of the 8x4 board, a value in \[0, .5, 1\] whether the position was lost, drawn, or won, and finally a 4x(8x4) policy tensor that encodes how many times a move was selected in the monte-carlo simulations of that position. It is important to note that this policy tensor is a tensor of all 'semi-legal' moves, one plane for each jumping direction for all squares on the board. Meanwhile, the policy output of the network has the same shape, but it is from a linear layer. When utilizing this output in the prior  probabilities of the tree search, we select the activations corresponding to the valid moves and softmax them to get a bonafide probability distribution.

The network outputs a value \[0, 1\] and probability logits as described above. At each state, the loss is the sum of the mean squared error of value and the score, plus the cross entropy loss of the predicted probability and the simulation counts (L1 normalized to get a distribution). For sake of your understanding, here is a  piece of the pseudocode from the AlphaZero paper:

[https://pastebin.com/YPkH8kdf](https://pastebin.com/YPkH8kdf)

Here is tidied-up (psuedo) code from my implementation:

[https://pastebin.com/Vtuhc7HU](https://pastebin.com/Vtuhc7HU)

&amp;#x200B;

The reason I am making this post is that I would like to use built in functions like torch.nn.crossentropyloss and torch.nn.MSE since I assume they are much faster/more stable. It would be great if I didn't have to iterate and use indexing for the individual data points, since I'm sure this is very expensive and probably doesn't allow me to take full advantage of CUDA. I also am not entirely sure if my math for CEL is correct or if my implementation of mini-batch SGD is correct. If you read this thank you and I look forward to any comments.",t2_ew2cf,False,,0,False,AlphaZero: How can I use built-in Pytorch functions to make my hacky implementation faster?,[],r/pytorch,False,6,,0,,,False,t3_end4wz,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,1578776508.0,,[],{},self,,True,,1578804961.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m doing alpha zero with checkers, and I&amp;#39;ve successfully implemented (afaik) the code that trains the network from the data. This data consists of a one-hot representation of the 8x4 board, a value in [0, .5, 1] whether the position was lost, drawn, or won, and finally a 4x(8x4) policy tensor that encodes how many times a move was selected in the monte-carlo simulations of that position. It is important to note that this policy tensor is a tensor of all &amp;#39;semi-legal&amp;#39; moves, one plane for each jumping direction for all squares on the board. Meanwhile, the policy output of the network has the same shape, but it is from a linear layer. When utilizing this output in the prior  probabilities of the tree search, we select the activations corresponding to the valid moves and softmax them to get a bonafide probability distribution.&lt;/p&gt;

&lt;p&gt;The network outputs a value [0, 1] and probability logits as described above. At each state, the loss is the sum of the mean squared error of value and the score, plus the cross entropy loss of the predicted probability and the simulation counts (L1 normalized to get a distribution). For sake of your understanding, here is a  piece of the pseudocode from the AlphaZero paper:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://pastebin.com/YPkH8kdf""&gt;https://pastebin.com/YPkH8kdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here is tidied-up (psuedo) code from my implementation:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://pastebin.com/Vtuhc7HU""&gt;https://pastebin.com/Vtuhc7HU&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The reason I am making this post is that I would like to use built in functions like torch.nn.crossentropyloss and torch.nn.MSE since I assume they are much faster/more stable. It would be great if I didn&amp;#39;t have to iterate and use indexing for the individual data points, since I&amp;#39;m sure this is very expensive and probably doesn&amp;#39;t allow me to take full advantage of CUDA. I also am not entirely sure if my math for CEL is correct or if my implementation of mini-batch SGD is correct. If you read this thank you and I look forward to any comments.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?auto=webp&amp;s=e28867cccd2864fc170848b64bc44e6a778116b9', 'width': 250, 'height': 250}, 'resolutions': [{'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b38a59d6140109bc38ed4f88b98bd4436dfe09b', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=203ad20eac023d67d122c5f834516a4670799f63', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'tEFaKdpbTuSBBWpWQ-kmQ1l_KwNUpQtPpUtOwmLiL-A'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,end4wz,True,,baskeyt,,1,True,all_ads,False,[],False,,/r/pytorch/comments/end4wz/alphazero_how_can_i_use_builtin_pytorch_functions/,all_ads,False,https://www.reddit.com/r/pytorch/comments/end4wz/alphazero_how_can_i_use_builtin_pytorch_functions/,7135,1578776161.0,0,,False,,,,,,,,
709,,pytorch,"The pytorch LSTM input dimensions require a tensor in the form of *seq_len * batch_size * input_size*, but the default collate function returns a tensor with dimensions *batch_size * seq_len * input_size* for me. How do I swap the axes to match?",t2_a3s1v,False,,0,False,Pytorch LSTM input dimensions,[],r/pytorch,False,6,,0,,,False,t3_emv107,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1578709763.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The pytorch LSTM input dimensions require a tensor in the form of &lt;em&gt;seq_len * batch_size * input_size&lt;/em&gt;, but the default collate function returns a tensor with dimensions &lt;em&gt;batch_size * seq_len * input_size&lt;/em&gt; for me. How do I swap the axes to match?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,emv107,True,,slipperysnail,,2,True,all_ads,False,[],False,,/r/pytorch/comments/emv107/pytorch_lstm_input_dimensions/,all_ads,False,https://www.reddit.com/r/pytorch/comments/emv107/pytorch_lstm_input_dimensions/,7135,1578680963.0,0,,False,,,,,,,,
710,,pytorch,"Have experience with TensorFlow but want to move towards PyTorch, does anyone have any suggestions of books to start with?",t2_e8ikp,False,,0,False,Best books for beginners?,[],r/pytorch,False,6,,0,,,False,t3_emxknd,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1578720797.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Have experience with TensorFlow but want to move towards PyTorch, does anyone have any suggestions of books to start with?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,emxknd,True,,andrewdood,,4,True,all_ads,False,[],False,,/r/pytorch/comments/emxknd/best_books_for_beginners/,all_ads,False,https://www.reddit.com/r/pytorch/comments/emxknd/best_books_for_beginners/,7135,1578691997.0,0,,False,,,,,,,,
711,,pytorch,,t2_2fv4yodo,False,,0,False,Facebook Open-Sources PySlowFast Codebase for Video Understanding,[],r/pytorch,False,6,,0,78.0,,False,t3_emtf55,False,dark,1.0,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/VNijTxkQg4gbKwoMkDbnsek-c_nP_usDWaovJsQxJ-Y.jpg,False,,[],{},link,,False,,1578703008.0,text,6,,,text,medium.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/J0zRGyZlkisk3p-JmHvhC2HrllzooxNMInqQmUikNIY.jpg?auto=webp&amp;s=433f9f58385aa50ac9c1a99dbbc45da8b6543c20', 'width': 1169, 'height': 657}, 'resolutions': [{'url': 'https://external-preview.redd.it/J0zRGyZlkisk3p-JmHvhC2HrllzooxNMInqQmUikNIY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=82b8edfcf53878159be4828a9318024fb2f24c0e', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/J0zRGyZlkisk3p-JmHvhC2HrllzooxNMInqQmUikNIY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1fab200b1a07827aaf589c02efb49ae55f9db1c4', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/J0zRGyZlkisk3p-JmHvhC2HrllzooxNMInqQmUikNIY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d21de073a8b617506c8a688d55bf701777657db', 'width': 320, 'height': 179}, {'url': 'https://external-preview.redd.it/J0zRGyZlkisk3p-JmHvhC2HrllzooxNMInqQmUikNIY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d747911336ec4e22e2d4288a5182f113230c289b', 'width': 640, 'height': 359}, {'url': 'https://external-preview.redd.it/J0zRGyZlkisk3p-JmHvhC2HrllzooxNMInqQmUikNIY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ec17421c58a05ba83e7d09e624628b1ebc74becf', 'width': 960, 'height': 539}, {'url': 'https://external-preview.redd.it/J0zRGyZlkisk3p-JmHvhC2HrllzooxNMInqQmUikNIY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=88a8c13cc1b53afa4f7f2a408fef1dd29c454b76', 'width': 1080, 'height': 606}], 'variants': {}, 'id': 'OQOyOLPAtQD08SjwnIBGgYMcAh9wnlS9Nt0421GZkYc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,emtf55,True,,Yuqing7,,0,True,all_ads,False,[],False,,/r/pytorch/comments/emtf55/facebook_opensources_pyslowfast_codebase_for/,all_ads,False,https://medium.com/syncedreview/facebook-open-sources-pyslowfast-codebase-for-video-understanding-89200ff35267,7135,1578674208.0,0,,False,https://medium.com/syncedreview/facebook-open-sources-pyslowfast-codebase-for-video-understanding-89200ff35267,,,,,,,
712,,pytorch,"I intend to use one of these frameworks for research purposes, where I will be writing many custom training loops, playing with the network architecture a lot, and I need a lot of flexibility. I have seen many comparisons on the web with the usual conclusion that PyTorch is more suitable for research because it is better designed and is more flexible, but these articles are usually from before Tensorflow 2.0 came out.

Can someone pitch in their opinion on the current state of these frameworks? Which one do you think is more suitable for research?",t2_69rstgl,False,,0,False,Tensorflow vs. PyTorch for research?,[],r/pytorch,False,6,,0,,,False,t3_empx4g,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1578685807.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I intend to use one of these frameworks for research purposes, where I will be writing many custom training loops, playing with the network architecture a lot, and I need a lot of flexibility. I have seen many comparisons on the web with the usual conclusion that PyTorch is more suitable for research because it is better designed and is more flexible, but these articles are usually from before Tensorflow 2.0 came out.&lt;/p&gt;

&lt;p&gt;Can someone pitch in their opinion on the current state of these frameworks? Which one do you think is more suitable for research?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,empx4g,True,,fori1to10,,3,True,all_ads,False,[],False,,/r/pytorch/comments/empx4g/tensorflow_vs_pytorch_for_research/,all_ads,False,https://www.reddit.com/r/pytorch/comments/empx4g/tensorflow_vs_pytorch_for_research/,7135,1578657007.0,0,,False,,,,,,,,
713,,pytorch,How does one go about creating a compatible dataset for use in pytorch? I have a 2D list of training samples and another list with corresponding labels. I have built a model architecture which I feel is suitable for this data. I am facing difficulties in loading/converting data I have into something which I can put into the Dataloader function. Please help me in this.,t2_2qtz34k0,False,,0,False,Creating custom dataset for loading into pytorch model,[],r/pytorch,False,6,,0,,,False,t3_emnfqp,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1578668671.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How does one go about creating a compatible dataset for use in pytorch? I have a 2D list of training samples and another list with corresponding labels. I have built a model architecture which I feel is suitable for this data. I am facing difficulties in loading/converting data I have into something which I can put into the Dataloader function. Please help me in this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,emnfqp,True,,thetarunbs,,0,True,all_ads,False,[],False,,/r/pytorch/comments/emnfqp/creating_custom_dataset_for_loading_into_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/emnfqp/creating_custom_dataset_for_loading_into_pytorch/,7135,1578639871.0,0,,False,,,,,,,,
714,,pytorch,I have videos. For each video i would want extract images at set intervals. I would like the images to be compatible with pytorch (channel first Tensors) What is a good way to this? Thank you very much.,t2_52i2r1g6,False,,0,False,Preparation of Images,[],r/pytorch,False,6,,0,,,False,t3_em6rln,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1578586384.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have videos. For each video i would want extract images at set intervals. I would like the images to be compatible with pytorch (channel first Tensors) What is a good way to this? Thank you very much.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,em6rln,True,,universerenade,,0,True,all_ads,False,[],False,,/r/pytorch/comments/em6rln/preparation_of_images/,all_ads,False,https://www.reddit.com/r/pytorch/comments/em6rln/preparation_of_images/,7135,1578557584.0,0,,False,,,,,,,,
715,,pytorch,Can someone help to modify the code to make it work for various sequence length as requested in this [post](https://stackoverflow.com/questions/59381695/lstm-in-pytorch-how-to-add-change-sequence-length-dimension)?,t2_4hao56xf,False,,0,False,LSTM in Pytorch: how to add/change sequence length dimension?,[],r/pytorch,False,6,,0,,,False,t3_eljkxj,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1578467636.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can someone help to modify the code to make it work for various sequence length as requested in this &lt;a href=""https://stackoverflow.com/questions/59381695/lstm-in-pytorch-how-to-add-change-sequence-length-dimension""&gt;post&lt;/a&gt;?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,eljkxj,True,,TTT2019,,0,True,all_ads,False,[],False,,/r/pytorch/comments/eljkxj/lstm_in_pytorch_how_to_addchange_sequence_length/,all_ads,False,https://www.reddit.com/r/pytorch/comments/eljkxj/lstm_in_pytorch_how_to_addchange_sequence_length/,7135,1578438836.0,0,,False,,,,,,,,
716,,pytorch,,t2_1ffz9tjt,False,,0,False,[Google Brain Object detection] EfficientDet: Scalable and Efficient Object Detection implementation by Signatrix,[],r/pytorch,False,6,,0,78.0,,False,t3_ek3unr,False,dark,0.91,,public,18,0,{},140.0,,False,[],,True,False,,{},,False,18,,False,https://b.thumbs.redditmedia.com/kb7T_1Dn3V95t2uvrirVI1ZCTmAyYm1O-4ZGdgUADSM.jpg,False,,[],{},image,,False,,1578206818.0,text,6,,,text,i.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/g7w6073ybu841.gif?format=png8&amp;s=3a46e7fe3b937279852df1abb0e0d13beab092ba', 'width': 640, 'height': 360}, 'resolutions': [{'url': 'https://preview.redd.it/g7w6073ybu841.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=b319b16dd262f2d1509130d3f8513408dc27c7ee', 'width': 108, 'height': 60}, {'url': 'https://preview.redd.it/g7w6073ybu841.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=4110a8968ec11577fe2c0ed3bee6746ab850ac89', 'width': 216, 'height': 121}, {'url': 'https://preview.redd.it/g7w6073ybu841.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=bd910959d011b3b30a1d0d3ceed132fc4eb62a3c', 'width': 320, 'height': 180}, {'url': 'https://preview.redd.it/g7w6073ybu841.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=22d5d0de349168964481eccf12780df9d728419e', 'width': 640, 'height': 360}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/g7w6073ybu841.gif?s=f49c4f4b22c6e2a1d00aa3f2bee309ae0abb3a7f', 'width': 640, 'height': 360}, 'resolutions': [{'url': 'https://preview.redd.it/g7w6073ybu841.gif?width=108&amp;crop=smart&amp;s=b8357bec95affcdd2b4249e52670c9344bea7ec6', 'width': 108, 'height': 60}, {'url': 'https://preview.redd.it/g7w6073ybu841.gif?width=216&amp;crop=smart&amp;s=b1d07f2536de1419f95a2f9c4e9731a288bcbae4', 'width': 216, 'height': 121}, {'url': 'https://preview.redd.it/g7w6073ybu841.gif?width=320&amp;crop=smart&amp;s=723cb7bc4abcb28776ead90d7420b4ffb737b481', 'width': 320, 'height': 180}, {'url': 'https://preview.redd.it/g7w6073ybu841.gif?width=640&amp;crop=smart&amp;s=3bbdd621c86d6f36c6248c683ca3ab96c859d4fe', 'width': 640, 'height': 360}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/g7w6073ybu841.gif?format=mp4&amp;s=ea835b7b3c9044153c5cc887b4d91c74f4c8f4cb', 'width': 640, 'height': 360}, 'resolutions': [{'url': 'https://preview.redd.it/g7w6073ybu841.gif?width=108&amp;format=mp4&amp;s=13bc6d468b611ee13455c3b3249565dd67bc1bba', 'width': 108, 'height': 60}, {'url': 'https://preview.redd.it/g7w6073ybu841.gif?width=216&amp;format=mp4&amp;s=8ac1e9adc38b62842806a67b70f10356f2e64f7d', 'width': 216, 'height': 121}, {'url': 'https://preview.redd.it/g7w6073ybu841.gif?width=320&amp;format=mp4&amp;s=ee0d9c9f125b6dceb753da63e4ba08f83ecb12b3', 'width': 320, 'height': 180}, {'url': 'https://preview.redd.it/g7w6073ybu841.gif?width=640&amp;format=mp4&amp;s=4c2324374353e0909ef163df9dc121ea279167e2', 'width': 640, 'height': 360}]}}, 'id': 'Awc7FGPS4s7nApq8PHUqCITVpUth_8B10cxebpcAAmU'}], 'enabled': True}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ek3unr,True,,1991viet,,1,True,all_ads,False,[],False,,/r/pytorch/comments/ek3unr/google_brain_object_detection_efficientdet/,all_ads,False,https://i.redd.it/g7w6073ybu841.gif,7135,1578178018.0,0,,False,https://i.redd.it/g7w6073ybu841.gif,,,,,,,
717,,pytorch,"Hi! I am following this tutorial: https://github.com/MarwanDebbiche/post-tuto-deployment/tree/master/src/training and noticed that I need PyTorch 0.4.1. The previous versions are here: https://pytorch.org/get-started/previous-versions/. Which wheel do I need to download? Thank you for any help you can offer! I am new to this!

Edit: I should also mention i am on Windows 10 64 bit with Python 3.7!",t2_4tvpi,False,,0,False,"New to PyTorch - Following a guide that requires 0.4.1, which wheel do I need to download?",[],r/pytorch,False,6,,0,,,False,t3_ek78t1,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1578248010.0,,[],{},self,,True,,1578223178.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi! I am following this tutorial: &lt;a href=""https://github.com/MarwanDebbiche/post-tuto-deployment/tree/master/src/training""&gt;https://github.com/MarwanDebbiche/post-tuto-deployment/tree/master/src/training&lt;/a&gt; and noticed that I need PyTorch 0.4.1. The previous versions are here: &lt;a href=""https://pytorch.org/get-started/previous-versions/""&gt;https://pytorch.org/get-started/previous-versions/&lt;/a&gt;. Which wheel do I need to download? Thank you for any help you can offer! I am new to this!&lt;/p&gt;

&lt;p&gt;Edit: I should also mention i am on Windows 10 64 bit with Python 3.7!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/6B4SnfN3WOu7aqjWXnLMaKdfTDY4xqHOxAqGfGddELQ.jpg?auto=webp&amp;s=16e7085408f7b11019f7272bb4b52417f503a770', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/6B4SnfN3WOu7aqjWXnLMaKdfTDY4xqHOxAqGfGddELQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2165f7dbe26f7fb774bb2109198c3a012d8dac4', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/6B4SnfN3WOu7aqjWXnLMaKdfTDY4xqHOxAqGfGddELQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f30e1ffe988a3f61aeb39ca956b2e51b7565f5c3', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/6B4SnfN3WOu7aqjWXnLMaKdfTDY4xqHOxAqGfGddELQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f00554c75fc34f946e19f8ac20a89b1f4d5fa133', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'NA_QnCRVmWoT5ICJDK2wFkUDaZXVQI_OIbsmYtJUUwE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ek78t1,True,,Firefly-ssa,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ek78t1/new_to_pytorch_following_a_guide_that_requires/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ek78t1/new_to_pytorch_following_a_guide_that_requires/,7135,1578194378.0,0,,False,,,,,,,,
718,,pytorch,"This is probably old news to anyone using Pytorch continuously but, as someone who hadn't been back to a project in a while I was really confused until I found that the MSELoss default parameters had changed.  Somewhere between Pytorch 0.5 and 1.3 (current) the default reduction became 'mean' instead of 'sum'.",t2_4bwhg,False,,0,False,PSA: MSELoss defaults changed...,[],r/pytorch,False,6,,0,,,False,t3_ein9q0,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1577939261.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is probably old news to anyone using Pytorch continuously but, as someone who hadn&amp;#39;t been back to a project in a while I was really confused until I found that the MSELoss default parameters had changed.  Somewhere between Pytorch 0.5 and 1.3 (current) the default reduction became &amp;#39;mean&amp;#39; instead of &amp;#39;sum&amp;#39;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ein9q0,True,,patniemeyer,,1,True,all_ads,False,[],False,,/r/pytorch/comments/ein9q0/psa_mseloss_defaults_changed/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ein9q0/psa_mseloss_defaults_changed/,7135,1577910461.0,0,,False,,,,,,,,
719,,pytorch,"I'm looking for an intuition behind why batch\_first is not default in LSTM/GRU layers while other layers don't follow this convention. Why was this behaviour chosen? I know I can set batch\_first to True but I'm wondering why is there the option to do this. What are the advantages of doing batch\_first vs not doing batch\_first.

&amp;#x200B;

Thanks",t2_5y7buqz,False,,0,False,Why batch_first is not default in LSTM/GRU?,[],r/pytorch,False,6,,0,,,False,t3_eii923,False,dark,1.0,,public,9,0,{},,,False,[],,False,False,,{},,False,9,,False,self,False,,[],{},,,True,,1577913464.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for an intuition behind why batch_first is not default in LSTM/GRU layers while other layers don&amp;#39;t follow this convention. Why was this behaviour chosen? I know I can set batch_first to True but I&amp;#39;m wondering why is there the option to do this. What are the advantages of doing batch_first vs not doing batch_first.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,eii923,True,,csyrup,,1,True,all_ads,False,[],False,,/r/pytorch/comments/eii923/why_batch_first_is_not_default_in_lstmgru/,all_ads,False,https://www.reddit.com/r/pytorch/comments/eii923/why_batch_first_is_not_default_in_lstmgru/,7135,1577884664.0,0,,False,,,,,,,,
720,,pytorch,,t2_mjlci,False,,0,False,Image Classification using Convolutional Neural Networks in PyTorch | 2 hour video tutorial | Part 4 of Deep Learning with PyTorch,[],r/pytorch,False,6,,0,105.0,,False,t3_ei2zc4,False,dark,0.71,,public,3,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/TNSO9oTRCMM?list=PLyMom0n-MBroupZiLfVSZqK5asX8KfoHL"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Image Classification using Convolutional Neural Networks | Deep Learning with PyTorch | Part 4 of 6', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/TNSO9oTRCMM?list=PLyMom0n-MBroupZiLfVSZqK5asX8KfoHL"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Jovian', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/TNSO9oTRCMM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCmKaoNn0OvxVAe7f_8sXYNQ'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/TNSO9oTRCMM?list=PLyMom0n-MBroupZiLfVSZqK5asX8KfoHL"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ei2zc4', 'height': 200}",,False,3,,False,https://b.thumbs.redditmedia.com/cP2tklGbQMDayjznDcSE_OKdQy5BvloVxM2hS3jEWrM.jpg,False,,[],{},rich:video,,False,,1577828418.0,text,6,,,text,youtube.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/JEtrTezAnghb1GCUTmamM1JbYNbMLapJnhTpwsH--88.jpg?auto=webp&amp;s=14e23784b90ed4af81fe09a6f78535be4604d503', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/JEtrTezAnghb1GCUTmamM1JbYNbMLapJnhTpwsH--88.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=021e65f8f036d008518bcbe18831aa8d706af07f', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/JEtrTezAnghb1GCUTmamM1JbYNbMLapJnhTpwsH--88.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0895538e3df1f6f64949eec74b592f736efbd62b', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/JEtrTezAnghb1GCUTmamM1JbYNbMLapJnhTpwsH--88.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ed6126422d2c600066494200518e9b679b37c5bb', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'tFMoS7n82PPz8ZU6V6y0WYZoyLbYOjnMqagQS7yg4Rg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ei2zc4,True,,aakashns,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ei2zc4/image_classification_using_convolutional_neural/,all_ads,False,https://www.youtube.com/watch?v=TNSO9oTRCMM&amp;list=PLyMom0n-MBroupZiLfVSZqK5asX8KfoHL&amp;index=4&amp;t=321,7135,1577799618.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Image Classification using Convolutional Neural Networks | Deep Learning with PyTorch | Part 4 of 6', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/TNSO9oTRCMM?list=PLyMom0n-MBroupZiLfVSZqK5asX8KfoHL"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Jovian', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/TNSO9oTRCMM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCmKaoNn0OvxVAe7f_8sXYNQ'}}",False,https://www.youtube.com/watch?v=TNSO9oTRCMM&amp;list=PLyMom0n-MBroupZiLfVSZqK5asX8KfoHL&amp;index=4&amp;t=321,,,,,,,
721,,pytorch,,t2_11oqtx,False,,0,False,"The limits of Deep Learning, and its future",[],r/pytorch,False,6,,0,74.0,,False,t3_ehz94s,False,dark,0.4,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://a.thumbs.redditmedia.com/3uHaE7vIA07c4CJDFLaJqGNXlt_umbqfYnjjeHaOSD8.jpg,False,,[],{},link,,False,,1577803331.0,text,6,,,text,guillaume-chevalier.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5bKoDyx0-ILD8_0Oz8WW1_VttlcnIJEF73Wi63thRss.jpg?auto=webp&amp;s=a53ee937115081f3b287305126ae1cafd6db3b73', 'width': 1586, 'height': 848}, 'resolutions': [{'url': 'https://external-preview.redd.it/5bKoDyx0-ILD8_0Oz8WW1_VttlcnIJEF73Wi63thRss.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=76899e425eb9b4153c7039a59f731837893928ff', 'width': 108, 'height': 57}, {'url': 'https://external-preview.redd.it/5bKoDyx0-ILD8_0Oz8WW1_VttlcnIJEF73Wi63thRss.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=74b6f4fc02dd9ca53ab5fac5f2f6505f04933529', 'width': 216, 'height': 115}, {'url': 'https://external-preview.redd.it/5bKoDyx0-ILD8_0Oz8WW1_VttlcnIJEF73Wi63thRss.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df4175f9ce19ef6b2567d423ba48faa0c2fece7c', 'width': 320, 'height': 171}, {'url': 'https://external-preview.redd.it/5bKoDyx0-ILD8_0Oz8WW1_VttlcnIJEF73Wi63thRss.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f3e31b97b4ae85e04e016e74d9a8cc35dca49c7', 'width': 640, 'height': 342}, {'url': 'https://external-preview.redd.it/5bKoDyx0-ILD8_0Oz8WW1_VttlcnIJEF73Wi63thRss.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=202b7c7418e9296d63a82dc7eb63ec635ae78c8c', 'width': 960, 'height': 513}, {'url': 'https://external-preview.redd.it/5bKoDyx0-ILD8_0Oz8WW1_VttlcnIJEF73Wi63thRss.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d35933eef51daa5ca97a0bc5334d61d4d25d308a', 'width': 1080, 'height': 577}], 'variants': {}, 'id': 'ierOH4VAZw9KWSmxuxCW-It0vJ-r02WcQbdMyl-CQWg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ehz94s,True,,GChe,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ehz94s/the_limits_of_deep_learning_and_its_future/,all_ads,False,https://guillaume-chevalier.com/limits-of-deep-learning-and-its-future/,7135,1577774531.0,0,,False,https://guillaume-chevalier.com/limits-of-deep-learning-and-its-future/,"[{'approved_at_utc': None, 'subreddit': 'singularity', 'selftext': '', 'author_fullname': 't2_11oqtx', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'The limits of Deep Learning, and its future', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/singularity', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'article', 'downs': 0, 'thumbnail_height': 74, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_ehz36d', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'article', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/3uHaE7vIA07c4CJDFLaJqGNXlt_umbqfYnjjeHaOSD8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1577802343.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'guillaume-chevalier.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://guillaume-chevalier.com/limits-of-deep-learning-and-its-future/', 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/5bKoDyx0-ILD8_0Oz8WW1_VttlcnIJEF73Wi63thRss.jpg?auto=webp&amp;s=a53ee937115081f3b287305126ae1cafd6db3b73', 'width': 1586, 'height': 848}, 'resolutions': [{'url': 'https://external-preview.redd.it/5bKoDyx0-ILD8_0Oz8WW1_VttlcnIJEF73Wi63thRss.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=76899e425eb9b4153c7039a59f731837893928ff', 'width': 108, 'height': 57}, {'url': 'https://external-preview.redd.it/5bKoDyx0-ILD8_0Oz8WW1_VttlcnIJEF73Wi63thRss.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=74b6f4fc02dd9ca53ab5fac5f2f6505f04933529', 'width': 216, 'height': 115}, {'url': 'https://external-preview.redd.it/5bKoDyx0-ILD8_0Oz8WW1_VttlcnIJEF73Wi63thRss.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df4175f9ce19ef6b2567d423ba48faa0c2fece7c', 'width': 320, 'height': 171}, {'url': 'https://external-preview.redd.it/5bKoDyx0-ILD8_0Oz8WW1_VttlcnIJEF73Wi63thRss.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f3e31b97b4ae85e04e016e74d9a8cc35dca49c7', 'width': 640, 'height': 342}, {'url': 'https://external-preview.redd.it/5bKoDyx0-ILD8_0Oz8WW1_VttlcnIJEF73Wi63thRss.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=202b7c7418e9296d63a82dc7eb63ec635ae78c8c', 'width': 960, 'height': 513}, {'url': 'https://external-preview.redd.it/5bKoDyx0-ILD8_0Oz8WW1_VttlcnIJEF73Wi63thRss.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d35933eef51daa5ca97a0bc5334d61d4d25d308a', 'width': 1080, 'height': 577}], 'variants': {}, 'id': 'ierOH4VAZw9KWSmxuxCW-It0vJ-r02WcQbdMyl-CQWg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'cb6167d4-dbb7-11e3-a5b0-12313d18e464', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qh8m', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ehz36d', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'GChe', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/singularity/comments/ehz36d/the_limits_of_deep_learning_and_its_future/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://guillaume-chevalier.com/limits-of-deep-learning-and-its-future/', 'subreddit_subscribers': 107748, 'created_utc': 1577773543.0, 'num_crossposts': 3, 'media': None, 'is_video': False}]",t3_ehz36d,,,,,
722,,pytorch,"github: [https://github.com/amarczew/pytorch\_model\_summary](https://github.com/amarczew/pytorch_model_summary)

&amp;#x200B;

This is an Improved PyTorch library of [modelsummary](https://github.com/graykode/modelsummary). Like in  modelsummary, \*\*It does not care with number of Input parameter!\*\*

\*\* Issues and PR are welcome!!

&amp;#x200B;

\### Improvements:

\- For user defined pytorch layers, now \`summary\` can show layers inside it

\-- some assumptions: when is an user defined layer, if any weight/params/bias is trainable, then it is assumed that this layer is trainable (but only trainable params are counted in Tr. Params #)

\- Adding column counting only trainable parameters (it makes sense when there are user defined layers)

\- Showing all input/output shapes, instead of showing only the first one

\-- example: LSTM layer return a Tensor and a tuple (Tensor, Tensor), then output\\\_shape has three set of values

\- Printing: table width defined dynamically

\- Adding option to add hierarchical summary in output

\- Adding batch\_size value (when provided) in table footer

\- fix bugs",t2_11nb0r,False,,0,False,Pytorch model summary - It is a Keras style model.summary() implementation for PyTorch,[],r/pytorch,False,6,,0,,,False,t3_eelc84,False,dark,0.86,,public,10,0,{},,,False,[],,False,False,,{},,False,10,,False,self,1577111946.0,,[],{},self,,True,,1577140542.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;github: &lt;a href=""https://github.com/amarczew/pytorch_model_summary""&gt;https://github.com/amarczew/pytorch_model_summary&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;This is an Improved PyTorch library of &lt;a href=""https://github.com/graykode/modelsummary""&gt;modelsummary&lt;/a&gt;. Like in  modelsummary, **It does not care with number of Input parameter!**&lt;/p&gt;

&lt;p&gt;** Issues and PR are welcome!!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;### Improvements:&lt;/p&gt;

&lt;p&gt;- For user defined pytorch layers, now `summary` can show layers inside it&lt;/p&gt;

&lt;p&gt;-- some assumptions: when is an user defined layer, if any weight/params/bias is trainable, then it is assumed that this layer is trainable (but only trainable params are counted in Tr. Params #)&lt;/p&gt;

&lt;p&gt;- Adding column counting only trainable parameters (it makes sense when there are user defined layers)&lt;/p&gt;

&lt;p&gt;- Showing all input/output shapes, instead of showing only the first one&lt;/p&gt;

&lt;p&gt;-- example: LSTM layer return a Tensor and a tuple (Tensor, Tensor), then output\_shape has three set of values&lt;/p&gt;

&lt;p&gt;- Printing: table width defined dynamically&lt;/p&gt;

&lt;p&gt;- Adding option to add hierarchical summary in output&lt;/p&gt;

&lt;p&gt;- Adding batch_size value (when provided) in table footer&lt;/p&gt;

&lt;p&gt;- fix bugs&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AMcByWjhhxIsVDJr-E0Rx3h3wEDjdw7NIIXrvS1cTMQ.jpg?auto=webp&amp;s=67dc15e2ac064a499e1f5359731d9054e0aa74bf', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/AMcByWjhhxIsVDJr-E0Rx3h3wEDjdw7NIIXrvS1cTMQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2217b269a69da5e937b66605804ee6fc30910a7b', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/AMcByWjhhxIsVDJr-E0Rx3h3wEDjdw7NIIXrvS1cTMQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=09f0402fc47f7238cc1b3d406460396037e5d089', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/AMcByWjhhxIsVDJr-E0Rx3h3wEDjdw7NIIXrvS1cTMQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a23df1f7a632f12fd0f4e4f43b4a9d83a3f71240', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'Qw7Iu4Z7x-1TlG9PtkSY0WDSC83gcprr2D4gySj24Y8'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,eelc84,True,,amarczew,,2,True,all_ads,False,[],False,,/r/pytorch/comments/eelc84/pytorch_model_summary_it_is_a_keras_style/,all_ads,False,https://www.reddit.com/r/pytorch/comments/eelc84/pytorch_model_summary_it_is_a_keras_style/,7135,1577111742.0,0,,False,,,,,,,,
723,,pytorch,"I think of iterator as a collection or a list -- that helps to fetch items (rather batches of items) in an ordered fashion. I understand that a generator is an infinite iterator of some sort. But I don't have a mental picture of Pytorch's dataloader. That is something I always create before I get an iterator out of it to fetch my batches. But I never use it. 

What is ur mental image of a dataloader?",t2_4xpn6csn,False,,0,False,Requesting a mental image for a dataloader!,[],r/pytorch,False,6,,0,,,False,t3_ebsdmd,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1576594594.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I think of iterator as a collection or a list -- that helps to fetch items (rather batches of items) in an ordered fashion. I understand that a generator is an infinite iterator of some sort. But I don&amp;#39;t have a mental picture of Pytorch&amp;#39;s dataloader. That is something I always create before I get an iterator out of it to fetch my batches. But I never use it. &lt;/p&gt;

&lt;p&gt;What is ur mental image of a dataloader?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ebsdmd,True,,punksfunk,,1,True,all_ads,False,[],False,,/r/pytorch/comments/ebsdmd/requesting_a_mental_image_for_a_dataloader/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ebsdmd/requesting_a_mental_image_for_a_dataloader/,7135,1576565794.0,0,,False,,,,,,,,
724,,pytorch,,t2_2fv4yodo,False,,0,False,PyTorch Deep Learning Framework: Speed + Usability,[],r/pytorch,False,6,,0,95.0,,False,t3_ebhcjl,False,dark,0.69,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/-quUv1sLQXjf5cR3obKt0LkSxFpxfxjiDQJAcbr7jhs.jpg,False,,[],{},link,,False,,1576542619.0,text,6,,,text,medium.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/r5X5gn0IxMuKo9EJofkZ_K-7GXUMZRSj_hNVgtahaU8.jpg?auto=webp&amp;s=6530b71098c553b0120c3d91c54ef77818d9c94c', 'width': 1200, 'height': 820}, 'resolutions': [{'url': 'https://external-preview.redd.it/r5X5gn0IxMuKo9EJofkZ_K-7GXUMZRSj_hNVgtahaU8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ac4aa02e9f4277b60ca1df479467cb838deffca', 'width': 108, 'height': 73}, {'url': 'https://external-preview.redd.it/r5X5gn0IxMuKo9EJofkZ_K-7GXUMZRSj_hNVgtahaU8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2d740c4066da1497217f1f6a486d0006396aed5d', 'width': 216, 'height': 147}, {'url': 'https://external-preview.redd.it/r5X5gn0IxMuKo9EJofkZ_K-7GXUMZRSj_hNVgtahaU8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=849c22d3bd721c3b293f8a1e28c062b4cb3cb9e2', 'width': 320, 'height': 218}, {'url': 'https://external-preview.redd.it/r5X5gn0IxMuKo9EJofkZ_K-7GXUMZRSj_hNVgtahaU8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5536a07f29e07c998a89030977c6da7f73fe9b55', 'width': 640, 'height': 437}, {'url': 'https://external-preview.redd.it/r5X5gn0IxMuKo9EJofkZ_K-7GXUMZRSj_hNVgtahaU8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c5bab8a9ed5666fdb7b8303c2336d8e4ff5ae96', 'width': 960, 'height': 656}, {'url': 'https://external-preview.redd.it/r5X5gn0IxMuKo9EJofkZ_K-7GXUMZRSj_hNVgtahaU8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0b9b0ae46493072340e6e78f57289ad4acb5d291', 'width': 1080, 'height': 738}], 'variants': {}, 'id': '72iUcvAAgLG1rqnIwtsF2LC0PvfWRmQTCucdsM8Ve60'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ebhcjl,True,,Yuqing7,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ebhcjl/pytorch_deep_learning_framework_speed_usability/,all_ads,False,https://medium.com/syncedreview/pytorch-deep-learning-framework-speed-usability-2de2de400237,7135,1576513819.0,0,,False,https://medium.com/syncedreview/pytorch-deep-learning-framework-speed-usability-2de2de400237,,,,,,,
725,,pytorch,I found [this](https://qph.fs.quoracdn.net/main-qimg-45c7ada8202c6f5e68fbdc368fefe1c9) image and I can see that AMD is doing pretty good when compared to NVidia cards and taking into account price difference AMD would by my choice but this chart is benchmark using TF. Is it possible to use AMD GPUs with pytorch easly and with similar performance(compared to nvidia) as shown on the graph?,t2_4cltnh8,False,,0,False,Is it possible to use AMD GPUs with better performance than on NVidia?,[],r/pytorch,False,6,,0,,,False,t3_ea37z8,False,dark,0.89,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},self,,True,,1576267891.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I found &lt;a href=""https://qph.fs.quoracdn.net/main-qimg-45c7ada8202c6f5e68fbdc368fefe1c9""&gt;this&lt;/a&gt; image and I can see that AMD is doing pretty good when compared to NVidia cards and taking into account price difference AMD would by my choice but this chart is benchmark using TF. Is it possible to use AMD GPUs with pytorch easly and with similar performance(compared to nvidia) as shown on the graph?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/OCrp73G5TjOGkNkYyCKwx62ZKM56P5TC5sYUUEN02hU.jpg?auto=webp&amp;s=71789b980f869ec4b5fdf63dc9a298f5336c72e0', 'width': 2052, 'height': 994}, 'resolutions': [{'url': 'https://external-preview.redd.it/OCrp73G5TjOGkNkYyCKwx62ZKM56P5TC5sYUUEN02hU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2be8bcd3e39bb6400f0043ae3247195d62cf9908', 'width': 108, 'height': 52}, {'url': 'https://external-preview.redd.it/OCrp73G5TjOGkNkYyCKwx62ZKM56P5TC5sYUUEN02hU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f86010e090e427c78e71f48b5700d687da3fa53', 'width': 216, 'height': 104}, {'url': 'https://external-preview.redd.it/OCrp73G5TjOGkNkYyCKwx62ZKM56P5TC5sYUUEN02hU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a5457fc966d3d8ddab4d37dd4c10f09ae2542b97', 'width': 320, 'height': 155}, {'url': 'https://external-preview.redd.it/OCrp73G5TjOGkNkYyCKwx62ZKM56P5TC5sYUUEN02hU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b127bca50956bbe5095c14ee9fe7fd9eb8e677e3', 'width': 640, 'height': 310}, {'url': 'https://external-preview.redd.it/OCrp73G5TjOGkNkYyCKwx62ZKM56P5TC5sYUUEN02hU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=14336feb234044fac49d61253065af5bbd1c15a3', 'width': 960, 'height': 465}, {'url': 'https://external-preview.redd.it/OCrp73G5TjOGkNkYyCKwx62ZKM56P5TC5sYUUEN02hU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a4e22df1c0afcb64656822516886d913f47ca2e4', 'width': 1080, 'height': 523}], 'variants': {}, 'id': '6xQE4SfZmRZ4V_RrZ7gxdt0edeizWDMU2rMVWqXI7vs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ea37z8,True,,IDontHaveNicknameToo,,9,True,all_ads,False,[],False,,/r/pytorch/comments/ea37z8/is_it_possible_to_use_amd_gpus_with_better/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ea37z8/is_it_possible_to_use_amd_gpus_with_better/,7135,1576239091.0,0,,False,,,,,,,,
726,,pytorch,"  

Hi, I need production based nmt.I have gone through 

1. Rnn(BILSTM-with attention) is good but long term dependency is not good enough.

2. Transformer  using vanilla transformer of open-seq2seq of Nvidia but result is not up to mark

I am searching for some nmt using BERT but unable to find any implementation. I guess BERT is to solve some of general problem in nlp field.

If any suggestion exist, please help&gt;&gt;

Open to all suggestion…..",t2_10re4nwz,False,,0,False,Different type of neural machine translation arch?,[],r/pytorch,False,6,,0,,,False,t3_ea0wzx,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1576251071.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I need production based nmt.I have gone through &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Rnn(BILSTM-with attention) is good but long term dependency is not good enough.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Transformer  using vanilla transformer of open-seq2seq of Nvidia but result is not up to mark&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I am searching for some nmt using BERT but unable to find any implementation. I guess BERT is to solve some of general problem in nlp field.&lt;/p&gt;

&lt;p&gt;If any suggestion exist, please help&amp;gt;&amp;gt;&lt;/p&gt;

&lt;p&gt;Open to all suggestion…..&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ea0wzx,True,,amiya_mandal,,1,True,all_ads,False,[],False,,/r/pytorch/comments/ea0wzx/different_type_of_neural_machine_translation_arch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ea0wzx/different_type_of_neural_machine_translation_arch/,7135,1576222271.0,0,,False,,,,,,,,
727,,pytorch,"I am using [TensorboardX](https://github.com/lanpa/tensorboardX) to help me plot my rewards for my RL runs. I am using this block of code to track my progress

    if e % log_interval == 0:
        print(f'{e} episode | score: {np.mean(running_score):.2f} | epsilon: {epsilon:.5f}')
        writer.add_scalar('log/avg', np.mean(running_score), e)
        writer.add_scalar('log/loss', float(loss), e)
    
    if np.mean(running_score) &gt; goal_score:
        writer.add_scalar('log/avg', np.mean(running_score), e)
        writer.add_scalar('log/loss', float(loss), e)
        print(f'{env_name} solved in {e} episodes!!')
        torch.save(live_net.state_dict(), 'trained.pth')
        break

To view the plots I install tensorboard alone using `pip install tensorboard` and run `tensorboard --logdir logs/` to view them. If the training ends at about 250 episodes, the graphs get plotted only until 220th episode or so only. Even the last point plotting doesn't take place when the environment is solved (the 2nd ""if"" in the code block)

How do I solve this? I've tried running it multiple times but a similar problem comes everytime",t2_3m668ghw,False,,0,False,TensorboardX not plotting last few values,[],r/pytorch,False,6,,0,,,False,t3_e9mv85,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1576182158.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am using &lt;a href=""https://github.com/lanpa/tensorboardX""&gt;TensorboardX&lt;/a&gt; to help me plot my rewards for my RL runs. I am using this block of code to track my progress&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if e % log_interval == 0:
    print(f&amp;#39;{e} episode | score: {np.mean(running_score):.2f} | epsilon: {epsilon:.5f}&amp;#39;)
    writer.add_scalar(&amp;#39;log/avg&amp;#39;, np.mean(running_score), e)
    writer.add_scalar(&amp;#39;log/loss&amp;#39;, float(loss), e)

if np.mean(running_score) &amp;gt; goal_score:
    writer.add_scalar(&amp;#39;log/avg&amp;#39;, np.mean(running_score), e)
    writer.add_scalar(&amp;#39;log/loss&amp;#39;, float(loss), e)
    print(f&amp;#39;{env_name} solved in {e} episodes!!&amp;#39;)
    torch.save(live_net.state_dict(), &amp;#39;trained.pth&amp;#39;)
    break
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To view the plots I install tensorboard alone using &lt;code&gt;pip install tensorboard&lt;/code&gt; and run &lt;code&gt;tensorboard --logdir logs/&lt;/code&gt; to view them. If the training ends at about 250 episodes, the graphs get plotted only until 220th episode or so only. Even the last point plotting doesn&amp;#39;t take place when the environment is solved (the 2nd &amp;quot;if&amp;quot; in the code block)&lt;/p&gt;

&lt;p&gt;How do I solve this? I&amp;#39;ve tried running it multiple times but a similar problem comes everytime&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jIEGZFVNAfcdfPW_goDOFf1wHqS-eOBqLi6SWgX3JLM.jpg?auto=webp&amp;s=d416949cef9f6c3d374ec80fcc24ef22d25f16bb', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/jIEGZFVNAfcdfPW_goDOFf1wHqS-eOBqLi6SWgX3JLM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eceffaabba3a0f724affb2c33902ceb47388bb82', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/jIEGZFVNAfcdfPW_goDOFf1wHqS-eOBqLi6SWgX3JLM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a68df0e6004f6c96d57384f03fcbdf4f5f3955e0', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/jIEGZFVNAfcdfPW_goDOFf1wHqS-eOBqLi6SWgX3JLM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8324f6ee73065ec5d725c0449b1119083cc9938', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'es_G5_-pcyKho9gz_C0LeZnpBYESINrx2VJjcqzfE_o'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,e9mv85,True,,Syzygianinfern0,,2,True,all_ads,False,[],False,,/r/pytorch/comments/e9mv85/tensorboardx_not_plotting_last_few_values/,all_ads,False,https://www.reddit.com/r/pytorch/comments/e9mv85/tensorboardx_not_plotting_last_few_values/,7135,1576153358.0,0,,False,,,,,,,,
728,,pytorch,,t2_2fv4yodo,False,,0,False,Japanese Unicorn Preferred Networks Migrates Its DL Platform to PyTorch,[],r/pytorch,False,6,,0,58.0,,False,t3_e8stwz,False,dark,1.0,,public,9,0,{},140.0,,False,[],,False,False,,{},,False,9,,False,https://a.thumbs.redditmedia.com/hKGHNwbpMwbVlIzVevg3ixOUj-khsiq11YucoGGUTt0.jpg,False,,[],{},link,,False,,1576024062.0,text,6,,,text,medium.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/lKXwd1EmJrPfndYCZAVAQnfiaIcmE4Yb3UNYhQMqGTw.jpg?auto=webp&amp;s=4d3e8ba8af1fdbfc1b4f567c169c6d99e9415f8b', 'width': 1200, 'height': 504}, 'resolutions': [{'url': 'https://external-preview.redd.it/lKXwd1EmJrPfndYCZAVAQnfiaIcmE4Yb3UNYhQMqGTw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=87dd2639306378b8a48b152fe2ff732ff89d6e17', 'width': 108, 'height': 45}, {'url': 'https://external-preview.redd.it/lKXwd1EmJrPfndYCZAVAQnfiaIcmE4Yb3UNYhQMqGTw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=da6a9b242b9058e90f8423095856d01b4a6f98b7', 'width': 216, 'height': 90}, {'url': 'https://external-preview.redd.it/lKXwd1EmJrPfndYCZAVAQnfiaIcmE4Yb3UNYhQMqGTw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e002817fe363bfce7daf73d97ba3a5aa25903671', 'width': 320, 'height': 134}, {'url': 'https://external-preview.redd.it/lKXwd1EmJrPfndYCZAVAQnfiaIcmE4Yb3UNYhQMqGTw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=85f711fc3bb91634a2e23e2d641748c3713cf2b8', 'width': 640, 'height': 268}, {'url': 'https://external-preview.redd.it/lKXwd1EmJrPfndYCZAVAQnfiaIcmE4Yb3UNYhQMqGTw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=416925b30bf8620a521756318ad5f61252e5dd52', 'width': 960, 'height': 403}, {'url': 'https://external-preview.redd.it/lKXwd1EmJrPfndYCZAVAQnfiaIcmE4Yb3UNYhQMqGTw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6201bbd897fcea5b9236b2437dc5569d4a645d66', 'width': 1080, 'height': 453}], 'variants': {}, 'id': 'lbgBYbZsCTY5QZQJ6djL_t5YYDtYHsaXCDolnbqsygw'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,e8stwz,True,,Yuqing7,,0,True,all_ads,False,[],False,,/r/pytorch/comments/e8stwz/japanese_unicorn_preferred_networks_migrates_its/,all_ads,False,https://medium.com/syncedreview/japanese-unicorn-preferred-networks-migrates-its-dl-platform-to-pytorch-a509ac8f4ba0,7135,1575995262.0,0,,False,https://medium.com/syncedreview/japanese-unicorn-preferred-networks-migrates-its-dl-platform-to-pytorch-a509ac8f4ba0,,,,,,,
729,,pytorch,"Hi PyTorch Folks! My project [C++ Implementation of PyTorch Tutorials for Deep Learning Researchers](https://github.com/prabhuomkar/pytorch-cpp) is updated with Intermediate Tutorials containing:


* Convolutional Neural Network
* Deep Residual Network
* Recurrent Neural Network
* Bidirectional Recurrent Neural Network
* Language Model (RNN-LM)  

Thanks to [Markus Fleischhacker](https://github.com/mfl28) for his daily contributions which made this happen. Show some love by starring/forking/contributing to more tutorials/reporting issues. Cheers!",t2_3sk7t1ft,False,,0,False,PyTorch Intermediate Tutorials for Deep Learning Researchers in C++,[],r/pytorch,False,6,,0,,,False,t3_e8me30,False,dark,0.84,,public,20,0,{},,,False,[],,False,False,,{},,False,20,,False,self,False,,[],{},self,,True,,1575984723.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi PyTorch Folks! My project &lt;a href=""https://github.com/prabhuomkar/pytorch-cpp""&gt;C++ Implementation of PyTorch Tutorials for Deep Learning Researchers&lt;/a&gt; is updated with Intermediate Tutorials containing:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Convolutional Neural Network&lt;/li&gt;
&lt;li&gt;Deep Residual Network&lt;/li&gt;
&lt;li&gt;Recurrent Neural Network&lt;/li&gt;
&lt;li&gt;Bidirectional Recurrent Neural Network&lt;/li&gt;
&lt;li&gt;Language Model (RNN-LM)&lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thanks to &lt;a href=""https://github.com/mfl28""&gt;Markus Fleischhacker&lt;/a&gt; for his daily contributions which made this happen. Show some love by starring/forking/contributing to more tutorials/reporting issues. Cheers!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vYd8VKGQX9OaxAc5wndNSwIGlgHHhWgbkdG0nCE1AH4.jpg?auto=webp&amp;s=63c3d84df70176956c75074a9aaeb540b5189b37', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/vYd8VKGQX9OaxAc5wndNSwIGlgHHhWgbkdG0nCE1AH4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=adb8d0ee5c270ca980dcd4bcbb441203ad3ead7b', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/vYd8VKGQX9OaxAc5wndNSwIGlgHHhWgbkdG0nCE1AH4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=48ab078900c81087855d2c1527e892a88d65e108', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/vYd8VKGQX9OaxAc5wndNSwIGlgHHhWgbkdG0nCE1AH4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6882df409ae36ca06d13f72b648011df24f81614', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'kEq2_PteosY4v8ixLLUxyXw8ploOV8eSqfr7nqetP_o'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,e8me30,True,,op_prabhuomkar,,3,True,all_ads,False,[],False,,/r/pytorch/comments/e8me30/pytorch_intermediate_tutorials_for_deep_learning/,all_ads,False,https://www.reddit.com/r/pytorch/comments/e8me30/pytorch_intermediate_tutorials_for_deep_learning/,7135,1575955923.0,0,,False,,,,,,,,
730,,pytorch,How do I download all the documentation so that I can view all of it offline like a browsable HTML file?,t2_3m668ghw,False,,0,False,Make the Documentation Offline,[],r/pytorch,False,6,,0,,,False,t3_e86fqj,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1575903071.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do I download all the documentation so that I can view all of it offline like a browsable HTML file?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,e86fqj,True,,Syzygianinfern0,,1,True,all_ads,False,[],False,,/r/pytorch/comments/e86fqj/make_the_documentation_offline/,all_ads,False,https://www.reddit.com/r/pytorch/comments/e86fqj/make_the_documentation_offline/,7135,1575874271.0,0,,False,,,,,,,,
731,,pytorch,"We have a `dockerised` `flask` application, it takes an image via `API` call process it and returns the result in `JSON` format.

There is one module in the code where we need to use `PyTorch` model for inference, is there any way to deploy PyTorch model so that it doesn’t have to be loaded on each API call.",t2_pqthu,False,,0,False,How to deploy PyTorch model in production,[],r/pytorch,False,6,,0,,,False,t3_e5vamo,False,dark,1.0,,public,10,0,{},,,False,[],,False,False,,{},,False,10,,False,self,False,,[],{},,,True,,1575470804.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We have a &lt;code&gt;dockerised&lt;/code&gt; &lt;code&gt;flask&lt;/code&gt; application, it takes an image via &lt;code&gt;API&lt;/code&gt; call process it and returns the result in &lt;code&gt;JSON&lt;/code&gt; format.&lt;/p&gt;

&lt;p&gt;There is one module in the code where we need to use &lt;code&gt;PyTorch&lt;/code&gt; model for inference, is there any way to deploy PyTorch model so that it doesn’t have to be loaded on each API call.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,e5vamo,True,,atinesh229,,7,True,all_ads,False,[],False,,/r/pytorch/comments/e5vamo/how_to_deploy_pytorch_model_in_production/,all_ads,False,https://www.reddit.com/r/pytorch/comments/e5vamo/how_to_deploy_pytorch_model_in_production/,7135,1575442004.0,0,,False,,,,,,,,
732,,pytorch,"My encoder-decoder outputs a tensor of size (*N,s,b*), where *N* is the batch size, *s* is the sequence length, and *b* is the vocabulary size; this corresponds to a 3D tensor where the each word in the sequence is represented by a 1-hot vector, My target is another tensor of size (*N,s*); this corresponds to a 2D tensor of sequences of indices, where each index corresponds to the non-zero entry of a 1-hot vector. Which shapes should I use in order to evaluate my prediction and the target using `torch.nn.NLLLoss`?",t2_g70f4,False,,0,False,Using NLL Loss in a seq2seq model: how do I implement it?,[],r/pytorch,False,6,,0,,,False,t3_e3map6,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1575068169.0,,[],{},,,True,,1575095812.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My encoder-decoder outputs a tensor of size (&lt;em&gt;N,s,b&lt;/em&gt;), where &lt;em&gt;N&lt;/em&gt; is the batch size, &lt;em&gt;s&lt;/em&gt; is the sequence length, and &lt;em&gt;b&lt;/em&gt; is the vocabulary size; this corresponds to a 3D tensor where the each word in the sequence is represented by a 1-hot vector, My target is another tensor of size (&lt;em&gt;N,s&lt;/em&gt;); this corresponds to a 2D tensor of sequences of indices, where each index corresponds to the non-zero entry of a 1-hot vector. Which shapes should I use in order to evaluate my prediction and the target using &lt;code&gt;torch.nn.NLLLoss&lt;/code&gt;?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,e3map6,True,,Feasinde,,1,True,all_ads,False,[],False,,/r/pytorch/comments/e3map6/using_nll_loss_in_a_seq2seq_model_how_do_i/,all_ads,False,https://www.reddit.com/r/pytorch/comments/e3map6/using_nll_loss_in_a_seq2seq_model_how_do_i/,7135,1575067012.0,0,,False,,,,,,,,
733,,pytorch,"Hey I noticed my model learns a lot more when I’m passing big images although my transformations are the following:

    train_trans = transforms.Compose([
        transforms.Resize(256),
        transforms.RandomCrop(_image_size),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(_mean, _std),
    ])
    

When I feed 256x256 images directly, the model doesn’t learn much. This leads me to believe, something is learned prior to transformations. Can anyone confirm? Thanks",t2_16u5s20q,False,,0,False,Does the model learn before or after transformations?,[],r/pytorch,False,6,,0,,,False,t3_e19pip,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1574679807.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey I noticed my model learns a lot more when I’m passing big images although my transformations are the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_trans = transforms.Compose([
    transforms.Resize(256),
    transforms.RandomCrop(_image_size),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(_mean, _std),
])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When I feed 256x256 images directly, the model doesn’t learn much. This leads me to believe, something is learned prior to transformations. Can anyone confirm? Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,e19pip,True,,RegularConstant,,6,True,all_ads,False,[],False,,/r/pytorch/comments/e19pip/does_the_model_learn_before_or_after/,all_ads,False,https://www.reddit.com/r/pytorch/comments/e19pip/does_the_model_learn_before_or_after/,7135,1574651007.0,0,,False,,,,,,,,
734,,pytorch,"Is there a way to initialize it from a normal distribution?

&amp;#x200B;

I saw this [http://anie.me/On-Torchtext/](http://anie.me/On-Torchtext/) but I was hoping there might be a better way now",t2_174d6j,False,,0,False,build_vocab - randomly initialize word vectors instead of using one hot vector encoding,[],r/pytorch,False,6,,0,,,False,t3_e180ys,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1574672028.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there a way to initialize it from a normal distribution?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I saw this &lt;a href=""http://anie.me/On-Torchtext/""&gt;http://anie.me/On-Torchtext/&lt;/a&gt; but I was hoping there might be a better way now&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/p0EdZb5n148wSyO9oibTOKGHE9x4cdM6T-0k_TUtA_w.jpg?auto=webp&amp;s=1446fc1ed2e0a97f467f9e6fc4d9655aeaf6a369', 'width': 256, 'height': 256}, 'resolutions': [{'url': 'https://external-preview.redd.it/p0EdZb5n148wSyO9oibTOKGHE9x4cdM6T-0k_TUtA_w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7b8ee0e1755b6e1aed1ae7ca72ca1fb8fb2a43a0', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/p0EdZb5n148wSyO9oibTOKGHE9x4cdM6T-0k_TUtA_w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=62cb43bc2c8b54de9bc60ab62f9ee6c91df738e5', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'RKIJLPQOKIcEOb6C2G4TyPHx5NOrlkIwx2NvlhQGmUA'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,e180ys,True,,Algogator,,0,True,all_ads,False,[],False,,/r/pytorch/comments/e180ys/build_vocab_randomly_initialize_word_vectors/,all_ads,False,https://www.reddit.com/r/pytorch/comments/e180ys/build_vocab_randomly_initialize_word_vectors/,7135,1574643228.0,0,,False,,,,,,,,
735,,pytorch,"This is probably posted often, but I cant seem to get pytorch installed. I tried using pycharm to install it but it came up with an error. So I ran this command 

 pip3 install torch===1.3.1 torchvision===0.4.2 -f https://download.pytorch.org/whl/torch\_stable.html 

in the cmd, but it came up with the error of 

Could not find a version that satisfies the requirement 1.3.1 (from versions: )                                                                                 No matching distribution found for 1.3.1

Any help would be great. Probably a simple fix.",t2_bwhxp,False,,0,False,Trying to install pycharm,[],r/pytorch,False,6,,0,,,False,t3_dyzmen,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1574270305.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is probably posted often, but I cant seem to get pytorch installed. I tried using pycharm to install it but it came up with an error. So I ran this command &lt;/p&gt;

&lt;p&gt;pip3 install torch===1.3.1 torchvision===0.4.2 -f &lt;a href=""https://download.pytorch.org/whl/torch%5C_stable.html""&gt;https://download.pytorch.org/whl/torch\_stable.html&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;in the cmd, but it came up with the error of &lt;/p&gt;

&lt;p&gt;Could not find a version that satisfies the requirement 1.3.1 (from versions: )                                                                                 No matching distribution found for 1.3.1&lt;/p&gt;

&lt;p&gt;Any help would be great. Probably a simple fix.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dyzmen,True,,Highfivesghost,,4,True,all_ads,False,[],False,,/r/pytorch/comments/dyzmen/trying_to_install_pycharm/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dyzmen/trying_to_install_pycharm/,7135,1574241505.0,0,,False,,,,,,,,
736,,pytorch,"Hi, 

I was wondering what the default dtype is when we initialize a pytorch tensor such as: 

`t = torch.tensor([1, 2, 3])`

Is it long?",t2_3v9mpxbv,False,,0,False,Default tensor type in PyTorch?,[],r/pytorch,False,6,,0,,,False,t3_dyrxle,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1574230926.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;I was wondering what the default dtype is when we initialize a pytorch tensor such as: &lt;/p&gt;

&lt;p&gt;&lt;code&gt;t = torch.tensor([1, 2, 3])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Is it long?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dyrxle,True,,UniformlyConvergent,,7,True,all_ads,False,[],False,,/r/pytorch/comments/dyrxle/default_tensor_type_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dyrxle/default_tensor_type_in_pytorch/,7135,1574202126.0,0,,False,,,,,,,,
737,,pytorch,"Hi guys, 

I have created a deep learning quick start template for PyTorch. It includes:

* modularity: we split each logic piece into a different python submodule
* data-augmentation: we included [imgaug](https://imgaug.readthedocs.io/en/latest/)
* ready to go: by using [poutyne](https://pypi.org/project/Poutyne/) a Keras-like framework you don't have to write any train loop.
* [torchsummary](https://github.com/sksq96/pytorch-summary) to show a summary of your models
* reduce the learning rate on a plateau
* auto-saving the best model
* experiment tracking with [comet](https://www.comet.ml/)

[https://github.com/FrancescoSaverioZuppichini/PyTorch-Deep-Learning-Template](https://github.com/FrancescoSaverioZuppichini/PyTorch-Deep-Learning-Template)

Let me know if you find it useful :)",t2_gw9fw2x,False,,0,False,Deep Learning Template,[],r/pytorch,False,6,,0,,,False,t3_dyhg1k,False,dark,0.93,,public,11,0,{},,,False,[],,False,False,,{},,False,11,,False,self,False,,[],{},self,,True,,1574179037.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys, &lt;/p&gt;

&lt;p&gt;I have created a deep learning quick start template for PyTorch. It includes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;modularity: we split each logic piece into a different python submodule&lt;/li&gt;
&lt;li&gt;data-augmentation: we included &lt;a href=""https://imgaug.readthedocs.io/en/latest/""&gt;imgaug&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ready to go: by using &lt;a href=""https://pypi.org/project/Poutyne/""&gt;poutyne&lt;/a&gt; a Keras-like framework you don&amp;#39;t have to write any train loop.&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/sksq96/pytorch-summary""&gt;torchsummary&lt;/a&gt; to show a summary of your models&lt;/li&gt;
&lt;li&gt;reduce the learning rate on a plateau&lt;/li&gt;
&lt;li&gt;auto-saving the best model&lt;/li&gt;
&lt;li&gt;experiment tracking with &lt;a href=""https://www.comet.ml/""&gt;comet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=""https://github.com/FrancescoSaverioZuppichini/PyTorch-Deep-Learning-Template""&gt;https://github.com/FrancescoSaverioZuppichini/PyTorch-Deep-Learning-Template&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let me know if you find it useful :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BydiJYdIOF8H7w8d5HObYw3dmIU-w3j0AtAUdSTYsZE.jpg?auto=webp&amp;s=0fbb1eccd84897e510e019459c4932323023d394', 'width': 521, 'height': 261}, 'resolutions': [{'url': 'https://external-preview.redd.it/BydiJYdIOF8H7w8d5HObYw3dmIU-w3j0AtAUdSTYsZE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e24776651cf2dd5cd0bf1364c8703cd7ecacb1c', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/BydiJYdIOF8H7w8d5HObYw3dmIU-w3j0AtAUdSTYsZE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1522315556d9f06ca7e50cea6439894ee14d17bf', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/BydiJYdIOF8H7w8d5HObYw3dmIU-w3j0AtAUdSTYsZE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c3de2659020044eed2a6c3626eed5131fb8e1f3', 'width': 320, 'height': 160}], 'variants': {}, 'id': 'OvmpHWs3fu2rB6Tk0f8NZNVDenpEp-HWdQ5w8PJIUvc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dyhg1k,True,,FrancescoSZ,,3,True,all_ads,False,[],False,,/r/pytorch/comments/dyhg1k/deep_learning_template/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dyhg1k/deep_learning_template/,7135,1574150237.0,0,,False,,,,,,,,
738,,pytorch,,t2_4zecvko2,False,,0,False,[Q] Low performance with CUDA-enabled Pytorch,[],r/pytorch,False,6,,0,,,False,t3_dxcnk1,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,default,False,,[],{},,,False,,1573966929.0,text,6,,,text,self.MLQuestions,False,,,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dxcnk1,True,,SnakeAI,,4,True,all_ads,False,[],False,,/r/pytorch/comments/dxcnk1/q_low_performance_with_cudaenabled_pytorch/,all_ads,False,https://old.reddit.com/r/MLQuestions/comments/dub70v/q_low_performance_with_cuda_pytorch/,7135,1573938129.0,0,,False,https://old.reddit.com/r/MLQuestions/comments/dub70v/q_low_performance_with_cuda_pytorch/,"[{'approved_at_utc': None, 'subreddit': 'MLQuestions', 'selftext': ""Hello \n\nI would like to ask about the use of GPU with pytorch.\nI recently came from Tensorflow (2.0) to learn with Pytorch (1.13)\n\nBut when coming down to training, my model takes around ~30 to 40s per epoch where Tensorflow/Keras took 1s per epoch for the same dataset and same hyperparameters for the same network. While training with Pytorch, my CPU usage is peaking usually high which doesn't happen while using TF\n\nI am working with a small dataset (3 classes with ~400 training examples each)\n\nI declared my device with\n\n    if torch.cuda.is_available():\n        gpu = torch.device('cuda')\n\nTransferred my net to the gpu:\n\n    net = CNNet()\n    net.to(device=gpu)\n\nTraining code:\n\n    stop = time.time()\n    MAX_EPOCHS = 2\n    BATCH_SIZE = 15\n    MAX_BATCH = len(dataloader)-1\n    for epoch in range(MAX_EPOCHS):  # loop over the dataset multiple times\n        running_loss = 0.0\n        for i_batch, batch_data in enumerate(dataloader):\n            # get the inputs\n            inputs = batch_data['image'].to(gpu)\n            labels = batch_data['label'].to(gpu)\n            # zero the parameter gradients\n            optimizer.zero_grad()\n            # forward + backward + optimize\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            # print loss and elapsed time\n            running_loss += loss.item()\n            if i_batch == MAX_BATCH:\n                now = time.time()\n                elapsed = now - stop\n                print('Epoch # %d, loss: %.3f  Elapsed time %.4f s' %\n                (epoch + 1, running_loss / MAX_BATCH,elapsed))\n                running_loss = 0.0\n                stop = now\n\nI appreciate your help."", 'author_fullname': 't2_4zecvko2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[Q] Low performance with CUDA Pytorch', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MLQuestions', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_dub70v', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 11, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1573419970.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MLQuestions', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello &lt;/p&gt;\n\n&lt;p&gt;I would like to ask about the use of GPU with pytorch.\nI recently came from Tensorflow (2.0) to learn with Pytorch (1.13)&lt;/p&gt;\n\n&lt;p&gt;But when coming down to training, my model takes around ~30 to 40s per epoch where Tensorflow/Keras took 1s per epoch for the same dataset and same hyperparameters for the same network. While training with Pytorch, my CPU usage is peaking usually high which doesn&amp;#39;t happen while using TF&lt;/p&gt;\n\n&lt;p&gt;I am working with a small dataset (3 classes with ~400 training examples each)&lt;/p&gt;\n\n&lt;p&gt;I declared my device with&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;if torch.cuda.is_available():\n    gpu = torch.device(&amp;#39;cuda&amp;#39;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Transferred my net to the gpu:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;net = CNNet()\nnet.to(device=gpu)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Training code:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;stop = time.time()\nMAX_EPOCHS = 2\nBATCH_SIZE = 15\nMAX_BATCH = len(dataloader)-1\nfor epoch in range(MAX_EPOCHS):  # loop over the dataset multiple times\n    running_loss = 0.0\n    for i_batch, batch_data in enumerate(dataloader):\n        # get the inputs\n        inputs = batch_data[&amp;#39;image&amp;#39;].to(gpu)\n        labels = batch_data[&amp;#39;label&amp;#39;].to(gpu)\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        # print loss and elapsed time\n        running_loss += loss.item()\n        if i_batch == MAX_BATCH:\n            now = time.time()\n            elapsed = now - stop\n            print(&amp;#39;Epoch # %d, loss: %.3f  Elapsed time %.4f s&amp;#39; %\n            (epoch + 1, running_loss / MAX_BATCH,elapsed))\n            running_loss = 0.0\n            stop = now\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I appreciate your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_30rel', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'dub70v', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'SnakeAI', 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MLQuestions/comments/dub70v/q_low_performance_with_cuda_pytorch/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MLQuestions/comments/dub70v/q_low_performance_with_cuda_pytorch/', 'subreddit_subscribers': 31266, 'created_utc': 1573391170.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_dub70v,,,,,
739,,pytorch,"I've been learning and implementing the Neuro-Evolution of Augmented Topologies (NEAT) algorithm. It's like an evolutionary algorithm, where it starts with a very basic neural net, and makes random modifications by adding new nodes and/or modifying weights (changing values, making new connections, or even turning them off or back on).

I'm pretty much brand new to PyTorch, and a tiny bit familiar with FastAI.  I'm wanting to take my NEAT generated networks and pass data through them to find how accurate it is. That gives me a fitness score to help the NEAT algorithm continue.

FastAI is too high level, from what I can tell, to implement custom networks. What's the highest level data types, of PyTorch, that I can use to make custom networks from a given map of nodes and weights? Are there any functions that can help me automate some of this construction?",t2_10vyes,False,,0,False,Highest level data types for building custom network,[],r/pytorch,False,6,,0,,,False,t3_dw5l0k,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1573713890.0,,[],{},,,True,,1573742508.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been learning and implementing the Neuro-Evolution of Augmented Topologies (NEAT) algorithm. It&amp;#39;s like an evolutionary algorithm, where it starts with a very basic neural net, and makes random modifications by adding new nodes and/or modifying weights (changing values, making new connections, or even turning them off or back on).&lt;/p&gt;

&lt;p&gt;I&amp;#39;m pretty much brand new to PyTorch, and a tiny bit familiar with FastAI.  I&amp;#39;m wanting to take my NEAT generated networks and pass data through them to find how accurate it is. That gives me a fitness score to help the NEAT algorithm continue.&lt;/p&gt;

&lt;p&gt;FastAI is too high level, from what I can tell, to implement custom networks. What&amp;#39;s the highest level data types, of PyTorch, that I can use to make custom networks from a given map of nodes and weights? Are there any functions that can help me automate some of this construction?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dw5l0k,True,,Cupofcalculus,,1,True,all_ads,False,[],False,,/r/pytorch/comments/dw5l0k/highest_level_data_types_for_building_custom/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dw5l0k/highest_level_data_types_for_building_custom/,7135,1573713708.0,0,,False,,,,,,,,
740,,pytorch,"I am trying to understand exactly what torch.nn.LayerNorm is doing, when it is given elementwise\_affine = True and eps = 1e-5.

Let x be a tensor, where x.shape  
 returns torch.Size(\[1280\])

I want to understand how y is assigned below:

self.ln\_1 = nn.LayerNorm(1280, eps=1e-05)

y = self.ln\_1(x)

Is the above code equivalent to this?

y = (x - torch.mean(x))/((torch.var(x) + 1e-5)\*\*.5)

I tried it out and the outputs aren't equivalent. What do I have wrong?",t2_128h0c,False,,0,False,"I've read the documentation, still can't figure what exactly torch.nn.LayerNorm is doing",[],r/pytorch,False,6,,0,,,False,t3_dw3ker,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1573731215.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to understand exactly what torch.nn.LayerNorm is doing, when it is given elementwise_affine = True and eps = 1e-5.&lt;/p&gt;

&lt;p&gt;Let x be a tensor, where x.shape&lt;br/&gt;
 returns torch.Size([1280])&lt;/p&gt;

&lt;p&gt;I want to understand how y is assigned below:&lt;/p&gt;

&lt;p&gt;self.ln_1 = nn.LayerNorm(1280, eps=1e-05)&lt;/p&gt;

&lt;p&gt;y = self.ln_1(x)&lt;/p&gt;

&lt;p&gt;Is the above code equivalent to this?&lt;/p&gt;

&lt;p&gt;y = (x - torch.mean(x))/((torch.var(x) + 1e-5)**.5)&lt;/p&gt;

&lt;p&gt;I tried it out and the outputs aren&amp;#39;t equivalent. What do I have wrong?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dw3ker,True,,synysterbates,,4,True,all_ads,False,[],False,,/r/pytorch/comments/dw3ker/ive_read_the_documentation_still_cant_figure_what/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dw3ker/ive_read_the_documentation_still_cant_figure_what/,7135,1573702415.0,0,,False,,,,,,,,
741,,pytorch,,t2_1ffz9tjt,False,,0,False,[AI application with source code] Let your machine play Street Fighter!,[],r/pytorch,False,6,,0,81.0,,False,t3_dvicbv,False,dark,0.9,,public,17,0,{},140.0,,False,[],,True,False,,{},,False,17,,False,https://a.thumbs.redditmedia.com/IQw8K0izdrnvxVPgidzaPYNqYmq8GvtO2uPmmWrva78.jpg,False,,[],{},image,,False,,1573629146.0,text,6,,,text,i.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/acx30egz7cy31.gif?format=png8&amp;s=881029a65c154d7861b13a1d986ae06583d5b25c', 'width': 384, 'height': 224}, 'resolutions': [{'url': 'https://preview.redd.it/acx30egz7cy31.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=82d82d0526eb2b240762c35e12ca85b62920a04a', 'width': 108, 'height': 63}, {'url': 'https://preview.redd.it/acx30egz7cy31.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=eff843b035233e1eecfd1745daf01581a56a34b0', 'width': 216, 'height': 126}, {'url': 'https://preview.redd.it/acx30egz7cy31.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=e2f2afc8585cf3bc8cae8828c1b07cbfbd28d4cb', 'width': 320, 'height': 186}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/acx30egz7cy31.gif?s=fd969fe2edd4d97bf163da5b1435aecd3e6e09bb', 'width': 384, 'height': 224}, 'resolutions': [{'url': 'https://preview.redd.it/acx30egz7cy31.gif?width=108&amp;crop=smart&amp;s=007bc588e3370e7eaea5ecc8b7c9fd8742deda20', 'width': 108, 'height': 63}, {'url': 'https://preview.redd.it/acx30egz7cy31.gif?width=216&amp;crop=smart&amp;s=01e28bb250fbfc361e05457466d781e02dd0db9e', 'width': 216, 'height': 126}, {'url': 'https://preview.redd.it/acx30egz7cy31.gif?width=320&amp;crop=smart&amp;s=b3df21bcfc484ddf340bc43cad6df0c8b0a21d98', 'width': 320, 'height': 186}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/acx30egz7cy31.gif?format=mp4&amp;s=dbd1baebbb2b6ef3bf7c2375a3fac5bebdd8c439', 'width': 384, 'height': 224}, 'resolutions': [{'url': 'https://preview.redd.it/acx30egz7cy31.gif?width=108&amp;format=mp4&amp;s=c35cadda56e31428b0bc4aa5751d49a5078189db', 'width': 108, 'height': 63}, {'url': 'https://preview.redd.it/acx30egz7cy31.gif?width=216&amp;format=mp4&amp;s=27c2477b189edc718ae1506bea0e7aae193cb686', 'width': 216, 'height': 126}, {'url': 'https://preview.redd.it/acx30egz7cy31.gif?width=320&amp;format=mp4&amp;s=0ea551434c4f27e31b4069763ad3417b1dd8d926', 'width': 320, 'height': 186}]}}, 'id': 'Zw_73pnP3JS4VeaYcCrchDHvhmRy8NuRSSvxDpjg8vg'}], 'enabled': True}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dvicbv,True,,1991viet,,1,True,all_ads,False,[],False,,/r/pytorch/comments/dvicbv/ai_application_with_source_code_let_your_machine/,all_ads,False,https://i.redd.it/acx30egz7cy31.gif,7135,1573600346.0,0,,False,https://i.redd.it/acx30egz7cy31.gif,,,,,,,
742,,pytorch,"Hi everyone,
I am trying to work with the LibriSpeech dataset to build an ASR model with LSTM layers.  
It is still not very clear to me how I should preprocess the data correctly. I have a list of input arrays which are the MFCC values for each whole sentence file, and a list of labels.  
I padded all the input MFCC vectors to have the same lenght as the longest audio file.  
In order to get the labels I mapped character indexes to each character in the sentence, so that

```
label_i = ""how are you?""  
vectorlabel_i = np.array([4,2,5,0,11,22,0,25,2,14]) 
```
  
I then padded the labels as well with a blank character to the longest sentence length.
  
It is still not clear to me, however, how I should move from here in order to feed inputs and labels into the model  .
Is each whole sentence a single input? Or should I feed batches of frames (e.g. a sentence with sequence 640 fed in batches of 32)?  
Is the format of the label correct? I don’t think I should use an embedder given how the output is supposed to be.  
  
Thanks for reading",t2_ytvdx,False,,0,False,Trying to understand targets in ASR with CTC loss,[],r/pytorch,False,6,,0,,,False,t3_duwl8r,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1573525903.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,
I am trying to work with the LibriSpeech dataset to build an ASR model with LSTM layers.&lt;br/&gt;
It is still not very clear to me how I should preprocess the data correctly. I have a list of input arrays which are the MFCC values for each whole sentence file, and a list of labels.&lt;br/&gt;
I padded all the input MFCC vectors to have the same lenght as the longest audio file.&lt;br/&gt;
In order to get the labels I mapped character indexes to each character in the sentence, so that&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
label_i = &amp;quot;how are you?&amp;quot;  
vectorlabel_i = np.array([4,2,5,0,11,22,0,25,2,14]) 
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I then padded the labels as well with a blank character to the longest sentence length.&lt;/p&gt;

&lt;p&gt;It is still not clear to me, however, how I should move from here in order to feed inputs and labels into the model  .
Is each whole sentence a single input? Or should I feed batches of frames (e.g. a sentence with sequence 640 fed in batches of 32)?&lt;br/&gt;
Is the format of the label correct? I don’t think I should use an embedder given how the output is supposed to be.  &lt;/p&gt;

&lt;p&gt;Thanks for reading&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,duwl8r,True,,carnivorousdrew,,1,True,all_ads,False,[],False,,/r/pytorch/comments/duwl8r/trying_to_understand_targets_in_asr_with_ctc_loss/,all_ads,False,https://www.reddit.com/r/pytorch/comments/duwl8r/trying_to_understand_targets_in_asr_with_ctc_loss/,7135,1573497103.0,0,,False,,,,,,,,
743,,pytorch,I asked a question about this to a few people I know and they didn't know how to answer. I also asked it on the pytorch ([https://discuss.pytorch.org/t/does-pytorch-kl-divergence-take-care-of-reparameterization-and-elbo/60439?u=deltaskelta](https://discuss.pytorch.org/t/does-pytorch-kl-divergence-take-care-of-reparameterization-and-elbo/60439?u=deltaskelta)) forum and I have not gotten an answer yet so I am very curious...,t2_gwrnl,False,,0,False,Does the pytorch KL divergence function replace reparameterization and ELBO?,[],r/pytorch,False,6,,0,,,False,t3_durq66,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},self,,True,,1573504136.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I asked a question about this to a few people I know and they didn&amp;#39;t know how to answer. I also asked it on the pytorch (&lt;a href=""https://discuss.pytorch.org/t/does-pytorch-kl-divergence-take-care-of-reparameterization-and-elbo/60439?u=deltaskelta""&gt;https://discuss.pytorch.org/t/does-pytorch-kl-divergence-take-care-of-reparameterization-and-elbo/60439?u=deltaskelta&lt;/a&gt;) forum and I have not gotten an answer yet so I am very curious...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,durq66,True,,delta_skelta,,2,True,all_ads,False,[],False,,/r/pytorch/comments/durq66/does_the_pytorch_kl_divergence_function_replace/,all_ads,False,https://www.reddit.com/r/pytorch/comments/durq66/does_the_pytorch_kl_divergence_function_replace/,7135,1573475336.0,0,,False,,,,,,,,
744,,pytorch,"Hey, i wanted to know how good a Faster R CNN works on a 3000,4000px image which contains very small objects (they fit in a ~50,50px area and appear only 1 to ~10 times per image). Thank you :)",,False,,0,False,Faster R CNN performance question,[],r/pytorch,False,6,,0,,,False,t3_dtuys9,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,,self,False,,,{},,,True,,1573328849.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, i wanted to know how good a Faster R CNN works on a 3000,4000px image which contains very small objects (they fit in a ~50,50px area and appear only 1 to ~10 times per image). Thank you :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dtuys9,True,,[deleted],,9,True,all_ads,False,[],,dark,/r/pytorch/comments/dtuys9/faster_r_cnn_performance_question/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dtuys9/faster_r_cnn_performance_question/,7135,1573300049.0,0,,False,,,,,,,,
745,,pytorch,"I wrote a Deep Learning code to classify cats and dogs images using CNN compiling from here and there.

&amp;#x200B;

This is a great resource: [https://www.deeplearningwizard.com/](https://www.deeplearningwizard.com/)

&amp;#x200B;

My code doesn't go through all the number of iterations it should go through. It stops after the first iteration. Can anybody help finding me why so... I've tried looking for what's going wrong, but unable to find one. Thank You.

&amp;#x200B;

The problem must be in the training loop itself.

&amp;#x200B;

`import os`

`from PIL import Image`

`# import numpy as np`

`from torchvision import transforms`

`import torch.nn.functional as F`

`import pandas as pd`

`import torch`

`import` [`torch.utils.data`](https://torch.utils.data) `as utils`

`import torch.nn as nn`

&amp;#x200B;

`## Get the list of all images`

`files = os.listdir(""dataset/val/cats"")`

`files.remove("".DS_Store"")`

`print(files[0])`

&amp;#x200B;

`## list of images`

`Images_train = []`

`Images_val = []`

&amp;#x200B;

`############### IF IT'S A CAT, it's '0' and IF IT'S A DOG, it's '1' #######################`

&amp;#x200B;

`# Labels = []`

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

`## defining required transforms or preprocessing on the images`

`data_transforms = transforms.Compose([`

`transforms.RandomResizedCrop(32),`

`transforms.ToTensor(),`

`transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])`

`])`

&amp;#x200B;

`## reading the images applying the transformations, converting each of them to pytorch tensors and storing them in images list`

`for i in files:`

    `image = os.path.join(""dataset/val/cats"",i)`
    
    `im =` [`Image.open`](https://Image.open)`(image)`
    
    `# imm = np.asarray(im)`
    
    `im = data_transforms(im)`
    
    `# Images.append(im)`
    
    `# Labels.append(0)`
    
    `Images_val.append([im, 0])`

&amp;#x200B;

&amp;#x200B;

`files = os.listdir(""dataset/train/cats"")`

`files.remove("".DS_Store"")`

`print(files[0])`

&amp;#x200B;

&amp;#x200B;

`for i in files:`

    `image = os.path.join(""dataset/train/cats"",i)`
    
    `im =` [`Image.open`](https://Image.open)`(image)`
    
    `# imm = np.asarray(im)`
    
    `im = data_transforms(im)`
    
    `# Images.append(im)`
    
    `# Labels.append('cat')`
    
    `Images_train.append([im, 0])`

&amp;#x200B;

&amp;#x200B;

`files = os.listdir(""dataset/val/dogs"")`

`files.remove("".DS_Store"")`

`print(files[0])`

&amp;#x200B;

&amp;#x200B;

`for i in files:`

    `image = os.path.join(""dataset/val/dogs"",i)`
    
    `im =` [`Image.open`](https://Image.open)`(image)`
    
    `# imm = np.asarray(im)`
    
    `im = data_transforms(im)`
    
    `# Images.append(im)`
    
    `# Labels.append('dog')`
    
    `Images_val.append([im, 1])`

&amp;#x200B;

&amp;#x200B;

`files = os.listdir(""dataset/train/dogs"")`

`files.remove("".DS_Store"")`

`print(files[0])`

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

`for i in files:`

    `image = os.path.join(""dataset/train/dogs"",i)`
    
    `im =` [`Image.open`](https://Image.open)`(image)`
    
    `# imm = np.asarray(im)`
    
    `im = data_transforms(im)`
    
    `# Images.append(im)`
    
    `# Labels.append('dog')`
    
    `Images_train.append([im, 1])`

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

`# df = pd.DataFrame(Images, columns=['Image', 'Label'])`

`# print(df.head())`

&amp;#x200B;

`print(Images_val[0])`

`print(Images_val[0][0])`

`print(Images_val[0][0][0])`

&amp;#x200B;

`print(len(Images_train))`

&amp;#x200B;

&amp;#x200B;

`batch_size = 100`

`n_iters = 800`

`num_epochs = n_iters / (len(Images_train) / batch_size)`

`num_epochs = int(num_epochs)`

`#tensor_x = torch.stack([torch.Tensor(i) for i in Images_val])`

`train_loader = torch.utils.data.DataLoader(dataset=Images_val,`

`batch_size=batch_size,`

`shuffle=True)`

&amp;#x200B;

`test_loader = torch.utils.data.DataLoader(dataset=Images_val,`

`batch_size=batch_size,`

`shuffle=False)`

&amp;#x200B;

&amp;#x200B;

`class SimpleCNN(torch.nn.Module):`

`#Our batch shape for input x is (3, 32, 32)`

`def __init__(self):`

`super(SimpleCNN, self).__init__()`

`#Input channels = 3, output channels = 18`

`self.conv1 = torch.nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)`

`self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)`

`#4608 input features, 64 output features (see sizing flow below)`

`self.fc1 = torch.nn.Linear(18 * 16 * 16, 64)`

`#64 input features, 10 output features for our 10 defined classes`

`self.fc2 = torch.nn.Linear(64, 2)`

`def forward(self, x):`

`#Computes the activation of the first convolution`

`#Size changes from (3, 32, 32) to (18, 32, 32)`

`x = F.relu(self.conv1(x))`

`#Size changes from (18, 32, 32) to (18, 16, 16)`

`x = self.pool(x)`

`#Reshape data to input to the input layer of the neural net`

`#Size changes from (18, 16, 16) to (1, 4608)`

`#Recall that the -1 infers this dimension from the other given dimension`

`x = x.view(-1, 18 * 16 *16)`

`#Computes the activation of the first fully connected layer`

`#Size changes from (1, 4608) to (1, 64)`

`x = F.relu(self.fc1(x))`

`#Computes the second fully connected layer (activation applied later)`

`#Size changes from (1, 64) to (1, 10)`

`x = self.fc2(x)`

`return(x)`

&amp;#x200B;

&amp;#x200B;

`model = SimpleCNN()`

`criterion = nn.CrossEntropyLoss()`

&amp;#x200B;

`learning_rate = 0.01`

&amp;#x200B;

`optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)`

&amp;#x200B;

`iter = 0`

`for epoch in range(num_epochs):`

`for i, (images, labels) in enumerate(train_loader):`

`# Load images`

`images = images.requires_grad_()`

&amp;#x200B;

`# Clear gradients w.r.t. parameters`

`optimizer.zero_grad()`

&amp;#x200B;

`# Forward pass to get output/logits`

`outputs = model(images)`

&amp;#x200B;

`# Calculate Loss: softmax --&gt; cross entropy loss`

`loss = criterion(outputs, labels)`

&amp;#x200B;

`# Getting gradients w.r.t. parameters`

`loss.backward()`

&amp;#x200B;

`# Updating parameters`

`optimizer.step()`

&amp;#x200B;

`iter += 1`

&amp;#x200B;

`if iter % 200 == 0:`

`# Calculate Accuracy`

`correct = 0`

`total = 0`

`# Iterate through test dataset`

`for images, labels in test_loader:`

`# Load images`

`images = images.requires_grad_()`

&amp;#x200B;

`# Forward pass only to get logits/output`

`outputs = model(images)`

&amp;#x200B;

`# Get predictions from the maximum value`

`_, predicted = torch.max(`[`outputs.data`](https://outputs.data)`, 1)`

&amp;#x200B;

`# Total number of labels`

`total += labels.size(0)`

&amp;#x200B;

`# Total correct predictions`

`correct += (predicted == labels).sum()`

&amp;#x200B;

`accuracy = 100 * correct / total`

&amp;#x200B;

`# Print Loss`

`print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))`",t2_29ruz6z1,False,,0,False,"Hello everyone, I'm a beginner in Deep Learning and need your help on this one.",[],r/pytorch,False,6,,0,,,False,t3_dtu8ce,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1573323336.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I wrote a Deep Learning code to classify cats and dogs images using CNN compiling from here and there.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;This is a great resource: &lt;a href=""https://www.deeplearningwizard.com/""&gt;https://www.deeplearningwizard.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;My code doesn&amp;#39;t go through all the number of iterations it should go through. It stops after the first iteration. Can anybody help finding me why so... I&amp;#39;ve tried looking for what&amp;#39;s going wrong, but unable to find one. Thank You.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The problem must be in the training loop itself.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;import os&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;from PIL import Image&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# import numpy as np&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;from torchvision import transforms&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;import torch.nn.functional as F&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;import pandas as pd&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;import torch&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;import&lt;/code&gt; &lt;a href=""https://torch.utils.data""&gt;&lt;code&gt;torch.utils.data&lt;/code&gt;&lt;/a&gt; &lt;code&gt;as utils&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;import torch.nn as nn&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;## Get the list of all images&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;files = os.listdir(&amp;quot;dataset/val/cats&amp;quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;files.remove(&amp;quot;.DS_Store&amp;quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;## list of images&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Images_train = []&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Images_val = []&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;############### IF IT&amp;#39;S A CAT, it&amp;#39;s &amp;#39;0&amp;#39; and IF IT&amp;#39;S A DOG, it&amp;#39;s &amp;#39;1&amp;#39; #######################&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Labels = []&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;## defining required transforms or preprocessing on the images&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;data_transforms = transforms.Compose([&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;transforms.RandomResizedCrop(32),&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;transforms.ToTensor(),&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;## reading the images applying the transformations, converting each of them to pytorch tensors and storing them in images list&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for i in files:&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`image = os.path.join(&amp;quot;dataset/val/cats&amp;quot;,i)`

`im =` [`Image.open`](https://Image.open)`(image)`

`# imm = np.asarray(im)`

`im = data_transforms(im)`

`# Images.append(im)`

`# Labels.append(0)`

`Images_val.append([im, 0])`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;files = os.listdir(&amp;quot;dataset/train/cats&amp;quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;files.remove(&amp;quot;.DS_Store&amp;quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for i in files:&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`image = os.path.join(&amp;quot;dataset/train/cats&amp;quot;,i)`

`im =` [`Image.open`](https://Image.open)`(image)`

`# imm = np.asarray(im)`

`im = data_transforms(im)`

`# Images.append(im)`

`# Labels.append(&amp;#39;cat&amp;#39;)`

`Images_train.append([im, 0])`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;files = os.listdir(&amp;quot;dataset/val/dogs&amp;quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;files.remove(&amp;quot;.DS_Store&amp;quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for i in files:&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`image = os.path.join(&amp;quot;dataset/val/dogs&amp;quot;,i)`

`im =` [`Image.open`](https://Image.open)`(image)`

`# imm = np.asarray(im)`

`im = data_transforms(im)`

`# Images.append(im)`

`# Labels.append(&amp;#39;dog&amp;#39;)`

`Images_val.append([im, 1])`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;files = os.listdir(&amp;quot;dataset/train/dogs&amp;quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;files.remove(&amp;quot;.DS_Store&amp;quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for i in files:&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`image = os.path.join(&amp;quot;dataset/train/dogs&amp;quot;,i)`

`im =` [`Image.open`](https://Image.open)`(image)`

`# imm = np.asarray(im)`

`im = data_transforms(im)`

`# Images.append(im)`

`# Labels.append(&amp;#39;dog&amp;#39;)`

`Images_train.append([im, 1])`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# df = pd.DataFrame(Images, columns=[&amp;#39;Image&amp;#39;, &amp;#39;Label&amp;#39;])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# print(df.head())&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(Images_val[0])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(Images_val[0][0])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(Images_val[0][0][0])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(len(Images_train))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;batch_size = 100&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;n_iters = 800&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;num_epochs = n_iters / (len(Images_train) / batch_size)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;num_epochs = int(num_epochs)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#tensor_x = torch.stack([torch.Tensor(i) for i in Images_val])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;train_loader = torch.utils.data.DataLoader(dataset=Images_val,&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;batch_size=batch_size,&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;shuffle=True)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;test_loader = torch.utils.data.DataLoader(dataset=Images_val,&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;batch_size=batch_size,&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;shuffle=False)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;class SimpleCNN(torch.nn.Module):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#Our batch shape for input x is (3, 32, 32)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def __init__(self):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;super(SimpleCNN, self).__init__()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#Input channels = 3, output channels = 18&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.conv1 = torch.nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#4608 input features, 64 output features (see sizing flow below)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.fc1 = torch.nn.Linear(18 * 16 * 16, 64)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#64 input features, 10 output features for our 10 defined classes&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;self.fc2 = torch.nn.Linear(64, 2)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def forward(self, x):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#Computes the activation of the first convolution&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#Size changes from (3, 32, 32) to (18, 32, 32)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = F.relu(self.conv1(x))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#Size changes from (18, 32, 32) to (18, 16, 16)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = self.pool(x)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#Reshape data to input to the input layer of the neural net&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#Size changes from (18, 16, 16) to (1, 4608)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#Recall that the -1 infers this dimension from the other given dimension&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = x.view(-1, 18 * 16 *16)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#Computes the activation of the first fully connected layer&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#Size changes from (1, 4608) to (1, 64)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = F.relu(self.fc1(x))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#Computes the second fully connected layer (activation applied later)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#Size changes from (1, 64) to (1, 10)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = self.fc2(x)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;return(x)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model = SimpleCNN()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;criterion = nn.CrossEntropyLoss()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;learning_rate = 0.01&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;iter = 0&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for epoch in range(num_epochs):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for i, (images, labels) in enumerate(train_loader):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Load images&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;images = images.requires_grad_()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Clear gradients w.r.t. parameters&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;optimizer.zero_grad()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Forward pass to get output/logits&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;outputs = model(images)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Calculate Loss: softmax --&amp;gt; cross entropy loss&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;loss = criterion(outputs, labels)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Getting gradients w.r.t. parameters&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;loss.backward()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Updating parameters&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;optimizer.step()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;iter += 1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;if iter % 200 == 0:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Calculate Accuracy&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;correct = 0&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;total = 0&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Iterate through test dataset&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for images, labels in test_loader:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Load images&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;images = images.requires_grad_()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Forward pass only to get logits/output&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;outputs = model(images)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Get predictions from the maximum value&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;_, predicted = torch.max(&lt;/code&gt;&lt;a href=""https://outputs.data""&gt;&lt;code&gt;outputs.data&lt;/code&gt;&lt;/a&gt;&lt;code&gt;, 1)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Total number of labels&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;total += labels.size(0)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Total correct predictions&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;correct += (predicted == labels).sum()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;accuracy = 100 * correct / total&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Print Loss&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(&amp;#39;Iteration: {}. Loss: {}. Accuracy: {}&amp;#39;.format(iter, loss.item(), accuracy))&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dtu8ce,True,,debayon,,2,True,all_ads,False,[],False,,/r/pytorch/comments/dtu8ce/hello_everyone_im_a_beginner_in_deep_learning_and/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dtu8ce/hello_everyone_im_a_beginner_in_deep_learning_and/,7135,1573294536.0,0,,False,,,,,,,,
746,,pytorch,,t2_cd4qjhv,False,,0,False,"Nov 21, Free Talk on PyTorch with Its Co-Author and Maintainer, Adam Paszke",[],r/pytorch,False,6,,0,140.0,,False,t3_dthcxp,False,dark,1.0,,public,13,0,{},140.0,,False,[],,False,False,,{},,False,13,,False,https://a.thumbs.redditmedia.com/CJBzaM3uXHopU5pt-YIcQZJwXTitIInGRhalxJDRXH8.jpg,False,,[],{},link,,False,,1573257459.0,text,6,,,text,webinars.on24.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/bZPn4moonFHCZpj-q04QiUTCOq2x0XSliWaw4FX-ByU.jpg?auto=webp&amp;s=5e7948e89f760334c59c19d92184b3ca1b534d18', 'width': 567, 'height': 819}, 'resolutions': [{'url': 'https://external-preview.redd.it/bZPn4moonFHCZpj-q04QiUTCOq2x0XSliWaw4FX-ByU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ae7bae29a69fcf5eaeea0217c4af1eb827ab0da', 'width': 108, 'height': 156}, {'url': 'https://external-preview.redd.it/bZPn4moonFHCZpj-q04QiUTCOq2x0XSliWaw4FX-ByU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=31f4fc1121cc06796da22933739dcb0c3bd17a9b', 'width': 216, 'height': 312}, {'url': 'https://external-preview.redd.it/bZPn4moonFHCZpj-q04QiUTCOq2x0XSliWaw4FX-ByU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=24c3e438c5ea0b6be3725d08bcfe57cb259bdeff', 'width': 320, 'height': 462}], 'variants': {}, 'id': 'QN0Nm2QmjCTIQEONEnp4HQ3aJBhHRxy6RLjMkjUomF0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dthcxp,True,,ACMLearning,,3,True,all_ads,False,[],False,,/r/pytorch/comments/dthcxp/nov_21_free_talk_on_pytorch_with_its_coauthor_and/,all_ads,False,https://webinars.on24.com/acm/paszke?partnerref=red,7135,1573228659.0,0,,False,https://webinars.on24.com/acm/paszke?partnerref=red,,,,,,,
747,,pytorch,"Hi everyone!

I am working on C++ Implementation of PyTorch Tutorial for Deep Learning Researchers. I aim to help C++ programmers with easy to follow tutorials which cover all basics, machine learning examples and wide range of Deep Learning examples as well.

I am following [https://github.com/yunjey/pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial) as the base for my C++ tutorials. Programmers who would like to contribute can pick up topics from Table of Contents and can work on it. Also, feel free to add more topics :) 

GitHub: [https://github.com/prabhuomkar/pytorch-cpp](https://github.com/prabhuomkar/pytorch-cpp)",t2_3sk7t1ft,False,,0,False,Need Help/Contributions to PyTorch C++ Tutorial for Deep Learning,[],r/pytorch,False,6,,0,,,False,t3_dsx0w0,False,dark,0.94,,public,12,0,{},,,False,[],,False,False,,{},,False,12,,False,self,False,,[],{},self,,True,,1573158397.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;

&lt;p&gt;I am working on C++ Implementation of PyTorch Tutorial for Deep Learning Researchers. I aim to help C++ programmers with easy to follow tutorials which cover all basics, machine learning examples and wide range of Deep Learning examples as well.&lt;/p&gt;

&lt;p&gt;I am following &lt;a href=""https://github.com/yunjey/pytorch-tutorial""&gt;https://github.com/yunjey/pytorch-tutorial&lt;/a&gt; as the base for my C++ tutorials. Programmers who would like to contribute can pick up topics from Table of Contents and can work on it. Also, feel free to add more topics :) &lt;/p&gt;

&lt;p&gt;GitHub: &lt;a href=""https://github.com/prabhuomkar/pytorch-cpp""&gt;https://github.com/prabhuomkar/pytorch-cpp&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/IqsFgES6xRq2DPmPPr04Rz3hxGOm9xnsJY0aiy7MTQw.jpg?auto=webp&amp;s=a97abad29fac0686cf6da057dcaccd96b521a9b1', 'width': 367, 'height': 367}, 'resolutions': [{'url': 'https://external-preview.redd.it/IqsFgES6xRq2DPmPPr04Rz3hxGOm9xnsJY0aiy7MTQw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4db34cc41e1b09d080fa65809cd8cb035185498', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/IqsFgES6xRq2DPmPPr04Rz3hxGOm9xnsJY0aiy7MTQw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fee206712a1fb24507899dbccbdb52341b2c7da6', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/IqsFgES6xRq2DPmPPr04Rz3hxGOm9xnsJY0aiy7MTQw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d28178f0ebe7556b5e010272de9c689baab0b5b', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'jRVn3lBe9towzPyRvpUEOTvI6D7dX3ifR9tnlukymZE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dsx0w0,True,,op_prabhuomkar,,0,True,all_ads,False,[],False,,/r/pytorch/comments/dsx0w0/need_helpcontributions_to_pytorch_c_tutorial_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dsx0w0/need_helpcontributions_to_pytorch_c_tutorial_for/,7135,1573129597.0,0,,False,,,,,,,,
748,,pytorch,"&amp;#x200B;

[PyPI link](https://pypi.org/project/torch-intermediate-layer-getter/)

Useful for debugging, visualizations and more complex models. I think it is pretty straightforward and easy to use, no need of additional libraries. Here is an example:

&amp;#x200B;

    import torch
    import torch.nn as nn
    
    from torch_intermediate_layer_getter import IntermediateLayerGetter as MidGetter
    
    class Model(nn.Module):
        def __init__(self):
            super().__init__()
    
            self.fc1 = nn.Linear(2, 2)
            self.fc2 = nn.Linear(2, 2)
            self.nested = nn.Sequential(
                nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 3)),
                nn.Linear(3, 1),
            )
            self.interaction_idty = nn.Identity() # Simple trick for operations not performed as modules
    
        def forward(self, x):
            x1 = self.fc1(x)
            x2 = self.fc2(x)
    
            interaction = x1 * x2
            self.interaction_idty(interaction)
    
            x_out = self.nested(interaction)
    
            return x_out
            
    model = Model()
    return_layers = {
        'fc2': 'fc2',
        'nested.0.1': 'nested',
        'interaction_idty': 'interaction',
    }
    mid_getter = MidGetter(model, return_layers=return_layers, keep_output=True)
    mid_outputs, model_output = mid_getter(torch.randn(1, 2))
    
    print(model_output)
    &gt;&gt; tensor([[0.3219]], grad_fn=&lt;AddmmBackward&gt;)
    print(mid_outputs)
    &gt;&gt; OrderedDict([('fc2', tensor([[-1.5125,  0.9334]], grad_fn=&lt;AddmmBackward&gt;)),
      ('interaction', tensor([[-0.0687, -0.1462]], grad_fn=&lt;MulBackward0&gt;)),
      ('nested', tensor([[-0.1697,  0.1432,  0.2959]], grad_fn=&lt;AddmmBackward&gt;))])
    
    # model_output is None if keep_ouput is False
    # if keep_output is True the model_output contains the final model's output

Update: added pypi link",t2_1gkbhia,False,,0,False,Hi! I made a package to get the intermediate outputs of a model's submodules,[],r/pytorch,False,6,,0,,,False,t3_ds4p0o,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,1572984552.0,,[],{},self,,True,,1573013057.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://pypi.org/project/torch-intermediate-layer-getter/""&gt;PyPI link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Useful for debugging, visualizations and more complex models. I think it is pretty straightforward and easy to use, no need of additional libraries. Here is an example:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import torch
import torch.nn as nn

from torch_intermediate_layer_getter import IntermediateLayerGetter as MidGetter

class Model(nn.Module):
    def __init__(self):
        super().__init__()

        self.fc1 = nn.Linear(2, 2)
        self.fc2 = nn.Linear(2, 2)
        self.nested = nn.Sequential(
            nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 3)),
            nn.Linear(3, 1),
        )
        self.interaction_idty = nn.Identity() # Simple trick for operations not performed as modules

    def forward(self, x):
        x1 = self.fc1(x)
        x2 = self.fc2(x)

        interaction = x1 * x2
        self.interaction_idty(interaction)

        x_out = self.nested(interaction)

        return x_out

model = Model()
return_layers = {
    &amp;#39;fc2&amp;#39;: &amp;#39;fc2&amp;#39;,
    &amp;#39;nested.0.1&amp;#39;: &amp;#39;nested&amp;#39;,
    &amp;#39;interaction_idty&amp;#39;: &amp;#39;interaction&amp;#39;,
}
mid_getter = MidGetter(model, return_layers=return_layers, keep_output=True)
mid_outputs, model_output = mid_getter(torch.randn(1, 2))

print(model_output)
&amp;gt;&amp;gt; tensor([[0.3219]], grad_fn=&amp;lt;AddmmBackward&amp;gt;)
print(mid_outputs)
&amp;gt;&amp;gt; OrderedDict([(&amp;#39;fc2&amp;#39;, tensor([[-1.5125,  0.9334]], grad_fn=&amp;lt;AddmmBackward&amp;gt;)),
  (&amp;#39;interaction&amp;#39;, tensor([[-0.0687, -0.1462]], grad_fn=&amp;lt;MulBackward0&amp;gt;)),
  (&amp;#39;nested&amp;#39;, tensor([[-0.1697,  0.1432,  0.2959]], grad_fn=&amp;lt;AddmmBackward&amp;gt;))])

# model_output is None if keep_ouput is False
# if keep_output is True the model_output contains the final model&amp;#39;s output
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Update: added pypi link&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/p3HQXQzdkmyXyJ89enL6PkgmSdstFY5z1QkzOzRNUaU.jpg?auto=webp&amp;s=01b29ed2d2e90d072e1fc7295da2c1cb3797f686', 'width': 300, 'height': 300}, 'resolutions': [{'url': 'https://external-preview.redd.it/p3HQXQzdkmyXyJ89enL6PkgmSdstFY5z1QkzOzRNUaU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=54fdfef1cb192ed04e4ba25828970287fc1ecde9', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/p3HQXQzdkmyXyJ89enL6PkgmSdstFY5z1QkzOzRNUaU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=22ff535b36cf2b9412be48a21108ba34479e2ba5', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'POVR4AtJHvry29k6WQQwgYhMSdLrOeYwBodMqA6lPGk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ds4p0o,True,,sebamenabar,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ds4p0o/hi_i_made_a_package_to_get_the_intermediate/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ds4p0o/hi_i_made_a_package_to_get_the_intermediate/,7135,1572984257.0,0,,False,,,,,,,,
749,,pytorch,"Hi I'm using the PyTorch transformer module for time series forecasting and I have a couple questions related to the `tgt` sequence as well as few more general questions. For the transformer I'm aware that we generally feed in the actual target sequence (as opposed to generating the target sequence step by step as opposed). Therefore, my first question is that prior to the transformer I have a standard linear layer to transform my time series sequence  along with positional encodings. According to the documentation  the transformer module code the src and trg sequence need to be the same dimension. 

```

class TransformerTimeSeries(torch.nn.Module)

  def __init__(self, n_time_series, d_model=128):
     super()._init__()

     self.dense_shape = torch.nn.Linear(n_time_series, d_model)

     self.pe = SimplePositionalEncoding(d_model)

     self.transformer = Transformer(d_model, nhead=8)

```

So I was wondering can I simply do something like this or will this somehow leak information about the target? I'm still not actually sure how loss.backward() works so I'm not sure if this will cause problems.

```

def forward(self, x, t):

   x = self.dense_shape(x)

   x = self.pe(x)

   t = self.dense_shape(t)

   t = self.pe(t)

   x = self.transformer(x, t)
```

Secondly,  does the target sequence need any sort of offset? So for instance if I have the time series [0,1,2,3,4,5,6,7] and I want to feed in [0,1,2,3] to predict [4,5,6,7] (tgt)? Would I simply feed it in like that or is it more complicated? Typically BERT and those models have [CLS] and [SEP] tokens to denote the beginning and end of sentences however, for time series I assume I don't need a separator time step.

Final question is will I need a mask for the encoder as well? My inclination is yes as unlike with sentence I would want the current timestep to be formed solely by the previous ones. However, I'm not entirely sure. 

&amp;#x200B;

Thanks for the help and please let me know if I'm getting other things wrong too.",t2_353elmq7,False,,0,False,Using PyTorch transformer module,[],r/pytorch,False,6,,0,,,False,t3_dsa6ca,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1573010216.0,,[],{},,,True,,1573038085.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi I&amp;#39;m using the PyTorch transformer module for time series forecasting and I have a couple questions related to the &lt;code&gt;tgt&lt;/code&gt; sequence as well as few more general questions. For the transformer I&amp;#39;m aware that we generally feed in the actual target sequence (as opposed to generating the target sequence step by step as opposed). Therefore, my first question is that prior to the transformer I have a standard linear layer to transform my time series sequence  along with positional encodings. According to the documentation  the transformer module code the src and trg sequence need to be the same dimension. &lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;class TransformerTimeSeries(torch.nn.Module)&lt;/p&gt;

&lt;p&gt;def &lt;strong&gt;init&lt;/strong&gt;(self, n&lt;em&gt;time_series, d_model=128):
     super()._init&lt;/em&gt;_()&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; self.dense_shape = torch.nn.Linear(n_time_series, d_model)

 self.pe = SimplePositionalEncoding(d_model)

 self.transformer = Transformer(d_model, nhead=8)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;So I was wondering can I simply do something like this or will this somehow leak information about the target? I&amp;#39;m still not actually sure how loss.backward() works so I&amp;#39;m not sure if this will cause problems.&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;def forward(self, x, t):&lt;/p&gt;

&lt;p&gt;x = self.dense_shape(x)&lt;/p&gt;

&lt;p&gt;x = self.pe(x)&lt;/p&gt;

&lt;p&gt;t = self.dense_shape(t)&lt;/p&gt;

&lt;p&gt;t = self.pe(t)&lt;/p&gt;

&lt;p&gt;x = self.transformer(x, t)
```&lt;/p&gt;

&lt;p&gt;Secondly,  does the target sequence need any sort of offset? So for instance if I have the time series [0,1,2,3,4,5,6,7] and I want to feed in [0,1,2,3] to predict [4,5,6,7] (tgt)? Would I simply feed it in like that or is it more complicated? Typically BERT and those models have [CLS] and [SEP] tokens to denote the beginning and end of sentences however, for time series I assume I don&amp;#39;t need a separator time step.&lt;/p&gt;

&lt;p&gt;Final question is will I need a mask for the encoder as well? My inclination is yes as unlike with sentence I would want the current timestep to be formed solely by the previous ones. However, I&amp;#39;m not entirely sure. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks for the help and please let me know if I&amp;#39;m getting other things wrong too.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dsa6ca,True,,svpadd3,,0,True,all_ads,False,[],False,,/r/pytorch/comments/dsa6ca/using_pytorch_transformer_module/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dsa6ca/using_pytorch_transformer_module/,7135,1573009285.0,0,,False,,,,,,,,
750,,pytorch,"There are few resource about horovod. 

I have written  a post about the basic concepts in distributed training in pytorch using horovod. The link is https://jdhao.github.io/2019/11/01/pytorch_distributed_training/.

Hope it can help you.",t2_gvcroc9,False,,0,False,Distributed traning in pytorch with horovod basic concepts,[],r/pytorch,False,6,,0,,,False,t3_drzlrc,False,dark,0.76,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1572991656.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;There are few resource about horovod. &lt;/p&gt;

&lt;p&gt;I have written  a post about the basic concepts in distributed training in pytorch using horovod. The link is &lt;a href=""https://jdhao.github.io/2019/11/01/pytorch_distributed_training/""&gt;https://jdhao.github.io/2019/11/01/pytorch_distributed_training/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Hope it can help you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/2SIY0sCmGC94Hmg66__DRxWL9hB2D84KxeQJwWxgTLI.jpg?auto=webp&amp;s=ccfb993000300c33f7f46ec098ee68400a853d70', 'width': 1239, 'height': 1635}, 'resolutions': [{'url': 'https://external-preview.redd.it/2SIY0sCmGC94Hmg66__DRxWL9hB2D84KxeQJwWxgTLI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ca0a58ba9530efede59e4d95328baaaf6cca9a58', 'width': 108, 'height': 142}, {'url': 'https://external-preview.redd.it/2SIY0sCmGC94Hmg66__DRxWL9hB2D84KxeQJwWxgTLI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=27a83fc342ba7208ff4c41214b7e28142e526a78', 'width': 216, 'height': 285}, {'url': 'https://external-preview.redd.it/2SIY0sCmGC94Hmg66__DRxWL9hB2D84KxeQJwWxgTLI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=402314367ed61db4d2c0128b2fee40b73e4e747f', 'width': 320, 'height': 422}, {'url': 'https://external-preview.redd.it/2SIY0sCmGC94Hmg66__DRxWL9hB2D84KxeQJwWxgTLI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc798e923868673b156e0cbb5426ef52591edc63', 'width': 640, 'height': 844}, {'url': 'https://external-preview.redd.it/2SIY0sCmGC94Hmg66__DRxWL9hB2D84KxeQJwWxgTLI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f56876ceade5b9de107185f9486072e2e8390c6e', 'width': 960, 'height': 1266}, {'url': 'https://external-preview.redd.it/2SIY0sCmGC94Hmg66__DRxWL9hB2D84KxeQJwWxgTLI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=030e23b130cb7b079764a0f53a08c4eadda0b4c7', 'width': 1080, 'height': 1425}], 'variants': {}, 'id': '17fYbvbQSt1YdmGLboVlbA-czaDaAFr8qPX36csHEys'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,drzlrc,True,,jdhao,,0,True,all_ads,False,[],False,,/r/pytorch/comments/drzlrc/distributed_traning_in_pytorch_with_horovod_basic/,all_ads,False,https://www.reddit.com/r/pytorch/comments/drzlrc/distributed_traning_in_pytorch_with_horovod_basic/,7135,1572962856.0,0,,False,,,,,,,,
751,,pytorch,,t2_1h230ytw,False,,0,False,Why Facebook open source Pytorch?,[],r/pytorch,False,6,,0,,,False,t3_drrzy9,False,dark,0.82,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1572946589.0,text,6,,,text,self.pytorch,False,,,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,drrzy9,True,,_spicyramen,,3,True,all_ads,False,[],False,,/r/pytorch/comments/drrzy9/why_facebook_open_source_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/drrzy9/why_facebook_open_source_pytorch/,7135,1572917789.0,0,,False,,,,,,,,
752,,pytorch,"Hi everyone, I recently tried to implement attention mechanism in Pytorch. I searched lots of github repos and also the official pytorch implementation [here](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html). also detailed tutorials such as  [this one on floydhub](https://blog.floydhub.com/attention-mechanism/). However, it seems to me all of them have implemented the attention mechanism incorrectly!

The problem that I see, is that, in the papers (both of them both, bahdanaus and luoungs), the score is calculated by comparing all encoders hidden states with a single timestep in the decoder, the cuurent/previous timestep to be exact.

The score is calculated and then used along the way so ultimately **the current timestep in the decoder can produce one output**.

Now if you look at the codes, you can see, all of them used an lstm/gru layer instead of a lstm/gru cell where each timestep is exposed!

They simply define the attention mechanism as a module and in its foward() pass, they simply feed the input and what not to the lstm/gru layer and get its outputs! and then use that as the hidden states and multiply/etc it with the encoders states and carry on the rest of the formula.

The idea was to prior to creating **an output in each timestep in the decoder**, **its** current/previous hidden state be used. This is not the case here, the lstm layer simply just does its job, produces all outputs **normally** and we are are using these hiddenstates **in each iteration(as apposed to in each sample)**.

The hidden states were supposed to belong to one sample(one german sentence/paragraph to one english sentence/parageraph) . what happens here is we are using the hidden states from previous iterations that has nothing to do with the current sample! (its as if for translating the current sentence, I'm looking at the hidden state for previous **sentence** (as if for translating the sentence : hey buddy whats up?! I look into the previous sample sentence, mama mia beat the hell out of his son Jose!)

&amp;#x200B;

Can someone please explain whats wrong here? am I missing something here?",t2_12rvgcg3,False,,0,False,Wrong implementation of Attention in Pytorch examples,[],r/pytorch,False,6,,0,,,False,t3_dmu8e0,False,dark,1.0,,public,14,0,{},,,False,[],,False,False,,{},,False,14,,False,self,False,,[],{},self,,True,,1572019439.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, I recently tried to implement attention mechanism in Pytorch. I searched lots of github repos and also the official pytorch implementation &lt;a href=""https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html""&gt;here&lt;/a&gt;. also detailed tutorials such as  &lt;a href=""https://blog.floydhub.com/attention-mechanism/""&gt;this one on floydhub&lt;/a&gt;. However, it seems to me all of them have implemented the attention mechanism incorrectly!&lt;/p&gt;

&lt;p&gt;The problem that I see, is that, in the papers (both of them both, bahdanaus and luoungs), the score is calculated by comparing all encoders hidden states with a single timestep in the decoder, the cuurent/previous timestep to be exact.&lt;/p&gt;

&lt;p&gt;The score is calculated and then used along the way so ultimately &lt;strong&gt;the current timestep in the decoder can produce one output&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Now if you look at the codes, you can see, all of them used an lstm/gru layer instead of a lstm/gru cell where each timestep is exposed!&lt;/p&gt;

&lt;p&gt;They simply define the attention mechanism as a module and in its foward() pass, they simply feed the input and what not to the lstm/gru layer and get its outputs! and then use that as the hidden states and multiply/etc it with the encoders states and carry on the rest of the formula.&lt;/p&gt;

&lt;p&gt;The idea was to prior to creating &lt;strong&gt;an output in each timestep in the decoder&lt;/strong&gt;, &lt;strong&gt;its&lt;/strong&gt; current/previous hidden state be used. This is not the case here, the lstm layer simply just does its job, produces all outputs &lt;strong&gt;normally&lt;/strong&gt; and we are are using these hiddenstates &lt;strong&gt;in each iteration(as apposed to in each sample)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The hidden states were supposed to belong to one sample(one german sentence/paragraph to one english sentence/parageraph) . what happens here is we are using the hidden states from previous iterations that has nothing to do with the current sample! (its as if for translating the current sentence, I&amp;#39;m looking at the hidden state for previous &lt;strong&gt;sentence&lt;/strong&gt; (as if for translating the sentence : hey buddy whats up?! I look into the previous sample sentence, mama mia beat the hell out of his son Jose!)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Can someone please explain whats wrong here? am I missing something here?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/paiG2tIaoR3vXsdH56ZZu2D82Cmwh1G_GjBqGhxbVa0.jpg?auto=webp&amp;s=3f26b3e24755138d801dde010cc0adcb69627dfc', 'width': 800, 'height': 480}, 'resolutions': [{'url': 'https://external-preview.redd.it/paiG2tIaoR3vXsdH56ZZu2D82Cmwh1G_GjBqGhxbVa0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6803114c8e886e8ea14f811afb152cef97eb5b2', 'width': 108, 'height': 64}, {'url': 'https://external-preview.redd.it/paiG2tIaoR3vXsdH56ZZu2D82Cmwh1G_GjBqGhxbVa0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4db04672ad2bec99ba4911db1ead83c724bb5456', 'width': 216, 'height': 129}, {'url': 'https://external-preview.redd.it/paiG2tIaoR3vXsdH56ZZu2D82Cmwh1G_GjBqGhxbVa0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e602a9e8597442f8e96eec11ae40fc0b1300d830', 'width': 320, 'height': 192}, {'url': 'https://external-preview.redd.it/paiG2tIaoR3vXsdH56ZZu2D82Cmwh1G_GjBqGhxbVa0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b4dfbf01918591c371e3dbf5786d0be452bfea60', 'width': 640, 'height': 384}], 'variants': {}, 'id': 'LedsfHOby-ngVmtmfjPXQelevAW22OmaP87xtPr1kPE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dmu8e0,True,,MasterSama,,2,True,all_ads,False,[],False,,/r/pytorch/comments/dmu8e0/wrong_implementation_of_attention_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dmu8e0/wrong_implementation_of_attention_in_pytorch/,7135,1571990639.0,0,,False,,,,,,,,
753,,pytorch,"I know Conv2d is more optimized than F.conv2d but it seems to take a long time to converge

&amp;#x200B;

Is there somwthing wrong with my code?

&amp;#x200B;

[https://gist.github.com/Algogator/81f2e91b29af7ef5017d5bb1ff86b694](https://gist.github.com/Algogator/81f2e91b29af7ef5017d5bb1ff86b694)",t2_174d6j,False,,0,False,Using F.conv2d instead of Conv2d,[],r/pytorch,False,6,,0,,,False,t3_dmarkw,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1571916991.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know Conv2d is more optimized than F.conv2d but it seems to take a long time to converge&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is there somwthing wrong with my code?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://gist.github.com/Algogator/81f2e91b29af7ef5017d5bb1ff86b694""&gt;https://gist.github.com/Algogator/81f2e91b29af7ef5017d5bb1ff86b694&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?auto=webp&amp;s=079a7260ec149880c73263d64811698adb22760a', 'width': 1280, 'height': 640}, 'resolutions': [{'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5811c5bda5fece1040636a6af8702ba790f0fd4', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eee576fd4da7535eb53ceb88dd8b52f073048441', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=72872d880460efa723918c000adca0ed259cf775', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3545b9335d763c9da9c16bf7bf9a3f907dbd6f6', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d241ace0f1c07088fac3f8469dbad3b05d2d419', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9055f11bdc00beb0b3589e1cae5817d6070d83bc', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dmarkw,True,,Algogator,,4,True,all_ads,False,[],False,,/r/pytorch/comments/dmarkw/using_fconv2d_instead_of_conv2d/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dmarkw/using_fconv2d_instead_of_conv2d/,7135,1571888191.0,0,,False,,,,,,,,
754,,pytorch,"I have been working on Implementing ODE RNN specified in the paper [Latent ODEs for Irregularly-Sampled Time Series](https://arxiv.org/abs/1907.03907). I observed that, when I train my model, it is unable to learn the *Func Parameters* but able to learn all the other parameters of the model. I have created a GitHub issue [https://github.com/rtqichen/torchdiffeq/issues/79](https://github.com/rtqichen/torchdiffeq/issues/79)

If someone has an Idea on using Neural ODE with PyTorch, could you please look into it and suggest me any errors?

Thank you.",t2_2yrxc2o9,False,,0,False,Neural ODE Function weights not getting updated.,[],r/pytorch,False,6,,0,,,False,t3_dlhaml,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1571776526.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been working on Implementing ODE RNN specified in the paper &lt;a href=""https://arxiv.org/abs/1907.03907""&gt;Latent ODEs for Irregularly-Sampled Time Series&lt;/a&gt;. I observed that, when I train my model, it is unable to learn the &lt;em&gt;Func Parameters&lt;/em&gt; but able to learn all the other parameters of the model. I have created a GitHub issue &lt;a href=""https://github.com/rtqichen/torchdiffeq/issues/79""&gt;https://github.com/rtqichen/torchdiffeq/issues/79&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If someone has an Idea on using Neural ODE with PyTorch, could you please look into it and suggest me any errors?&lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dlhaml,True,,al10101,,0,True,all_ads,False,[],False,,/r/pytorch/comments/dlhaml/neural_ode_function_weights_not_getting_updated/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dlhaml/neural_ode_function_weights_not_getting_updated/,7135,1571747726.0,0,,False,,,,,,,,
755,,pytorch,"Hi, I'm trying to run a ResNeXt model on a SLURM cluster node with 4 GPUs, of which I'm using 2. However, when I run my code, the moment I begin reading image data files stored on disk, the dataloader runs for two iterations before throwing an ""exceeded job memory"" error. I give it 64GB of RAM, and it requests for 341GB of RAM! That seems a little unreasonable. And this happens only when I set num_workers &gt; 0. Any suggestions?",t2_dvna520,False,,0,False,Out of memory error with num_workers&gt;0?,[],r/pytorch,False,6,,0,,,False,t3_dldaa8,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1571750961.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m trying to run a ResNeXt model on a SLURM cluster node with 4 GPUs, of which I&amp;#39;m using 2. However, when I run my code, the moment I begin reading image data files stored on disk, the dataloader runs for two iterations before throwing an &amp;quot;exceeded job memory&amp;quot; error. I give it 64GB of RAM, and it requests for 341GB of RAM! That seems a little unreasonable. And this happens only when I set num_workers &amp;gt; 0. Any suggestions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dldaa8,True,,BreakingTheBadBread,,1,True,all_ads,False,[],False,,/r/pytorch/comments/dldaa8/out_of_memory_error_with_num_workers0/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dldaa8/out_of_memory_error_with_num_workers0/,7135,1571722161.0,0,,False,,,,,,,,
756,,pytorch,"Hey guys, I have a 1d signal with length 220480. How do I max-pool this to a dimension of 22030 since 22030 doesn't exactly divide it? Do I add 0s at the end? Because I've heard that that could reduce accuracy.

Thanks",t2_u2si6,False,,0,False,Pooling dimensions issue,[],r/pytorch,False,6,,0,,,False,t3_dkjmf2,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1571606997.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, I have a 1d signal with length 220480. How do I max-pool this to a dimension of 22030 since 22030 doesn&amp;#39;t exactly divide it? Do I add 0s at the end? Because I&amp;#39;ve heard that that could reduce accuracy.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dkjmf2,True,,SEAsFinest,,0,True,all_ads,False,[],False,,/r/pytorch/comments/dkjmf2/pooling_dimensions_issue/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dkjmf2/pooling_dimensions_issue/,7135,1571578197.0,0,,False,,,,,,,,
757,,pytorch,"I'm experimenting with multi-head DQN, a deep reinforcement learning method.

I have a working implementation, however I am not sure it is using resources efficiently.

So in practice my network has the following architecture:

    input -&gt; 128x (separate fully connected layers) -&gt; output averaging

I am using a ModuleList to hold the list of fully connected layers. Here's how it looks at this point:

    class MultiHead(nn.Module):
        def __init__(self, dim_state, dim_action, hidden_size=32, nb_heads=1):
            super(MultiHead, self).__init__()
    
            self.networks = nn.ModuleList()
            for _ in range(nb_heads):
                network = nn.Sequential(
                    nn.Linear(dim_state, hidden_size),
                    nn.Tanh(),
                    nn.Linear(hidden_size, dim_action)
                )
                self.networks.append(network)
    
            self.cuda()
            self.optimizer = optim.Adam(self.parameters())

Then, when I need to calculate the output, I use a `for ... in` construct to perform the forward and backward pass through all the layers:

    q_values = torch.cat([net(observations) for net in self.networks])
    
    # skipped code to the loss as needed
    
    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()

**This works!** But I am wondering if I couldn't do this more efficiently. I feel like by doing a for...in, I am actually going through each separate FC layer one by one, while I'd expect this operation could be done in parallel.

(Cross-posted from [/r/reinforcementlearning](https://www.reddit.com/r/reinforcementlearning/comments/dhybku/need_help_implementing_multihead_dqn_with_pytorch/))",t2_13c9j,False,,0,False,Need help implementing a multi-head network,[],r/pytorch,False,6,,0,,,False,t3_dk85xz,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1571540856.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m experimenting with multi-head DQN, a deep reinforcement learning method.&lt;/p&gt;

&lt;p&gt;I have a working implementation, however I am not sure it is using resources efficiently.&lt;/p&gt;

&lt;p&gt;So in practice my network has the following architecture:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input -&amp;gt; 128x (separate fully connected layers) -&amp;gt; output averaging
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I am using a ModuleList to hold the list of fully connected layers. Here&amp;#39;s how it looks at this point:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class MultiHead(nn.Module):
    def __init__(self, dim_state, dim_action, hidden_size=32, nb_heads=1):
        super(MultiHead, self).__init__()

        self.networks = nn.ModuleList()
        for _ in range(nb_heads):
            network = nn.Sequential(
                nn.Linear(dim_state, hidden_size),
                nn.Tanh(),
                nn.Linear(hidden_size, dim_action)
            )
            self.networks.append(network)

        self.cuda()
        self.optimizer = optim.Adam(self.parameters())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, when I need to calculate the output, I use a &lt;code&gt;for ... in&lt;/code&gt; construct to perform the forward and backward pass through all the layers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;q_values = torch.cat([net(observations) for net in self.networks])

# skipped code to the loss as needed

self.optimizer.zero_grad()
loss.backward()
self.optimizer.step()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;This works!&lt;/strong&gt; But I am wondering if I couldn&amp;#39;t do this more efficiently. I feel like by doing a for...in, I am actually going through each separate FC layer one by one, while I&amp;#39;d expect this operation could be done in parallel.&lt;/p&gt;

&lt;p&gt;(Cross-posted from &lt;a href=""https://www.reddit.com/r/reinforcementlearning/comments/dhybku/need_help_implementing_multihead_dqn_with_pytorch/""&gt;/r/reinforcementlearning&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dk85xz,True,,MasterScrat,,0,True,all_ads,False,[],False,,/r/pytorch/comments/dk85xz/need_help_implementing_a_multihead_network/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dk85xz/need_help_implementing_a_multihead_network/,7135,1571512056.0,0,,False,,,,,,,,
758,,pytorch,"Trying to implement some sampling in 3d space but grid_sample function doesnt support trilinear interpolation(which is weird). Issue on github seems to be dead. Maybe someone implemented it and can share, cause i didnt find anything on the internet and i almost zero in cuda :(",t2_g3uaa,False,,0,False,Does anyone have implementation of tri-linear interpolation for grid_sample function?,[],r/pytorch,False,6,,0,,,False,t3_djdbby,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1571377698.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Trying to implement some sampling in 3d space but grid_sample function doesnt support trilinear interpolation(which is weird). Issue on github seems to be dead. Maybe someone implemented it and can share, cause i didnt find anything on the internet and i almost zero in cuda :(&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,djdbby,True,,creotiv,,0,True,all_ads,False,[],False,,/r/pytorch/comments/djdbby/does_anyone_have_implementation_of_trilinear/,all_ads,False,https://www.reddit.com/r/pytorch/comments/djdbby/does_anyone_have_implementation_of_trilinear/,7135,1571348898.0,0,,False,,,,,,,,
759,,pytorch,"I have a bunch segmentation labels I am using for training a segmentation network, in the format of colored png images such as this: [https://github.com/BrianSantoso/images/blob/master/unet/cmp\_b0004.png](https://github.com/BrianSantoso/images/blob/master/unet/cmp_b0004.png)

Using torchvision's transforms, is there a way to transform the RGB tensors (3xHxW) into N channels (NxHxW), where each channel is a one-hot encoded mask for each unique color (class)? For example, the channel for walls would have 0's at every pixel there is not a wall, and 1's for every pixel there is a wall. The window channel would have 0's everywhere there is not a window, and 1's everywhere there is a window, and so on",t2_25liejo,False,,0,False,Transform RGB Image label to N Classes?,[],r/pytorch,False,6,,0,,,False,t3_dhn5o8,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1571038155.0,,[],{},self,,True,,1571066716.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a bunch segmentation labels I am using for training a segmentation network, in the format of colored png images such as this: &lt;a href=""https://github.com/BrianSantoso/images/blob/master/unet/cmp_b0004.png""&gt;https://github.com/BrianSantoso/images/blob/master/unet/cmp_b0004.png&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Using torchvision&amp;#39;s transforms, is there a way to transform the RGB tensors (3xHxW) into N channels (NxHxW), where each channel is a one-hot encoded mask for each unique color (class)? For example, the channel for walls would have 0&amp;#39;s at every pixel there is not a wall, and 1&amp;#39;s for every pixel there is a wall. The window channel would have 0&amp;#39;s everywhere there is not a window, and 1&amp;#39;s everywhere there is a window, and so on&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/yqwXZZFB16UgNZgo-3u-WJZdyLqNv1Cn5VSY-BV5j7c.jpg?auto=webp&amp;s=8237446fc933f158a0e076b48fda0b69c34b65d8', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/yqwXZZFB16UgNZgo-3u-WJZdyLqNv1Cn5VSY-BV5j7c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=50fb4d9c2502e09dbac220ec5bf1df004a147595', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/yqwXZZFB16UgNZgo-3u-WJZdyLqNv1Cn5VSY-BV5j7c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=453e015d255f430f34c357e6ac3aacc4ed5e7f3b', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/yqwXZZFB16UgNZgo-3u-WJZdyLqNv1Cn5VSY-BV5j7c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e783daf083386c7deadbb9b21ca7c4f26eccf0ab', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'QtcYI7kZsH2qmVNUHF17OROeKM8fgTwiq3VlhoAPlqo'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dhn5o8,True,,santoso-sheep,,0,True,all_ads,False,[],False,,/r/pytorch/comments/dhn5o8/transform_rgb_image_label_to_n_classes/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dhn5o8/transform_rgb_image_label_to_n_classes/,7135,1571037916.0,0,,False,,,,,,,,
760,,pytorch,,t2_15n8fb,False,,0,False,What is an example of using Hessian Vector Product in Learning using Pytorch?,[],r/pytorch,False,6,,0,140.0,,False,t3_dhcn8c,False,dark,0.72,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/rXd8dDPnXNmTQmZZ8yMOHsbgXtEvoeCD9121M6oyMEc.jpg,False,,[],{},link,,False,,1571012254.0,text,6,,,text,discuss.pytorch.org,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dhcn8c,True,,real_pinocchio,,1,True,all_ads,False,[],False,,/r/pytorch/comments/dhcn8c/what_is_an_example_of_using_hessian_vector/,all_ads,False,https://discuss.pytorch.org/t/what-is-an-example-of-using-hessian-vector-product-in-learning-using-pytorch/58061,7135,1570983454.0,0,,False,https://discuss.pytorch.org/t/what-is-an-example-of-using-hessian-vector-product-in-learning-using-pytorch/58061,,,,,,,
761,,pytorch,"I have followed the instructions mentioned in [pytorch.org](https://pytorch.org) but it is installing 1.0.0. 

&gt; conda install pytorch torchvision cudatoolkit=10.1 -c pytorch

I couldn't find a way around that so I tried updating after that which brought it to 1.0.1. 

&gt; conda **update** pytorch torchvision

Can anyone please help me installing the latest versions of Pytorch and Torchvision? I am new to this so, I appreciate any help that I can get. Thanks!",t2_162teh,False,,0,False,Pytorch 1.3 is not getting installed. How do I install it?,[],r/pytorch,False,6,,0,,,False,t3_dgvt59,False,dark,0.71,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1570919149.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have followed the instructions mentioned in &lt;a href=""https://pytorch.org""&gt;pytorch.org&lt;/a&gt; but it is installing 1.0.0. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;conda install pytorch torchvision cudatoolkit=10.1 -c pytorch&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I couldn&amp;#39;t find a way around that so I tried updating after that which brought it to 1.0.1. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;conda &lt;strong&gt;update&lt;/strong&gt; pytorch torchvision&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Can anyone please help me installing the latest versions of Pytorch and Torchvision? I am new to this so, I appreciate any help that I can get. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?auto=webp&amp;s=d0da5dbb7aff983a5173aba7d2bdd36fa28abe08', 'width': 2500, 'height': 2500}, 'resolutions': [{'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fef05ec37ddf39032979380cc7acc403e05834be', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e1de8e408f33ef3c93425e30e0bb5afffd5c631', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89562dde9cea0af5ebaaba50a9437e484d732513', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d6125bcd02c2fed8d84dac49cc8d73dbe205d81', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=40d7ca8d88566f27054acb632514012b5026acc9', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=757464a70e69532931dcbf0fbd397b9d433cf205', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'Mg0HAz_PmYkYWkaGTVyhCEzp15PpLW_TWWqmn9VNVFs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dgvt59,True,,spawnofdexter,,2,True,all_ads,False,[],False,,/r/pytorch/comments/dgvt59/pytorch_13_is_not_getting_installed_how_do_i/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dgvt59/pytorch_13_is_not_getting_installed_how_do_i/,7135,1570890349.0,0,,False,,,,,,,,True
762,,pytorch,"Hey guys I have been using Keras for quite some time now but want to switch to PyTorch for obvious reasons. Can someone suggest be really good books to get started with PyTorch (only books, no courses please)",,False,,0,False,Best PyTorch Book,[],r/pytorch,False,6,,0,,,False,t3_dgo1wr,False,dark,1.0,,public,11,0,{},,,False,[],,False,False,,{},,False,11,,,self,False,,,{},,,True,,1570870268.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys I have been using Keras for quite some time now but want to switch to PyTorch for obvious reasons. Can someone suggest be really good books to get started with PyTorch (only books, no courses please)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dgo1wr,True,,[deleted],,10,True,all_ads,False,[],,dark,/r/pytorch/comments/dgo1wr/best_pytorch_book/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dgo1wr/best_pytorch_book/,7135,1570841468.0,0,,False,,,,,,,,
763,,pytorch,,t2_7eslkpz,False,,0,False,PyTorch 1.3 released,[],r/pytorch,False,6,,0,140.0,,False,t3_dg9zhr,False,dark,1.0,,public,42,0,{},140.0,,False,[],,False,False,,{},,False,42,,False,https://b.thumbs.redditmedia.com/q6aAXoLZ6El02sO3Hcw9YR-lcolZ1h7mj9j4SRjteTs.jpg,False,,[],{},link,,False,,1570797106.0,text,6,,,text,pytorch.org,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?auto=webp&amp;s=d0da5dbb7aff983a5173aba7d2bdd36fa28abe08', 'width': 2500, 'height': 2500}, 'resolutions': [{'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fef05ec37ddf39032979380cc7acc403e05834be', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e1de8e408f33ef3c93425e30e0bb5afffd5c631', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89562dde9cea0af5ebaaba50a9437e484d732513', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d6125bcd02c2fed8d84dac49cc8d73dbe205d81', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=40d7ca8d88566f27054acb632514012b5026acc9', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=757464a70e69532931dcbf0fbd397b9d433cf205', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'Mg0HAz_PmYkYWkaGTVyhCEzp15PpLW_TWWqmn9VNVFs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dg9zhr,True,,Marha01,,2,False,all_ads,False,[],False,,/r/pytorch/comments/dg9zhr/pytorch_13_released/,all_ads,False,https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/,7135,1570768306.0,0,,False,https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/,,,,,,,
764,,pytorch,"Hi guys,

Im trying to use the pretrained inception v3 model offered by pytorch to extract features ( general purpose features) from images from the Fully Connected Layer (Fc/Fc-7) for clustering purposes.  
Could you please tell me how to get the proper vectors from the fc layer?",t2_29yyz0jr,False,,0,False,Extract features from Inception_V3 torchvision pretrained model,[],r/pytorch,False,6,,0,,,False,t3_dggt6g,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1570836667.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys,&lt;/p&gt;

&lt;p&gt;Im trying to use the pretrained inception v3 model offered by pytorch to extract features ( general purpose features) from images from the Fully Connected Layer (Fc/Fc-7) for clustering purposes.&lt;br/&gt;
Could you please tell me how to get the proper vectors from the fc layer?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dggt6g,True,,ihababdk,,6,True,all_ads,False,[],False,,/r/pytorch/comments/dggt6g/extract_features_from_inception_v3_torchvision/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dggt6g/extract_features_from_inception_v3_torchvision/,7135,1570807867.0,0,,False,,,,,,,,
765,,pytorch,"I am looking to optimize my PyTorch code for production.  Using TorchScript seems to be the standard way to do it. However, I am unable to find information regarding the speedup due to TorchScript w.r.t. vanilla Pytorch code. Is there any blog or article I can refer to? 

PS: I understand that the speedup will differ for different models but some comparison would be really helpful.",t2_n5tr6,False,,0,False,[Question] Performance of TorchScript,[],r/pytorch,False,6,,0,,,False,t3_dfhabo,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1570657661.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking to optimize my PyTorch code for production.  Using TorchScript seems to be the standard way to do it. However, I am unable to find information regarding the speedup due to TorchScript w.r.t. vanilla Pytorch code. Is there any blog or article I can refer to? &lt;/p&gt;

&lt;p&gt;PS: I understand that the speedup will differ for different models but some comparison would be really helpful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dfhabo,True,,lt007,,1,True,all_ads,False,[],False,,/r/pytorch/comments/dfhabo/question_performance_of_torchscript/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dfhabo/question_performance_of_torchscript/,7135,1570628861.0,0,,False,,,,,,,,
766,,pytorch,"When I train the model, my loss is weird values and I am not sure what is causing that. Here is the [Code](https://github.com/mikeyyg96/machine-learning-tutorial/blob/master/pytorch_example.ipynb).

    epoch 0, loss 623258112.0
    epoch 1, loss 2.3203421500556297e+25
    epoch 2, loss inf
    epoch 3, loss inf
    epoch 4, loss inf
    epoch 5, loss nan
    epoch 6, loss nan
    epoch 7, loss nan
    epoch 8, loss nan
    epoch 9, loss nan",t2_1k1ina8l,False,,0,False,I (Beginner Newbie) need some help with a simple Linear Regression Model I am trying to train,[],r/pytorch,False,6,,0,,,False,t3_ddqwxn,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1570327912.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When I train the model, my loss is weird values and I am not sure what is causing that. Here is the &lt;a href=""https://github.com/mikeyyg96/machine-learning-tutorial/blob/master/pytorch_example.ipynb""&gt;Code&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;epoch 0, loss 623258112.0
epoch 1, loss 2.3203421500556297e+25
epoch 2, loss inf
epoch 3, loss inf
epoch 4, loss inf
epoch 5, loss nan
epoch 6, loss nan
epoch 7, loss nan
epoch 8, loss nan
epoch 9, loss nan
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/CScxeq_rIEzW6Uh5J28iqBmNjV07gjhTnDE92UIgs7M.jpg?auto=webp&amp;s=5c8c93f931952dbb42f7de2e304b6b738c71fced', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/CScxeq_rIEzW6Uh5J28iqBmNjV07gjhTnDE92UIgs7M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d71ad01b6853fc7c8a654ac9585d6b80cd6c5c3', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/CScxeq_rIEzW6Uh5J28iqBmNjV07gjhTnDE92UIgs7M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1977989265f821712d7a11b45b8fee7d26aefe06', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/CScxeq_rIEzW6Uh5J28iqBmNjV07gjhTnDE92UIgs7M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5de747f2f28f25d864e2c4db3a29529ad864c1a1', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'jjeLt7ST2nln-sb1nEqOVUD6aSCjymk3hWnEO7Xzo-8'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ddqwxn,True,,mikeyyg58,,8,True,all_ads,False,[],False,,/r/pytorch/comments/ddqwxn/i_beginner_newbie_need_some_help_with_a_simple/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ddqwxn/i_beginner_newbie_need_some_help_with_a_simple/,7135,1570299112.0,0,,False,,,,,,,,
767,,pytorch,"Interview with the CTO of Hugging Face, Julien Chaumond all about his journey into the field and his path as a CTO of Hugging Face. We also discuss all the amazing work being done at Hugging Face, research and open source as well as about their team.

Audio: https://anchor.fm/chaitimedatascience/episodes/Hugging-Face--Transformers--NLP-Research-and-Open-Source--Interview-with-Julien-Chaumond-e5o819/a-apcirt

Video: https://www.youtube.com/watch?v=kqPEwJVkpnA

AMA Announcements: 
I’ll also be interviewing 3 more amazing people in the upcoming week and they have been kind enough to agree to an AMA interview, here are the links to submit your questions:


AMA Interview with Victor Sanh, Research Scientist at Hugging Face, AMA: https://twitter.com/bhutanisanyam1/status/1179836740569223168?s=21

Kaggle IEEE-CIS Fraud Detection Comp, 2nd Pos winner (Team: “2 Uncles and 3 Puppies”), Grandmaster CPMP AMA: https://www.kaggle.com/c/ieee-fraud-detection/discussion/111242

6th Pos winner (Team: “Zoo”), Kaggle Master, Philipp Singer, AMA Interview: https://www.kaggle.com/c/ieee-fraud-detection/discussion/111281",t2_2s77fkel,False,,0,False,"Interview with the CTO of Hugging Face: Julien Chaumond | Hugging Face, Transformers | NLP Research and Open Source + 3 AMA Announcements",[],r/pytorch,False,6,,0,,,False,t3_ddk4lj,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},self,,True,,1570289957.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Interview with the CTO of Hugging Face, Julien Chaumond all about his journey into the field and his path as a CTO of Hugging Face. We also discuss all the amazing work being done at Hugging Face, research and open source as well as about their team.&lt;/p&gt;

&lt;p&gt;Audio: &lt;a href=""https://anchor.fm/chaitimedatascience/episodes/Hugging-Face--Transformers--NLP-Research-and-Open-Source--Interview-with-Julien-Chaumond-e5o819/a-apcirt""&gt;https://anchor.fm/chaitimedatascience/episodes/Hugging-Face--Transformers--NLP-Research-and-Open-Source--Interview-with-Julien-Chaumond-e5o819/a-apcirt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Video: &lt;a href=""https://www.youtube.com/watch?v=kqPEwJVkpnA""&gt;https://www.youtube.com/watch?v=kqPEwJVkpnA&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;AMA Announcements: 
I’ll also be interviewing 3 more amazing people in the upcoming week and they have been kind enough to agree to an AMA interview, here are the links to submit your questions:&lt;/p&gt;

&lt;p&gt;AMA Interview with Victor Sanh, Research Scientist at Hugging Face, AMA: &lt;a href=""https://twitter.com/bhutanisanyam1/status/1179836740569223168?s=21""&gt;https://twitter.com/bhutanisanyam1/status/1179836740569223168?s=21&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Kaggle IEEE-CIS Fraud Detection Comp, 2nd Pos winner (Team: “2 Uncles and 3 Puppies”), Grandmaster CPMP AMA: &lt;a href=""https://www.kaggle.com/c/ieee-fraud-detection/discussion/111242""&gt;https://www.kaggle.com/c/ieee-fraud-detection/discussion/111242&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;6th Pos winner (Team: “Zoo”), Kaggle Master, Philipp Singer, AMA Interview: &lt;a href=""https://www.kaggle.com/c/ieee-fraud-detection/discussion/111281""&gt;https://www.kaggle.com/c/ieee-fraud-detection/discussion/111281&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?auto=webp&amp;s=96c78f7af0d55bf809aea5e3aa74cbbcef56dd55', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb0b9f0fe0880768f4673eb0b0de4950a98b6b17', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c358b62ea4934234264e445b13378fcfa493bf6d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db1864c49e213ead3ece1212938c1640ab7b1535', 'width': 320, 'height': 320}], 'variants': {}, 'id': '3vbWKckhU81Qq--awNv_DObYSXMuYxFCxRDsUJAgfB0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ddk4lj,True,,init__27,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ddk4lj/interview_with_the_cto_of_hugging_face_julien/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ddk4lj/interview_with_the_cto_of_hugging_face_julien/,7135,1570261157.0,0,,False,,,,,,,,
768,,pytorch,,t2_2fv4yodo,False,,0,False,Hugging Face Implements SOTA Transformer Architectures for PyTorch and TensorFlow 2.0,[],r/pytorch,False,6,,0,44.0,,False,t3_dd8mp6,False,dark,0.91,,public,8,0,{},140.0,,False,[],,False,False,,{},,False,8,,False,https://b.thumbs.redditmedia.com/mUU6ng03p_afDAA-krXhDCOoQLhYx762ZJJwEqUBrbw.jpg,False,,[],{},link,,False,,1570230508.0,text,6,,,text,medium.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/UlOeEOUZ6Fh6QUiKaOkEIiGTR3NScRaPdUVvF70xL3A.jpg?auto=webp&amp;s=29790309289eb4b61882c8a208b1f63c63bae160', 'width': 1190, 'height': 382}, 'resolutions': [{'url': 'https://external-preview.redd.it/UlOeEOUZ6Fh6QUiKaOkEIiGTR3NScRaPdUVvF70xL3A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecc97c1aabe2ab5c647f95b21ada27b6c55c33bc', 'width': 108, 'height': 34}, {'url': 'https://external-preview.redd.it/UlOeEOUZ6Fh6QUiKaOkEIiGTR3NScRaPdUVvF70xL3A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bd3615b612c99522b549f695a1914baf469188d9', 'width': 216, 'height': 69}, {'url': 'https://external-preview.redd.it/UlOeEOUZ6Fh6QUiKaOkEIiGTR3NScRaPdUVvF70xL3A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=711fe311b8c072951998b9745da164bc32c5a80c', 'width': 320, 'height': 102}, {'url': 'https://external-preview.redd.it/UlOeEOUZ6Fh6QUiKaOkEIiGTR3NScRaPdUVvF70xL3A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=61180fef9261610eb6f1782a1633510f72809dce', 'width': 640, 'height': 205}, {'url': 'https://external-preview.redd.it/UlOeEOUZ6Fh6QUiKaOkEIiGTR3NScRaPdUVvF70xL3A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6664f177711e50e6ac042f56f6c75dea985ef737', 'width': 960, 'height': 308}, {'url': 'https://external-preview.redd.it/UlOeEOUZ6Fh6QUiKaOkEIiGTR3NScRaPdUVvF70xL3A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ab4c1553f40961c484e25beb10d2e26ffc977ed', 'width': 1080, 'height': 346}], 'variants': {}, 'id': 'G7i2WaqJVZl0wYJN38jSXhGZd8H0TUTcwNmGEcQKdus'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dd8mp6,True,,Yuqing7,,0,True,all_ads,False,[],False,,/r/pytorch/comments/dd8mp6/hugging_face_implements_sota_transformer/,all_ads,False,https://medium.com/syncedreview/hugging-face-implements-sota-transformer-architectures-for-pytorch-and-tensorflow-2-0-2e821dcb498d,7135,1570201708.0,0,,False,https://medium.com/syncedreview/hugging-face-implements-sota-transformer-architectures-for-pytorch-and-tensorflow-2-0-2e821dcb498d,,,,,,,
769,,pytorch,"I was recently working on a problem where I was required to read data from an 11 GB file. The traditional Dataset API of pytorch for this problem will require you to store whole data in memory, which in this case wasn't feasible. So I came across this IterableDataset class introduced in Pytorch 1.2 which helped me solve this problem. However, I couldn't find a good documentation on how to use this feature, so I decided to write a [tutorial](https://medium.com/@f2015791/how-to-use-pytorch-dataloaders-to-work-with-enormously-large-text-files-bbd672e955a0) on the same. I hope someone will find this useful.",t2_2ptyfgks,False,,0,False,Using Pytorch's dataloaders to deal with big size text files.,[],r/pytorch,False,6,,0,,,False,t3_dd72yx,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1570223033.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was recently working on a problem where I was required to read data from an 11 GB file. The traditional Dataset API of pytorch for this problem will require you to store whole data in memory, which in this case wasn&amp;#39;t feasible. So I came across this IterableDataset class introduced in Pytorch 1.2 which helped me solve this problem. However, I couldn&amp;#39;t find a good documentation on how to use this feature, so I decided to write a &lt;a href=""https://medium.com/@f2015791/how-to-use-pytorch-dataloaders-to-work-with-enormously-large-text-files-bbd672e955a0""&gt;tutorial&lt;/a&gt; on the same. I hope someone will find this useful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/cd1M0DmKS12YkYOphCyDW-wVv94CvKz_O8atoEsheDU.jpg?auto=webp&amp;s=4766315c742e5736ae940959666f373bdbeb190f', 'width': 1200, 'height': 786}, 'resolutions': [{'url': 'https://external-preview.redd.it/cd1M0DmKS12YkYOphCyDW-wVv94CvKz_O8atoEsheDU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=51545e0962568b0a4954d63b6b2b3cac8aea227c', 'width': 108, 'height': 70}, {'url': 'https://external-preview.redd.it/cd1M0DmKS12YkYOphCyDW-wVv94CvKz_O8atoEsheDU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3833b9299de6758ac060dfa60f5fa8a4f2162769', 'width': 216, 'height': 141}, {'url': 'https://external-preview.redd.it/cd1M0DmKS12YkYOphCyDW-wVv94CvKz_O8atoEsheDU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a8a3919ce175f68d6a1ec2e089051732a14657a', 'width': 320, 'height': 209}, {'url': 'https://external-preview.redd.it/cd1M0DmKS12YkYOphCyDW-wVv94CvKz_O8atoEsheDU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ab149c9ba66f7257c8964dc142723dd4780cfa0c', 'width': 640, 'height': 419}, {'url': 'https://external-preview.redd.it/cd1M0DmKS12YkYOphCyDW-wVv94CvKz_O8atoEsheDU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ceaf8e68d75c84b77243c5ba895832b9da2321ae', 'width': 960, 'height': 628}, {'url': 'https://external-preview.redd.it/cd1M0DmKS12YkYOphCyDW-wVv94CvKz_O8atoEsheDU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e10a33de5798f0cdf97d6eb0e6373473c6ddc3ed', 'width': 1080, 'height': 707}], 'variants': {}, 'id': 'PxWtL7Od6jM55Nkb8vrh1O0YFcdosVNnisdCqQzmMLY'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dd72yx,True,,kabirahuja2431,,2,True,all_ads,False,[],False,,/r/pytorch/comments/dd72yx/using_pytorchs_dataloaders_to_deal_with_big_size/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dd72yx/using_pytorchs_dataloaders_to_deal_with_big_size/,7135,1570194233.0,0,,False,,,,,,,,
770,,pytorch,"Here is the link to my repo:  [https://github.com/itisyeetimetoday/pytorch\_reggression/blob/master/sentdex.ipynb](https://github.com/itisyeetimetoday/pytorch_reggression/blob/master/sentdex.ipynb) 

When I run my code, I get this error: tensor(nan, grad\_fn=&lt;MseLossBackward&gt;) . I've tried batch normalization, however, that doesn't seem to work. How do I fix this error? Thank you for your time.",t2_2jezkxap,False,,0,False,"tensor(nan, grad_fn=&lt;MseLossBackward&gt;) when training CNN",[],r/pytorch,False,6,,0,,,False,t3_dd0o8w,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1570182105.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Here is the link to my repo:  &lt;a href=""https://github.com/itisyeetimetoday/pytorch_reggression/blob/master/sentdex.ipynb""&gt;https://github.com/itisyeetimetoday/pytorch_reggression/blob/master/sentdex.ipynb&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;When I run my code, I get this error: tensor(nan, grad_fn=&amp;lt;MseLossBackward&amp;gt;) . I&amp;#39;ve tried batch normalization, however, that doesn&amp;#39;t seem to work. How do I fix this error? Thank you for your time.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jT5_21tGYUqnXMX6CiYrHZNJyGUJFT0ibip7ZSXLMiE.jpg?auto=webp&amp;s=0432a32eff736aabbaf37ce24882dfb42b28ee25', 'width': 420, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/jT5_21tGYUqnXMX6CiYrHZNJyGUJFT0ibip7ZSXLMiE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f27e00d384b0ef781a7b1fd1f3eb7fff48735c4f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/jT5_21tGYUqnXMX6CiYrHZNJyGUJFT0ibip7ZSXLMiE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b4ee34b64c46c23f263d61ff3ac7e3499e4e2c70', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/jT5_21tGYUqnXMX6CiYrHZNJyGUJFT0ibip7ZSXLMiE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ffd7850116f912ad1d5199245864fe70476f89f', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'MUWZULkbxd9RmYJzFladxqa9t85M3lfRpyWyFmBXKPU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dd0o8w,True,,Stanley_C,,10,True,all_ads,False,[],False,,/r/pytorch/comments/dd0o8w/tensornan_grad_fnmselossbackward_when_training_cnn/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dd0o8w/tensornan_grad_fnmselossbackward_when_training_cnn/,7135,1570153305.0,0,,False,,,,,,,,
771,,pytorch,,t2_1259yeec,False,,0,False,what is your thoughts of TF 2.0 ? will you leave PyTorch on your next project in favour for Eager Execution ?,[],r/pytorch,False,6,,0,,,False,t3_dcqfk6,False,dark,0.7,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1570135766.0,text,6,,,text,self.pytorch,False,,,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dcqfk6,True,,MohamedRashad,,3,True,all_ads,False,[],False,,/r/pytorch/comments/dcqfk6/what_is_your_thoughts_of_tf_20_will_you_leave/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dcqfk6/what_is_your_thoughts_of_tf_20_will_you_leave/,7135,1570106966.0,0,,False,,,,,,,,
772,,pytorch,I have seen nn.conv2d and F.conv2d being used interchangeably and similarly for the ReLu function as well. I have tried to go through forums online but couldn't quite grasp the advantage one method has over another. Can anyone please point me towards any example in which the difference between the method is more highlighted?,t2_2k2kq16g,False,,0,False,[Question] What is the difference between torch.nn.x and torch.nn.fuctional.x,[],r/pytorch,False,6,,0,,,False,t3_dcpq3m,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1570131975.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have seen nn.conv2d and F.conv2d being used interchangeably and similarly for the ReLu function as well. I have tried to go through forums online but couldn&amp;#39;t quite grasp the advantage one method has over another. Can anyone please point me towards any example in which the difference between the method is more highlighted?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dcpq3m,True,,karrhikey97,,3,True,all_ads,False,[],False,,/r/pytorch/comments/dcpq3m/question_what_is_the_difference_between_torchnnx/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dcpq3m/question_what_is_the_difference_between_torchnnx/,7135,1570103175.0,1,,False,,,,,,,,
773,,pytorch,Is anyone using pytorch on a Mac with an eGPU setup?  All of the discussions I see about this are a year or two old... not sure what that means.,t2_4bwhg,False,,0,False,Pytorch on Mac + eGPU viable option?,[],r/pytorch,False,6,,0,,,False,t3_dbxtjv,False,dark,1.0,,public,9,0,{},,,False,[],,False,False,,{},,False,9,,False,self,False,,[],{},,,True,,1569984505.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is anyone using pytorch on a Mac with an eGPU setup?  All of the discussions I see about this are a year or two old... not sure what that means.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dbxtjv,True,,patniemeyer,,5,True,all_ads,False,[],False,,/r/pytorch/comments/dbxtjv/pytorch_on_mac_egpu_viable_option/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dbxtjv/pytorch_on_mac_egpu_viable_option/,7135,1569955705.0,0,,False,,,,,,,,
774,,pytorch," [ODSC India](https://india.odsc.com/) are looking for speakers for their 2020 conference in Bengaluru, India. If you've got an innovative application of PyTorch or cutting edge insights into data science or AI, you should check it out. It's a great way to engage with the community and share your knowledge! **Proposal submissions close on October 9:** [Submit here](https://confng.in/ySfDg6gX)

Conference Focus Areas:

* AI for Engineers
* Open Data Science
* Data Visualization
* Machine Learning &amp; Deep Learning
* Data Science at Scale
* Data Science Kick Start
* Math Behind AI
* AI for Good
* Data Management
* DataOps

Conference dates: 16-19 September, 2020",t2_4fjkjj7g,False,,0,False,[CFP] Got Pytorch expertise to share with the Data Science and AI community in India?,[],r/pytorch,False,6,,0,,,False,t3_dbqr6k,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1569950403.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://india.odsc.com/""&gt;ODSC India&lt;/a&gt; are looking for speakers for their 2020 conference in Bengaluru, India. If you&amp;#39;ve got an innovative application of PyTorch or cutting edge insights into data science or AI, you should check it out. It&amp;#39;s a great way to engage with the community and share your knowledge! &lt;strong&gt;Proposal submissions close on October 9:&lt;/strong&gt; &lt;a href=""https://confng.in/ySfDg6gX""&gt;Submit here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Conference Focus Areas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AI for Engineers&lt;/li&gt;
&lt;li&gt;Open Data Science&lt;/li&gt;
&lt;li&gt;Data Visualization&lt;/li&gt;
&lt;li&gt;Machine Learning &amp;amp; Deep Learning&lt;/li&gt;
&lt;li&gt;Data Science at Scale&lt;/li&gt;
&lt;li&gt;Data Science Kick Start&lt;/li&gt;
&lt;li&gt;Math Behind AI&lt;/li&gt;
&lt;li&gt;AI for Good&lt;/li&gt;
&lt;li&gt;Data Management&lt;/li&gt;
&lt;li&gt;DataOps&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Conference dates: 16-19 September, 2020&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jbm7eQ8t-GWyleXKcNcWdG37QR0aE72KLMeLbgmETag.jpg?auto=webp&amp;s=ce5470e690df2bcdbf550e56538e1f67c9888e5b', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/jbm7eQ8t-GWyleXKcNcWdG37QR0aE72KLMeLbgmETag.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4e7a82128b5c17513995057c7a12743ec38352a', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/jbm7eQ8t-GWyleXKcNcWdG37QR0aE72KLMeLbgmETag.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d49dfc38f1113e90f19b58f326bd18ea1794af3b', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/jbm7eQ8t-GWyleXKcNcWdG37QR0aE72KLMeLbgmETag.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7fdebaef19af5242c525fe855d2ac9fbca4cc4b5', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/jbm7eQ8t-GWyleXKcNcWdG37QR0aE72KLMeLbgmETag.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=51346a47b0b78c1d4864b7a58987576ec0d3afb2', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/jbm7eQ8t-GWyleXKcNcWdG37QR0aE72KLMeLbgmETag.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f211c33f247fbf167767654d3c6ba9482610de30', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/jbm7eQ8t-GWyleXKcNcWdG37QR0aE72KLMeLbgmETag.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ff8a8e19f94b91ef592384a90a0eac0aa1b8c6b1', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'JSSc2ZfGzw9SYSBHnfP7_iEOwRxw5P1hKGVKhJjmOqc'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dbqr6k,True,,yourdigitalvoice,,0,True,all_ads,False,[],False,,/r/pytorch/comments/dbqr6k/cfp_got_pytorch_expertise_to_share_with_the_data/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dbqr6k/cfp_got_pytorch_expertise_to_share_with_the_data/,7135,1569921603.0,0,,False,,,,,,,,
775,,pytorch,"Hi everyone. I’m trying to implement a video classification scheme, everything seems fine so far except one thing: exploding gradients in validation loop. I know it sounds strange because there’s not supposed to be gradients in the validation process, but that’s also what I don’t get. I’ve made sure to turn on eval() mode, and use torch.no_grad(), and somehow exploding gradients (with NaN outputs) still happens ONLY when there is a validation loop. I’ve tried commented out the validation code and the training code ran smoothly, so I figured something must be wrong with the validation code but I can’t put my hands on it. I’d really appreciate some help to point me in the right direction.

```

    for epoch in range(params.getint('num_epochs')):
        print('Starting epoch %i:' % (epoch + 1))
        print('*********Training*********')
        training_loss = 0
        training_losses = []
        training_progress = tqdm(enumerate(train_loader))
        artnet.train()
        for batch_index, (frames, label) in training_progress:
            training_progress.set_description('Batch no. %i: ' % batch_index)
            frames = frames.to(device)
            label = label.to(device)
            optimizer.zero_grad()

            output = artnet.forward(frames)
            loss = criterion(output, label)
            training_loss += loss.item()
            loss.backward()
            optimizer.step()
        else:
            avg_loss = training_loss / len(train_loader)
            training_losses.append(avg_loss)
            print(f'Training loss: {avg_loss}')

        print('*********Validating*********')
        validating_loss = 0
        validating_losses = []
        validating_progress = tqdm(enumerate(validation_loader))
        artnet.eval()
        with torch.no_grad():
            for batch_index, (frames, label) in validating_progress:
                validating_progress.set_description('Batch no. %i: ' % batch_index)
                frames = frames.to(device)
                label = label.to(device)
                output = artnet.forward(frames)
                loss = criterion(output, label)
                validating_loss += loss.item()
            else:
                avg_loss = validating_loss / len(validation_loader)
                validating_losses.append(avg_loss)
                print(f'Validating loss: {avg_loss}')

        print('=============================================')
        print('Epoch %i complete' % (epoch + 1))
        if (epoch + 1) % params.getint('ckpt') == 0:
            print('Saving checkpoint...' )
            torch.save(artnet.state_dict(), os.path.join(params['ckpt_path'], 'arnet_%i' % (epoch + 1)))
        # Update LR
        scheduler.step()
    print('Training complete, saving final model....')
    torch.save(artnet.state_dict(), os.path.join(params['ckpt_path'], 'arnet_final'))
    return training_losses, validating_losses
```",t2_y73ff,False,,0,False,Exploding gradients in validation,[],r/pytorch,False,6,,0,,,False,t3_db8trl,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1569834730.0,,[],{},,,True,,1569863341.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone. I’m trying to implement a video classification scheme, everything seems fine so far except one thing: exploding gradients in validation loop. I know it sounds strange because there’s not supposed to be gradients in the validation process, but that’s also what I don’t get. I’ve made sure to turn on eval() mode, and use torch.no_grad(), and somehow exploding gradients (with NaN outputs) still happens ONLY when there is a validation loop. I’ve tried commented out the validation code and the training code ran smoothly, so I figured something must be wrong with the validation code but I can’t put my hands on it. I’d really appreciate some help to point me in the right direction.&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for epoch in range(params.getint(&amp;#39;num_epochs&amp;#39;)):
    print(&amp;#39;Starting epoch %i:&amp;#39; % (epoch + 1))
    print(&amp;#39;*********Training*********&amp;#39;)
    training_loss = 0
    training_losses = []
    training_progress = tqdm(enumerate(train_loader))
    artnet.train()
    for batch_index, (frames, label) in training_progress:
        training_progress.set_description(&amp;#39;Batch no. %i: &amp;#39; % batch_index)
        frames = frames.to(device)
        label = label.to(device)
        optimizer.zero_grad()

        output = artnet.forward(frames)
        loss = criterion(output, label)
        training_loss += loss.item()
        loss.backward()
        optimizer.step()
    else:
        avg_loss = training_loss / len(train_loader)
        training_losses.append(avg_loss)
        print(f&amp;#39;Training loss: {avg_loss}&amp;#39;)

    print(&amp;#39;*********Validating*********&amp;#39;)
    validating_loss = 0
    validating_losses = []
    validating_progress = tqdm(enumerate(validation_loader))
    artnet.eval()
    with torch.no_grad():
        for batch_index, (frames, label) in validating_progress:
            validating_progress.set_description(&amp;#39;Batch no. %i: &amp;#39; % batch_index)
            frames = frames.to(device)
            label = label.to(device)
            output = artnet.forward(frames)
            loss = criterion(output, label)
            validating_loss += loss.item()
        else:
            avg_loss = validating_loss / len(validation_loader)
            validating_losses.append(avg_loss)
            print(f&amp;#39;Validating loss: {avg_loss}&amp;#39;)

    print(&amp;#39;=============================================&amp;#39;)
    print(&amp;#39;Epoch %i complete&amp;#39; % (epoch + 1))
    if (epoch + 1) % params.getint(&amp;#39;ckpt&amp;#39;) == 0:
        print(&amp;#39;Saving checkpoint...&amp;#39; )
        torch.save(artnet.state_dict(), os.path.join(params[&amp;#39;ckpt_path&amp;#39;], &amp;#39;arnet_%i&amp;#39; % (epoch + 1)))
    # Update LR
    scheduler.step()
print(&amp;#39;Training complete, saving final model....&amp;#39;)
torch.save(artnet.state_dict(), os.path.join(params[&amp;#39;ckpt_path&amp;#39;], &amp;#39;arnet_final&amp;#39;))
return training_losses, validating_losses
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,db8trl,True,,ryanaleksander,,10,True,all_ads,False,[],False,,/r/pytorch/comments/db8trl/exploding_gradients_in_validation/,all_ads,False,https://www.reddit.com/r/pytorch/comments/db8trl/exploding_gradients_in_validation/,7135,1569834541.0,0,,False,,,,,,,,
776,,pytorch,"How to fix CUDA out of memory 

I tested Super-SloMo from a person from github, and after long use, a message popped up: ""CUDA out of memory"" - I tried to change BrenchSize from BrenchSize = 4 to BrenchSize = 1 but it did not help",t2_146ga4,False,,0,False,CUDA out of memory,[],r/pytorch,False,6,,0,,,False,t3_danweh,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1569744276.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How to fix CUDA out of memory &lt;/p&gt;

&lt;p&gt;I tested Super-SloMo from a person from github, and after long use, a message popped up: &amp;quot;CUDA out of memory&amp;quot; - I tried to change BrenchSize from BrenchSize = 4 to BrenchSize = 1 but it did not help&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,danweh,True,,MrTajniak,,5,True,all_ads,False,[],False,,/r/pytorch/comments/danweh/cuda_out_of_memory/,all_ads,False,https://www.reddit.com/r/pytorch/comments/danweh/cuda_out_of_memory/,7135,1569715476.0,0,,False,,,,,,,,
777,,pytorch,"[Link to code snippet](https://drive.google.com/open?id=1npYIuXb8F6vU5HGNdXuZTkIO0KPUVh_S)

I am trying something like the above code. When I call output\_tensor.sum().backward(), I don't get any errors. But when I try stacked\_tensor.sum().backward(), I get the following error:

Error:  one of the variables needed for gradient computation has been modified by an inplace operation

The network is a single layer RNN. What am I doing wrong / missing?

P.S.: Pytorch newbie and stuck on this for hours! Please let me know if any additional info is required",t2_ul4iisf,False,,0,False,Unable to backprop through torch.stack,[],r/pytorch,False,6,,0,,,False,t3_dal1ga,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1569701187.0,,[],{},self,,True,,1569729614.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://drive.google.com/open?id=1npYIuXb8F6vU5HGNdXuZTkIO0KPUVh_S""&gt;Link to code snippet&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I am trying something like the above code. When I call output_tensor.sum().backward(), I don&amp;#39;t get any errors. But when I try stacked_tensor.sum().backward(), I get the following error:&lt;/p&gt;

&lt;p&gt;Error:  one of the variables needed for gradient computation has been modified by an inplace operation&lt;/p&gt;

&lt;p&gt;The network is a single layer RNN. What am I doing wrong / missing?&lt;/p&gt;

&lt;p&gt;P.S.: Pytorch newbie and stuck on this for hours! Please let me know if any additional info is required&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/luqXN0FrBs1O4_XFKDzz_f6eBAhXM-pfQVk6IMQbJJg.jpg?auto=webp&amp;s=34f0ff8f3fe0dce07b36d365bd6a978a0c29265e', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/luqXN0FrBs1O4_XFKDzz_f6eBAhXM-pfQVk6IMQbJJg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0c03fee382a259be5b0980f922a82440c4a263cc', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/luqXN0FrBs1O4_XFKDzz_f6eBAhXM-pfQVk6IMQbJJg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a1a2138a83f9e6b3abdb7bc69964d2ff253b2b1e', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/luqXN0FrBs1O4_XFKDzz_f6eBAhXM-pfQVk6IMQbJJg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=96c5b9ef92d5e2b1739ad279b9d478260c34166d', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/luqXN0FrBs1O4_XFKDzz_f6eBAhXM-pfQVk6IMQbJJg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec455bce4ca60aed10f4dce8e9130a9e46bfdc8e', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/luqXN0FrBs1O4_XFKDzz_f6eBAhXM-pfQVk6IMQbJJg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bef3084287afebb3ecdc15fe382dc5122ed3cd45', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/luqXN0FrBs1O4_XFKDzz_f6eBAhXM-pfQVk6IMQbJJg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c4100bb88c24f9e9814522f2d714a6090721db70', 'width': 1080, 'height': 567}], 'variants': {}, 'id': '2qdEnBbh8-ocf0GqAYBH1V5IhhTnSyeyKuXJl0CSdDQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,dal1ga,True,,xicor7017,,5,True,all_ads,False,[],False,,/r/pytorch/comments/dal1ga/unable_to_backprop_through_torchstack/,all_ads,False,https://www.reddit.com/r/pytorch/comments/dal1ga/unable_to_backprop_through_torchstack/,7135,1569700814.0,0,,False,,,,,,,,
778,,pytorch,"Hello my Pytorch fellows,

I am trying to implement some binarized neural networks (XNOR - net, ABC net, BiReal Net) but I am coping with the problem of reducing training/inference times and memory usage.

My question is, does anybody know how to implement custom Kernels for substituting the normal convolution with a XNOR one in Pytorch?

Greetings.",t2_28o0k78w,False,,0,False,Binarizing Networks (custom Kernels),[],r/pytorch,False,6,,0,,,False,t3_d9mqju,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1569547204.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello my Pytorch fellows,&lt;/p&gt;

&lt;p&gt;I am trying to implement some binarized neural networks (XNOR - net, ABC net, BiReal Net) but I am coping with the problem of reducing training/inference times and memory usage.&lt;/p&gt;

&lt;p&gt;My question is, does anybody know how to implement custom Kernels for substituting the normal convolution with a XNOR one in Pytorch?&lt;/p&gt;

&lt;p&gt;Greetings.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,d9mqju,True,,luzbel316,,0,True,all_ads,False,[],False,,/r/pytorch/comments/d9mqju/binarizing_networks_custom_kernels/,all_ads,False,https://www.reddit.com/r/pytorch/comments/d9mqju/binarizing_networks_custom_kernels/,7135,1569518404.0,0,,False,,,,,,,,
779,,pytorch,"I'm trying to implement a sentiment/affect analysis network using the lstm module. I followed [this](https://www.kaggle.com/ashukr/sentiment-analysis-using-lstm/notebook) tutorial and adapted it to my needs. 

My input consist of roughly 12.000 vectors of size 85. My targets are one-hot-vectors of size 4. I'm basically trying to get a prediction like so: 

    [12, 65, 98, .... 12, 34] -&gt; [0.121, 0.758, 0.011, 0.322]

My net is defined as follows: 

    class Lstm(nn.Module):
        def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):
            super(Lstm, self).__init__()

            self.output_size = output_size
            self.n_layers = n_layers
            self.hidden_dim = hidden_dim

            self.embedding = nn.Embedding(vocab_size,embedding_dim)
            self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers)
            self.FC = nn.Linear(hidden_dim, output_size)
            self.sig = nn.Sigmoid()

        def forward(self, x, hidden):	
            batch_size = x.size(0)
            
            embeds = self.embedding(x)
            lstm_out, hidden = self.lstm(embeds, hidden)

            #stack_up lstm outputs
            lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)
		
		out = self.FC(lstm_out) #lstm_out.shape -&gt; [4250, 4]
                sig_out = self.sig(out) #sig_out.shape -&gt; [4250, 4]
                sig_out = sig_out.view(batch_size, -1) #sig_out.shape -&gt; [50, 340]
                sig_out = sig_out[:, -1] #sig_out.shape -&gt; [50]
                return last sigmoid output and hidden state
	
		return sig_out, hidden

I run initiate the forward pass as follows: 

    batch_size = 50
    train_data = TensorDataset(train_x, train_y)
    test_data = TensorDataset(test_x, test_y)
    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)
    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)

    num_layers = 2
    vocab_size = 18 + 1 # there are 18 unique words
    embedding_dim = 85 
    hidden_dim = 256
    output_dim = 4

    lstm = Lstm(vocab_size, output_dim, embedding_dim, hidden_dim, num_layers)

    for inputs, targets in train_loader: 
        hidden = tuple([each.data for each in hidden])
        lstm.zero_grad()

        # get the output from the model
        output, hidden = lstm(inputs, hidden)
        loss = criterion(output.squeeze(), targets.float())
        loss.backward()
        optimizer.step()

I'm struggling with the forward pass. I can't get my head around what the shapes of the outputs of the layer mean, and how i can transform these outputs to get a 4-size vector at the end. 

Any help would be greatly appreciated.",t2_sf1br,False,,0,False,Transforming LSTM output for further calculations in linear layers,[],r/pytorch,False,6,,0,,,False,t3_d7rfvm,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1569192751.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to implement a sentiment/affect analysis network using the lstm module. I followed &lt;a href=""https://www.kaggle.com/ashukr/sentiment-analysis-using-lstm/notebook""&gt;this&lt;/a&gt; tutorial and adapted it to my needs. &lt;/p&gt;

&lt;p&gt;My input consist of roughly 12.000 vectors of size 85. My targets are one-hot-vectors of size 4. I&amp;#39;m basically trying to get a prediction like so: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[12, 65, 98, .... 12, 34] -&amp;gt; [0.121, 0.758, 0.011, 0.322]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My net is defined as follows: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Lstm(nn.Module):
    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):
        super(Lstm, self).__init__()

        self.output_size = output_size
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim

        self.embedding = nn.Embedding(vocab_size,embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers)
        self.FC = nn.Linear(hidden_dim, output_size)
        self.sig = nn.Sigmoid()

    def forward(self, x, hidden):   
        batch_size = x.size(0)

        embeds = self.embedding(x)
        lstm_out, hidden = self.lstm(embeds, hidden)

        #stack_up lstm outputs
        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)

    out = self.FC(lstm_out) #lstm_out.shape -&amp;gt; [4250, 4]
            sig_out = self.sig(out) #sig_out.shape -&amp;gt; [4250, 4]
            sig_out = sig_out.view(batch_size, -1) #sig_out.shape -&amp;gt; [50, 340]
            sig_out = sig_out[:, -1] #sig_out.shape -&amp;gt; [50]
            return last sigmoid output and hidden state

    return sig_out, hidden
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I run initiate the forward pass as follows: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;batch_size = 50
train_data = TensorDataset(train_x, train_y)
test_data = TensorDataset(test_x, test_y)
train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)
test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)

num_layers = 2
vocab_size = 18 + 1 # there are 18 unique words
embedding_dim = 85 
hidden_dim = 256
output_dim = 4

lstm = Lstm(vocab_size, output_dim, embedding_dim, hidden_dim, num_layers)

for inputs, targets in train_loader: 
    hidden = tuple([each.data for each in hidden])
    lstm.zero_grad()

    # get the output from the model
    output, hidden = lstm(inputs, hidden)
    loss = criterion(output.squeeze(), targets.float())
    loss.backward()
    optimizer.step()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;#39;m struggling with the forward pass. I can&amp;#39;t get my head around what the shapes of the outputs of the layer mean, and how i can transform these outputs to get a 4-size vector at the end. &lt;/p&gt;

&lt;p&gt;Any help would be greatly appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5Tkh6gz4EE0cyYn4NmDZG1Uub5uhL-EKFwj2SCtIHwk.jpg?auto=webp&amp;s=40998a0459471322772b58596cff31dacdf46b6c', 'width': 100, 'height': 100}, 'resolutions': [], 'variants': {}, 'id': 'vPtmgU0_61ttVvYdHgyJBNHQvGFq53XeFke7MwVLuOE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,d7rfvm,True,,tostre,,8,True,all_ads,False,[],False,,/r/pytorch/comments/d7rfvm/transforming_lstm_output_for_further_calculations/,all_ads,False,https://www.reddit.com/r/pytorch/comments/d7rfvm/transforming_lstm_output_for_further_calculations/,7135,1569163951.0,0,,False,,,,,,,,
780,,pytorch,"Hello I am training a neural network in Pytorch with Optimizer Adam.

The thing is that data keeps coming every certain time and I need to retrain it but without reinitialization. For that I thing I should reset Adam Optimizer, is it correct? Does it resets automatically? If not how to reset it?

To make things more clear:

a) Dataset 1 -&gt; train parameters starting from random weights.

After some time...

b) Dataset 2 ( Dataset 1 is subset of dataset ) -&gt; Train again but starting from parameters obtained first time training but I would like to reset Adam parameters.

Thanks in advance.",t2_11t3v5,False,,0,False,Retrain neural network,[],r/pytorch,False,6,,0,,,False,t3_d7p1r6,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1569179512.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello I am training a neural network in Pytorch with Optimizer Adam.&lt;/p&gt;

&lt;p&gt;The thing is that data keeps coming every certain time and I need to retrain it but without reinitialization. For that I thing I should reset Adam Optimizer, is it correct? Does it resets automatically? If not how to reset it?&lt;/p&gt;

&lt;p&gt;To make things more clear:&lt;/p&gt;

&lt;p&gt;a) Dataset 1 -&amp;gt; train parameters starting from random weights.&lt;/p&gt;

&lt;p&gt;After some time...&lt;/p&gt;

&lt;p&gt;b) Dataset 2 ( Dataset 1 is subset of dataset ) -&amp;gt; Train again but starting from parameters obtained first time training but I would like to reset Adam parameters.&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,d7p1r6,True,,LazyButAmbitious,,1,True,all_ads,False,[],False,,/r/pytorch/comments/d7p1r6/retrain_neural_network/,all_ads,False,https://www.reddit.com/r/pytorch/comments/d7p1r6/retrain_neural_network/,7135,1569150712.0,0,,False,,,,,,,,
781,,pytorch,"**Hi /r/pytorch**,

# What is [torchdata](https://github.com/szymonmaszke/torchdata)

I would like to present you a new open source PyTorch based project ([__torchdata__](https://szymonmaszke.github.io/torchdata/)) which extends capabilities of `torch.utils.data.Dataset` by bringing `map`, `cache` and other operations known from `tensorflow.data.Dataset` (and actually a little more than that).

### All that with a single line of code: `super().__init__()`

For more, check [documentation](https://szymonmaszke.github.io/torchdata/) or [github repository](https://github.com/szymonmaszke/torchdata).

# Functionalities Overview

* Use `map`, `apply`, `reduce` or `filter`
* `cache` data in RAM or on disk (even partial caching, say first `20%` RAM and the rest on disk)
* Full PyTorch's [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and [`IterableDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset&gt;) support (including [`torchvision`](https://pytorch.org/docs/stable/torchvision/index.html))
* General `torchdata.maps` like `Flatten` or `Select`
* Concrete `torchdata.datasets` designed for file reading and other general tasks

# Example

- Create image reading dataset 

        import torchdata
        import torchvision


        class Images(torchdata.Dataset): # Different inheritance
             def __init__(self, path: str):
                super().__init__() # This is the only change
            self.files = [file for file in pathlib.Path(path).glob(""*"")]

        def __getitem__(self, index):
            return Image.open(self.files[index])

        def __len__(self):
            return len(self.files)

- `map` each element to `torch.Tensor` and `cache()` everything in memory:

        images = Images(""./data"").map(torchvision.transforms.ToTensor()).cache()

- concatenate with labels (another `torchdata.Dataset` instance) and iterate over:

        for data, label in images | labels:
            # Do whatever you want with your data

# Installation 

`pip` is the easiest of course:

        pip install torchdata

You can also use `nightly` releases (`torchdata-nightly`) or GPU/CPU Docker based images (check documentation). Hopefully `conda` will be released soon as well, stay tuned

BTW. You can also checkout [__torchfunc__](https://github.com/szymonmaszke/torchfunc), I plan to make a separate post about that in a week or so.

Thanks for checking",t2_1bq357tt,False,,0,False,"[Project] torchdata: Implement map, cache, filter etc. within PyTorch's Datasets (like Tensorflow's tf.data and more)",[],r/pytorch,False,6,,0,,,False,t3_d79z0c,False,dark,1.0,,public,8,0,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},self,,True,,1569098326.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;strong&gt;Hi &lt;a href=""/r/pytorch""&gt;/r/pytorch&lt;/a&gt;&lt;/strong&gt;,&lt;/p&gt;

&lt;h1&gt;What is &lt;a href=""https://github.com/szymonmaszke/torchdata""&gt;torchdata&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;I would like to present you a new open source PyTorch based project (&lt;a href=""https://szymonmaszke.github.io/torchdata/""&gt;&lt;strong&gt;torchdata&lt;/strong&gt;&lt;/a&gt;) which extends capabilities of &lt;code&gt;torch.utils.data.Dataset&lt;/code&gt; by bringing &lt;code&gt;map&lt;/code&gt;, &lt;code&gt;cache&lt;/code&gt; and other operations known from &lt;code&gt;tensorflow.data.Dataset&lt;/code&gt; (and actually a little more than that).&lt;/p&gt;

&lt;h3&gt;All that with a single line of code: &lt;code&gt;super().__init__()&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;For more, check &lt;a href=""https://szymonmaszke.github.io/torchdata/""&gt;documentation&lt;/a&gt; or &lt;a href=""https://github.com/szymonmaszke/torchdata""&gt;github repository&lt;/a&gt;.&lt;/p&gt;

&lt;h1&gt;Functionalities Overview&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Use &lt;code&gt;map&lt;/code&gt;, &lt;code&gt;apply&lt;/code&gt;, &lt;code&gt;reduce&lt;/code&gt; or &lt;code&gt;filter&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cache&lt;/code&gt; data in RAM or on disk (even partial caching, say first &lt;code&gt;20%&lt;/code&gt; RAM and the rest on disk)&lt;/li&gt;
&lt;li&gt;Full PyTorch&amp;#39;s &lt;a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset""&gt;&lt;code&gt;Dataset&lt;/code&gt;&lt;/a&gt; and &lt;a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset""&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; support (including &lt;a href=""https://pytorch.org/docs/stable/torchvision/index.html""&gt;&lt;code&gt;torchvision&lt;/code&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;General &lt;code&gt;torchdata.maps&lt;/code&gt; like &lt;code&gt;Flatten&lt;/code&gt; or &lt;code&gt;Select&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Concrete &lt;code&gt;torchdata.datasets&lt;/code&gt; designed for file reading and other general tasks&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;Example&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Create image reading dataset &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import torchdata
import torchvision

class Images(torchdata.Dataset): # Different inheritance
     def __init__(self, path: str):
        super().__init__() # This is the only change
    self.files = [file for file in pathlib.Path(path).glob(&amp;quot;*&amp;quot;)]

def __getitem__(self, index):
    return Image.open(self.files[index])

def __len__(self):
    return len(self.files)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;map&lt;/code&gt; each element to &lt;code&gt;torch.Tensor&lt;/code&gt; and &lt;code&gt;cache()&lt;/code&gt; everything in memory:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;images = Images(&amp;quot;./data&amp;quot;).map(torchvision.transforms.ToTensor()).cache()
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;concatenate with labels (another &lt;code&gt;torchdata.Dataset&lt;/code&gt; instance) and iterate over:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for data, label in images | labels:
    # Do whatever you want with your data
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;Installation&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;pip&lt;/code&gt; is the easiest of course:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    pip install torchdata
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also use &lt;code&gt;nightly&lt;/code&gt; releases (&lt;code&gt;torchdata-nightly&lt;/code&gt;) or GPU/CPU Docker based images (check documentation). Hopefully &lt;code&gt;conda&lt;/code&gt; will be released soon as well, stay tuned&lt;/p&gt;

&lt;p&gt;BTW. You can also checkout &lt;a href=""https://github.com/szymonmaszke/torchfunc""&gt;&lt;strong&gt;torchfunc&lt;/strong&gt;&lt;/a&gt;, I plan to make a separate post about that in a week or so.&lt;/p&gt;

&lt;p&gt;Thanks for checking&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jzGWvkTTccuSQzDdQb60oxImw17s14f940b7f9LcfpY.jpg?auto=webp&amp;s=33ac43da1bd14d49a3b920ade7b6b5f62c83e44a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/jzGWvkTTccuSQzDdQb60oxImw17s14f940b7f9LcfpY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=217f9806047602b4dd0ffd922ead2bf9f0cb1476', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/jzGWvkTTccuSQzDdQb60oxImw17s14f940b7f9LcfpY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1bd535e4506e607faed0a3726a99b43eeb94c98e', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/jzGWvkTTccuSQzDdQb60oxImw17s14f940b7f9LcfpY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b96130017586140a80c75a33a92ddff4af3eb298', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'S4UiFajST-sn1ptkHDBThjToZTcyghHRSajwYoGsv4k'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,d79z0c,True,,szymonmaszke,,0,True,all_ads,False,[],False,,/r/pytorch/comments/d79z0c/project_torchdata_implement_map_cache_filter_etc/,all_ads,False,https://www.reddit.com/r/pytorch/comments/d79z0c/project_torchdata_implement_map_cache_filter_etc/,7135,1569069526.0,0,,False,,,,,,,,
782,,pytorch,Does anyone care to explain how to use the Caffe2 C++ framework today? Does it come with pytorch now since they are merged? I am trying to build a simple project in Visual Studio 2019 using Caffe2 in Windows 10.,t2_euthh,False,,0,False,Using Caffe2 C++ framework in 2019,[],r/pytorch,False,6,,0,,,False,t3_d7a3d7,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1569099063.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone care to explain how to use the Caffe2 C++ framework today? Does it come with pytorch now since they are merged? I am trying to build a simple project in Visual Studio 2019 using Caffe2 in Windows 10.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,d7a3d7,True,,Fulafirren,,2,True,all_ads,False,[],False,,/r/pytorch/comments/d7a3d7/using_caffe2_c_framework_in_2019/,all_ads,False,https://www.reddit.com/r/pytorch/comments/d7a3d7/using_caffe2_c_framework_in_2019/,7135,1569070263.0,0,,False,,,,,,,,
783,,pytorch,"Just wanted to say that I'm rewriting the GDL book in Pytorch here [https://github.com/MLSlayer/Generative-Deep-Learning-Code-in-Pytorch](https://github.com/MLSlayer/Generative-Deep-Learning-Code-in-Pytorch)

Any feedback or help would be appreciated!",t2_4maltkik,False,,0,False,Rewriting Generative Deep Learning Book from David Foster in Pytorch,[],r/pytorch,False,6,,0,,,False,t3_d6ldev,False,dark,1.0,,public,17,0,{},,,False,[],,False,False,,{},,False,17,,False,self,False,,[],{},self,,True,,1568960629.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just wanted to say that I&amp;#39;m rewriting the GDL book in Pytorch here &lt;a href=""https://github.com/MLSlayer/Generative-Deep-Learning-Code-in-Pytorch""&gt;https://github.com/MLSlayer/Generative-Deep-Learning-Code-in-Pytorch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Any feedback or help would be appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/W8Danl-3HivXiNW6X9f_u509XPWwXdbOFgWbpJdhnzU.jpg?auto=webp&amp;s=e36697a3b2e66c0334c9ffd80b1367c1eaa70b66', 'width': 318, 'height': 318}, 'resolutions': [{'url': 'https://external-preview.redd.it/W8Danl-3HivXiNW6X9f_u509XPWwXdbOFgWbpJdhnzU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eb4b01c7f65fa24f5ee4800ae28102eb2ac168f3', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/W8Danl-3HivXiNW6X9f_u509XPWwXdbOFgWbpJdhnzU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2b71a5062738969294d13d03d82a69a59a904254', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'v3jKTLNX844VIZaIKkddYCs_OQihEeNP7Th2HPSXfkA'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,d6ldev,True,,mlslayer,,1,True,all_ads,False,[],False,,/r/pytorch/comments/d6ldev/rewriting_generative_deep_learning_book_from/,all_ads,False,https://www.reddit.com/r/pytorch/comments/d6ldev/rewriting_generative_deep_learning_book_from/,7135,1568931829.0,0,,False,,,,,,,,
784,,pytorch,"Following is my code:

`from torchvision import datasets, models, transforms` 

`import matplotlib.image as mpimg` 

`import matplotlib.pyplot as plt` 

`import torch` 

`data_transforms = transforms.Compose([ transforms.RandomResizedCrop(256), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])` 

`img = mpimg.imread('dataset/train/cats/cat.1.jpg')` 

`print(type(img))` 

`print(len(img))`  

`img = data_transforms(img)`

Error Message:

`Traceback (most recent call last):`

 `File ""Image_Preprocessing.py"", line 47, in &lt;module&gt;` 

`img = data_transforms(img)` 

`File ""/home/debayon/anaconda3/envs/ENV2/lib/python3.7/site-packages/torchvision/transforms/transforms.py"", line 61, in __call__ img = t(img)`

 `File ""/home/debayon/anaconda3/envs/ENV2/lib/python3.7/site-packages/torchvision/transforms/transforms.py"", line 676, in __call__ i, j, h, w = self.get_params(img, self.scale, self.ratio)` 

`File ""/home/debayon/anaconda3/envs/ENV2/lib/python3.7/site-packages/torchvision/transforms/transforms.py"", line 638, in get_params area = img.size[0] * img.size[1] TypeError: 'int' object is not subscriptable`

&amp;#x200B;

I am unable to understand why is that occuring and please tell how can I solve it?Thank You.",t2_29ruz6z1,False,,0,False,Applying Transforms to a single image: ERROR,[],r/pytorch,False,6,,0,,,False,t3_d5vipn,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1568829701.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Following is my code:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;from torchvision import datasets, models, transforms&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;import matplotlib.image as mpimg&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;import matplotlib.pyplot as plt&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;import torch&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;data_transforms = transforms.Compose([ transforms.RandomResizedCrop(256), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;img = mpimg.imread(&amp;#39;dataset/train/cats/cat.1.jpg&amp;#39;)&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(type(img))&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(len(img))&lt;/code&gt;  &lt;/p&gt;

&lt;p&gt;&lt;code&gt;img = data_transforms(img)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Error Message:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Traceback (most recent call last):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;File &amp;quot;Image_Preprocessing.py&amp;quot;, line 47, in &amp;lt;module&amp;gt;&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;img = data_transforms(img)&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;File &amp;quot;/home/debayon/anaconda3/envs/ENV2/lib/python3.7/site-packages/torchvision/transforms/transforms.py&amp;quot;, line 61, in __call__ img = t(img)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;File &amp;quot;/home/debayon/anaconda3/envs/ENV2/lib/python3.7/site-packages/torchvision/transforms/transforms.py&amp;quot;, line 676, in __call__ i, j, h, w = self.get_params(img, self.scale, self.ratio)&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;File &amp;quot;/home/debayon/anaconda3/envs/ENV2/lib/python3.7/site-packages/torchvision/transforms/transforms.py&amp;quot;, line 638, in get_params area = img.size[0] * img.size[1] TypeError: &amp;#39;int&amp;#39; object is not subscriptable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am unable to understand why is that occuring and please tell how can I solve it?Thank You.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,d5vipn,True,,debayon,,0,True,all_ads,False,[],False,,/r/pytorch/comments/d5vipn/applying_transforms_to_a_single_image_error/,all_ads,False,https://www.reddit.com/r/pytorch/comments/d5vipn/applying_transforms_to_a_single_image_error/,7135,1568800901.0,0,,False,,,,,,,,
785,,pytorch,"So I am new to PyTorch and just learned about PyTorch's autograd functionality. So I tried to create a simple linear regression program. But I am getting a very high error and not able to figure out the reason. Can you help me, what is wrong in the following program?

```python
mport torch

torch.manual_seed(42)

noise = torch.rand(100, 1) / 10
x = torch.randn(100, 1) * 10
y = (2 * x) + 3 + noise

theta = torch.rand(1, requires_grad=True)
beta = torch.rand(1, requires_grad=True)

print(""Theta = {}, Beta = {}"".format(theta, beta))


alpha = 0.1

for i in range(20):
    pred = beta + theta * x
    diff = (y - pred)
    error = (diff ** 2).mean()
    error.backward()
    print(""epoch = {}, error = {}"".format(i+1, error))
    
    with torch.no_grad():
        theta -= alpha * theta.grad
        beta -= alpha * beta.grad
        
    theta.grad.zero_()
    beta.grad.zero_()
    
print(theta, beta)
```

output 
```
Theta = tensor([0.5723], requires_grad=True), Beta = tensor([0.3705], requires_grad=True)
epoch = 1, error = 145.42373657226562
epoch = 2, error = 22058.08203125
epoch = 3, error = 3518530.25
epoch = 4, error = 561364224.0
epoch = 5, error = 89563004928.0
epoch = 6, error = 14289343610880.0
epoch = 7, error = 2279797631746048.0
epoch = 8, error = 3.637308159741133e+17
epoch = 9, error = 5.803153761876155e+19
epoch = 10, error = 9.25865548813541e+21
epoch = 11, error = 1.4771741925940593e+24
epoch = 12, error = 2.356761741347795e+26
epoch = 13, error = 3.760102142808153e+28
epoch = 14, error = 5.999064079468961e+30
epoch = 15, error = 9.571222390675241e+32
epoch = 16, error = 1.5270433071943214e+35
epoch = 17, error = inf
epoch = 18, error = inf
epoch = 19, error = inf
epoch = 20, error = inf
tensor([-1.5219e+22], requires_grad=True) tensor([1.6865e+19], requires_grad=True)
```",,False,,0,False,"Error shooting for linear regression, using autograd",[],r/pytorch,False,6,,0,,,False,t3_d5lclr,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,,self,1568746802.0,,,{},,,True,,1568774987.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I am new to PyTorch and just learned about PyTorch&amp;#39;s autograd functionality. So I tried to create a simple linear regression program. But I am getting a very high error and not able to figure out the reason. Can you help me, what is wrong in the following program?&lt;/p&gt;

&lt;p&gt;```python
mport torch&lt;/p&gt;

&lt;p&gt;torch.manual_seed(42)&lt;/p&gt;

&lt;p&gt;noise = torch.rand(100, 1) / 10
x = torch.randn(100, 1) * 10
y = (2 * x) + 3 + noise&lt;/p&gt;

&lt;p&gt;theta = torch.rand(1, requires_grad=True)
beta = torch.rand(1, requires_grad=True)&lt;/p&gt;

&lt;p&gt;print(&amp;quot;Theta = {}, Beta = {}&amp;quot;.format(theta, beta))&lt;/p&gt;

&lt;p&gt;alpha = 0.1&lt;/p&gt;

&lt;p&gt;for i in range(20):
    pred = beta + theta * x
    diff = (y - pred)
    error = (diff ** 2).mean()
    error.backward()
    print(&amp;quot;epoch = {}, error = {}&amp;quot;.format(i+1, error))&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;with torch.no_grad():
    theta -= alpha * theta.grad
    beta -= alpha * beta.grad

theta.grad.zero_()
beta.grad.zero_()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;print(theta, beta)
```&lt;/p&gt;

&lt;p&gt;output 
&lt;code&gt;
Theta = tensor([0.5723], requires_grad=True), Beta = tensor([0.3705], requires_grad=True)
epoch = 1, error = 145.42373657226562
epoch = 2, error = 22058.08203125
epoch = 3, error = 3518530.25
epoch = 4, error = 561364224.0
epoch = 5, error = 89563004928.0
epoch = 6, error = 14289343610880.0
epoch = 7, error = 2279797631746048.0
epoch = 8, error = 3.637308159741133e+17
epoch = 9, error = 5.803153761876155e+19
epoch = 10, error = 9.25865548813541e+21
epoch = 11, error = 1.4771741925940593e+24
epoch = 12, error = 2.356761741347795e+26
epoch = 13, error = 3.760102142808153e+28
epoch = 14, error = 5.999064079468961e+30
epoch = 15, error = 9.571222390675241e+32
epoch = 16, error = 1.5270433071943214e+35
epoch = 17, error = inf
epoch = 18, error = inf
epoch = 19, error = inf
epoch = 20, error = inf
tensor([-1.5219e+22], requires_grad=True) tensor([1.6865e+19], requires_grad=True)
&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,d5lclr,True,,[deleted],,2,True,all_ads,False,[],,dark,/r/pytorch/comments/d5lclr/error_shooting_for_linear_regression_using/,all_ads,False,https://www.reddit.com/r/pytorch/comments/d5lclr/error_shooting_for_linear_regression_using/,7135,1568746187.0,0,,False,,,,,,,,
786,,pytorch,,t2_10efjmjx,False,,0,False,"[P] SpeedTorch. 4x faster pinned CPU -&gt; GPU data transfer than Pytorch pinned CPU tensors, and 110x faster GPU -&gt; CPU transfer. Augment parameter size by hosting on CPU. Use non sparse optimizers (Adadelta, Adamax, RMSprop, Rprop, etc.) for sparse training (word2vec, node2vec, GloVe, NCF, etc.).",[],r/pytorch,False,6,,0,,,False,t3_d5ah9r,False,dark,1.0,,public,12,0,{},,,False,[],,False,False,,{},,False,12,,False,default,False,,[],{},link,,False,,1568714825.0,text,6,,,text,self.MachineLearning,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/HXaD9AXcJOYhOEi1lQmyu3EPPVIozvFqLonNGQiL5vU.png?auto=webp&amp;s=3201dd60a9fe9946560f053a6162c7fd84b7b8e0', 'width': 2000, 'height': 1696}, 'resolutions': [{'url': 'https://external-preview.redd.it/HXaD9AXcJOYhOEi1lQmyu3EPPVIozvFqLonNGQiL5vU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=943f048a1f9302107d74cae43fed52cdb19fc641', 'width': 108, 'height': 91}, {'url': 'https://external-preview.redd.it/HXaD9AXcJOYhOEi1lQmyu3EPPVIozvFqLonNGQiL5vU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=06a61aa713ab71e804a6d3a987b68623f583d5fe', 'width': 216, 'height': 183}, {'url': 'https://external-preview.redd.it/HXaD9AXcJOYhOEi1lQmyu3EPPVIozvFqLonNGQiL5vU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6bf15785b93c7854237b888c5fe068a9bdcee28a', 'width': 320, 'height': 271}, {'url': 'https://external-preview.redd.it/HXaD9AXcJOYhOEi1lQmyu3EPPVIozvFqLonNGQiL5vU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=28f75dce306c09a64d07705f4ec726f486e45120', 'width': 640, 'height': 542}, {'url': 'https://external-preview.redd.it/HXaD9AXcJOYhOEi1lQmyu3EPPVIozvFqLonNGQiL5vU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a3e911acc5b07a96bdd5148df5413b3346575e68', 'width': 960, 'height': 814}, {'url': 'https://external-preview.redd.it/HXaD9AXcJOYhOEi1lQmyu3EPPVIozvFqLonNGQiL5vU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=85c373395ba70278807b0d6d6e83f299c30f42ef', 'width': 1080, 'height': 915}], 'variants': {}, 'id': 'xDnKPq63bVvsewA_WZEYqqEWiX7c_adm_shS6ghIUXM'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,d5ah9r,True,,BatmantoshReturns,,0,True,all_ads,False,[],False,,/r/pytorch/comments/d5ah9r/p_speedtorch_4x_faster_pinned_cpu_gpu_data/,all_ads,False,https://www.reddit.com/r/MachineLearning/comments/d4recl/p_speedtorch_4x_faster_pinned_cpu_gpu_data/,7135,1568686025.0,0,,False,https://www.reddit.com/r/MachineLearning/comments/d4recl/p_speedtorch_4x_faster_pinned_cpu_gpu_data/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': ""https://i.imgur.com/wr4VaUV.png\n\nhttps://github.com/Santosh-Gupta/SpeedTorch\n\nThis is library I made for Pytorch, for fast transfer between pinned CPU tensors and GPU pytorch variables. The inspiration came from needing to train large number of embeddings, which don't all fit on GPU ram at a desired embedding size, so I needed a faster CPU &lt;-&gt; GPU transfer method. This also allows using any optimizer for sparse training, since every embedding contained in the Pytorch embedding variable receives an update, previously only Pytorch's SGD, Adagrad, and SparseAdam were suitable for such training. \n\nIn addition to augmenting parameter sizes, you can use to increase the speed of which data on your CPU is transferred to Pytorch Cuda variables. \n\nAlso, SpeedTorch's GPU tensors are also overall faster then Pytorch cuda tensors, when taking into account both transferring two and from (overall 2.6x faster). For just transfering to a Pytorch Cuda, Pytorch is still faster, but significantly slower when transfering from a Pytorch Cuda variable. \n\nI have personally used this to nearly double the embedding size of embeddings in two other projects, by holding half the parameters on CPU. The training speed is decent thanks to the fast CPU&lt;-&gt;GPU exchange. \n\nhttps://github.com/Santosh-Gupta/Research2Vec2\n\nhttps://github.com/Santosh-Gupta/lit2vec2\n\nThere's a bit of a learning curve for the very first time getting started with it, so as soon as you run into any sort of friction, feel free to ask a question on the project gitter\n\nhttps://gitter.im/SpeedTorch\n\nAnd I'll answer them. \n\nhttps://i.imgur.com/6o8C1BP.gif"", 'author_fullname': 't2_10efjmjx', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[P] SpeedTorch. 4x faster pinned CPU -&gt; GPU data transfer than Pytorch pinned CPU tensors, and 110x faster GPU -&gt; CPU transfer. Augment parameter size by hosting on CPU. Use non sparse optimizers (Adadelta, Adamax, RMSprop, Rprop, etc.) for sparse training (word2vec, node2vec, GloVe, NCF, etc.).', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'four', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_d4recl', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 151, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 151, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': 1568768118.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1568614007.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://i.imgur.com/wr4VaUV.png""&gt;https://i.imgur.com/wr4VaUV.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://github.com/Santosh-Gupta/SpeedTorch""&gt;https://github.com/Santosh-Gupta/SpeedTorch&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is library I made for Pytorch, for fast transfer between pinned CPU tensors and GPU pytorch variables. The inspiration came from needing to train large number of embeddings, which don&amp;#39;t all fit on GPU ram at a desired embedding size, so I needed a faster CPU &amp;lt;-&amp;gt; GPU transfer method. This also allows using any optimizer for sparse training, since every embedding contained in the Pytorch embedding variable receives an update, previously only Pytorch&amp;#39;s SGD, Adagrad, and SparseAdam were suitable for such training. &lt;/p&gt;\n\n&lt;p&gt;In addition to augmenting parameter sizes, you can use to increase the speed of which data on your CPU is transferred to Pytorch Cuda variables. &lt;/p&gt;\n\n&lt;p&gt;Also, SpeedTorch&amp;#39;s GPU tensors are also overall faster then Pytorch cuda tensors, when taking into account both transferring two and from (overall 2.6x faster). For just transfering to a Pytorch Cuda, Pytorch is still faster, but significantly slower when transfering from a Pytorch Cuda variable. &lt;/p&gt;\n\n&lt;p&gt;I have personally used this to nearly double the embedding size of embeddings in two other projects, by holding half the parameters on CPU. The training speed is decent thanks to the fast CPU&amp;lt;-&amp;gt;GPU exchange. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://github.com/Santosh-Gupta/Research2Vec2""&gt;https://github.com/Santosh-Gupta/Research2Vec2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://github.com/Santosh-Gupta/lit2vec2""&gt;https://github.com/Santosh-Gupta/lit2vec2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a bit of a learning curve for the very first time getting started with it, so as soon as you run into any sort of friction, feel free to ask a question on the project gitter&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://gitter.im/SpeedTorch""&gt;https://gitter.im/SpeedTorch&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And I&amp;#39;ll answer them. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://i.imgur.com/6o8C1BP.gif""&gt;https://i.imgur.com/6o8C1BP.gif&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/HXaD9AXcJOYhOEi1lQmyu3EPPVIozvFqLonNGQiL5vU.png?auto=webp&amp;s=3201dd60a9fe9946560f053a6162c7fd84b7b8e0', 'width': 2000, 'height': 1696}, 'resolutions': [{'url': 'https://external-preview.redd.it/HXaD9AXcJOYhOEi1lQmyu3EPPVIozvFqLonNGQiL5vU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=943f048a1f9302107d74cae43fed52cdb19fc641', 'width': 108, 'height': 91}, {'url': 'https://external-preview.redd.it/HXaD9AXcJOYhOEi1lQmyu3EPPVIozvFqLonNGQiL5vU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=06a61aa713ab71e804a6d3a987b68623f583d5fe', 'width': 216, 'height': 183}, {'url': 'https://external-preview.redd.it/HXaD9AXcJOYhOEi1lQmyu3EPPVIozvFqLonNGQiL5vU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6bf15785b93c7854237b888c5fe068a9bdcee28a', 'width': 320, 'height': 271}, {'url': 'https://external-preview.redd.it/HXaD9AXcJOYhOEi1lQmyu3EPPVIozvFqLonNGQiL5vU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=28f75dce306c09a64d07705f4ec726f486e45120', 'width': 640, 'height': 542}, {'url': 'https://external-preview.redd.it/HXaD9AXcJOYhOEi1lQmyu3EPPVIozvFqLonNGQiL5vU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a3e911acc5b07a96bdd5148df5413b3346575e68', 'width': 960, 'height': 814}, {'url': 'https://external-preview.redd.it/HXaD9AXcJOYhOEi1lQmyu3EPPVIozvFqLonNGQiL5vU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=85c373395ba70278807b0d6d6e83f299c30f42ef', 'width': 1080, 'height': 915}], 'variants': {}, 'id': 'xDnKPq63bVvsewA_WZEYqqEWiX7c_adm_shS6ghIUXM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'd4recl', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'BatmantoshReturns', 'discussion_type': None, 'num_comments': 15, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/d4recl/p_speedtorch_4x_faster_pinned_cpu_gpu_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/d4recl/p_speedtorch_4x_faster_pinned_cpu_gpu_data/', 'subreddit_subscribers': 1740778, 'created_utc': 1568585207.0, 'num_crossposts': 9, 'media': None, 'is_video': False}]",t3_d4recl,,,,,
787,,pytorch,"Hi, What is the difference between the two libraries and which one is better to work on Bayesian optimization?",t2_ts2al,False,,0,False,What is the difference pyro and Botorch ?,[],r/pytorch,False,6,,0,,,False,t3_d2xhue,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1568266753.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, What is the difference between the two libraries and which one is better to work on Bayesian optimization?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,d2xhue,True,,sriharsha_0806,,1,True,all_ads,False,[],False,,/r/pytorch/comments/d2xhue/what_is_the_difference_pyro_and_botorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/d2xhue/what_is_the_difference_pyro_and_botorch/,7135,1568237953.0,0,,False,,,,,,,,
788,,pytorch,,t2_2fv4yodo,False,,0,False,"R.I.P. Python 2: October 16, 2000 — January 1, 2020 | Survey indicates 84 percent Python developers had adopted Python 3",[],r/pytorch,False,6,,0,44.0,,False,t3_d1xm8b,False,dark,0.85,,public,13,0,{},140.0,,False,[],,False,False,,{},,False,13,,False,https://b.thumbs.redditmedia.com/bApodDG73wka4blXIFWaEzB1vHyAdv1pUykX50wOvXU.jpg,False,,[],{},link,,False,,1568094120.0,text,6,,,text,medium.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/1JYNRq5h5dwhRbNpRROCR6yPH4jwFSeKVEBbm0AWZVg.jpg?auto=webp&amp;s=6fc169c488a54a33139a0d6fb4ca31e2d1d759e2', 'width': 1200, 'height': 380}, 'resolutions': [{'url': 'https://external-preview.redd.it/1JYNRq5h5dwhRbNpRROCR6yPH4jwFSeKVEBbm0AWZVg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=14e72b0522a8d7cd0ff8e9da2170e322c1cafe77', 'width': 108, 'height': 34}, {'url': 'https://external-preview.redd.it/1JYNRq5h5dwhRbNpRROCR6yPH4jwFSeKVEBbm0AWZVg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=987ce5416652b96720d70c8d135f30a4d7ee0906', 'width': 216, 'height': 68}, {'url': 'https://external-preview.redd.it/1JYNRq5h5dwhRbNpRROCR6yPH4jwFSeKVEBbm0AWZVg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d07c9def4aa6c2d8e85463949c90490d2a235d9a', 'width': 320, 'height': 101}, {'url': 'https://external-preview.redd.it/1JYNRq5h5dwhRbNpRROCR6yPH4jwFSeKVEBbm0AWZVg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=785b6368ba42992908cd393d21990de83275b87c', 'width': 640, 'height': 202}, {'url': 'https://external-preview.redd.it/1JYNRq5h5dwhRbNpRROCR6yPH4jwFSeKVEBbm0AWZVg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0d6317a70addc08c163929c6e2bf3f6cfb99afcc', 'width': 960, 'height': 304}, {'url': 'https://external-preview.redd.it/1JYNRq5h5dwhRbNpRROCR6yPH4jwFSeKVEBbm0AWZVg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4509cee614f96a528d8a29eec3834b6b64e2461c', 'width': 1080, 'height': 342}], 'variants': {}, 'id': 'A_N4Lr-AJpboJIWG1xbNmaHWdepHBhcl_n7Jui3TXK4'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,d1xm8b,True,,Yuqing7,,2,True,all_ads,False,[],False,,/r/pytorch/comments/d1xm8b/rip_python_2_october_16_2000_january_1_2020/,all_ads,False,https://medium.com/syncedreview/r-i-p-python-2-october-16-2000-january-1-2020-6d68d436b3c2,7135,1568065320.0,0,,False,https://medium.com/syncedreview/r-i-p-python-2-october-16-2000-january-1-2020-6d68d436b3c2,,,,,,,
789,,pytorch,"Hi, I had a question regarding the official tutorial on [Finetuning Torchvision Models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html), in that tutorial they set all the parameters of the network except the new ones ( from the new classifier layer ) to requires\_grad = False  
 and then they use this code to build the Optimizer :

\`params\_to\_update = model\_ft.parameters()  
print(“Params to learn:”)  
if feature\_extract:  
params\_to\_update = \[\]  
for name,param in model\_ft.named\_parameters():  
if param.requires\_grad == True:  
params\_to\_update.append(param)  
print(""\\t"",name)  
else:  
for name,param in model\_ft.named\_parameters():  
if param.requires\_grad == True:  
print(""\\t"",name)

optimizer\_ft = optim.SGD(params\_to\_update, lr=0.001, momentum=0.9)  
\`

( The else here is for finetuning ). But settings requires\_grad = False  
 is not enough to not train the parameters even if we pass them to the Optimizer? In my understanding of the autograd module, both cases here are doing the same thing ( training only the parameters with requires\_grad = True  
 ) even if they don’t have the same parameters in the Optimizer. Am I missing something ?

Thank you very much for your help !",t2_38gv3quu,False,,0,False,Question on Finetuning Torchvision Models Tutorial,[],r/pytorch,False,6,,0,,,False,t3_d1dsjp,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1567991811.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I had a question regarding the official tutorial on &lt;a href=""https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html""&gt;Finetuning Torchvision Models&lt;/a&gt;, in that tutorial they set all the parameters of the network except the new ones ( from the new classifier layer ) to requires_grad = False&lt;br/&gt;
 and then they use this code to build the Optimizer :&lt;/p&gt;

&lt;p&gt;`params_to_update = model_ft.parameters()&lt;br/&gt;
print(“Params to learn:”)&lt;br/&gt;
if feature_extract:&lt;br/&gt;
params_to_update = []&lt;br/&gt;
for name,param in model_ft.named_parameters():&lt;br/&gt;
if param.requires_grad == True:&lt;br/&gt;
params_to_update.append(param)&lt;br/&gt;
print(&amp;quot;\t&amp;quot;,name)&lt;br/&gt;
else:&lt;br/&gt;
for name,param in model_ft.named_parameters():&lt;br/&gt;
if param.requires_grad == True:&lt;br/&gt;
print(&amp;quot;\t&amp;quot;,name)&lt;/p&gt;

&lt;p&gt;optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)&lt;br/&gt;
`&lt;/p&gt;

&lt;p&gt;( The else here is for finetuning ). But settings requires_grad = False&lt;br/&gt;
 is not enough to not train the parameters even if we pass them to the Optimizer? In my understanding of the autograd module, both cases here are doing the same thing ( training only the parameters with requires_grad = True&lt;br/&gt;
 ) even if they don’t have the same parameters in the Optimizer. Am I missing something ?&lt;/p&gt;

&lt;p&gt;Thank you very much for your help !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,d1dsjp,True,,eracro,,1,True,all_ads,False,[],False,,/r/pytorch/comments/d1dsjp/question_on_finetuning_torchvision_models_tutorial/,all_ads,False,https://www.reddit.com/r/pytorch/comments/d1dsjp/question_on_finetuning_torchvision_models_tutorial/,7135,1567963011.0,0,,False,,,,,,,,
790,,pytorch,"Interview with Pierre Stock about DL Research, Research at FAIR, and their recent work: [""And the Bit Goes Down: Revisiting the Quantization of Neural Networks""](https://arxiv.org/pdf/1907.05686.pdf):

&amp;#x200B;

Audio: [https://anchor.fm/chaitimedatascience/episodes/And-the-Bit-Goes-Down--Deep-Learning-Research--Research-at-FAIR--Interview-with-Pierre-Stock-e52rpk/a-alh51k](https://anchor.fm/chaitimedatascience/episodes/And-the-Bit-Goes-Down--Deep-Learning-Research--Research-at-FAIR--Interview-with-Pierre-Stock-e52rpk/a-alh51k)

&amp;#x200B;

Video: [https://www.youtube.com/watch?v=okFcSGShE10&amp;list=PLyMom0n-MBrrGL8w-nD9kc2q5dOwN7Hb1&amp;index=16](https://www.youtube.com/watch?v=okFcSGShE10&amp;list=PLyMom0n-MBrrGL8w-nD9kc2q5dOwN7Hb1&amp;index=16)",t2_2s77fkel,False,,0,False,"""And the Bit Goes Down"", Deep Learning Research, Research at Facebook | Interview with Pierre Stock",[],r/pytorch,False,6,,0,,,False,t3_d0vtj0,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1567890826.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Interview with Pierre Stock about DL Research, Research at FAIR, and their recent work: &lt;a href=""https://arxiv.org/pdf/1907.05686.pdf""&gt;&amp;quot;And the Bit Goes Down: Revisiting the Quantization of Neural Networks&amp;quot;&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Audio: &lt;a href=""https://anchor.fm/chaitimedatascience/episodes/And-the-Bit-Goes-Down--Deep-Learning-Research--Research-at-FAIR--Interview-with-Pierre-Stock-e52rpk/a-alh51k""&gt;https://anchor.fm/chaitimedatascience/episodes/And-the-Bit-Goes-Down--Deep-Learning-Research--Research-at-FAIR--Interview-with-Pierre-Stock-e52rpk/a-alh51k&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Video: &lt;a href=""https://www.youtube.com/watch?v=okFcSGShE10&amp;amp;list=PLyMom0n-MBrrGL8w-nD9kc2q5dOwN7Hb1&amp;amp;index=16""&gt;https://www.youtube.com/watch?v=okFcSGShE10&amp;amp;list=PLyMom0n-MBrrGL8w-nD9kc2q5dOwN7Hb1&amp;amp;index=16&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,d0vtj0,True,,init__27,,0,True,all_ads,False,[],False,,/r/pytorch/comments/d0vtj0/and_the_bit_goes_down_deep_learning_research/,all_ads,False,https://www.reddit.com/r/pytorch/comments/d0vtj0/and_the_bit_goes_down_deep_learning_research/,7135,1567862026.0,0,,False,,,,,,,,
791,,pytorch,"Hello all, I have been trying to use this method but fail each time. basically I have two images that I stored in a list (using `img_lst.append((img1,img2))`. one image is the input image and the other is its reconstruction. 

They are both gray scale images. 

This is how I tried to do : 

    def visualize(imgs_list, rows=2, cols=10):
        fig = plt.figure(figsize=(10,5))
        print(f'number of samples: {len(imgs_list)}')
        for i in range(len(imgs_list)):
            img,recons = imgs_list[i]
            img = img.cpu().permute(1,2,0)
            recons = recons.cpu().detach().permute(1,2,0)
            f = [img,recons]
            ax = fig.add_subplot(rows, cols, i+1, xticks=[], yticks=[])
            ax.imshow(torchvision.utils.make_grid(f), cmap='Greys_r')

but this fails with the error : 

&gt; RuntimeError: The expanded size of the tensor (3) must match the existing size (28) at non-singleton dimension 0.  Target sizes: \[3, 28, 1\].  Tensor sizes: \[28, 28, 1\]

if I remove permute, it complains about the dimensions being invalid :

&gt;\*\*TypeError\*\* : Invalid dimensions for image data

I'm completely lost here! how is this supposed to work? 

Can anyone kindly please help me understand this? 

&amp;#x200B;

Edit: 

Actually I'm following this [youtube channel](https://www.youtube.com/watch?v=DRsrjExb2q8) but so far, the same exact code wont work! and I have no idea why!

&amp;#x200B;

Thanks a lot in advance",t2_12rvgcg3,False,,0,False,How can I use torchvision.utils.make_grid() in Pytorch?,[],r/pytorch,False,6,,0,,,False,t3_czl9cq,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1567634413.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all, I have been trying to use this method but fail each time. basically I have two images that I stored in a list (using &lt;code&gt;img_lst.append((img1,img2))&lt;/code&gt;. one image is the input image and the other is its reconstruction. &lt;/p&gt;

&lt;p&gt;They are both gray scale images. &lt;/p&gt;

&lt;p&gt;This is how I tried to do : &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def visualize(imgs_list, rows=2, cols=10):
    fig = plt.figure(figsize=(10,5))
    print(f&amp;#39;number of samples: {len(imgs_list)}&amp;#39;)
    for i in range(len(imgs_list)):
        img,recons = imgs_list[i]
        img = img.cpu().permute(1,2,0)
        recons = recons.cpu().detach().permute(1,2,0)
        f = [img,recons]
        ax = fig.add_subplot(rows, cols, i+1, xticks=[], yticks=[])
        ax.imshow(torchvision.utils.make_grid(f), cmap=&amp;#39;Greys_r&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;but this fails with the error : &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;RuntimeError: The expanded size of the tensor (3) must match the existing size (28) at non-singleton dimension 0.  Target sizes: [3, 28, 1].  Tensor sizes: [28, 28, 1]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;if I remove permute, it complains about the dimensions being invalid :&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;**TypeError** : Invalid dimensions for image data&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I&amp;#39;m completely lost here! how is this supposed to work? &lt;/p&gt;

&lt;p&gt;Can anyone kindly please help me understand this? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Edit: &lt;/p&gt;

&lt;p&gt;Actually I&amp;#39;m following this &lt;a href=""https://www.youtube.com/watch?v=DRsrjExb2q8""&gt;youtube channel&lt;/a&gt; but so far, the same exact code wont work! and I have no idea why!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks a lot in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/tsO0g0ZzHHG9m7suqTk_o77Oftznoh2rJjDtViOp-UU.jpg?auto=webp&amp;s=b0674c80dc54390841010e49b26512cedf6cecd1', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/tsO0g0ZzHHG9m7suqTk_o77Oftznoh2rJjDtViOp-UU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d53bf5d3147a2d8aaad21520ed8fe8fbefc973f', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/tsO0g0ZzHHG9m7suqTk_o77Oftznoh2rJjDtViOp-UU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4642eaed232973ae4cf4fc5b04940f37d63dcb42', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/tsO0g0ZzHHG9m7suqTk_o77Oftznoh2rJjDtViOp-UU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba2aeb86f54ccae19c85406be9724f6e932d39c5', 'width': 320, 'height': 240}], 'variants': {}, 'id': '9HgClvXFNY3X3eNxjSvj1fLp3rPI8Zd9Iox1CVGH0y0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,czl9cq,True,,MasterSama,,2,True,all_ads,False,[],False,,/r/pytorch/comments/czl9cq/how_can_i_use_torchvisionutilsmake_grid_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/czl9cq/how_can_i_use_torchvisionutilsmake_grid_in_pytorch/,7135,1567605613.0,0,,False,,,,,,,,
792,,pytorch,"I am new in Deep Learning and PyTorch. I have been trying to upload images for ""cat v/s dog"" classification using some CNN architecture. The question is, how do I input the data that is in my hard disk. Wherever I search for in the Internet, all the solutions import either MNIST dataset directly from some data library or they are uploading csv file. In my case I have folder named dataset, in it two folders named train and test, in which two folders named cats and dogs are there. The images are in jpg format, not some csv file.

I'm stuck here and my learning too. Feeling frustrated at this point. Please help.",t2_3o18l0ky,False,,0,False,Data Loading from local hard drive (Image Specifically),[],r/pytorch,False,6,,0,,,False,t3_czj2a9,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1567622088.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am new in Deep Learning and PyTorch. I have been trying to upload images for &amp;quot;cat v/s dog&amp;quot; classification using some CNN architecture. The question is, how do I input the data that is in my hard disk. Wherever I search for in the Internet, all the solutions import either MNIST dataset directly from some data library or they are uploading csv file. In my case I have folder named dataset, in it two folders named train and test, in which two folders named cats and dogs are there. The images are in jpg format, not some csv file.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m stuck here and my learning too. Feeling frustrated at this point. Please help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,czj2a9,True,,DebayonDC,,1,True,all_ads,False,[],False,,/r/pytorch/comments/czj2a9/data_loading_from_local_hard_drive_image/,all_ads,False,https://www.reddit.com/r/pytorch/comments/czj2a9/data_loading_from_local_hard_drive_image/,7135,1567593288.0,0,,False,,,,,,,,
793,,pytorch,"    import torch
    pytorchGPUDirectCreateWEmpty = torch.empty(size=(20000000, 128), dtype=torch.float, device='cuda', requires_grad=False, pin_memory=False).uniform_(-1, 1)
    pytorchGPUDirectCreateWEmpty

results in 

    tensor([[0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            ...,
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')

and 

    import torch
    torch.set_default_tensor_type('torch.cuda.FloatTensor')
    u_embeddings = torch.nn.Embedding(20000000, 128, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None)
    u_embeddings.weight.data.uniform_(-1, 1)
    u_embeddings.weight.data

results in 

    tensor([[0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            ...,
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.]])

If I initialize with `double` instead of `float` and the initialization works fine. I could convert to float later, but I am working with limited memory and unable to initialize a double first before converting. 

Why is the initialization not working for float tensors?",t2_10efjmjx,False,,0,False,Performing normal or uniform initializations on Float tensors result in only zeros,[],r/pytorch,False,6,,0,,,False,t3_czefr8,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1567591121.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;pre&gt;&lt;code&gt;import torch
pytorchGPUDirectCreateWEmpty = torch.empty(size=(20000000, 128), dtype=torch.float, device=&amp;#39;cuda&amp;#39;, requires_grad=False, pin_memory=False).uniform_(-1, 1)
pytorchGPUDirectCreateWEmpty
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;results in &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device=&amp;#39;cuda:0&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import torch
torch.set_default_tensor_type(&amp;#39;torch.cuda.FloatTensor&amp;#39;)
u_embeddings = torch.nn.Embedding(20000000, 128, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None)
u_embeddings.weight.data.uniform_(-1, 1)
u_embeddings.weight.data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;results in &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If I initialize with &lt;code&gt;double&lt;/code&gt; instead of &lt;code&gt;float&lt;/code&gt; and the initialization works fine. I could convert to float later, but I am working with limited memory and unable to initialize a double first before converting. &lt;/p&gt;

&lt;p&gt;Why is the initialization not working for float tensors?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,czefr8,True,,BatmantoshReturns,,2,True,all_ads,False,[],False,,/r/pytorch/comments/czefr8/performing_normal_or_uniform_initializations_on/,all_ads,False,https://www.reddit.com/r/pytorch/comments/czefr8/performing_normal_or_uniform_initializations_on/,7135,1567562321.0,0,,False,,,,,,,,
794,,pytorch,"[https://github.com/blue-season/pywarm](https://github.com/blue-season/pywarm) 

[PyWarm](https://github.com/blue-season/pywarm) is a high-level neural network construction API for PyTorch.

Using PyWarm, you can put *all* network data flow logic in the `forward()` method of your model, without the need to define children modules in the `__init__()` method. This result in a much more readable model definition in fewer lines of code. Check the GitHub repository for code examples and more details!",t2_4ia9rd5f,False,,0,False,PyWarm: A cleaner way to build neural networks for PyTorch,[],r/pytorch,False,6,,0,,,False,t3_cz0v0q,False,dark,0.85,,public,17,0,{},,,False,[],,False,False,,{},,False,17,,False,self,1567524890.0,,[],{},self,,True,,1567519252.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://github.com/blue-season/pywarm""&gt;https://github.com/blue-season/pywarm&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/blue-season/pywarm""&gt;PyWarm&lt;/a&gt; is a high-level neural network construction API for PyTorch.&lt;/p&gt;

&lt;p&gt;Using PyWarm, you can put &lt;em&gt;all&lt;/em&gt; network data flow logic in the &lt;code&gt;forward()&lt;/code&gt; method of your model, without the need to define children modules in the &lt;code&gt;__init__()&lt;/code&gt; method. This result in a much more readable model definition in fewer lines of code. Check the GitHub repository for code examples and more details!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/eq8UmNKAjjGAyBseKvpqFuJ9SN9-B7v4m3fzOD2pp-M.jpg?auto=webp&amp;s=5b8ef4bf168b310f40c65e6dded599970fb7b37e', 'width': 420, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/eq8UmNKAjjGAyBseKvpqFuJ9SN9-B7v4m3fzOD2pp-M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2d1be6a85631bece9d5afbf2168621151d2bdf5', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/eq8UmNKAjjGAyBseKvpqFuJ9SN9-B7v4m3fzOD2pp-M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7f7f6c39d54c813a22e0cadd8224f76ded7d93e', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/eq8UmNKAjjGAyBseKvpqFuJ9SN9-B7v4m3fzOD2pp-M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff8214d8a143502b0316aece76e3ce1942e49258', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'CHRgl_8SMEIsMjo5vgcwVippY-45H2l1lgKVMxzP0PQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cz0v0q,True,,very-blue-season,,7,True,all_ads,False,[],False,,/r/pytorch/comments/cz0v0q/pywarm_a_cleaner_way_to_build_neural_networks_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cz0v0q/pywarm_a_cleaner_way_to_build_neural_networks_for/,7135,1567490452.0,0,,False,,,,,,,,
795,,pytorch,"Greetings, I found there exists [`torch.utils.tensorboard`](https://pytorch.org/docs/stable/tensorboard.html) and [`tensorboardX`](https://github.com/lanpa/tensorboardX). From the provided examples I see no difference between the two. What is the difference and what should I use?",t2_1t2xjw,False,,0,False,tensorboardX vs torch.utils.tensorboard ?,[],r/pytorch,False,6,,0,,,False,t3_cz2bj2,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1567530316.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Greetings, I found there exists &lt;a href=""https://pytorch.org/docs/stable/tensorboard.html""&gt;&lt;code&gt;torch.utils.tensorboard&lt;/code&gt;&lt;/a&gt; and &lt;a href=""https://github.com/lanpa/tensorboardX""&gt;&lt;code&gt;tensorboardX&lt;/code&gt;&lt;/a&gt;. From the provided examples I see no difference between the two. What is the difference and what should I use?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ul4Te3xREpRzY3QAbi4mXCvCDwvmxA5QpIOQIWh2THQ.jpg?auto=webp&amp;s=53cea7fd05a4b787fe72ccaabf26aadd13d59844', 'width': 730, 'height': 822}, 'resolutions': [{'url': 'https://external-preview.redd.it/ul4Te3xREpRzY3QAbi4mXCvCDwvmxA5QpIOQIWh2THQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa1440eadb67af448d9c97609c76c1996d59d74d', 'width': 108, 'height': 121}, {'url': 'https://external-preview.redd.it/ul4Te3xREpRzY3QAbi4mXCvCDwvmxA5QpIOQIWh2THQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=75a674e864e03adb5190d28633cf7bb3c99d7e55', 'width': 216, 'height': 243}, {'url': 'https://external-preview.redd.it/ul4Te3xREpRzY3QAbi4mXCvCDwvmxA5QpIOQIWh2THQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ffa8f0a5a3e4c4ccf84841dcefb85cb185baefdb', 'width': 320, 'height': 360}, {'url': 'https://external-preview.redd.it/ul4Te3xREpRzY3QAbi4mXCvCDwvmxA5QpIOQIWh2THQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4337d3688057157b3a83827ddfe9892b8f2305a4', 'width': 640, 'height': 720}], 'variants': {}, 'id': 'M1d8KayD20DgG__g2PxxCCc9Tjq4SnWaZvDrXippCUo'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cz2bj2,True,,mike239x,,1,True,all_ads,False,[],False,,/r/pytorch/comments/cz2bj2/tensorboardx_vs_torchutilstensorboard/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cz2bj2/tensorboardx_vs_torchutilstensorboard/,7135,1567501516.0,0,,False,,,,,,,,
796,,pytorch,"```
# Embedding
embedding = tf.Variable(tf.random_normal([params['max_words'], params['dim']]))
embedding_lookup_for_x = tf.nn.embedding_lookup(embedding, words)

# LSTM
lstm_cell_fw = tf.nn.rnn_cell.BasicLSTMCell(params['lstm_size'], state_is_tuple = True)
lstm_cell_bw = tf.nn.rnn_cell.BasicLSTMCell(params['lstm_size'], state_is_tuple = True)
states, final_state = tf.nn.bidirectional_dynamic_rnn(
                                    cell_fw = lstm_cell_fw, 
                                    cell_bw = lstm_cell_bw,
                                    inputs = embedding_lookup_for_x, 
                                    dtype = tf.float32,
                                    time_major = False,
                                    sequence_length = length)
lstm_out = tf.concat([states[0], states[1]], axis = 2)

# Conditional random fields
logits = tf.layers.dense(lstm_out, params['num_classes'])
crf_params = tf.get_variable(""crf"", [params['num_classes'], params['num_classes']],
                             dtype=tf.float32)
pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, length)
training = (mode == tf.estimator.ModeKeys.TRAIN)

# Prediction
if mode == tf.estimator.ModeKeys.PREDICT:
    predictions = { 
        'pred_ids': pred_ids,
        'tags': words,
        'length' : length,
    }
    export_outputs = {
      'prediction': tf.estimator.export.PredictOutput(predictions)
  }

    return tf.estimator.EstimatorSpec(mode, predictions=predictions,
                                      export_outputs=export_outputs)

# Loss functions and optimizers
log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(
    logits, labels, length, crf_params)

loss = tf.reduce_mean(-log_likelihood)
train_op = tf.train.AdamOptimizer().minimize(
    loss, global_step = tf.train.get_or_create_global_step())

# Training
if mode == tf.estimator.ModeKeys.TRAIN:
    return tf.estimator.EstimatorSpec(mode = mode,
                                       loss = loss,
                                       train_op = train_op)
```
It'd be really helpful for me if someone could give me the pytorch equivalent to this code. The code is from https://aihub.cloud.google.com/u/0/p/products%2F2290fc65-0041-4c87-a898-0289f59aa8ba section 3. Named entity recognition with Tensorflow",t2_1oxohlkg,False,,0,False,Could someone please translate this from tensorflow to pytorch?,[],r/pytorch,False,6,,0,,,False,t3_cz43zy,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1567541983.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;```&lt;/p&gt;

&lt;h1&gt;Embedding&lt;/h1&gt;

&lt;p&gt;embedding = tf.Variable(tf.random_normal([params[&amp;#39;max_words&amp;#39;], params[&amp;#39;dim&amp;#39;]]))
embedding_lookup_for_x = tf.nn.embedding_lookup(embedding, words)&lt;/p&gt;

&lt;h1&gt;LSTM&lt;/h1&gt;

&lt;p&gt;lstm_cell_fw = tf.nn.rnn_cell.BasicLSTMCell(params[&amp;#39;lstm_size&amp;#39;], state_is_tuple = True)
lstm_cell_bw = tf.nn.rnn_cell.BasicLSTMCell(params[&amp;#39;lstm_size&amp;#39;], state_is_tuple = True)
states, final_state = tf.nn.bidirectional_dynamic_rnn(
                                    cell_fw = lstm_cell_fw, 
                                    cell_bw = lstm_cell_bw,
                                    inputs = embedding_lookup_for_x, 
                                    dtype = tf.float32,
                                    time_major = False,
                                    sequence_length = length)
lstm_out = tf.concat([states[0], states[1]], axis = 2)&lt;/p&gt;

&lt;h1&gt;Conditional random fields&lt;/h1&gt;

&lt;p&gt;logits = tf.layers.dense(lstm_out, params[&amp;#39;num_classes&amp;#39;])
crf_params = tf.get_variable(&amp;quot;crf&amp;quot;, [params[&amp;#39;num_classes&amp;#39;], params[&amp;#39;num_classes&amp;#39;]],
                             dtype=tf.float32)
pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, length)
training = (mode == tf.estimator.ModeKeys.TRAIN)&lt;/p&gt;

&lt;h1&gt;Prediction&lt;/h1&gt;

&lt;p&gt;if mode == tf.estimator.ModeKeys.PREDICT:
    predictions = { 
        &amp;#39;pred_ids&amp;#39;: pred_ids,
        &amp;#39;tags&amp;#39;: words,
        &amp;#39;length&amp;#39; : length,
    }
    export_outputs = {
      &amp;#39;prediction&amp;#39;: tf.estimator.export.PredictOutput(predictions)
  }&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;return tf.estimator.EstimatorSpec(mode, predictions=predictions,
                                  export_outputs=export_outputs)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;Loss functions and optimizers&lt;/h1&gt;

&lt;p&gt;log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(
    logits, labels, length, crf_params)&lt;/p&gt;

&lt;p&gt;loss = tf.reduce_mean(-log_likelihood)
train_op = tf.train.AdamOptimizer().minimize(
    loss, global_step = tf.train.get_or_create_global_step())&lt;/p&gt;

&lt;h1&gt;Training&lt;/h1&gt;

&lt;p&gt;if mode == tf.estimator.ModeKeys.TRAIN:
    return tf.estimator.EstimatorSpec(mode = mode,
                                       loss = loss,
                                       train_op = train_op)
```
It&amp;#39;d be really helpful for me if someone could give me the pytorch equivalent to this code. The code is from &lt;a href=""https://aihub.cloud.google.com/u/0/p/products%2F2290fc65-0041-4c87-a898-0289f59aa8ba""&gt;https://aihub.cloud.google.com/u/0/p/products%2F2290fc65-0041-4c87-a898-0289f59aa8ba&lt;/a&gt; section 3. Named entity recognition with Tensorflow&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/68hXOYYKal26uvgqqQSwcY3XD94vev0RzZrN1b599c4.jpg?auto=webp&amp;s=3484a97bd27699daeb197d648191e49b7240a458', 'width': 192, 'height': 192}, 'resolutions': [{'url': 'https://external-preview.redd.it/68hXOYYKal26uvgqqQSwcY3XD94vev0RzZrN1b599c4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc4b2540ce8898104fbca6f5dd2ce3da27c22bed', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'zya98H8y4wzp85nH8sFFH8L4JB5_6ESTSMdiJnrpb0o'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cz43zy,True,,katiex7,,2,True,all_ads,False,[],False,,/r/pytorch/comments/cz43zy/could_someone_please_translate_this_from/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cz43zy/could_someone_please_translate_this_from/,7135,1567513183.0,0,,False,,,,,,,,
797,,pytorch,Do Pytorch and Cuda 10.1 work together or do I need to downgrade to Cuda 10.0.,t2_k6h0x0r,False,,0,False,Pytorch and Cuda 10.1,[],r/pytorch,False,6,,0,,,False,t3_cyligl,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1567435551.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Do Pytorch and Cuda 10.1 work together or do I need to downgrade to Cuda 10.0.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cyligl,True,,Suprem_Motu,,2,True,all_ads,False,[],False,,/r/pytorch/comments/cyligl/pytorch_and_cuda_101/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cyligl/pytorch_and_cuda_101/,7135,1567406751.0,0,,False,,,,,,,,
798,,pytorch,"Recently, fine tuning pre-trained models for your purpose has received a lot of attention. Transfer learning allows one to use a model trained for a source domain and task for a target domain and task from few samples. In this post, I'll cover how to use a pre-trained semantic segmentation DeepLabv3 model for the task of road crack detection in PyTorch by using transfer learning.

[https://expoundai.wordpress.com/2019/08/30/transfer-learning-for-segmentation-using-deeplabv3-in-pytorch/](https://expoundai.wordpress.com/2019/08/30/transfer-learning-for-segmentation-using-deeplabv3-in-pytorch/)

The code can be found at the following repository. [https://github.com/msminhas93/DeepLabv3FineTuning](https://github.com/msminhas93/DeepLabv3FineTuning)

\#pytorch #transferlearning #segmentation #roadcrackdetection #deeplearning #deeplabv3 #tutorial #blog #expoundai #torchvision",t2_dem70,False,,0,False,Transfer Learning for Segmentation Using DeepLabv3 in PyTorch for Road Crack Detection,[],r/pytorch,False,6,,0,,,False,t3_cxr7ur,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1567251422.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Recently, fine tuning pre-trained models for your purpose has received a lot of attention. Transfer learning allows one to use a model trained for a source domain and task for a target domain and task from few samples. In this post, I&amp;#39;ll cover how to use a pre-trained semantic segmentation DeepLabv3 model for the task of road crack detection in PyTorch by using transfer learning.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://expoundai.wordpress.com/2019/08/30/transfer-learning-for-segmentation-using-deeplabv3-in-pytorch/""&gt;https://expoundai.wordpress.com/2019/08/30/transfer-learning-for-segmentation-using-deeplabv3-in-pytorch/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The code can be found at the following repository. &lt;a href=""https://github.com/msminhas93/DeepLabv3FineTuning""&gt;https://github.com/msminhas93/DeepLabv3FineTuning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#pytorch #transferlearning #segmentation #roadcrackdetection #deeplearning #deeplabv3 #tutorial #blog #expoundai #torchvision&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Ih9F3pETOJjBp8sHdQSn84s1vxhS9ToMAlzp9BR_tHA.jpg?auto=webp&amp;s=9a5f9511df70bde58fcc8933e7a74a24cb4adab3', 'width': 880, 'height': 480}, 'resolutions': [{'url': 'https://external-preview.redd.it/Ih9F3pETOJjBp8sHdQSn84s1vxhS9ToMAlzp9BR_tHA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e0e11db61757c122dbf62955d8b6e19816b52fea', 'width': 108, 'height': 58}, {'url': 'https://external-preview.redd.it/Ih9F3pETOJjBp8sHdQSn84s1vxhS9ToMAlzp9BR_tHA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=78aa35a67e5030b20489224118f0e2de2fb923a3', 'width': 216, 'height': 117}, {'url': 'https://external-preview.redd.it/Ih9F3pETOJjBp8sHdQSn84s1vxhS9ToMAlzp9BR_tHA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ae9ba0ad69a06643f1149cbd713e8e2bf379a5d', 'width': 320, 'height': 174}, {'url': 'https://external-preview.redd.it/Ih9F3pETOJjBp8sHdQSn84s1vxhS9ToMAlzp9BR_tHA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ba951343443ee32e68d551f61b6dc09d8129d957', 'width': 640, 'height': 349}], 'variants': {}, 'id': 'uzvb3thyk4zvD7Q9K6LdXXq4fbWnIgfUyveMbBJOses'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cxr7ur,True,,msminhas93,,0,True,all_ads,False,[],False,,/r/pytorch/comments/cxr7ur/transfer_learning_for_segmentation_using/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cxr7ur/transfer_learning_for_segmentation_using/,7135,1567222622.0,0,,False,,,,,,,,
799,,pytorch,"My network is 1 Gbit ethernet and i am trying to use pytorch distributed training on two 8-gpu servers. Training procedure is simple classification objective with feed-forward network. I experience significant slowdown in comparison with single 8-gpu server training. Also ""nload"" tool shows full bandwidth usage even for small model (resnet18).

Is my network too slow for distributed training? If it is, what bandwidth (in Gbit/s) do I need to train heavy models like resnet101?",t2_wwxv4bq,False,,0,False,How wide should network bandwidth be for distributed training?,[],r/pytorch,False,6,,0,,,False,t3_cx5eub,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1567134837.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My network is 1 Gbit ethernet and i am trying to use pytorch distributed training on two 8-gpu servers. Training procedure is simple classification objective with feed-forward network. I experience significant slowdown in comparison with single 8-gpu server training. Also &amp;quot;nload&amp;quot; tool shows full bandwidth usage even for small model (resnet18).&lt;/p&gt;

&lt;p&gt;Is my network too slow for distributed training? If it is, what bandwidth (in Gbit/s) do I need to train heavy models like resnet101?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cx5eub,True,,borislestsov,,3,True,all_ads,False,[],False,,/r/pytorch/comments/cx5eub/how_wide_should_network_bandwidth_be_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cx5eub/how_wide_should_network_bandwidth_be_for/,7135,1567106037.0,0,,False,,,,,,,,
800,,pytorch,"Hi all! So i wanted to get into PyTorch hovewer most Udemy courses are like 5-10 hours of talking without any ""real-life"" sort of little projects to actually make with the instructor and learn, are there any such courses around (dont have to be udemy)? I much prefer to write code and understand it that way than to look at powerpoint-like presentations",t2_cn5xthv,False,,0,False,What are the best Project oriented courses for PyTorch?,[],r/pytorch,False,6,,0,,,False,t3_cw8c2q,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1566958823.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all! So i wanted to get into PyTorch hovewer most Udemy courses are like 5-10 hours of talking without any &amp;quot;real-life&amp;quot; sort of little projects to actually make with the instructor and learn, are there any such courses around (dont have to be udemy)? I much prefer to write code and understand it that way than to look at powerpoint-like presentations&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cw8c2q,True,,Flamyngoo,,2,True,all_ads,False,[],False,,/r/pytorch/comments/cw8c2q/what_are_the_best_project_oriented_courses_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cw8c2q/what_are_the_best_project_oriented_courses_for/,7135,1566930023.0,0,,False,,,,,,,,
801,,pytorch,"Hello, I'm interested to hear your experiences using PyTorch with TPUs. What kind of models did you train? What issues did you encounter?

I'm considering to start using PyTorch but I want to be able to use TPUs to train my large models (sequence to sequence architectures using RNNs).

**Background**  
Support for running PyTorch on TPUs is in the works for about 10 months now : https://github.com/pytorch/xla",t2_j6q7p,False,,0,False,Your experience using PyTorch with TPUs,[],r/pytorch,False,6,,0,,,False,t3_cui826,False,dark,0.94,,public,12,0,{},,,False,[],,False,False,,{},,False,12,,False,self,1566608113.0,,[],{},self,,True,,1566615403.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I&amp;#39;m interested to hear your experiences using PyTorch with TPUs. What kind of models did you train? What issues did you encounter?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m considering to start using PyTorch but I want to be able to use TPUs to train my large models (sequence to sequence architectures using RNNs).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;br/&gt;
Support for running PyTorch on TPUs is in the works for about 10 months now : &lt;a href=""https://github.com/pytorch/xla""&gt;https://github.com/pytorch/xla&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?auto=webp&amp;s=08c43430ff8793a25b767aa172468acb35be611a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4adad794ed3c0e2f76e2da992ae9eef8c074cae4', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5b3cb400fea60e0dfe87ebc5314f7db8016f8ab', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f149723b636ef3087022ddc1efa6ab53e3d4bc69', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_YPUxJ8AyYwgpO9r-tD7JUoQ-Nx8IlATLoEfmjW48iQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cui826,True,,visionscaper,,13,True,all_ads,False,[],False,,/r/pytorch/comments/cui826/your_experience_using_pytorch_with_tpus/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cui826/your_experience_using_pytorch_with_tpus/,7135,1566586603.0,0,,False,,,,,,,,
802,,pytorch,"Hello,

I am trying to implement the model of the following paper: https://pdfs.semanticscholar.org/db62/5c4c26c7df67c9099e78961d479532628ec7.pdf

They are using the WARP loss for the ranking loss. Since the WARP loss performs bad using pytorch, I wanted to ask if you guys have any ideas how to implement the ranking loss.

The documents I am working with can have multiple labels. I already came across an approximation of the WARP loss (https://github.com/NegatioN/WARP-Pytorch/blob/master/warp_loss.py) but mayby you have some more input for me.",t2_14inmw,False,,0,False,Pairwise Ranking Loss Pytorch,[],r/pytorch,False,6,,0,,,False,t3_ctgtlm,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1566422714.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am trying to implement the model of the following paper: &lt;a href=""https://pdfs.semanticscholar.org/db62/5c4c26c7df67c9099e78961d479532628ec7.pdf""&gt;https://pdfs.semanticscholar.org/db62/5c4c26c7df67c9099e78961d479532628ec7.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;They are using the WARP loss for the ranking loss. Since the WARP loss performs bad using pytorch, I wanted to ask if you guys have any ideas how to implement the ranking loss.&lt;/p&gt;

&lt;p&gt;The documents I am working with can have multiple labels. I already came across an approximation of the WARP loss (&lt;a href=""https://github.com/NegatioN/WARP-Pytorch/blob/master/warp_loss.py""&gt;https://github.com/NegatioN/WARP-Pytorch/blob/master/warp_loss.py&lt;/a&gt;) but mayby you have some more input for me.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ctgtlm,True,,CuppiCuppi,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ctgtlm/pairwise_ranking_loss_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ctgtlm/pairwise_ranking_loss_pytorch/,7135,1566393914.0,0,,False,,,,,,,,
803,,pytorch,"Hi, I have a pytorch model with runs in less than 2 ms on average. I'm working in a very fast application when we take images and analyze them with the model in less than 10 ms. I'm facing the issue of having to initialize the dataset and dataloader class everytime a new set of images (18) are recorded. And process them as 1 batch. However, this takes 30 ms per image instead of 2 as the first batch of the dataloader is always slower than the rest.

I would like to know if there are different alternatives to dataloader for my case.

Thanks!",t2_3b7uazps,False,,0,False,Pytorch for continuous stream of images,[],r/pytorch,False,6,,0,,,False,t3_csi41n,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1566255083.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I have a pytorch model with runs in less than 2 ms on average. I&amp;#39;m working in a very fast application when we take images and analyze them with the model in less than 10 ms. I&amp;#39;m facing the issue of having to initialize the dataset and dataloader class everytime a new set of images (18) are recorded. And process them as 1 batch. However, this takes 30 ms per image instead of 2 as the first batch of the dataloader is always slower than the rest.&lt;/p&gt;

&lt;p&gt;I would like to know if there are different alternatives to dataloader for my case.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,csi41n,True,,drr21,,4,True,all_ads,False,[],False,,/r/pytorch/comments/csi41n/pytorch_for_continuous_stream_of_images/,all_ads,False,https://www.reddit.com/r/pytorch/comments/csi41n/pytorch_for_continuous_stream_of_images/,7135,1566226283.0,0,,False,,,,,,,,
804,,pytorch,"Hi reddit,

I am new to PyTorch and trying to implement a simple LSTM. When I run the forward pass, I have the following error : 

&gt;RuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 'mat2'

I do not understand what is happening because I explicitely converted everything to LongTensors, but the error still remains.

The code is the following :

    class RecurrentModel(nn.Module):
        
        def __init__(self, input_size, hidden_size, output_size):
            super(RecurrentModel, self).__init__()
            """"""
            Arguments:
            - input_size (int): dimension of the input.
            - hidden_size (int): dimension of the hidden state of the recurrent layer.
            - output_size (int): size of output (3 in our case).
            """"""
            
            self.hidden_size = hidden_size
            
            self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)
            self.fully_connected = nn.Linear(hidden_size, output_size)
    
    
        def forward(self, x):
            """"""  
            Perform a forward pass.
            
            Arguments:
            - x (tensor): tensor of shape [batch_size, sequence_length, input_size].
            
            Returns:
            - Output of the linear layer of shape [batch_size, output_size].
            """"""
            
            # x : [batch_size, seq_len, input_size]
            # h0 : [1, batch_size, hidden_size]
            # c0 : [1, batch_size, hidden_size]
            h0, c0 = self._init_hidden(x.shape[0], self.hidden_size)
            
            print(x.type())
            print(h0.type())
            print(c0.type())
            
            _, (hn, _) = self.lstm(x, (h0, c0))  # hn : [1, batch_size, hidden_size]
                
            # hn : [1, batch_size, hidden_size]
            # torch.squeeze(hn) : [batch_size, hidden_size]
            output = self.fully_connected(torch.squeeze(hn))  # output : [batch_size, output_size]
            
            return output
        
        def _init_hidden(self, batch_size, hidden_size):
            """"""
            Initialize hidden states for the recurrent layers.
            
            Arguments:
            - batch_size (int): batch size.
            - hidden_size (int): hidden dimension.
            
            Returns:
            - A tuple (h0, c0) containing hidden and cell states.
            """"""
            
            h0 = torch.zeros(1, batch_size, hidden_size).long()
            c0 = torch.zeros(1, batch_size, hidden_size).long()
                
            return h0, c0

In the above code, I have printed x, h0 and c0 which are all LongTensors. The error appears when trying to compute this line :

    _, (hn, _) = self.lstm(x, (h0, c0)) 

I would really appreciate if you could give me help on this matter, as I am about to throw my computer through the window :)

&amp;#x200B;

Thank you !",t2_2knk68fc,False,,0,False,What's wrong with my implementation ?,[],r/pytorch,False,6,,0,,,False,t3_cr4wlh,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1565982754.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi reddit,&lt;/p&gt;

&lt;p&gt;I am new to PyTorch and trying to implement a simple LSTM. When I run the forward pass, I have the following error : &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;RuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 &amp;#39;mat2&amp;#39;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I do not understand what is happening because I explicitely converted everything to LongTensors, but the error still remains.&lt;/p&gt;

&lt;p&gt;The code is the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class RecurrentModel(nn.Module):

    def __init__(self, input_size, hidden_size, output_size):
        super(RecurrentModel, self).__init__()
        &amp;quot;&amp;quot;&amp;quot;
        Arguments:
        - input_size (int): dimension of the input.
        - hidden_size (int): dimension of the hidden state of the recurrent layer.
        - output_size (int): size of output (3 in our case).
        &amp;quot;&amp;quot;&amp;quot;

        self.hidden_size = hidden_size

        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)
        self.fully_connected = nn.Linear(hidden_size, output_size)


    def forward(self, x):
        &amp;quot;&amp;quot;&amp;quot;  
        Perform a forward pass.

        Arguments:
        - x (tensor): tensor of shape [batch_size, sequence_length, input_size].

        Returns:
        - Output of the linear layer of shape [batch_size, output_size].
        &amp;quot;&amp;quot;&amp;quot;

        # x : [batch_size, seq_len, input_size]
        # h0 : [1, batch_size, hidden_size]
        # c0 : [1, batch_size, hidden_size]
        h0, c0 = self._init_hidden(x.shape[0], self.hidden_size)

        print(x.type())
        print(h0.type())
        print(c0.type())

        _, (hn, _) = self.lstm(x, (h0, c0))  # hn : [1, batch_size, hidden_size]

        # hn : [1, batch_size, hidden_size]
        # torch.squeeze(hn) : [batch_size, hidden_size]
        output = self.fully_connected(torch.squeeze(hn))  # output : [batch_size, output_size]

        return output

    def _init_hidden(self, batch_size, hidden_size):
        &amp;quot;&amp;quot;&amp;quot;
        Initialize hidden states for the recurrent layers.

        Arguments:
        - batch_size (int): batch size.
        - hidden_size (int): hidden dimension.

        Returns:
        - A tuple (h0, c0) containing hidden and cell states.
        &amp;quot;&amp;quot;&amp;quot;

        h0 = torch.zeros(1, batch_size, hidden_size).long()
        c0 = torch.zeros(1, batch_size, hidden_size).long()

        return h0, c0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the above code, I have printed x, h0 and c0 which are all LongTensors. The error appears when trying to compute this line :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;_, (hn, _) = self.lstm(x, (h0, c0)) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I would really appreciate if you could give me help on this matter, as I am about to throw my computer through the window :)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cr4wlh,True,,lazywiing,,5,True,all_ads,False,[],False,,/r/pytorch/comments/cr4wlh/whats_wrong_with_my_implementation/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cr4wlh/whats_wrong_with_my_implementation/,7135,1565953954.0,0,,False,,,,,,,,
805,,pytorch,"Interview about Image Augmentations, NVIDIA's DALI Library with James Dellinger. 

We talk about performance, convenience, and comparisons of the frameworks against other frameworks.

&amp;#x200B;

Audio: [https://anchor.fm/chaitimedatascience/episodes/NVIDIAs-DALI-Library--Image-Augmentations-Discussion-Interview-with-James-Dellinger-e4r6f7/a-ak5426](https://anchor.fm/chaitimedatascience/episodes/NVIDIAs-DALI-Library--Image-Augmentations-Discussion-Interview-with-James-Dellinger-e4r6f7/a-ak5426)

&amp;#x200B;

Video: [https://www.youtube.com/watch?v=IMOqrf5NPUU](https://www.youtube.com/watch?v=IMOqrf5NPUU)",t2_2s77fkel,False,,0,False,"NVIDIA's DALI Library, Image Augmentations | Interview with James Dellinger",[],r/pytorch,False,6,,0,,,False,t3_cr078a,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1565952282.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Interview about Image Augmentations, NVIDIA&amp;#39;s DALI Library with James Dellinger. &lt;/p&gt;

&lt;p&gt;We talk about performance, convenience, and comparisons of the frameworks against other frameworks.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Audio: &lt;a href=""https://anchor.fm/chaitimedatascience/episodes/NVIDIAs-DALI-Library--Image-Augmentations-Discussion-Interview-with-James-Dellinger-e4r6f7/a-ak5426""&gt;https://anchor.fm/chaitimedatascience/episodes/NVIDIAs-DALI-Library--Image-Augmentations-Discussion-Interview-with-James-Dellinger-e4r6f7/a-ak5426&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Video: &lt;a href=""https://www.youtube.com/watch?v=IMOqrf5NPUU""&gt;https://www.youtube.com/watch?v=IMOqrf5NPUU&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?auto=webp&amp;s=96c78f7af0d55bf809aea5e3aa74cbbcef56dd55', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb0b9f0fe0880768f4673eb0b0de4950a98b6b17', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c358b62ea4934234264e445b13378fcfa493bf6d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db1864c49e213ead3ece1212938c1640ab7b1535', 'width': 320, 'height': 320}], 'variants': {}, 'id': '3vbWKckhU81Qq--awNv_DObYSXMuYxFCxRDsUJAgfB0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cr078a,True,,init__27,,2,True,all_ads,False,[],False,,/r/pytorch/comments/cr078a/nvidias_dali_library_image_augmentations/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cr078a/nvidias_dali_library_image_augmentations/,7135,1565923482.0,0,,False,,,,,,,,
806,,pytorch,,t2_a6lub,False,,0,False,"[N] Facebook launches online Global Pytorch Hackathon. $61,000 in prizes. Submissions due Sept 16th.",[],r/pytorch,False,6,,0,,,False,t3_cq2ypj,False,dark,0.92,,public,10,0,{},,,False,[],,False,False,,{},,False,10,,False,default,False,,[],{},link,,False,,1565776179.0,text,6,,,text,self.MachineLearning,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?auto=webp&amp;s=d7f8b71760fdc25bb8ef53bd84cf50083b1df02e', 'width': 1250, 'height': 1250}, 'resolutions': [{'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff22d20651e07ae43fd0a3afd8a735df47860454', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7010a041fd91cfb407e77d8d821448c09af030da', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=76490e027391880faf4539feebb2b1925a1bda87', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=91cdd9690de1bf8bdf4864deb818294317cff42f', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b2b2cddc393150413d17feb746125e861a19ef45', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1c8ee5a9b57969de7be9713d95cdc4aeb614f4e9', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'Efsm4-zJf8YPejIOsrC08zjLcz6i69Npd1RDr4QRJ1c'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cq2ypj,True,,MonstarGaming,,0,True,all_ads,False,[],False,,/r/pytorch/comments/cq2ypj/n_facebook_launches_online_global_pytorch/,all_ads,False,https://www.reddit.com/r/MachineLearning/comments/cot2xm/n_facebook_launches_online_global_pytorch/,7135,1565747379.0,0,,False,https://www.reddit.com/r/MachineLearning/comments/cot2xm/n_facebook_launches_online_global_pytorch/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': ""https://pytorch.devpost.com/\n\nI had the pleasure of attending their in person hackathon at Menlo Park yesterday. If you want some inspiration for potential projects, checkout their submissions page here, they were really good. \n\nhttps://pytorchmpk.devpost.com/submissions\n\nPytorch rolled a bunch of new features out a few days ago. They seem to be really stepping up in response to TF 2.0. \n\nIf you're looking for teammates, signup on the page, then you can look at other profiles of those looking for teammates\n\nhttps://pytorch.devpost.com/participants"", 'author_fullname': 't2_25c2pypq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[N] Facebook launches online Global Pytorch Hackathon. $61,000 in prizes. Submissions due Sept 16th.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'two', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_cot2xm', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 277, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'News', 'can_mod_post': False, 'score': 277, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1565530818.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://pytorch.devpost.com/""&gt;https://pytorch.devpost.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I had the pleasure of attending their in person hackathon at Menlo Park yesterday. If you want some inspiration for potential projects, checkout their submissions page here, they were really good. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://pytorchmpk.devpost.com/submissions""&gt;https://pytorchmpk.devpost.com/submissions&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Pytorch rolled a bunch of new features out a few days ago. They seem to be really stepping up in response to TF 2.0. &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re looking for teammates, signup on the page, then you can look at other profiles of those looking for teammates&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://pytorch.devpost.com/participants""&gt;https://pytorch.devpost.com/participants&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?auto=webp&amp;s=d7f8b71760fdc25bb8ef53bd84cf50083b1df02e', 'width': 1250, 'height': 1250}, 'resolutions': [{'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff22d20651e07ae43fd0a3afd8a735df47860454', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7010a041fd91cfb407e77d8d821448c09af030da', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=76490e027391880faf4539feebb2b1925a1bda87', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=91cdd9690de1bf8bdf4864deb818294317cff42f', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b2b2cddc393150413d17feb746125e861a19ef45', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1c8ee5a9b57969de7be9713d95cdc4aeb614f4e9', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'Efsm4-zJf8YPejIOsrC08zjLcz6i69Npd1RDr4QRJ1c'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'cot2xm', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Research2Vec', 'discussion_type': None, 'num_comments': 46, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/cot2xm/n_facebook_launches_online_global_pytorch/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/cot2xm/n_facebook_launches_online_global_pytorch/', 'subreddit_subscribers': 1740779, 'created_utc': 1565502018.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_cot2xm,,,,,
807,,pytorch,,t2_12i1bz,False,,0,False,"PyTorch 1.2 released (+torchvision, torchaudio): New TorchScript API with Improved Python Language Coverage, Expanded ONNX Export, NN.Transformer",[],r/pytorch,False,6,,0,140.0,,False,t3_cp0yjf,False,dark,0.95,,public,32,0,{},140.0,,False,[],,False,False,,{},,False,32,,False,https://a.thumbs.redditmedia.com/W8EB8zVNZt6Sk8xweEghVW3XcKPLRkx7mUtazGwesy4.jpg,False,,[],{},link,,False,,1565579705.0,text,6,,,text,pytorch.org,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?auto=webp&amp;s=d0da5dbb7aff983a5173aba7d2bdd36fa28abe08', 'width': 2500, 'height': 2500}, 'resolutions': [{'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fef05ec37ddf39032979380cc7acc403e05834be', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e1de8e408f33ef3c93425e30e0bb5afffd5c631', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89562dde9cea0af5ebaaba50a9437e484d732513', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d6125bcd02c2fed8d84dac49cc8d73dbe205d81', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=40d7ca8d88566f27054acb632514012b5026acc9', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=757464a70e69532931dcbf0fbd397b9d433cf205', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'Mg0HAz_PmYkYWkaGTVyhCEzp15PpLW_TWWqmn9VNVFs'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cp0yjf,True,,101testing,,1,True,all_ads,False,[],False,,/r/pytorch/comments/cp0yjf/pytorch_12_released_torchvision_torchaudio_new/,all_ads,False,https://pytorch.org/blog/pytorch-1.2-and-domain-api-release/,7135,1565550905.0,0,,False,https://pytorch.org/blog/pytorch-1.2-and-domain-api-release/,,,,,,,
808,,pytorch,"Just Released the interview with the King of Kaggle Kernels (currently ranked #1) Grandmaster ""Artgor"": Andrew Lukyanenko

We talk about his journey into DS, his current projects at work, his pipeline, insights, and many tips for writing Kaggle Kernels

[https://anchor.fm/chaitimedatascience/episodes/Interview-with-Kaggle-Kernels-Grandmaster-1-Artgor--Andrew-Lukyanenko-e4r6du/a-ak53o7](https://anchor.fm/chaitimedatascience/episodes/Interview-with-Kaggle-Kernels-Grandmaster-1-Artgor--Andrew-Lukyanenko-e4r6du/a-ak53o7)",t2_2s77fkel,False,,0,False,"Interview with Kaggle Kernels Grandmaster Ranked #1, ""Artgor"": Andrew Lukyanenko",[],r/pytorch,False,6,,0,,,False,t3_cp8pkv,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1565621291.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just Released the interview with the King of Kaggle Kernels (currently ranked #1) Grandmaster &amp;quot;Artgor&amp;quot;: Andrew Lukyanenko&lt;/p&gt;

&lt;p&gt;We talk about his journey into DS, his current projects at work, his pipeline, insights, and many tips for writing Kaggle Kernels&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://anchor.fm/chaitimedatascience/episodes/Interview-with-Kaggle-Kernels-Grandmaster-1-Artgor--Andrew-Lukyanenko-e4r6du/a-ak53o7""&gt;https://anchor.fm/chaitimedatascience/episodes/Interview-with-Kaggle-Kernels-Grandmaster-1-Artgor--Andrew-Lukyanenko-e4r6du/a-ak53o7&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?auto=webp&amp;s=96c78f7af0d55bf809aea5e3aa74cbbcef56dd55', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb0b9f0fe0880768f4673eb0b0de4950a98b6b17', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c358b62ea4934234264e445b13378fcfa493bf6d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db1864c49e213ead3ece1212938c1640ab7b1535', 'width': 320, 'height': 320}], 'variants': {}, 'id': '3vbWKckhU81Qq--awNv_DObYSXMuYxFCxRDsUJAgfB0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cp8pkv,True,,init__27,,0,True,all_ads,False,[],False,,/r/pytorch/comments/cp8pkv/interview_with_kaggle_kernels_grandmaster_ranked/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cp8pkv/interview_with_kaggle_kernels_grandmaster_ranked/,7135,1565592491.0,0,,False,,,,,,,,
809,,pytorch,"I am looking to develop a library on top of pytorch, for sparse training. A lot of the work will involve speed/performance optimization, particularly parallelizing IO memmap operations so that it doesn't slow down training. 

Basically at after every training step, embeddings are swapped out with the pytorch variable weights with the memmap, and vise versa. 
The biggest problem is that it adds a ton of slowdown, especially since we'll be swapping out thousands of embeddings. A base case would be swapping out 65000 embeddings during each training step, and each embedding has 512 dimensions. 

So initial tasks would be to parallelize the memmap operations using CUDA. Adaptive fetching size to optimize speed on any system. Figure what's the best datastore for this (i used numpy.memmap just for a simple test case, but didn't compare speeds to other formats such as feather, hdf5, etc.), and maybe use multiprocessing/multithreading to perform fetching during training steps and/or input data fetching steps. 

If this sounds like something you would be interested in, please PM. 

btw, here's the hackathon page 

https://pytorch.devpost.com/",t2_25c2pypq,False,,0,False,"Anyone would like to help us develop a sparse training library for Pytorch? This is for the Facebook Pytorch hackathon ( $61,000 in prizes. Submissions due Sept 16th ).",[],r/pytorch,False,6,,0,,,False,t3_cp6bih,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1565606572.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking to develop a library on top of pytorch, for sparse training. A lot of the work will involve speed/performance optimization, particularly parallelizing IO memmap operations so that it doesn&amp;#39;t slow down training. &lt;/p&gt;

&lt;p&gt;Basically at after every training step, embeddings are swapped out with the pytorch variable weights with the memmap, and vise versa. 
The biggest problem is that it adds a ton of slowdown, especially since we&amp;#39;ll be swapping out thousands of embeddings. A base case would be swapping out 65000 embeddings during each training step, and each embedding has 512 dimensions. &lt;/p&gt;

&lt;p&gt;So initial tasks would be to parallelize the memmap operations using CUDA. Adaptive fetching size to optimize speed on any system. Figure what&amp;#39;s the best datastore for this (i used numpy.memmap just for a simple test case, but didn&amp;#39;t compare speeds to other formats such as feather, hdf5, etc.), and maybe use multiprocessing/multithreading to perform fetching during training steps and/or input data fetching steps. &lt;/p&gt;

&lt;p&gt;If this sounds like something you would be interested in, please PM. &lt;/p&gt;

&lt;p&gt;btw, here&amp;#39;s the hackathon page &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://pytorch.devpost.com/""&gt;https://pytorch.devpost.com/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?auto=webp&amp;s=d7f8b71760fdc25bb8ef53bd84cf50083b1df02e', 'width': 1250, 'height': 1250}, 'resolutions': [{'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff22d20651e07ae43fd0a3afd8a735df47860454', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7010a041fd91cfb407e77d8d821448c09af030da', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=76490e027391880faf4539feebb2b1925a1bda87', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=91cdd9690de1bf8bdf4864deb818294317cff42f', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b2b2cddc393150413d17feb746125e861a19ef45', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/fFHCDInEW-oBEip-0Wi614K_DiVRtGCE1BoKAYwi8dI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1c8ee5a9b57969de7be9713d95cdc4aeb614f4e9', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'Efsm4-zJf8YPejIOsrC08zjLcz6i69Npd1RDr4QRJ1c'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cp6bih,True,,Research2Vec,,1,True,all_ads,False,[],False,,/r/pytorch/comments/cp6bih/anyone_would_like_to_help_us_develop_a_sparse/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cp6bih/anyone_would_like_to_help_us_develop_a_sparse/,7135,1565577772.0,0,,False,,,,,,,,
810,,pytorch,"Hi, i'm just implementing a statistical model to manage dialogues and I'm struggling to understand how RNNs really work with batches.

The input is of the following shape: \[DIALOGUES\_NUM x SENTENCES\_NUM x SENTENCE\_EMBEDDING\_SIZE \]

Where DIALOGUES\_NUM is the batch size (supposing batch\_first=True)

I want to feed this input to a RNN in such a way that the computation is intra-dialogue and so independent between 2 different dialogues. I mean, I want the hidden-state to flow inside the single batch (single dialogue) and not between 2 batches.

Ex. (Snm is the m-th sentence of the n-th dialogue)

H 11+ S11 -&gt; \[RNN \] -&gt; H12+S12 -&gt; \[RNN\] -&gt; ...

H21+S21 -&gt; \[RNN\] -&gt; H22+S22 -&gt; \[RNN\] -&gt; ...

&amp;#x200B;

Maybe this is just how RNNs simply work but I cannot find any clear reference on the web. I so need to pass an hidden state of shape  \[DIALOGUES\_NUM x HIDDEN\_DIM\] and I will obtain an output of  shape:

output = \[DIALOGUES\_NUM x SENTENCES\_NUM x HIDDEN\_DIM\]

&amp;#x200B;

&amp;#x200B;

Is this correct?",t2_34dbezd6,False,,0,False,RNN with batches,[],r/pytorch,False,6,,0,,,False,t3_cog2z0,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1565458964.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, i&amp;#39;m just implementing a statistical model to manage dialogues and I&amp;#39;m struggling to understand how RNNs really work with batches.&lt;/p&gt;

&lt;p&gt;The input is of the following shape: [DIALOGUES_NUM x SENTENCES_NUM x SENTENCE_EMBEDDING_SIZE ]&lt;/p&gt;

&lt;p&gt;Where DIALOGUES_NUM is the batch size (supposing batch_first=True)&lt;/p&gt;

&lt;p&gt;I want to feed this input to a RNN in such a way that the computation is intra-dialogue and so independent between 2 different dialogues. I mean, I want the hidden-state to flow inside the single batch (single dialogue) and not between 2 batches.&lt;/p&gt;

&lt;p&gt;Ex. (Snm is the m-th sentence of the n-th dialogue)&lt;/p&gt;

&lt;p&gt;H 11+ S11 -&amp;gt; [RNN ] -&amp;gt; H12+S12 -&amp;gt; [RNN] -&amp;gt; ...&lt;/p&gt;

&lt;p&gt;H21+S21 -&amp;gt; [RNN] -&amp;gt; H22+S22 -&amp;gt; [RNN] -&amp;gt; ...&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Maybe this is just how RNNs simply work but I cannot find any clear reference on the web. I so need to pass an hidden state of shape  [DIALOGUES_NUM x HIDDEN_DIM] and I will obtain an output of  shape:&lt;/p&gt;

&lt;p&gt;output = [DIALOGUES_NUM x SENTENCES_NUM x HIDDEN_DIM]&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is this correct?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cog2z0,True,,_Seoo,,6,True,all_ads,False,[],False,,/r/pytorch/comments/cog2z0/rnn_with_batches/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cog2z0/rnn_with_batches/,7135,1565430164.0,0,,False,,,,,,,,
811,,pytorch," 

I've trained a model that is to be used for text generation. I'm still kind of new to saving and loading models for inference so, for experience's sake I coded two versions. One where I train the model, save it, and directly generate text, and another where I load the saved model and generate the text from that point.

The first version generates text as I wanted it to, however the text generated from the loaded model is just gibberish. I noticed that the softmax results returned from the loaded model are all near equal as well...

This is how I save the model:

    model_name = 'model.pth' 
    checkpoint = {'n_hidden': net.n_hidden, 
                'n_layers': net.n_layers, 
                'state_dict': net.state_dict(), 
                'tokens': net.chars} 
    with open(model_name, 'wb') as f:
         torch.save(checkpoint, f)

And this is how it is loaded:

    model_path = 'model.pth' 
    checkpoint = torch.load(model_path) 
    model = charRNN(checkpoint['tokens'], checkpoint['n_hidden'], checkpoint['n_layers'], lr, dropout) 
    model.load_state_dict(checkpoint['state_dict']) 
    model.eval()",t2_xlwsi,False,,0,False,Loaded model not giving me the same results as it did before saving it,[],r/pytorch,False,6,,0,,,False,t3_co7xi3,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1565385454.0,,[],{},,,True,,1565410826.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve trained a model that is to be used for text generation. I&amp;#39;m still kind of new to saving and loading models for inference so, for experience&amp;#39;s sake I coded two versions. One where I train the model, save it, and directly generate text, and another where I load the saved model and generate the text from that point.&lt;/p&gt;

&lt;p&gt;The first version generates text as I wanted it to, however the text generated from the loaded model is just gibberish. I noticed that the softmax results returned from the loaded model are all near equal as well...&lt;/p&gt;

&lt;p&gt;This is how I save the model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model_name = &amp;#39;model.pth&amp;#39; 
checkpoint = {&amp;#39;n_hidden&amp;#39;: net.n_hidden, 
            &amp;#39;n_layers&amp;#39;: net.n_layers, 
            &amp;#39;state_dict&amp;#39;: net.state_dict(), 
            &amp;#39;tokens&amp;#39;: net.chars} 
with open(model_name, &amp;#39;wb&amp;#39;) as f:
     torch.save(checkpoint, f)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And this is how it is loaded:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model_path = &amp;#39;model.pth&amp;#39; 
checkpoint = torch.load(model_path) 
model = charRNN(checkpoint[&amp;#39;tokens&amp;#39;], checkpoint[&amp;#39;n_hidden&amp;#39;], checkpoint[&amp;#39;n_layers&amp;#39;], lr, dropout) 
model.load_state_dict(checkpoint[&amp;#39;state_dict&amp;#39;]) 
model.eval()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,co7xi3,True,,atomicalexx,,6,True,all_ads,False,[],False,,/r/pytorch/comments/co7xi3/loaded_model_not_giving_me_the_same_results_as_it/,all_ads,False,https://www.reddit.com/r/pytorch/comments/co7xi3/loaded_model_not_giving_me_the_same_results_as_it/,7135,1565382026.0,0,,False,,,,,,,,
812,,pytorch,"Hello All, I'm trying to finetune a resnet18 model.

I want to freeze all layers except the last one. I did

    resnet18 = models.resnet18(pretrained=True)
    resnet18.fc = nn.Linear(512, 10) 
    for param in resnet18.parameters():
        param.requires_grad = False 

However, doing

    for param in resnet18.fc.parameters():
        param.requires_grad = True

Fails. How can I set a specific layers parameters to have requires\_grad to True?

Thank you all in advance

&amp;#x200B;

Note:

I specifically dont want to swap the order of assigning a new layer with setting all the grads to false

I want to learn how this specific thing can be done.",t2_12rvgcg3,False,,0,False,How can I disable all layers gradient expect the last layer?,[],r/pytorch,False,6,,0,,,False,t3_co492s,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1565368931.0,,[],{},,,True,,1565394746.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello All, I&amp;#39;m trying to finetune a resnet18 model.&lt;/p&gt;

&lt;p&gt;I want to freeze all layers except the last one. I did&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resnet18 = models.resnet18(pretrained=True)
resnet18.fc = nn.Linear(512, 10) 
for param in resnet18.parameters():
    param.requires_grad = False 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, doing&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for param in resnet18.fc.parameters():
    param.requires_grad = True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fails. How can I set a specific layers parameters to have requires_grad to True?&lt;/p&gt;

&lt;p&gt;Thank you all in advance&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;

&lt;p&gt;I specifically dont want to swap the order of assigning a new layer with setting all the grads to false&lt;/p&gt;

&lt;p&gt;I want to learn how this specific thing can be done.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,co492s,True,,MasterSama,,3,True,all_ads,False,[],False,,/r/pytorch/comments/co492s/how_can_i_disable_all_layers_gradient_expect_the/,all_ads,False,https://www.reddit.com/r/pytorch/comments/co492s/how_can_i_disable_all_layers_gradient_expect_the/,7135,1565365946.0,0,,False,,,,,,,,
813,,pytorch,"Interview with Shivam Bansal, Kernels GM Ranked 4: 

Did you know Shivam made it to the Top 2 Rankings in Kernels after just 9 months of getting started on Kaggle!

We talk about his journey into Data Science, Kaggle. Shivam also shares his pipeline and motivation behind writing kernels. 

(Audio) https://anchor.fm/chaitimedatascience/episodes/Interview-with-Kaggle-Kernels-GM-Shivam-Bansal-e4qcbe/a-ak0id3

(Video) https://www.youtube.com/watch?v=0K4C1FMVbgQ",t2_2s77fkel,False,,0,False,Getting Started to Achieving Kaggle Kernels GM &amp; Rank #2 in just 9 months | Interview with Shivam Bansal,[],r/pytorch,False,6,,0,,,False,t3_cnw5z0,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1565347421.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Interview with Shivam Bansal, Kernels GM Ranked 4: &lt;/p&gt;

&lt;p&gt;Did you know Shivam made it to the Top 2 Rankings in Kernels after just 9 months of getting started on Kaggle!&lt;/p&gt;

&lt;p&gt;We talk about his journey into Data Science, Kaggle. Shivam also shares his pipeline and motivation behind writing kernels. &lt;/p&gt;

&lt;p&gt;(Audio) &lt;a href=""https://anchor.fm/chaitimedatascience/episodes/Interview-with-Kaggle-Kernels-GM-Shivam-Bansal-e4qcbe/a-ak0id3""&gt;https://anchor.fm/chaitimedatascience/episodes/Interview-with-Kaggle-Kernels-GM-Shivam-Bansal-e4qcbe/a-ak0id3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(Video) &lt;a href=""https://www.youtube.com/watch?v=0K4C1FMVbgQ""&gt;https://www.youtube.com/watch?v=0K4C1FMVbgQ&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?auto=webp&amp;s=96c78f7af0d55bf809aea5e3aa74cbbcef56dd55', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb0b9f0fe0880768f4673eb0b0de4950a98b6b17', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c358b62ea4934234264e445b13378fcfa493bf6d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db1864c49e213ead3ece1212938c1640ab7b1535', 'width': 320, 'height': 320}], 'variants': {}, 'id': '3vbWKckhU81Qq--awNv_DObYSXMuYxFCxRDsUJAgfB0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cnw5z0,True,,init__27,,0,True,all_ads,False,[],False,,/r/pytorch/comments/cnw5z0/getting_started_to_achieving_kaggle_kernels_gm/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cnw5z0/getting_started_to_achieving_kaggle_kernels_gm/,7135,1565318621.0,0,,False,,,,,,,,
814,,pytorch,"Hey I'm trying to implement a neuroevolution algotithm using pytorch and I haven't used pytorch before and I think I;m doing something very wrong, so any help will be greatly appreciated!

Everything seems to be working fine and for the all iterations except the last one the learning rate, accuracy(my dataset is balanced) and loss are perfectly okay and then on the last one my accuracy becomes 0.25 (I have 4 classes, so ABSOLUTELY random), my lr= 0.00000 and everything just Fs up. Please help! 

Code snippet at [https://pastebin.com/cpGT6gWb](https://pastebin.com/cpGT6gWb)",t2_3qwcovlu,False,,0,False,Model performing weirdly​! Please help!,[],r/pytorch,False,6,,0,,,False,t3_cnloiw,False,dark,0.76,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1565299246.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey I&amp;#39;m trying to implement a neuroevolution algotithm using pytorch and I haven&amp;#39;t used pytorch before and I think I;m doing something very wrong, so any help will be greatly appreciated!&lt;/p&gt;

&lt;p&gt;Everything seems to be working fine and for the all iterations except the last one the learning rate, accuracy(my dataset is balanced) and loss are perfectly okay and then on the last one my accuracy becomes 0.25 (I have 4 classes, so ABSOLUTELY random), my lr= 0.00000 and everything just Fs up. Please help! &lt;/p&gt;

&lt;p&gt;Code snippet at &lt;a href=""https://pastebin.com/cpGT6gWb""&gt;https://pastebin.com/cpGT6gWb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?auto=webp&amp;s=e28867cccd2864fc170848b64bc44e6a778116b9', 'width': 250, 'height': 250}, 'resolutions': [{'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b38a59d6140109bc38ed4f88b98bd4436dfe09b', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=203ad20eac023d67d122c5f834516a4670799f63', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'tEFaKdpbTuSBBWpWQ-kmQ1l_KwNUpQtPpUtOwmLiL-A'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cnloiw,True,,kartinko28,,1,True,all_ads,False,[],False,,/r/pytorch/comments/cnloiw/model_performing_weirdly_please_help/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cnloiw/model_performing_weirdly_please_help/,7135,1565270446.0,0,,False,,,,,,,,
815,,pytorch,"Hello everyone. 

I made a simple network and tried to access the modules.(print them for now) 

This is the network I'm talking about: 

&amp;#x200B;

    def convlayer(input_dim, output_dim, kernel_size=3, stride=1, padding=1, batchnorm=False):
        layers = []
        conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride, padding)
        layers.append(conv)
        if batchnorm: 
            layers.append(nn.BatchNorm2d(output_dim))
        layers.append(nn.ReLU())
    
        return nn.Sequential(*layers)
    
    class sequential_net3_2(nn.Module):
        def __init__(self):
            super().__init__()
            self.features = nn.Sequential(
                                        convlayer(3, 6, 3, stride=2), 
                                        convlayer(6, 12, 3, stride=2, batchnorm=True),
                                        convlayer(12, 12, 3, stride=2, batchnorm=True)
                                        )
            self.classifer = nn.Linear(12, 2)
    
        def forward(self, x):
            output = self.features(x)
            output = output.view(x.size(0), -1)
            output = self.classifer(output)
            return output
    
    sequential_net3_2 = sequential_net3_2()
    for i, m in enumerate(sequential_net3_2.modules()):
        print(f'{i}, {m}')

I expected to see the modules I see when printing the model which is : 

&amp;#x200B;

    sequential_net3_2(
      (features): Sequential(
        (0): Sequential(
          (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): ReLU()
        )
        (1): Sequential(
          (0): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (2): Sequential(
          (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
      (classifer): Linear(in_features=12, out_features=2, bias=True)
    )

But instead  I got: 

&amp;#x200B;

    0, sequential_net3_2(
      (features): Sequential(
        (0): Sequential(
          (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): ReLU()
        )
        (1): Sequential(
          (0): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (2): Sequential(
          (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
      (classifer): Linear(in_features=12, out_features=2, bias=True)
    )
    1, Sequential(
      (0): Sequential(
        (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): ReLU()
      )
      (1): Sequential(
        (0): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (2): Sequential(
        (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    2, Sequential(
      (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): ReLU()
    )
    3, Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    4, ReLU()
    5, Sequential(
      (0): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    6, Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    7, BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    8, ReLU()
    9, Sequential(
      (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    10, Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    11, BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    12, ReLU()
    13, Linear(in_features=12, out_features=2, bias=True)

I was only expecting : 

&amp;#x200B;

    0, sequential_net3_2(
      (features): Sequential(
        (0): Sequential(
          (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): ReLU()
        )
        (1): Sequential(
          (0): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (2): Sequential(
          (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
      (classifer): Linear(in_features=12, out_features=2, bias=True)
    )

and not all the combinations of the modules!

Is this a bug or is it just expected behavior?

if it is expected behavior what is the use of such spurious information? 

&amp;#x200B;

Thank you all very much",t2_12rvgcg3,False,,0,False,Is this a Pytorch bug?,[],r/pytorch,False,6,,0,,,False,t3_cnk2xm,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1565290512.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone. &lt;/p&gt;

&lt;p&gt;I made a simple network and tried to access the modules.(print them for now) &lt;/p&gt;

&lt;p&gt;This is the network I&amp;#39;m talking about: &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def convlayer(input_dim, output_dim, kernel_size=3, stride=1, padding=1, batchnorm=False):
    layers = []
    conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride, padding)
    layers.append(conv)
    if batchnorm: 
        layers.append(nn.BatchNorm2d(output_dim))
    layers.append(nn.ReLU())

    return nn.Sequential(*layers)

class sequential_net3_2(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
                                    convlayer(3, 6, 3, stride=2), 
                                    convlayer(6, 12, 3, stride=2, batchnorm=True),
                                    convlayer(12, 12, 3, stride=2, batchnorm=True)
                                    )
        self.classifer = nn.Linear(12, 2)

    def forward(self, x):
        output = self.features(x)
        output = output.view(x.size(0), -1)
        output = self.classifer(output)
        return output

sequential_net3_2 = sequential_net3_2()
for i, m in enumerate(sequential_net3_2.modules()):
    print(f&amp;#39;{i}, {m}&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I expected to see the modules I see when printing the model which is : &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sequential_net3_2(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): ReLU()
    )
    (1): Sequential(
      (0): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (2): Sequential(
      (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (classifer): Linear(in_features=12, out_features=2, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But instead  I got: &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0, sequential_net3_2(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): ReLU()
    )
    (1): Sequential(
      (0): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (2): Sequential(
      (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (classifer): Linear(in_features=12, out_features=2, bias=True)
)
1, Sequential(
  (0): Sequential(
    (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ReLU()
  )
  (1): Sequential(
    (0): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (2): Sequential(
    (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
)
2, Sequential(
  (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (1): ReLU()
)
3, Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
4, ReLU()
5, Sequential(
  (0): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU()
)
6, Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
7, BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
8, ReLU()
9, Sequential(
  (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU()
)
10, Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
11, BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
12, ReLU()
13, Linear(in_features=12, out_features=2, bias=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I was only expecting : &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0, sequential_net3_2(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): ReLU()
    )
    (1): Sequential(
      (0): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (2): Sequential(
      (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (classifer): Linear(in_features=12, out_features=2, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and not all the combinations of the modules!&lt;/p&gt;

&lt;p&gt;Is this a bug or is it just expected behavior?&lt;/p&gt;

&lt;p&gt;if it is expected behavior what is the use of such spurious information? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you all very much&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cnk2xm,True,,MasterSama,,4,True,all_ads,False,[],False,,/r/pytorch/comments/cnk2xm/is_this_a_pytorch_bug/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cnk2xm/is_this_a_pytorch_bug/,7135,1565261712.0,0,,False,,,,,,,,
816,,pytorch,"Pytorch newbie here.

&amp;#x200B;

for m in [self.Network](https://self.Network):

if hasattr(m, 'weight') or hasattr(m, 'bias'):

for name, param in m.named\_parameters():

if name == 'weight':

nn.init.orthogonal\_(param, gain=nn.init.calculate\_gain('relu'))

if name == 'bias':

nn.init.constant\_(param, 0.0)

&amp;#x200B;

for name, param in self.mean.named\_parameters():

if name == 'weight':

nn.init.orthogonal\_(param, gain=0.01)

if name == 'bias':

nn.init.constant\_(param, 0.0)

&amp;#x200B;

PS: Sorry but I am unable to index the code on reddit. Can someone please explain that too.

Thanks in Advance",t2_ul4iisf,False,,0,False,Can someone please explain what this code does?,[],r/pytorch,False,6,,0,,,False,t3_cn9feb,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1565230567.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Pytorch newbie here.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;for m in &lt;a href=""https://self.Network""&gt;self.Network&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;if hasattr(m, &amp;#39;weight&amp;#39;) or hasattr(m, &amp;#39;bias&amp;#39;):&lt;/p&gt;

&lt;p&gt;for name, param in m.named_parameters():&lt;/p&gt;

&lt;p&gt;if name == &amp;#39;weight&amp;#39;:&lt;/p&gt;

&lt;p&gt;nn.init.orthogonal_(param, gain=nn.init.calculate_gain(&amp;#39;relu&amp;#39;))&lt;/p&gt;

&lt;p&gt;if name == &amp;#39;bias&amp;#39;:&lt;/p&gt;

&lt;p&gt;nn.init.constant_(param, 0.0)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;for name, param in self.mean.named_parameters():&lt;/p&gt;

&lt;p&gt;if name == &amp;#39;weight&amp;#39;:&lt;/p&gt;

&lt;p&gt;nn.init.orthogonal_(param, gain=0.01)&lt;/p&gt;

&lt;p&gt;if name == &amp;#39;bias&amp;#39;:&lt;/p&gt;

&lt;p&gt;nn.init.constant_(param, 0.0)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;PS: Sorry but I am unable to index the code on reddit. Can someone please explain that too.&lt;/p&gt;

&lt;p&gt;Thanks in Advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cn9feb,True,,xicor7017,,2,True,all_ads,False,[],False,,/r/pytorch/comments/cn9feb/can_someone_please_explain_what_this_code_does/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cn9feb/can_someone_please_explain_what_this_code_does/,7135,1565201767.0,0,,False,,,,,,,,
817,,pytorch,"I recently came across the [Maestro dataset](https://magenta.tensorflow.org/datasets/maestro) and wanted to design a deep learning model for generating music compositions from these MIDI files. I know only Pytorch unfortunately and haven't had the time to work with TensorFlow. With this in mind can you give me some advice on how I should proceed to building this model from scratch.

&amp;#x200B;

PS: I've just used Pytorch for 4 months or so only.  Not at all a pro. But I have the ML concepts right I think. :-|",t2_3mechp98,False,,0,False,Music Generation with Pytorch - guidance please,[],r/pytorch,False,6,,0,,,False,t3_cn4d44,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1565204661.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I recently came across the &lt;a href=""https://magenta.tensorflow.org/datasets/maestro""&gt;Maestro dataset&lt;/a&gt; and wanted to design a deep learning model for generating music compositions from these MIDI files. I know only Pytorch unfortunately and haven&amp;#39;t had the time to work with TensorFlow. With this in mind can you give me some advice on how I should proceed to building this model from scratch.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;PS: I&amp;#39;ve just used Pytorch for 4 months or so only.  Not at all a pro. But I have the ML concepts right I think. :-|&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5UpSfA2YpNtbdOoH_9H6d1Qe2OHBX-n_EtbIl0tUJFE.jpg?auto=webp&amp;s=198dcebecbbc415e597a25a498f30455ca21a4f8', 'width': 1140, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/5UpSfA2YpNtbdOoH_9H6d1Qe2OHBX-n_EtbIl0tUJFE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=53cdb652768726c4ac2614000425dd28806e601e', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/5UpSfA2YpNtbdOoH_9H6d1Qe2OHBX-n_EtbIl0tUJFE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e9f0e803ca88ea606cb4e9f810551bda94280120', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/5UpSfA2YpNtbdOoH_9H6d1Qe2OHBX-n_EtbIl0tUJFE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1c4ca301e452664f3230e2d8c6eb6f6397894326', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/5UpSfA2YpNtbdOoH_9H6d1Qe2OHBX-n_EtbIl0tUJFE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b6be0b5f12994cb416d6278fd6d3e4744ddb652e', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/5UpSfA2YpNtbdOoH_9H6d1Qe2OHBX-n_EtbIl0tUJFE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d27a56c8508265a5474b96c78687370cd5897761', 'width': 960, 'height': 505}, {'url': 'https://external-preview.redd.it/5UpSfA2YpNtbdOoH_9H6d1Qe2OHBX-n_EtbIl0tUJFE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=801e797e5ab272b5c8471d4110081e462ce99d1f', 'width': 1080, 'height': 568}], 'variants': {}, 'id': 'ESwPzRifOXHhruumAG2kaUtXjj6w0HKpgfz9PNuSe1M'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cn4d44,True,,stygianrex,,1,True,all_ads,False,[],False,,/r/pytorch/comments/cn4d44/music_generation_with_pytorch_guidance_please/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cn4d44/music_generation_with_pytorch_guidance_please/,7135,1565175861.0,0,,False,,,,,,,,
818,,pytorch,"Hi! I keep coming across an issue when running a python script from the Windows command line. 

`RuntimeError: Found 0 files in subfolders of: dataset\images\trainA`

`Supported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif`

This is the error. My directory hierarchy is shaped like this

.\\dataset

\\images

\\trainA

xxx.png

I've searched all over for a solution and the most common one I found was the need for a subfolder. I tried creating a subfolder, simply titled ""1"" within \\trainA and I continued to get the same error. The README for the code I'm using calls for the hierarchy to be shaped like the one shown above. I've also tried using jpg, png, and jpeg file formats and none of them have been 'seen' by the program. Anyone have any ideas?

&amp;#x200B;

Thanks!",t2_1nf8dzl,False,,0,False,RuntimeError: Found 0 files in subfolders of: dataset\images\trainA,[],r/pytorch,False,6,,0,,,False,t3_cmxww1,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1565162751.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi! I keep coming across an issue when running a python script from the Windows command line. &lt;/p&gt;

&lt;p&gt;&lt;code&gt;RuntimeError: Found 0 files in subfolders of: dataset\images\trainA&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Supported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is the error. My directory hierarchy is shaped like this&lt;/p&gt;

&lt;p&gt;.\dataset&lt;/p&gt;

&lt;p&gt;\images&lt;/p&gt;

&lt;p&gt;\trainA&lt;/p&gt;

&lt;p&gt;xxx.png&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve searched all over for a solution and the most common one I found was the need for a subfolder. I tried creating a subfolder, simply titled &amp;quot;1&amp;quot; within \trainA and I continued to get the same error. The README for the code I&amp;#39;m using calls for the hierarchy to be shaped like the one shown above. I&amp;#39;ve also tried using jpg, png, and jpeg file formats and none of them have been &amp;#39;seen&amp;#39; by the program. Anyone have any ideas?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cmxww1,True,,reddit-moose,,3,True,all_ads,False,[],False,,/r/pytorch/comments/cmxww1/runtimeerror_found_0_files_in_subfolders_of/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cmxww1/runtimeerror_found_0_files_in_subfolders_of/,7135,1565133951.0,0,,False,,,,,,,,
819,,pytorch,,t2_1uzfxf9a,False,,0,False,LSTM help: trouble making a model that uses several timeseries (think 10 timesteps of 12 input categories [10x12]) to generate single output ([1x1]). Experiencing lots of mtx size issues. Any examples would be super appreciated!,[],r/pytorch,False,6,,0,,,False,t3_cmfmxn,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1565062048.0,text,6,,,text,self.pytorch,False,,,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cmfmxn,True,,athenysus,,8,True,all_ads,False,[],False,,/r/pytorch/comments/cmfmxn/lstm_help_trouble_making_a_model_that_uses/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cmfmxn/lstm_help_trouble_making_a_model_that_uses/,7135,1565033248.0,0,,False,,,,,,,,
820,,pytorch,"Interview with Tim Dettmers, PhD Student at University of Washington. 

We talk a lot about DL Research, DL Hardware :) and Tim's Kaggle experience 

Audio: https://anchor.fm/chaitimedatascience/episodes/Deep-Learning-Research--Hardware--Kaggle--Interview-with-Tim-Dettmers-e4qcad/a-ak0hr6

Video: https://www.youtube.com/watch?v=4857Lph2ndk&amp;feature=youtu.be",t2_2s77fkel,False,,0,False,"Deep Learning Research, Deep Learning Hardware, Sparse Networks | Interview with Tim Dettmers",[],r/pytorch,False,6,,0,,,False,t3_clz8pw,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1564969835.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Interview with Tim Dettmers, PhD Student at University of Washington. &lt;/p&gt;

&lt;p&gt;We talk a lot about DL Research, DL Hardware :) and Tim&amp;#39;s Kaggle experience &lt;/p&gt;

&lt;p&gt;Audio: &lt;a href=""https://anchor.fm/chaitimedatascience/episodes/Deep-Learning-Research--Hardware--Kaggle--Interview-with-Tim-Dettmers-e4qcad/a-ak0hr6""&gt;https://anchor.fm/chaitimedatascience/episodes/Deep-Learning-Research--Hardware--Kaggle--Interview-with-Tim-Dettmers-e4qcad/a-ak0hr6&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Video: &lt;a href=""https://www.youtube.com/watch?v=4857Lph2ndk&amp;amp;feature=youtu.be""&gt;https://www.youtube.com/watch?v=4857Lph2ndk&amp;amp;feature=youtu.be&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?auto=webp&amp;s=96c78f7af0d55bf809aea5e3aa74cbbcef56dd55', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb0b9f0fe0880768f4673eb0b0de4950a98b6b17', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c358b62ea4934234264e445b13378fcfa493bf6d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db1864c49e213ead3ece1212938c1640ab7b1535', 'width': 320, 'height': 320}], 'variants': {}, 'id': '3vbWKckhU81Qq--awNv_DObYSXMuYxFCxRDsUJAgfB0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,clz8pw,True,,init__27,,0,True,all_ads,False,[],False,,/r/pytorch/comments/clz8pw/deep_learning_research_deep_learning_hardware/,all_ads,False,https://www.reddit.com/r/pytorch/comments/clz8pw/deep_learning_research_deep_learning_hardware/,7135,1564941035.0,0,,False,,,,,,,,
821,,pytorch,"I'm going to a 2 day machine learning hackathon. 2 days isn't a lot of time to train a brand new model, so I'm hoping to make something that would assist other ML people. 2 days will probably just be enough time for a prototype with core functionality.",t2_10efjmjx,False,,0,False,"[D] Do you have an idea for a machine learning library or framework that you'll probably never get around to making, and wish someone else would make it?",[],r/pytorch,False,6,,0,,,False,t3_cl7hia,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1564797215.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m going to a 2 day machine learning hackathon. 2 days isn&amp;#39;t a lot of time to train a brand new model, so I&amp;#39;m hoping to make something that would assist other ML people. 2 days will probably just be enough time for a prototype with core functionality.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cl7hia,True,,BatmantoshReturns,,2,True,all_ads,False,[],False,,/r/pytorch/comments/cl7hia/d_do_you_have_an_idea_for_a_machine_learning/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cl7hia/d_do_you_have_an_idea_for_a_machine_learning/,7135,1564768415.0,0,,False,,,,,,,,
822,,pytorch,"TL;DR:  
I want to read how the forward and backward passes are implemented in Pytorch underneath the hood.

&amp;#x200B;

I am currently doing assignment 2 of cs231n where one exercise is to implement batch normalization from scratch.  
The exercises notices that there is a clever trick to make the backward pass faster than by ""naively"" writing the formula for the gradient shown in the paper. This is described in [https://kevinzakka.github.io/2016/09/14/batch\_normalization/](https://kevinzakka.github.io/2016/09/14/batch_normalization/)

Since this trick is clever, I wanted to see how a real life implementation would look like.

* [https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py)  
on line 78 return `F.batch_norm(`
* [https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/\_functions.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/_functions.py)  
shows several calls to `torch.batch_norm_*`
* After some searching, I found [https://github.com/pytorch/pytorch/blob/e5b5154768ca2ba26742d75d13df4ea89a0d814b/torch/csrc/autograd/functions/batch\_normalization.cpp](https://github.com/pytorch/pytorch/blob/e5b5154768ca2ba26742d75d13df4ea89a0d814b/torch/csrc/autograd/functions/batch_normalization.cpp)  
it looks like `torch::nn::BatchNormalization_updateOutput` is where it's at
* But [https://github.com/pytorch/pytorch/tree/e5b5154768ca2ba26742d75d13df4ea89a0d814b/torch/csrc/nn](https://github.com/pytorch/pytorch/tree/e5b5154768ca2ba26742d75d13df4ea89a0d814b/torch/csrc/nn) is ""empty""

&amp;#x200B;

So, how can I find where batch norm is implemented?

Additionally, how can I ease up searching a large codebase (for future references)? I feel like following a thread in a messy unfamiliar place, but actually I have no idea what I'm doing.",t2_2b6wlz4w,False,,0,False,Where can I read the implementation of Batch Normalization?,[],r/pytorch,False,6,,0,,,False,t3_cl231i,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1564768799.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;TL;DR:&lt;br/&gt;
I want to read how the forward and backward passes are implemented in Pytorch underneath the hood.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am currently doing assignment 2 of cs231n where one exercise is to implement batch normalization from scratch.&lt;br/&gt;
The exercises notices that there is a clever trick to make the backward pass faster than by &amp;quot;naively&amp;quot; writing the formula for the gradient shown in the paper. This is described in &lt;a href=""https://kevinzakka.github.io/2016/09/14/batch_normalization/""&gt;https://kevinzakka.github.io/2016/09/14/batch_normalization/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Since this trick is clever, I wanted to see how a real life implementation would look like.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py""&gt;https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py&lt;/a&gt;&lt;br/&gt;
on line 78 return &lt;code&gt;F.batch_norm(&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/_functions.py""&gt;https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/_functions.py&lt;/a&gt;&lt;br/&gt;
shows several calls to &lt;code&gt;torch.batch_norm_*&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;After some searching, I found &lt;a href=""https://github.com/pytorch/pytorch/blob/e5b5154768ca2ba26742d75d13df4ea89a0d814b/torch/csrc/autograd/functions/batch_normalization.cpp""&gt;https://github.com/pytorch/pytorch/blob/e5b5154768ca2ba26742d75d13df4ea89a0d814b/torch/csrc/autograd/functions/batch_normalization.cpp&lt;/a&gt;&lt;br/&gt;
it looks like &lt;code&gt;torch::nn::BatchNormalization_updateOutput&lt;/code&gt; is where it&amp;#39;s at&lt;/li&gt;
&lt;li&gt;But &lt;a href=""https://github.com/pytorch/pytorch/tree/e5b5154768ca2ba26742d75d13df4ea89a0d814b/torch/csrc/nn""&gt;https://github.com/pytorch/pytorch/tree/e5b5154768ca2ba26742d75d13df4ea89a0d814b/torch/csrc/nn&lt;/a&gt; is &amp;quot;empty&amp;quot;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So, how can I find where batch norm is implemented?&lt;/p&gt;

&lt;p&gt;Additionally, how can I ease up searching a large codebase (for future references)? I feel like following a thread in a messy unfamiliar place, but actually I have no idea what I&amp;#39;m doing.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cl231i,True,,hagetarou,,2,True,all_ads,False,[],False,,/r/pytorch/comments/cl231i/where_can_i_read_the_implementation_of_batch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cl231i/where_can_i_read_the_implementation_of_batch/,7135,1564739999.0,0,,False,,,,,,,,
823,,pytorch,"I have a target array of 609 possible outputs. They are all 1s and 0s and in this particular case it seems to not be learning at all and default to 0 because there are 600 0s and 9 1s. Here is the model and the training loop

 

    class little_model(nn.Module):
          def __init__(self, inp, out):
            super(little_model, self).__init__()
            self.lil_fc = nn.Linear(inp, 1024)
            self.lil_fc2 = nn.Linear(1024, 2048)
            self.lil_fc3 = nn.Linear(2048, 2048)
            self.lil_fc4 = nn.Linear(2048, 1024)
            self.lil_fc5 = nn.Linear(1024, out)
    
            
          def forward(self, x):
            first = self.lil_fc(x)
            second = self.lil_fc2(first)
            third = self.lil_fc3(second)
            fourth = self.lil_fc4(third)
            fourth = torch.tanh(fourth)
            fifth= self.lil_fc5(fourth)
            return fifth
    
    mod = little_model(613, 609)
    mod = mod.to(device)
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    op =  torch.optim.SGD(mod.parameters(),
                                 lr=0.001, momentum=0.9, weight_decay = 0.2)
    crit = torch.nn.BCEWithLogitsLoss()
    inp = inps[0].to(device)
    teg = tags[0].to(device)
    for i in range(3000):
            
        out = mod(inp.data)
        loss = crit(out, teg.data)
        op.zero_grad()
        loss.backward()
        op.step()
    

I have more data but at this point I am just trying to overfit on this particular sample just for testing. The loss also seems to stay around 0.69. Any idea why it seems to not be training at all and just defaults everything to 0?",t2_qjb55,False,,0,False,"Pytorch Sparse outputs seems to default to 0, not training",[],r/pytorch,False,6,,0,,,False,t3_ckpsnt,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1564673282.0,,[],{},,,True,,1564701819.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a target array of 609 possible outputs. They are all 1s and 0s and in this particular case it seems to not be learning at all and default to 0 because there are 600 0s and 9 1s. Here is the model and the training loop&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class little_model(nn.Module):
      def __init__(self, inp, out):
        super(little_model, self).__init__()
        self.lil_fc = nn.Linear(inp, 1024)
        self.lil_fc2 = nn.Linear(1024, 2048)
        self.lil_fc3 = nn.Linear(2048, 2048)
        self.lil_fc4 = nn.Linear(2048, 1024)
        self.lil_fc5 = nn.Linear(1024, out)


      def forward(self, x):
        first = self.lil_fc(x)
        second = self.lil_fc2(first)
        third = self.lil_fc3(second)
        fourth = self.lil_fc4(third)
        fourth = torch.tanh(fourth)
        fifth= self.lil_fc5(fourth)
        return fifth

mod = little_model(613, 609)
mod = mod.to(device)
device = torch.device(&amp;#39;cuda:0&amp;#39; if torch.cuda.is_available() else &amp;#39;cpu&amp;#39;)
op =  torch.optim.SGD(mod.parameters(),
                             lr=0.001, momentum=0.9, weight_decay = 0.2)
crit = torch.nn.BCEWithLogitsLoss()
inp = inps[0].to(device)
teg = tags[0].to(device)
for i in range(3000):

    out = mod(inp.data)
    loss = crit(out, teg.data)
    op.zero_grad()
    loss.backward()
    op.step()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have more data but at this point I am just trying to overfit on this particular sample just for testing. The loss also seems to stay around 0.69. Any idea why it seems to not be training at all and just defaults everything to 0?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ckpsnt,True,,Awill1aB,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ckpsnt/pytorch_sparse_outputs_seems_to_default_to_0_not/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ckpsnt/pytorch_sparse_outputs_seems_to_default_to_0_not/,7135,1564673019.0,0,,False,,,,,,,,
824,,pytorch,"hey guys I've been trying to get my pytorch segmentation model to coreML, but looks like I have to convert it to onnx first but I can't seem to get it to work with everything I tried, is there anyone who's really experienced In converting models? I would love your help.

link to model - [https://drive.google.com/file/d/1yOkUCmlm8X8VQuh-hl1v4IeJe39PxnuP/view?usp=sharing](https://drive.google.com/file/d/1yOkUCmlm8X8VQuh-hl1v4IeJe39PxnuP/view?usp=sharing)",t2_1305uo,False,,0,False,Need help converting pytorch model to onnx,[],r/pytorch,False,6,,0,,,False,t3_ck2eqs,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1564570538.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;hey guys I&amp;#39;ve been trying to get my pytorch segmentation model to coreML, but looks like I have to convert it to onnx first but I can&amp;#39;t seem to get it to work with everything I tried, is there anyone who&amp;#39;s really experienced In converting models? I would love your help.&lt;/p&gt;

&lt;p&gt;link to model - &lt;a href=""https://drive.google.com/file/d/1yOkUCmlm8X8VQuh-hl1v4IeJe39PxnuP/view?usp=sharing""&gt;https://drive.google.com/file/d/1yOkUCmlm8X8VQuh-hl1v4IeJe39PxnuP/view?usp=sharing&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ck2eqs,True,,ewelumokeke,,1,True,all_ads,False,[],False,,/r/pytorch/comments/ck2eqs/need_help_converting_pytorch_model_to_onnx/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ck2eqs/need_help_converting_pytorch_model_to_onnx/,7135,1564541738.0,0,,False,,,,,,,,
825,,pytorch,"Hi everyone!
I working on image classification and I have a project where we made the data loading part ourselves. The code is capable to load and preprocess images for the next batch on a different threads (using an output Tensor in shared memory for efficiency), while the current batch is being processed by the GPU.

But I want to implement a more complex data sampling scheme so I need something like the pytorch dataloader.

Is there a way to keep the efficiency of the old design (load next batch during inference and backprop, as few Tensors as possible) while using DataLoader?

I tried implementing something using Dataloader but it was very unefficient, especially the execution collate_fn.

Any advice on efficient dataloading that could be interesting?",t2_fqkup,False,,0,False,Dataloader and multiprocessing,[],r/pytorch,False,6,,0,,,False,t3_cjp992,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1564507335.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!
I working on image classification and I have a project where we made the data loading part ourselves. The code is capable to load and preprocess images for the next batch on a different threads (using an output Tensor in shared memory for efficiency), while the current batch is being processed by the GPU.&lt;/p&gt;

&lt;p&gt;But I want to implement a more complex data sampling scheme so I need something like the pytorch dataloader.&lt;/p&gt;

&lt;p&gt;Is there a way to keep the efficiency of the old design (load next batch during inference and backprop, as few Tensors as possible) while using DataLoader?&lt;/p&gt;

&lt;p&gt;I tried implementing something using Dataloader but it was very unefficient, especially the execution collate_fn.&lt;/p&gt;

&lt;p&gt;Any advice on efficient dataloading that could be interesting?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cjp992,True,,Mulcyber,,5,True,all_ads,False,[],False,,/r/pytorch/comments/cjp992/dataloader_and_multiprocessing/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cjp992/dataloader_and_multiprocessing/,7135,1564478535.0,0,,False,,,,,,,,
826,,pytorch,,t2_j1toq,False,,0,False,Understanding PyTorch with an example: a step-by-step tutorial,[],r/pytorch,False,6,,0,93.0,,False,t3_cjiyvw,False,dark,0.75,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://a.thumbs.redditmedia.com/ii-PkTtp2UwhsE8FkvxN61BKVfJVNDAt6d4djHqapp0.jpg,False,,[],{},link,,False,,1564470225.0,text,6,,,text,towardsdatascience.com,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/63tQbOnBpLJVSbHUbpFpSDnwNDZV1DFAFgnnz8GW_lg.jpg?auto=webp&amp;s=dae76153b1e2d7e08361e1405adda482ab9beebf', 'width': 1200, 'height': 799}, 'resolutions': [{'url': 'https://external-preview.redd.it/63tQbOnBpLJVSbHUbpFpSDnwNDZV1DFAFgnnz8GW_lg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=750667afdb760013375e7a0021bba1f42ca2e254', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/63tQbOnBpLJVSbHUbpFpSDnwNDZV1DFAFgnnz8GW_lg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e15a05abc655925fae11c0a309b053dc3df074d', 'width': 216, 'height': 143}, {'url': 'https://external-preview.redd.it/63tQbOnBpLJVSbHUbpFpSDnwNDZV1DFAFgnnz8GW_lg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4fa7bf1bebb58ae3807e2b46a48312cfdfade81f', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/63tQbOnBpLJVSbHUbpFpSDnwNDZV1DFAFgnnz8GW_lg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c7031b9affd8c944b76de7f1b0e8a0f3ee30d50', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/63tQbOnBpLJVSbHUbpFpSDnwNDZV1DFAFgnnz8GW_lg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=59b564ccab295ce7b619ac3d5ca4a6ba2a48ccd1', 'width': 960, 'height': 639}, {'url': 'https://external-preview.redd.it/63tQbOnBpLJVSbHUbpFpSDnwNDZV1DFAFgnnz8GW_lg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=049dbf9dedfa9579ece25051ab1a4a8d8ec7765a', 'width': 1080, 'height': 719}], 'variants': {}, 'id': 'tQi5Uf4uZ1No8GgMyocXpEAUodXY9aO4wrhVn5Nq2YE'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cjiyvw,True,,mjgcfb,,0,True,all_ads,False,[],False,,/r/pytorch/comments/cjiyvw/understanding_pytorch_with_an_example_a/,all_ads,False,https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e#58f2,7135,1564441425.0,0,,False,https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e#58f2,,,,,,,
827,,pytorch,"Hello,


I am having a hard time trying to speed up the models I develop. I have a desktop with a GTX 1080ti (single GPU) and a Ryzen 7 2700x and I use PyTorch for my models. Since I've started studying the field not long ago, most of my models are small and I used to run them solely on CPU. However, lately I tried to speed up the training and I decided to train my models on my GPU, but I don't see any improvement in the training time when compared to training on CPU.


What I do to start training the models on the GPU is to include .cuda() in all tensors and in the network. Is there something I am missing?


Thanks",t2_mwu7o,False,,0,False,GPU vs CPU,[],r/pytorch,False,6,,0,,,False,t3_chmj0f,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1564083451.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am having a hard time trying to speed up the models I develop. I have a desktop with a GTX 1080ti (single GPU) and a Ryzen 7 2700x and I use PyTorch for my models. Since I&amp;#39;ve started studying the field not long ago, most of my models are small and I used to run them solely on CPU. However, lately I tried to speed up the training and I decided to train my models on my GPU, but I don&amp;#39;t see any improvement in the training time when compared to training on CPU.&lt;/p&gt;

&lt;p&gt;What I do to start training the models on the GPU is to include .cuda() in all tensors and in the network. Is there something I am missing?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,chmj0f,True,,arivar,,9,True,all_ads,False,[],False,,/r/pytorch/comments/chmj0f/gpu_vs_cpu/,all_ads,False,https://www.reddit.com/r/pytorch/comments/chmj0f/gpu_vs_cpu/,7135,1564054651.0,0,,False,,,,,,,,
828,,pytorch,"So I'm moving right now and I don't have access to my computer.

&amp;#x200B;

So recently I purchased a 1060 for myself. I did everything in this link to verify that it's installed correctly etc.

 [https://stackoverflow.com/questions/48152674/how-to-check-if-pytorch-is-using-the-gpu](https://stackoverflow.com/questions/48152674/how-to-check-if-pytorch-is-using-the-gpu) 

However, I have not been calling .cuda() anywhere in my code. Am I a fool and my GPU has been doing nothing?

&amp;#x200B;

Follow up question: my project is a game playing engine, which will require very frequent inferences of small batches (thousands per second, ideally). How do I best optimize this? Is it possible to 'compile' a model?",t2_ew2cf,False,,0,False,quick dumb question about CUDA,[],r/pytorch,False,6,,0,,,False,t3_chgba7,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1564042457.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I&amp;#39;m moving right now and I don&amp;#39;t have access to my computer.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So recently I purchased a 1060 for myself. I did everything in this link to verify that it&amp;#39;s installed correctly etc.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://stackoverflow.com/questions/48152674/how-to-check-if-pytorch-is-using-the-gpu""&gt;https://stackoverflow.com/questions/48152674/how-to-check-if-pytorch-is-using-the-gpu&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;However, I have not been calling .cuda() anywhere in my code. Am I a fool and my GPU has been doing nothing?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Follow up question: my project is a game playing engine, which will require very frequent inferences of small batches (thousands per second, ideally). How do I best optimize this? Is it possible to &amp;#39;compile&amp;#39; a model?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,chgba7,True,,baskeyt,,5,True,all_ads,False,[],False,,/r/pytorch/comments/chgba7/quick_dumb_question_about_cuda/,all_ads,False,https://www.reddit.com/r/pytorch/comments/chgba7/quick_dumb_question_about_cuda/,7135,1564013657.0,0,,False,,,,,,,,
829,,pytorch,"So I have been thinking of switching from tensorflow to pytorch, because the latter is more pythonic etc.. I'm reading the tutorials online. One thing I like about tensorflow is tensorflow-gpu, I just install it and use it and don't think about my gpu anymore as long as it is big enough. :)

Going through the pytorch tutorials, in the tutorial on tensors there's a little section at the end on moving tensors onto the gpu using the `to` method (https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#cuda-tensors). Then a couple of tutorials later in the bit on training networks, there's a little section at the end on how to train on a GPU  (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-on-gpu). It says:    

&gt; Just like how you transfer a Tensor onto the GPU, you transfer the neural net onto the GPU. Let’s first define our device as the first visible cuda device if we have CUDA available:
    
        device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
    
&gt; The rest of this section assumes that device is a CUDA device. Then these methods will recursively go over all modules and convert their parameters and buffers to CUDA tensors:
    
        net.to(device)

This is fine. I sort of wish it did this by default, but ok I have a bit of fine-grained control over what goes where, I guess.  Then it goes on:

&gt; Remember [?] that you will have to send the inputs and targets at every step to the GPU too:
    
        inputs, labels = data[0].to(device), data[1].to(device)

OK, so I add this to the for loop in the previous training steps (though frankly it would be nice if the tutorial just worked out a full example using GPU from start to finish, with profiling thrown in for good measure). It does seem to have sped things up some, but I'm confused for a few reasons.

First, I'm not seeing *any* GPU memory usage increase when I enter `nvidia-smi` during training. When I run tensorflow, it pretty much fills my gpu even with small networks. I'm not saying this is a good thing because that's one complaint I have about tensorflow it is a memory-grubbing framework, but I feel like at least I know the GPU is getting used.

In these tutorials the GPU is kind of an afterthought, whereas in this era shouldn't it be integrated into the tutorials from the beginning? 

in general I feel like I don't really understand the best way to integrate GPU in my code going forward. If I just want all tensors/models/training to go on my GPU, is there just a toggle I can set or some configuration file where I can just say `pytorch.gpu = True` or whatever? Is there an authoritative but friendly guide on this? I feel like it should be simpler and I'm missing something.  (But maybe in pytorch it isn't)?",t2_3vnf4prh,False,,0,False,gpu in pytorch good resource for general guidelines/advice? I feel very lost with the tutorial afterthought-like treatment,[],r/pytorch,False,6,,0,,,False,t3_ch8zr4,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1563979401.0,,[],{},,,True,,1564007861.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I have been thinking of switching from tensorflow to pytorch, because the latter is more pythonic etc.. I&amp;#39;m reading the tutorials online. One thing I like about tensorflow is tensorflow-gpu, I just install it and use it and don&amp;#39;t think about my gpu anymore as long as it is big enough. :)&lt;/p&gt;

&lt;p&gt;Going through the pytorch tutorials, in the tutorial on tensors there&amp;#39;s a little section at the end on moving tensors onto the gpu using the &lt;code&gt;to&lt;/code&gt; method (&lt;a href=""https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#cuda-tensors""&gt;https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#cuda-tensors&lt;/a&gt;). Then a couple of tutorials later in the bit on training networks, there&amp;#39;s a little section at the end on how to train on a GPU  (&lt;a href=""https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-on-gpu""&gt;https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-on-gpu&lt;/a&gt;). It says:    &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Just like how you transfer a Tensor onto the GPU, you transfer the neural net onto the GPU. Let’s first define our device as the first visible cuda device if we have CUDA available:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;    device = torch.device(&amp;quot;cuda:0&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;The rest of this section assumes that device is a CUDA device. Then these methods will recursively go over all modules and convert their parameters and buffers to CUDA tensors:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;    net.to(device)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is fine. I sort of wish it did this by default, but ok I have a bit of fine-grained control over what goes where, I guess.  Then it goes on:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Remember [?] that you will have to send the inputs and targets at every step to the GPU too:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;    inputs, labels = data[0].to(device), data[1].to(device)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK, so I add this to the for loop in the previous training steps (though frankly it would be nice if the tutorial just worked out a full example using GPU from start to finish, with profiling thrown in for good measure). It does seem to have sped things up some, but I&amp;#39;m confused for a few reasons.&lt;/p&gt;

&lt;p&gt;First, I&amp;#39;m not seeing &lt;em&gt;any&lt;/em&gt; GPU memory usage increase when I enter &lt;code&gt;nvidia-smi&lt;/code&gt; during training. When I run tensorflow, it pretty much fills my gpu even with small networks. I&amp;#39;m not saying this is a good thing because that&amp;#39;s one complaint I have about tensorflow it is a memory-grubbing framework, but I feel like at least I know the GPU is getting used.&lt;/p&gt;

&lt;p&gt;In these tutorials the GPU is kind of an afterthought, whereas in this era shouldn&amp;#39;t it be integrated into the tutorials from the beginning? &lt;/p&gt;

&lt;p&gt;in general I feel like I don&amp;#39;t really understand the best way to integrate GPU in my code going forward. If I just want all tensors/models/training to go on my GPU, is there just a toggle I can set or some configuration file where I can just say &lt;code&gt;pytorch.gpu = True&lt;/code&gt; or whatever? Is there an authoritative but friendly guide on this? I feel like it should be simpler and I&amp;#39;m missing something.  (But maybe in pytorch it isn&amp;#39;t)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ch8zr4,True,,ml_runway,,6,True,all_ads,False,[],False,,/r/pytorch/comments/ch8zr4/gpu_in_pytorch_good_resource_for_general/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ch8zr4/gpu_in_pytorch_good_resource_for_general/,7135,1563979061.0,0,,False,,,,,,,,
830,,pytorch,"Is there any plan to add large model support to Pytorch - specifically, the ability to transfer tensors from GPU memory to host memory? I am working with medical data imaging segmentation (whole-brain), and loading data as patches would be less ideal than loading the 3D data all at once. I know Tensorflow/Keras already has support for this ([https://github.com/IBM/tensorflow-large-model-support](https://github.com/IBM/tensorflow-large-model-support)), and IBM's PowerAI cloud servers also have implemented this functionality into PyTorch ([https://www.ibm.com/support/knowledgecenter/en/SS5SF7\_1.6.0/navigation/pai\_getstarted\_pytorch.html](https://www.ibm.com/support/knowledgecenter/en/SS5SF7_1.6.0/navigation/pai_getstarted_pytorch.html)), but this is available only within an IBM environment.",t2_4agn6,False,,0,False,Large Model Support for PyTorch?,[],r/pytorch,False,6,,0,,,False,t3_cgyppk,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1563945816.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there any plan to add large model support to Pytorch - specifically, the ability to transfer tensors from GPU memory to host memory? I am working with medical data imaging segmentation (whole-brain), and loading data as patches would be less ideal than loading the 3D data all at once. I know Tensorflow/Keras already has support for this (&lt;a href=""https://github.com/IBM/tensorflow-large-model-support""&gt;https://github.com/IBM/tensorflow-large-model-support&lt;/a&gt;), and IBM&amp;#39;s PowerAI cloud servers also have implemented this functionality into PyTorch (&lt;a href=""https://www.ibm.com/support/knowledgecenter/en/SS5SF7_1.6.0/navigation/pai_getstarted_pytorch.html""&gt;https://www.ibm.com/support/knowledgecenter/en/SS5SF7_1.6.0/navigation/pai_getstarted_pytorch.html&lt;/a&gt;), but this is available only within an IBM environment.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/784KfDT_ewQ8QAhylUXuK3iqwlWwNTOjs2PclGMDYVY.jpg?auto=webp&amp;s=fde1c0fb5e611d45b58b671f2601685412f05b7e', 'width': 196, 'height': 196}, 'resolutions': [{'url': 'https://external-preview.redd.it/784KfDT_ewQ8QAhylUXuK3iqwlWwNTOjs2PclGMDYVY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=accc88c016c816189fa2c126099d9fae9f316b99', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'x9_V8uwXsqzOXeJh74fgIKIoAV3-zXb06yxhS96QEiI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cgyppk,True,,embolized,,3,True,all_ads,False,[],False,,/r/pytorch/comments/cgyppk/large_model_support_for_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cgyppk/large_model_support_for_pytorch/,7135,1563917016.0,0,,False,,,,,,,,
831,,pytorch,,t2_ft66c9,False,,0,False,Question on pytorch dataloaders,[],r/pytorch,False,6,,0,140.0,,False,t3_cgxkp5,False,dark,0.6,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/cDsflHW94dYY0PA0ta6Nc5ZWTKpMdHRTdFrxANlCb1M.jpg,False,,[],{},link,,False,,1563940537.0,text,6,,,text,discuss.pytorch.org,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?auto=webp&amp;s=25578284f23558a4b75f01ba8e9eeac86b51ff7b', 'width': 512, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=483db2a9221411f5f23a5ff6343682aad440a479', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70aec466c6831e1a172caf5b9702244e398cfb5f', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ptTnDZUx3X9A3hi4wxKik9qFkPv8iZAiRYu9fRZmU0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f338411ff9dd85acf34f5a688d3e05e52000c39', 'width': 320, 'height': 320}], 'variants': {}, 'id': '0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cgxkp5,True,,styx97,,0,True,all_ads,False,[],False,,/r/pytorch/comments/cgxkp5/question_on_pytorch_dataloaders/,all_ads,False,https://discuss.pytorch.org/t/a-dataloader-for-multiple-similar-inputs/51284?u=styx97,7135,1563911737.0,0,,False,https://discuss.pytorch.org/t/a-dataloader-for-multiple-similar-inputs/51284?u=styx97,,,,,,,
832,,pytorch,"It's really an honor for me to kick-off The Chai Time Data Science Show with an interview with the Only Triple Grandmaster on Kaggle: 

&amp;#x200B;

Here are the links to the interview. You can find it both in audio and video (of the interview) format:

[Podcast](https://anchor.fm/chaitimedatascience/episodes/Kaggle-Triple-Grandmaster--Abhishek-Thakur-Interview-e4mjoi)

&amp;#x200B;

[Video](https://www.youtube.com/watch?v=vMtORPcjDn8&amp;list=PLyMom0n-MBrrGL8w-nD9kc2q5dOwN7Hb1&amp;index=2)

&amp;#x200B;

I'll be releasing 1 episode every Thursday, Sunday. 

Hope you like it, If you have any ideas/comments/suggestions, I'd really love to hear them.",t2_2s77fkel,False,,0,False,Interview with World's First Triple Kaggle Grandmaster: Abhishek Thakur,[],r/pytorch,False,6,,0,,,False,t3_cg27ff,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},self,,True,,1563762170.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;It&amp;#39;s really an honor for me to kick-off The Chai Time Data Science Show with an interview with the Only Triple Grandmaster on Kaggle: &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Here are the links to the interview. You can find it both in audio and video (of the interview) format:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://anchor.fm/chaitimedatascience/episodes/Kaggle-Triple-Grandmaster--Abhishek-Thakur-Interview-e4mjoi""&gt;Podcast&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=vMtORPcjDn8&amp;amp;list=PLyMom0n-MBrrGL8w-nD9kc2q5dOwN7Hb1&amp;amp;index=2""&gt;Video&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ll be releasing 1 episode every Thursday, Sunday. &lt;/p&gt;

&lt;p&gt;Hope you like it, If you have any ideas/comments/suggestions, I&amp;#39;d really love to hear them.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?auto=webp&amp;s=96c78f7af0d55bf809aea5e3aa74cbbcef56dd55', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb0b9f0fe0880768f4673eb0b0de4950a98b6b17', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c358b62ea4934234264e445b13378fcfa493bf6d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/DAiW62Z9rkz0Sd_IS_8WYDg8Ucw2SuYmF4GwXYXqlgw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db1864c49e213ead3ece1212938c1640ab7b1535', 'width': 320, 'height': 320}], 'variants': {}, 'id': '3vbWKckhU81Qq--awNv_DObYSXMuYxFCxRDsUJAgfB0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cg27ff,True,,init__27,,0,True,all_ads,False,[],False,,/r/pytorch/comments/cg27ff/interview_with_worlds_first_triple_kaggle/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cg27ff/interview_with_worlds_first_triple_kaggle/,7135,1563733370.0,0,,False,,,,,,,,
833,,pytorch,,t2_41wv6t6w,False,,0,False,Anyone can suggest online resources for learning pytorch (for a beginner),[],r/pytorch,False,6,,0,,,False,t3_cfshnl,False,dark,0.78,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1563694648.0,text,6,,,text,self.pytorch,False,,,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cfshnl,True,,DM9667,,8,True,all_ads,False,[],False,,/r/pytorch/comments/cfshnl/anyone_can_suggest_online_resources_for_learning/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cfshnl/anyone_can_suggest_online_resources_for_learning/,7135,1563665848.0,0,,False,,,,,,,,
834,,pytorch,,t2_o7pqfa2,False,,0,False,Train ML Models on Free Cloud GPUs ⚡,[],r/pytorch,False,6,,0,78.0,,False,t3_ce35hw,False,dark,0.86,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://a.thumbs.redditmedia.com/oJUjNpvRL3ZeT9eGhuyLBnd0UYtgOOQ1HfyplQu8mf4.jpg,False,,[],{},link,,False,,1563340425.0,text,6,,,text,blog.paperspace.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/isPNVQKtHNIFysqcY-clA3fbKOoxKQr659oDxvd9GbI.jpg?auto=webp&amp;s=38574c091146676585f2484f2769393fad5ae211', 'width': 2000, 'height': 1125}, 'resolutions': [{'url': 'https://external-preview.redd.it/isPNVQKtHNIFysqcY-clA3fbKOoxKQr659oDxvd9GbI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04536eb058c587d0b50f1060db22242e584dbdd1', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/isPNVQKtHNIFysqcY-clA3fbKOoxKQr659oDxvd9GbI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f0a1ee52c8b8a774066b7ae45be4852d709933c', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/isPNVQKtHNIFysqcY-clA3fbKOoxKQr659oDxvd9GbI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8cceeb7b7298c7277a1715c185d954cbd3c4576d', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/isPNVQKtHNIFysqcY-clA3fbKOoxKQr659oDxvd9GbI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ab1ba3a7cda6433d6861cb7b05aaa8b721cd3629', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/isPNVQKtHNIFysqcY-clA3fbKOoxKQr659oDxvd9GbI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fd6bc0112ad39e997fcdd67453777bd958eca9bd', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/isPNVQKtHNIFysqcY-clA3fbKOoxKQr659oDxvd9GbI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3a6a6fe3eb8e839ad916ce2e40c5313c85fec0ff', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'hLzPiQHsDJDQFi8WgVCM0FustxO33l3QNEyxbADVc0s'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ce35hw,True,,coffeepants87,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ce35hw/train_ml_models_on_free_cloud_gpus/,all_ads,False,https://blog.paperspace.com/free-cloud-gpu?utm_source=reddit&amp;utm_medium=r-deeplearning&amp;utm_campaign=free-gpu-notebook-waitlist-launch,7135,1563311625.0,0,,False,https://blog.paperspace.com/free-cloud-gpu?utm_source=reddit&amp;utm_medium=r-deeplearning&amp;utm_campaign=free-gpu-notebook-waitlist-launch,,,,,,,
835,,pytorch,"Hey all, I'm working with the MNIST dataset which has a single input channel for grayscale. I'd like to use one of the models in torchvision which require 3 input channels for rgb but I cant seem to find a way to transform the entire dataset. Any clarification would be greatly appreciated!",t2_v0nnd,False,,0,False,Converting grayscale to rgb?,[],r/pytorch,False,6,,0,,,False,t3_cdp8mj,False,dark,0.76,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1563262470.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all, I&amp;#39;m working with the MNIST dataset which has a single input channel for grayscale. I&amp;#39;d like to use one of the models in torchvision which require 3 input channels for rgb but I cant seem to find a way to transform the entire dataset. Any clarification would be greatly appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cdp8mj,True,,___AJ___,,6,True,all_ads,False,[],False,,/r/pytorch/comments/cdp8mj/converting_grayscale_to_rgb/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cdp8mj/converting_grayscale_to_rgb/,7135,1563233670.0,0,,False,,,,,,,,
836,,pytorch,"I want to run Pytorch on a Raspberry Pi Zero (it sounds silly but the customer asked for it, so here I am), but the only precompiled packages are for ARMv7.

I am right now trying to compile from source, ON the Pi Zero, but as you can imagine it's a challenge. Has anyone done this or know whether there are precompiled binaries for this?",t2_jjy1c,False,,0,False,PyTorch on ARMv6?,[],r/pytorch,False,6,,0,,,False,t3_cd4lpw,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1563148772.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to run Pytorch on a Raspberry Pi Zero (it sounds silly but the customer asked for it, so here I am), but the only precompiled packages are for ARMv7.&lt;/p&gt;

&lt;p&gt;I am right now trying to compile from source, ON the Pi Zero, but as you can imagine it&amp;#39;s a challenge. Has anyone done this or know whether there are precompiled binaries for this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cd4lpw,True,,rumborak,,5,True,all_ads,False,[],False,,/r/pytorch/comments/cd4lpw/pytorch_on_armv6/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cd4lpw/pytorch_on_armv6/,7135,1563119972.0,0,,False,,,,,,,,
837,,pytorch,"I've been using Tensorflow mostly because of popularity and extensive documentation. I just went through pytorch and was impressed with ease of use of PyTorch.

I know, there are several getting started tutorials, I wrote this one with programmers in mind without ML theory or mathematics behind it.

[https://medium.com/@srohit0/pytorch-in-2-minutes-9e18875990fd?source=friends\_link&amp;sk=012cbb2381f7271ff72c7763ef1002b7](https://medium.com/@srohit0/pytorch-in-2-minutes-9e18875990fd?source=friends_link&amp;sk=012cbb2381f7271ff72c7763ef1002b7)

Comments are welcome ! 🙏",t2_101ge5y9,False,,0,False,ABCs of PyTorch in 4 minutes,[],r/pytorch,False,6,,0,,,False,t3_cctj77,False,dark,0.89,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},self,,True,,1563075845.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been using Tensorflow mostly because of popularity and extensive documentation. I just went through pytorch and was impressed with ease of use of PyTorch.&lt;/p&gt;

&lt;p&gt;I know, there are several getting started tutorials, I wrote this one with programmers in mind without ML theory or mathematics behind it.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://medium.com/@srohit0/pytorch-in-2-minutes-9e18875990fd?source=friends_link&amp;amp;sk=012cbb2381f7271ff72c7763ef1002b7""&gt;https://medium.com/@srohit0/pytorch-in-2-minutes-9e18875990fd?source=friends_link&amp;amp;sk=012cbb2381f7271ff72c7763ef1002b7&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Comments are welcome ! 🙏&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/CYOoLzmFQdos9EhO6-lesaEDqUOvXvwmKBluAN-PPk0.jpg?auto=webp&amp;s=c9b0b5e0f5d7c0f16873eb4587625e95c5f04cc0', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/CYOoLzmFQdos9EhO6-lesaEDqUOvXvwmKBluAN-PPk0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0dcb595d9848bd1462bd40862109271292246fef', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/CYOoLzmFQdos9EhO6-lesaEDqUOvXvwmKBluAN-PPk0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d44f29b382b61debf8b363e861824a04949b6a85', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/CYOoLzmFQdos9EhO6-lesaEDqUOvXvwmKBluAN-PPk0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f654ab8f0193232f09561f7184085560b4970cdd', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/CYOoLzmFQdos9EhO6-lesaEDqUOvXvwmKBluAN-PPk0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=acff8f73b5c9a57946c32c7c7748586d485f8d2e', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/CYOoLzmFQdos9EhO6-lesaEDqUOvXvwmKBluAN-PPk0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7b3b2a6bb4aae4be11dbe0450de553d9bdcd226e', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/CYOoLzmFQdos9EhO6-lesaEDqUOvXvwmKBluAN-PPk0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6fa61e93efb615c89f028bdd1659da6c2a8c6b7f', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'kSQ4WOUDfxF-Ri8--yEPcuYTfXZjXCMPcChmuN1qi5k'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cctj77,True,,srohit0,,2,True,all_ads,False,[],False,,/r/pytorch/comments/cctj77/abcs_of_pytorch_in_4_minutes/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cctj77/abcs_of_pytorch_in_4_minutes/,7135,1563047045.0,0,,False,,,,,,,,
838,,pytorch,"I'm trying to use a pretrained faster rcnn *torchvision.models.detection.fasterrcnn\_resnet50\_fpn().* For object detection project. I have created a **CustomDataset(Dataset)** class to handle the custom dataset.

While performing *model.train(input, target)*  I am getting this weird error **\_thnn\_upsample\_bilinear2d\_forward not supported on CUDAType for Byte** of which I am not aware neither I am getting any help online. 

&amp;#x200B;

If anyone can help me out it will be really appreciable.

Thanks",t2_227v2s6x,False,,0,False,Need help regarding Transfer Learning a Faster RCNN ResNet50FPN in PyTorch,[],r/pytorch,False,6,,0,,,False,t3_cc90sq,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1562955330.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to use a pretrained faster rcnn &lt;em&gt;torchvision.models.detection.fasterrcnn_resnet50_fpn().&lt;/em&gt; For object detection project. I have created a &lt;strong&gt;CustomDataset(Dataset)&lt;/strong&gt; class to handle the custom dataset.&lt;/p&gt;

&lt;p&gt;While performing &lt;em&gt;model.train(input, target)&lt;/em&gt;  I am getting this weird error &lt;strong&gt;_thnn_upsample_bilinear2d_forward not supported on CUDAType for Byte&lt;/strong&gt; of which I am not aware neither I am getting any help online. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;If anyone can help me out it will be really appreciable.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cc90sq,True,,r42in,,1,True,all_ads,False,[],False,,/r/pytorch/comments/cc90sq/need_help_regarding_transfer_learning_a_faster/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cc90sq/need_help_regarding_transfer_learning_a_faster/,7135,1562926530.0,0,,False,,,,,,,,
839,,pytorch,"I want to use pytorch to match feature descriptors efficiently.

The main time consuming problem is: Suppose I have two matrices containing m and n vectors. How do I efficiently compute distances between them?

What I currently do is:

`m, n, d = 10000, 5000, 2`

`x = torch.Tensor(size=(m, d))`

`y = torch.Tensor(size=(n, d))`

`dist = torch.sum(torch.stack([x-y_i for y_i in y]), (2,))`

Rest of the code:

`matches = np.argsort(dist)[:,:2]`

`good_matches = []`

`for i, (a, b) in enumerate(matches):`

`if dist[i,b] &gt; ratio*dist[i, a]:`

`good_matches.append([i,a])`

`return good_matches`

&amp;#x200B;

Any suggestions? I was looking for an operation that does the fourth line of code as a single operation i.e. subtract two matrices and return a tensor.",t2_4502thj6,False,,0,False,How to efficiently find distances between vectors?,[],r/pytorch,False,6,,0,,,False,t3_ccamip,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1562965670.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to use pytorch to match feature descriptors efficiently.&lt;/p&gt;

&lt;p&gt;The main time consuming problem is: Suppose I have two matrices containing m and n vectors. How do I efficiently compute distances between them?&lt;/p&gt;

&lt;p&gt;What I currently do is:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;m, n, d = 10000, 5000, 2&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x = torch.Tensor(size=(m, d))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;y = torch.Tensor(size=(n, d))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;dist = torch.sum(torch.stack([x-y_i for y_i in y]), (2,))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Rest of the code:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;matches = np.argsort(dist)[:,:2]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;good_matches = []&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for i, (a, b) in enumerate(matches):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;if dist[i,b] &amp;gt; ratio*dist[i, a]:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;good_matches.append([i,a])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;return good_matches&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Any suggestions? I was looking for an operation that does the fourth line of code as a single operation i.e. subtract two matrices and return a tensor.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ccamip,True,,comp_vision_,,2,True,all_ads,False,[],False,,/r/pytorch/comments/ccamip/how_to_efficiently_find_distances_between_vectors/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ccamip/how_to_efficiently_find_distances_between_vectors/,7135,1562936870.0,0,,False,,,,,,,,
840,,pytorch,"For my current research project the current bottleneck is computing Jacobians in Autograd (the library from Harvard). 

Having heard good things about Pytorch's C++ API, I was thinking I could port my Python Autograd code to compiled Pytorch with a light wrapper. Does this sound like a good idea?",t2_5u5tc,False,,0,False,Will C++ Pytorch be faster than Autograd?,[],r/pytorch,False,6,,0,,,False,t3_cbwwv6,False,dark,0.87,,public,10,0,{},,,False,[],,False,False,,{},,False,10,,False,self,False,,[],{},,,True,,1562887619.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For my current research project the current bottleneck is computing Jacobians in Autograd (the library from Harvard). &lt;/p&gt;

&lt;p&gt;Having heard good things about Pytorch&amp;#39;s C++ API, I was thinking I could port my Python Autograd code to compiled Pytorch with a light wrapper. Does this sound like a good idea?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cbwwv6,True,,internet_ham,,0,True,all_ads,False,[],False,,/r/pytorch/comments/cbwwv6/will_c_pytorch_be_faster_than_autograd/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cbwwv6/will_c_pytorch_be_faster_than_autograd/,7135,1562858819.0,0,,False,,,,,,,,
841,,pytorch,"I know it is possible to get most of the deeplearning networks (VGG16, RESNET18 etc.) easily on pytorch (both pre-trained and plain), but is it possible to use these standard implementations and pre-train them easily with your own unlabeled data (like an autoencoder)?",t2_sds7fgv,False,,0,False,manual pre-training of deeplearning networks included in pytorch?,[],r/pytorch,False,6,,0,,,False,t3_cbulyz,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1562873261.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know it is possible to get most of the deeplearning networks (VGG16, RESNET18 etc.) easily on pytorch (both pre-trained and plain), but is it possible to use these standard implementations and pre-train them easily with your own unlabeled data (like an autoencoder)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cbulyz,True,,Lanze-Spezzate,,7,True,all_ads,False,[],False,,/r/pytorch/comments/cbulyz/manual_pretraining_of_deeplearning_networks/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cbulyz/manual_pretraining_of_deeplearning_networks/,7135,1562844461.0,0,,False,,,,,,,,
842,,pytorch,"So I am trying to build a real-time object tracking algorithm and I am still quite new to ML.

My current pipeline is for each frame:

1. Use OpenCV to obtain webcam frame
2. frame -&gt; pytorch model -&gt; output
3. applying object track algorithm

But I think doing this would be faster, for each frame:

1. Use OpenCV to obtain webcam frame
2. stack 30 frames
3. frame -&gt; pytorch model -&gt; output
4. apply object track algorithm

However I am not sure how to batch 30 frames together? Can anyone give me some guidance?

&amp;#x200B;

`## Original Version - One Image Array at a Time ##`

`""""""`

`H_org, W_org, _ = frame.shape`

`img, *_ = letterbox(frame, new_shape=416)`

`img, *_ = letterbox(test_img, new_shape=416) # convert img from original Height to target Height 416`

`H_tran, W_tran, _ = img.shape`

&amp;#x200B;

`# Normalise`

`img = img[:, :, ::-1] # BGR to RGB`

`img = img.transpose(2, 0, 1)`

`img = np.ascontiguousarray(img, dtype=np.float32)  # uint8 to float32`

`img /= 255.0  # 0 - 255 to 0.0 - 1.0`

&amp;#x200B;

`""""""`

&amp;#x200B;

`## Batch - Thirty Images Array at a Time ##`

`H_org, W_org, _ = frame.shape`

`batch_size=30`

&amp;#x200B;

`img, *_ = letterbox(test_img, new_shape=416)`

`H_tran, W_tran, _ = img.shape`

`print(img.shape)`

&amp;#x200B;

`input_array = np.zeros((batch_size, H_tran, W_tran, 3))`

`for i in range(batch_size):`

`input_array[i, :, : ,:] = img`

&amp;#x200B;

`# Normalize RGB`

`img = input_array`

`img = img[:, :, :, ::-1] # BGR to RGB`

`print(img.shape)`

`img = img.transpose(0, 3, 1, 2)`

`print(img.shape)`

`img = np.ascontiguousarray(img, dtype=np.float32)  # uint8 to float32`

`img /= 255.0  # 0 - 255 to 0.0 - 1.0`

&amp;#x200B;

`## Feed Into Model ##`

&amp;#x200B;

`conf_thres = 0.4`

`nms_thres = 0.4`

&amp;#x200B;

`device = torch_utils.select_device()`

`torch.backends.cudnn.benchmark = False  # set False for reproducible results`

&amp;#x200B;

`model = Darknet(""../prod_model/yolov3-tiny.cfg"", 416)`

&amp;#x200B;

`_ = load_darknet_weights(model, ""../prod_model/yolov3-tiny_final-TL.weights"")`

&amp;#x200B;

`# Fuse Conv2d + BatchNorm2d layers`

`model.fuse()`

&amp;#x200B;

`# Eval mode`

[`model.to`](https://model.to)`(device).eval()`

&amp;#x200B;

`# Get classes and colors`

`classes = load_classes(parse_data_cfg('data/coco.data')['names'])`

`colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(classes))]`

&amp;#x200B;

`# Get detections`

`img = torch.from_numpy(img).unsqueeze(0).to(device)`

`pred, _ = model(img)`

`det = non_max_suppression(pred, conf_thres, nms_thres)[0]`

&amp;#x200B;

`H_ratio, W_ratio = H_org / H_tran, W_org / W_tran`

&amp;#x200B;

`if det is not None and len(det) &gt; 0:`

`# Rescale boxes from 416 to true image size`

`det[:, 0] = det[:, 0] * H_ratio`

`det[:, 1] = det[:, 1] * W_ratio`

`det[:, 2] = det[:, 2] * H_ratio`

`det[:, 3] = det[:, 3] * W_ratio`

&amp;#x200B;

`print(det.detach().numpy())`",t2_wc9v2,False,,0,False,Real-Time Object Detection/Tracking - How to batch and process 30 frames in one go from webcam?,[],r/pytorch,False,6,,0,,,False,t3_cbehu7,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1562779287.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I am trying to build a real-time object tracking algorithm and I am still quite new to ML.&lt;/p&gt;

&lt;p&gt;My current pipeline is for each frame:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Use OpenCV to obtain webcam frame&lt;/li&gt;
&lt;li&gt;frame -&amp;gt; pytorch model -&amp;gt; output&lt;/li&gt;
&lt;li&gt;applying object track algorithm&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But I think doing this would be faster, for each frame:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Use OpenCV to obtain webcam frame&lt;/li&gt;
&lt;li&gt;stack 30 frames&lt;/li&gt;
&lt;li&gt;frame -&amp;gt; pytorch model -&amp;gt; output&lt;/li&gt;
&lt;li&gt;apply object track algorithm&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;However I am not sure how to batch 30 frames together? Can anyone give me some guidance?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;## Original Version - One Image Array at a Time ##&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;H_org, W_org, _ = frame.shape&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;img, *_ = letterbox(frame, new_shape=416)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;img, *_ = letterbox(test_img, new_shape=416) # convert img from original Height to target Height 416&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;H_tran, W_tran, _ = img.shape&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Normalise&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;img = img[:, :, ::-1] # BGR to RGB&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;img = img.transpose(2, 0, 1)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;img = np.ascontiguousarray(img, dtype=np.float32)  # uint8 to float32&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;img /= 255.0  # 0 - 255 to 0.0 - 1.0&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;## Batch - Thirty Images Array at a Time ##&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;H_org, W_org, _ = frame.shape&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;batch_size=30&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;img, *_ = letterbox(test_img, new_shape=416)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;H_tran, W_tran, _ = img.shape&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(img.shape)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;input_array = np.zeros((batch_size, H_tran, W_tran, 3))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for i in range(batch_size):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;input_array[i, :, : ,:] = img&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Normalize RGB&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;img = input_array&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;img = img[:, :, :, ::-1] # BGR to RGB&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(img.shape)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;img = img.transpose(0, 3, 1, 2)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(img.shape)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;img = np.ascontiguousarray(img, dtype=np.float32)  # uint8 to float32&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;img /= 255.0  # 0 - 255 to 0.0 - 1.0&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;## Feed Into Model ##&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;conf_thres = 0.4&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nms_thres = 0.4&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;device = torch_utils.select_device()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;torch.backends.cudnn.benchmark = False  # set False for reproducible results&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model = Darknet(&amp;quot;../prod_model/yolov3-tiny.cfg&amp;quot;, 416)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;_ = load_darknet_weights(model, &amp;quot;../prod_model/yolov3-tiny_final-TL.weights&amp;quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Fuse Conv2d + BatchNorm2d layers&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model.fuse()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Eval mode&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://model.to""&gt;&lt;code&gt;model.to&lt;/code&gt;&lt;/a&gt;&lt;code&gt;(device).eval()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Get classes and colors&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;classes = load_classes(parse_data_cfg(&amp;#39;data/coco.data&amp;#39;)[&amp;#39;names&amp;#39;])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(classes))]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Get detections&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;img = torch.from_numpy(img).unsqueeze(0).to(device)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pred, _ = model(img)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;det = non_max_suppression(pred, conf_thres, nms_thres)[0]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;H_ratio, W_ratio = H_org / H_tran, W_org / W_tran&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;if det is not None and len(det) &amp;gt; 0:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Rescale boxes from 416 to true image size&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;det[:, 0] = det[:, 0] * H_ratio&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;det[:, 1] = det[:, 1] * W_ratio&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;det[:, 2] = det[:, 2] * H_ratio&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;det[:, 3] = det[:, 3] * W_ratio&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(det.detach().numpy())&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,cbehu7,True,,keeperclone,,0,True,all_ads,False,[],False,,/r/pytorch/comments/cbehu7/realtime_object_detectiontracking_how_to_batch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/cbehu7/realtime_object_detectiontracking_how_to_batch/,7135,1562750487.0,0,,False,,,,,,,,
843,,pytorch,"Register for a free webinar on how to build your own AutoML computer vision pipeline: [https://info.cnvrg.io/automl-computer-vision](https://info.cnvrg.io/automl-computer-vision) 

The step-by-step tutorial will introduce how to use Keras, PyTorch, Tensorflow and other tools to build your model so you can build one on your own.  

Key webinar takeaways: 

* How to identify production value of computer vision projects
* How to manage datasets for computer vision and deep learning applications
* How to use AutoML for computer vision
* How to save time and achieve top performing results with transfer learning and reusable machine learning components
* How to build a proper MLOps setup so you can focus more on research and less on IT
* Monitor and track training with specific parameters and metrics
* Integrate computer vision into your application by deploying it as a REST endpoint

Only 50 spots left so register soon:  [https://info.cnvrg.io/automl-computer-vision](https://info.cnvrg.io/automl-computer-vision)",t2_2kul58k1,False,,0,False,Learn to build your own AutoML computer vision pipeline,[],r/pytorch,False,6,,0,,,False,t3_c8ozsy,False,dark,0.43,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1562194219.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Register for a free webinar on how to build your own AutoML computer vision pipeline: &lt;a href=""https://info.cnvrg.io/automl-computer-vision""&gt;https://info.cnvrg.io/automl-computer-vision&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;The step-by-step tutorial will introduce how to use Keras, PyTorch, Tensorflow and other tools to build your model so you can build one on your own.  &lt;/p&gt;

&lt;p&gt;Key webinar takeaways: &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How to identify production value of computer vision projects&lt;/li&gt;
&lt;li&gt;How to manage datasets for computer vision and deep learning applications&lt;/li&gt;
&lt;li&gt;How to use AutoML for computer vision&lt;/li&gt;
&lt;li&gt;How to save time and achieve top performing results with transfer learning and reusable machine learning components&lt;/li&gt;
&lt;li&gt;How to build a proper MLOps setup so you can focus more on research and less on IT&lt;/li&gt;
&lt;li&gt;Monitor and track training with specific parameters and metrics&lt;/li&gt;
&lt;li&gt;Integrate computer vision into your application by deploying it as a REST endpoint&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Only 50 spots left so register soon:  &lt;a href=""https://info.cnvrg.io/automl-computer-vision""&gt;https://info.cnvrg.io/automl-computer-vision&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BGRARcCuDSBZ_gZId5M634q3v8bsSQPw7amdBRN3IY8.jpg?auto=webp&amp;s=c5af1ad5a0bfc8b9286c51f2fb028440d7d145c0', 'width': 770, 'height': 430}, 'resolutions': [{'url': 'https://external-preview.redd.it/BGRARcCuDSBZ_gZId5M634q3v8bsSQPw7amdBRN3IY8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f53c320bdb50ca57359eec7940678b21a03299e', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/BGRARcCuDSBZ_gZId5M634q3v8bsSQPw7amdBRN3IY8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb0e5969a628bd23749acf96a63494405cfa33b5', 'width': 216, 'height': 120}, {'url': 'https://external-preview.redd.it/BGRARcCuDSBZ_gZId5M634q3v8bsSQPw7amdBRN3IY8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c1dc3090cda2926702b59d9b521eeb91ec2b0a15', 'width': 320, 'height': 178}, {'url': 'https://external-preview.redd.it/BGRARcCuDSBZ_gZId5M634q3v8bsSQPw7amdBRN3IY8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6599f2cc855872f35f8fbbe85e46ee4af8bdd8e', 'width': 640, 'height': 357}], 'variants': {}, 'id': 'zWbdc2yS1ptliABn3xbxykaFzWfZN3lxjMi6oOipGAA'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c8ozsy,True,,Mayalittlepony,,0,True,all_ads,False,[],False,,/r/pytorch/comments/c8ozsy/learn_to_build_your_own_automl_computer_vision/,all_ads,False,https://www.reddit.com/r/pytorch/comments/c8ozsy/learn_to_build_your_own_automl_computer_vision/,7135,1562165419.0,0,,False,,,,,,,,
844,,pytorch,"On Twitter I found rumors about an elusive PyTorch laptop sticker.
Is there any chance to know more about it here?
I'd love to get one and I am pretty sure I am not the only one interested 😁",t2_tl401sk,False,,0,False,Pytorch laptop sticker,[],r/pytorch,False,6,,0,,,False,t3_c8f4fq,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1562130696.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;On Twitter I found rumors about an elusive PyTorch laptop sticker.
Is there any chance to know more about it here?
I&amp;#39;d love to get one and I am pretty sure I am not the only one interested 😁&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c8f4fq,True,,acobobby,,6,True,all_ads,False,[],False,,/r/pytorch/comments/c8f4fq/pytorch_laptop_sticker/,all_ads,False,https://www.reddit.com/r/pytorch/comments/c8f4fq/pytorch_laptop_sticker/,7135,1562101896.0,0,,False,,,,,,,,
845,,pytorch,I don't wanna use onnx and tensor flow.,t2_14vny3,False,,0,False,How do I use pytorch with flutter/react-native?,[],r/pytorch,False,6,,0,,,False,t3_c87u9l,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1562093796.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I don&amp;#39;t wanna use onnx and tensor flow.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c87u9l,True,,b14cksh4d0w369,,4,True,all_ads,False,[],False,,/r/pytorch/comments/c87u9l/how_do_i_use_pytorch_with_flutterreactnative/,all_ads,False,https://www.reddit.com/r/pytorch/comments/c87u9l/how_do_i_use_pytorch_with_flutterreactnative/,7135,1562064996.0,0,,False,,,,,,,,
846,,pytorch,"Hi guys,

I’m new to PyTorch and I got a question about PyTorch model recently. How can I get the static computational graph of a PyTorch model? Like the prototxt file in Caffe.

I know that the .pth model contains the weight of each layer in a dictionary. But how can I know the connection of layers? (Like the prototxt in Caffe, it’s a serialized graph)

Many thanks!",t2_21mwpyk4,False,,0,False,Extract static graph from PyTorch model,[],r/pytorch,False,6,,0,,,False,t3_c86c6a,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1562081836.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys,&lt;/p&gt;

&lt;p&gt;I’m new to PyTorch and I got a question about PyTorch model recently. How can I get the static computational graph of a PyTorch model? Like the prototxt file in Caffe.&lt;/p&gt;

&lt;p&gt;I know that the .pth model contains the weight of each layer in a dictionary. But how can I know the connection of layers? (Like the prototxt in Caffe, it’s a serialized graph)&lt;/p&gt;

&lt;p&gt;Many thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c86c6a,True,,lucius323,,4,True,all_ads,False,[],False,,/r/pytorch/comments/c86c6a/extract_static_graph_from_pytorch_model/,all_ads,False,https://www.reddit.com/r/pytorch/comments/c86c6a/extract_static_graph_from_pytorch_model/,7135,1562053036.0,0,,False,,,,,,,,
847,,pytorch,"# High quality, fast, modular reference implementation of SSD in PyTorch 1.0

**A maskrcnnbenchmark-like SSD implementation, support customizing every component! And EfficientNet-B3 backbone is support now!**


## Highlights

- **PyTorch 1.0**: Support PyTorch 1.0 or higher.
- **Multi-GPU training and inference**: We use `DistributedDataParallel`, you can train or test with arbitrary GPU(s), the training schema will change accordingly.
- **Modular**: And you own modules without pain. We abstract `backbone`,`Detector`, `BoxHead`, `BoxPredictor`, etc. You can replace every component with your own code without change the code base. For example, You can add [EfficientNet](https://github.com/lukemelas/EfficientNet-PyTorch) as backbone, just add `efficient_net.py` (ALREADY ADDED) and register it, specific it in the config file, It's done!
- **CPU support for inference**: runs on CPU in inference time.
- **Smooth and enjoyable training procedure**: we save the state of model, optimizer, scheduler, training iter, you can stop your training and resume training exactly from the save point without change your training `CMD`.
- **Batched inference**: can perform inference using multiple images per batch per GPU.
- **Evaluating during training**: eval you model every `eval_step` to check performance improving or not.
- **Metrics Visualization**: visualize metrics details in tensorboard, like AP, APl, APm and APs for COCO dataset or mAP and 20 categories' AP for VOC dataset.
- **Auto download**: load pre-trained weights from URL and cache it.

## Model Zoo:

### COCO:

| Backbone       | Input Size  |          box AP                  | Model Size |  Download |
| :------------: | :----------:|   :--------------------------:   | :--------: | :-------: |
|  VGG16         |     300     |          25.2                    |  262MB     | [model](https://github.com/lufficc/SSD/releases/download/1.2/vgg_ssd300_coco_trainval35k.pth)   |
|  VGG16         |     512     |          xx.x                    |  xxx.xMB   |           |
|  Mobilenet V2  |     320     |          xx.x                    |  xxx.xMB   |           |

### PASCAL VOC:

| Backbone         | Input Size  |          mAP                     | Model Size | Download  |
| :--------------: | :----------:|   :--------------------------:   | :--------: | :-------: |
|  VGG16           |     300     |          77.6                    |   201MB    | [model](https://github.com/lufficc/SSD/releases/download/1.2/vgg_ssd300_voc0712.pth)  |
|  VGG16           |     512     |          xx.x                    |   xxx.xMB  |            |
|  Mobilenet V2    |     320     |          68.8                    |   25.5MB   | [model](https://github.com/lufficc/SSD/releases/download/1.2/mobilenet_v2_ssd320_voc0712.pth) |
|  EfficientNet-B3 |     300     |          73.9                    |   97.1MB   | [model](https://github.com/lufficc/SSD/releases/download/1.2/efficient_net_b3_ssd300_voc0712.pth) |",t2_11mpcg,False,,0,False,"A maskrcnnbenchmark-like implementation of SSD, and EfficientNet-B3 backbone is support now!",[],r/pytorch,False,6,,0,,,False,t3_c7ws4n,False,dark,0.79,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},self,,True,,1562032215.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h1&gt;High quality, fast, modular reference implementation of SSD in PyTorch 1.0&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;A maskrcnnbenchmark-like SSD implementation, support customizing every component! And EfficientNet-B3 backbone is support now!&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;Highlights&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PyTorch 1.0&lt;/strong&gt;: Support PyTorch 1.0 or higher.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-GPU training and inference&lt;/strong&gt;: We use &lt;code&gt;DistributedDataParallel&lt;/code&gt;, you can train or test with arbitrary GPU(s), the training schema will change accordingly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modular&lt;/strong&gt;: And you own modules without pain. We abstract &lt;code&gt;backbone&lt;/code&gt;,&lt;code&gt;Detector&lt;/code&gt;, &lt;code&gt;BoxHead&lt;/code&gt;, &lt;code&gt;BoxPredictor&lt;/code&gt;, etc. You can replace every component with your own code without change the code base. For example, You can add &lt;a href=""https://github.com/lukemelas/EfficientNet-PyTorch""&gt;EfficientNet&lt;/a&gt; as backbone, just add &lt;code&gt;efficient_net.py&lt;/code&gt; (ALREADY ADDED) and register it, specific it in the config file, It&amp;#39;s done!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU support for inference&lt;/strong&gt;: runs on CPU in inference time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smooth and enjoyable training procedure&lt;/strong&gt;: we save the state of model, optimizer, scheduler, training iter, you can stop your training and resume training exactly from the save point without change your training &lt;code&gt;CMD&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Batched inference&lt;/strong&gt;: can perform inference using multiple images per batch per GPU.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evaluating during training&lt;/strong&gt;: eval you model every &lt;code&gt;eval_step&lt;/code&gt; to check performance improving or not.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Metrics Visualization&lt;/strong&gt;: visualize metrics details in tensorboard, like AP, APl, APm and APs for COCO dataset or mAP and 20 categories&amp;#39; AP for VOC dataset.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Auto download&lt;/strong&gt;: load pre-trained weights from URL and cache it.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Model Zoo:&lt;/h2&gt;

&lt;h3&gt;COCO:&lt;/h3&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""center""&gt;Backbone&lt;/th&gt;
&lt;th align=""center""&gt;Input Size&lt;/th&gt;
&lt;th align=""center""&gt;box AP&lt;/th&gt;
&lt;th align=""center""&gt;Model Size&lt;/th&gt;
&lt;th align=""center""&gt;Download&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""center""&gt;VGG16&lt;/td&gt;
&lt;td align=""center""&gt;300&lt;/td&gt;
&lt;td align=""center""&gt;25.2&lt;/td&gt;
&lt;td align=""center""&gt;262MB&lt;/td&gt;
&lt;td align=""center""&gt;&lt;a href=""https://github.com/lufficc/SSD/releases/download/1.2/vgg_ssd300_coco_trainval35k.pth""&gt;model&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""center""&gt;VGG16&lt;/td&gt;
&lt;td align=""center""&gt;512&lt;/td&gt;
&lt;td align=""center""&gt;xx.x&lt;/td&gt;
&lt;td align=""center""&gt;xxx.xMB&lt;/td&gt;
&lt;td align=""center""&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""center""&gt;Mobilenet V2&lt;/td&gt;
&lt;td align=""center""&gt;320&lt;/td&gt;
&lt;td align=""center""&gt;xx.x&lt;/td&gt;
&lt;td align=""center""&gt;xxx.xMB&lt;/td&gt;
&lt;td align=""center""&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h3&gt;PASCAL VOC:&lt;/h3&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""center""&gt;Backbone&lt;/th&gt;
&lt;th align=""center""&gt;Input Size&lt;/th&gt;
&lt;th align=""center""&gt;mAP&lt;/th&gt;
&lt;th align=""center""&gt;Model Size&lt;/th&gt;
&lt;th align=""center""&gt;Download&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""center""&gt;VGG16&lt;/td&gt;
&lt;td align=""center""&gt;300&lt;/td&gt;
&lt;td align=""center""&gt;77.6&lt;/td&gt;
&lt;td align=""center""&gt;201MB&lt;/td&gt;
&lt;td align=""center""&gt;&lt;a href=""https://github.com/lufficc/SSD/releases/download/1.2/vgg_ssd300_voc0712.pth""&gt;model&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""center""&gt;VGG16&lt;/td&gt;
&lt;td align=""center""&gt;512&lt;/td&gt;
&lt;td align=""center""&gt;xx.x&lt;/td&gt;
&lt;td align=""center""&gt;xxx.xMB&lt;/td&gt;
&lt;td align=""center""&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""center""&gt;Mobilenet V2&lt;/td&gt;
&lt;td align=""center""&gt;320&lt;/td&gt;
&lt;td align=""center""&gt;68.8&lt;/td&gt;
&lt;td align=""center""&gt;25.5MB&lt;/td&gt;
&lt;td align=""center""&gt;&lt;a href=""https://github.com/lufficc/SSD/releases/download/1.2/mobilenet_v2_ssd320_voc0712.pth""&gt;model&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""center""&gt;EfficientNet-B3&lt;/td&gt;
&lt;td align=""center""&gt;300&lt;/td&gt;
&lt;td align=""center""&gt;73.9&lt;/td&gt;
&lt;td align=""center""&gt;97.1MB&lt;/td&gt;
&lt;td align=""center""&gt;&lt;a href=""https://github.com/lufficc/SSD/releases/download/1.2/efficient_net_b3_ssd300_voc0712.pth""&gt;model&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/RRRI_6_m4O6utGr9zdiJpoe95w3i3_GzdmG671ytNk4.jpg?auto=webp&amp;s=6a2473d65962a78326c20a5751a2eaee192bcd6b', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/RRRI_6_m4O6utGr9zdiJpoe95w3i3_GzdmG671ytNk4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=86896f75a5107ab55be4f9d909bf5677b4c22497', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/RRRI_6_m4O6utGr9zdiJpoe95w3i3_GzdmG671ytNk4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d9c5a98ab9c6365979e3401fa8323fc6f39995e', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/RRRI_6_m4O6utGr9zdiJpoe95w3i3_GzdmG671ytNk4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7803b0439d18ac3f4597d303c4a4c470f58ccf8b', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'h1h4mCXJ4UVNd9zvydEY1c611Xu9OMd2rNFeBRCjdFg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c7ws4n,True,,lufficc,,6,True,all_ads,False,[],False,,/r/pytorch/comments/c7ws4n/a_maskrcnnbenchmarklike_implementation_of_ssd_and/,all_ads,False,https://www.reddit.com/r/pytorch/comments/c7ws4n/a_maskrcnnbenchmarklike_implementation_of_ssd_and/,7135,1562003415.0,0,,False,,,,,,,,
848,,pytorch,"Hello !

&amp;#x200B;

After succesfully implementing a LSTM ""from scratch"" based on linear layers, I decided to start using the existing LSTM class to make things easier and gain in performance.

But somehow when I try it, it only returns tensors full of zeros.

Here is the model :

&amp;#x200B;

class pytorchLSTM(nn.Module):

def \_\_init\_\_(self,input\_size,hidden\_size):

super().\_\_init\_\_()

self.input\_size = input\_size

self.hidden\_size = hidden\_size

self.lstm = nn.LSTM(input\_size, hidden\_size)

self.softmax = nn.LogSoftmax(dim = 1)

def forward(self, input):

out, hidden = self.lstm(input)

out = self.softmax(out)

return out, hidden

&amp;#x200B;

the input is a (1,1,60) tensor representing a one-hot encoded letter :

&amp;#x200B;

tensor(\[\[\[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 1., 0.\]\]\])

&amp;#x200B;

and the models returns, invariably (I tried modifying the values inside of the input, but the result is always the same) :

&amp;#x200B;

tensor(\[\[\[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,

0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,

0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,

0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,

0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,

0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.\]\]\],

grad\_fn=&lt;LogSoftmaxBackward&gt;)

&amp;#x200B;

Any idea where my mistake is and what I understood wrong about the LSTM class ?

&amp;#x200B;

EDIT :

&amp;#x200B;

Solved it. I forgot the meaning of the dim argument of the softmax class. One has to sum in the direction of the output dimension (in this case 2, not one).",t2_3ovb406j,False,,0,False,LSTM returning only zeros,[],r/pytorch,False,6,,0,,,False,t3_c7qnpd,False,dark,0.76,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1561986965.0,,[],{},,,True,,1562003625.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello !&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;After succesfully implementing a LSTM &amp;quot;from scratch&amp;quot; based on linear layers, I decided to start using the existing LSTM class to make things easier and gain in performance.&lt;/p&gt;

&lt;p&gt;But somehow when I try it, it only returns tensors full of zeros.&lt;/p&gt;

&lt;p&gt;Here is the model :&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;class pytorchLSTM(nn.Module):&lt;/p&gt;

&lt;p&gt;def __init__(self,input_size,hidden_size):&lt;/p&gt;

&lt;p&gt;super().__init__()&lt;/p&gt;

&lt;p&gt;self.input_size = input_size&lt;/p&gt;

&lt;p&gt;self.hidden_size = hidden_size&lt;/p&gt;

&lt;p&gt;self.lstm = nn.LSTM(input_size, hidden_size)&lt;/p&gt;

&lt;p&gt;self.softmax = nn.LogSoftmax(dim = 1)&lt;/p&gt;

&lt;p&gt;def forward(self, input):&lt;/p&gt;

&lt;p&gt;out, hidden = self.lstm(input)&lt;/p&gt;

&lt;p&gt;out = self.softmax(out)&lt;/p&gt;

&lt;p&gt;return out, hidden&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;the input is a (1,1,60) tensor representing a one-hot encoded letter :&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 1., 0.]]])&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;and the models returns, invariably (I tried modifying the values inside of the input, but the result is always the same) :&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,&lt;/p&gt;

&lt;p&gt;0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,&lt;/p&gt;

&lt;p&gt;0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,&lt;/p&gt;

&lt;p&gt;0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,&lt;/p&gt;

&lt;p&gt;0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,&lt;/p&gt;

&lt;p&gt;0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],&lt;/p&gt;

&lt;p&gt;grad_fn=&amp;lt;LogSoftmaxBackward&amp;gt;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Any idea where my mistake is and what I understood wrong about the LSTM class ?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;EDIT :&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Solved it. I forgot the meaning of the dim argument of the softmax class. One has to sum in the direction of the output dimension (in this case 2, not one).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c7qnpd,True,,JohnCawotte,,1,True,all_ads,False,[],False,,/r/pytorch/comments/c7qnpd/lstm_returning_only_zeros/,all_ads,False,https://www.reddit.com/r/pytorch/comments/c7qnpd/lstm_returning_only_zeros/,7135,1561974825.0,0,,False,,,,,,,,
849,,pytorch,"Hi !

&amp;#x200B;

I'm interested in designing a model for melody generation (or prediction) based on LSTM, but it occured to me that it might not be the best option to just consider the validity of the next note prediciton in the training but maybe also a bit further into the ""future"" let's say, 5 steps. I've been giving it a bit of thoughts but I don't really have an idea of how I would concretely do it. 

I could ""freeze"" the hidden state and cell state at prediction time and ask the network, based on these informations to do the best predictions of the 5 next notes it can. Is it a good approach ?

Are you aware of a class or method that already takes care of multi-step prediction with LSTMs in pytorch ?",t2_3ovb406j,False,,0,False,How to create a multi-step LSTM in pytorch ?,[],r/pytorch,False,6,,0,,,False,t3_c7qu93,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1562005043.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi !&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m interested in designing a model for melody generation (or prediction) based on LSTM, but it occured to me that it might not be the best option to just consider the validity of the next note prediciton in the training but maybe also a bit further into the &amp;quot;future&amp;quot; let&amp;#39;s say, 5 steps. I&amp;#39;ve been giving it a bit of thoughts but I don&amp;#39;t really have an idea of how I would concretely do it. &lt;/p&gt;

&lt;p&gt;I could &amp;quot;freeze&amp;quot; the hidden state and cell state at prediction time and ask the network, based on these informations to do the best predictions of the 5 next notes it can. Is it a good approach ?&lt;/p&gt;

&lt;p&gt;Are you aware of a class or method that already takes care of multi-step prediction with LSTMs in pytorch ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c7qu93,True,,JohnCawotte,,0,True,all_ads,False,[],False,,/r/pytorch/comments/c7qu93/how_to_create_a_multistep_lstm_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/c7qu93/how_to_create_a_multistep_lstm_in_pytorch/,7135,1561976243.0,0,,False,,,,,,,,
850,,pytorch,,t2_12i1bz,False,,0,False,"fast.ai part 2 (2019) is online: ""Deep Learning from the Foundations""",[],r/pytorch,False,6,,0,95.0,,False,t3_c6hqg5,False,dark,0.96,,public,24,0,{},140.0,,False,[],,False,False,,{},,False,24,,False,https://b.thumbs.redditmedia.com/NyUrImWQcVyudnXwqAN_UuFRy0fwE9LVVkUnkSeHmHg.jpg,False,,[],{},link,,False,,1561739714.0,text,6,,,text,fast.ai,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jIml_TsC82hsSHwxkRuGCh1MDb28qDy_cSJhgEZsNO8.jpg?auto=webp&amp;s=d7faece877de8c055a28dfdbf42a7d8544358e8a', 'width': 1736, 'height': 1189}, 'resolutions': [{'url': 'https://external-preview.redd.it/jIml_TsC82hsSHwxkRuGCh1MDb28qDy_cSJhgEZsNO8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=637c9e21b9ac398737a02f720e748fc01fee621e', 'width': 108, 'height': 73}, {'url': 'https://external-preview.redd.it/jIml_TsC82hsSHwxkRuGCh1MDb28qDy_cSJhgEZsNO8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a0c80fcc779c9dfbe35c773db5e3a471f36d41a3', 'width': 216, 'height': 147}, {'url': 'https://external-preview.redd.it/jIml_TsC82hsSHwxkRuGCh1MDb28qDy_cSJhgEZsNO8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=533afd3b93afda82b91f76ac7147a9a23ed020ab', 'width': 320, 'height': 219}, {'url': 'https://external-preview.redd.it/jIml_TsC82hsSHwxkRuGCh1MDb28qDy_cSJhgEZsNO8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c00718389eb59fdeab94a6b998231ccc70c17e0a', 'width': 640, 'height': 438}, {'url': 'https://external-preview.redd.it/jIml_TsC82hsSHwxkRuGCh1MDb28qDy_cSJhgEZsNO8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5b7cca99fab39175893463a99ccab2e22e31236', 'width': 960, 'height': 657}, {'url': 'https://external-preview.redd.it/jIml_TsC82hsSHwxkRuGCh1MDb28qDy_cSJhgEZsNO8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d4a5a850dc013db045fc1bdba192fb99896f88e', 'width': 1080, 'height': 739}], 'variants': {}, 'id': 'FflJMoWeE-YtudeJOZ8Cld2EKcfojXBasR3Dmcc5NGY'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c6hqg5,True,,101testing,,1,True,all_ads,False,[],False,,/r/pytorch/comments/c6hqg5/fastai_part_2_2019_is_online_deep_learning_from/,all_ads,False,https://www.fast.ai/2019/06/28/course-p2v3/,7135,1561710914.0,0,,False,https://www.fast.ai/2019/06/28/course-p2v3/,,,,,,,
851,,pytorch,"(I'm new to this community, so if this post violates any rules, I'm so sorry. Edit: I'm also really sorry about the formatting - I have no idea what is going on with it :( )

&amp;#x200B;

I’m trying to write a model that will try to approximate the underlying function behind some data (I have a lot of x,y pairs).

&amp;#x200B;

Here is the model architecture:

&amp;#x200B;

`class Net(torch.nn.Module):`

`def __init__(self):`

&amp;#x200B;

    `super(Net, self).__init__()`

&amp;#x200B;

    `self.fc1 = Linear(1, 50)`

&amp;#x200B;

    `self.fc2 = Linear(50, 1)`

&amp;#x200B;

&amp;#x200B;

`def forward(self, x):`

&amp;#x200B;

    `sigmoid = torch.nn.Sigmoid()`

&amp;#x200B;

    `x = self.fc1(x)`

&amp;#x200B;

    `x = sigmoid(x)`

&amp;#x200B;

    `x = self.fc2(x)`

&amp;#x200B;

    `return x`

`model = Net()`

&amp;#x200B;

The X values are in the numpy array X = \[\[366922500\], \[696530521\], …\], the Y values are in a numpy array “res” formatted res = \[\[0.0\], \[-652.2099712329232\],…\].

&amp;#x200B;

And here is how I am attempting to training it:

&amp;#x200B;

`# parameters`

`num_epochs = 10`

`learning_rate = 0.1`

&amp;#x200B;

`# define the loss function`

`critereon = L1Loss()`

&amp;#x200B;

`# define the optimizer`

`optimizer = Rprop(model.parameters(), lr=learning_rate)`

&amp;#x200B;

`# store the predictions during training`

`predictions = []`

&amp;#x200B;

`# create our training loop`

`for epoch in range(num_epochs):`

    `epoch_loss = 0`
    
    `predictions = []`
    
    
    
    `for ix in range(len(X)):`
    
    	`y_pred = model(Variable(Tensor(X[ix])))`
    
    	`# print(X[ix], res[ix], y_pred.item())`
    
    	`predictions.append(y_pred.item())`
    
    
    
    	`loss = critereon(y_pred, Variable(Tensor(res[ix])))`
    
    	`epoch_loss += loss.data.item()`
    
    
    
    	`optimizer.zero_grad()`
    
    	`loss.backward()`
    
    	`optimizer.step()`

&amp;#x200B;

`epoch_loss = epoch_loss/len(X)`

&amp;#x200B;

`print(""Epoch: {} Loss: {}"".format(epoch, epoch_loss))`

&amp;#x200B;

`# print(""Epoch: {}"".format(epoch))`

&amp;#x200B;

&amp;#x200B;

`plt.plot(X,predictions)`

`plt.plot(X,res)`

[`plt.show`](https://plt.show)`()`

&amp;#x200B;

The final three lines of that snippet are so I can see how the predictions during the final training loop compare to the true Y values. Orange are the true values, blue are the predictions.

&amp;#x200B;

https://preview.redd.it/8es5yhoqgz631.png?width=1308&amp;format=png&amp;auto=webp&amp;s=aa5672dfe28c1fb7551a5e2d12341a06751bc48b

The loss values decrease over the course of training, and the predictions do improve after each round of training.

Now, I am trying to predict using this model, doing the following:

&amp;#x200B;

`preds = []`

`for idx in range(len(X)):`

    `pred = model(Variable(Tensor(X[idx])))`
    
    `# print(X[idx], res[idx], pred.item())`
    
    `preds.append(pred.item())`

&amp;#x200B;

`plt.plot(X,preds)`

`plt.plot(X,res)`

[`plt.show`](https://plt.show)`()`

&amp;#x200B;

However, when I do this, the model just predicts one value for the entirety of the X values, resulting in this plot:

&amp;#x200B;

&amp;#x200B;

https://preview.redd.it/udjjkw8xgz631.png?width=640&amp;format=png&amp;auto=webp&amp;s=6bce88314eacbb609cf94f7cd2973be2bcbaf0bd

&amp;#x200B;

Does anyone have any idea why this is happening? During training, the predictions seems to be correct, but after training, the model just predicts one value for all the inputs. These are the same inputs it was trained on, and the same inputs for the first plot, so I have no idea what is happening.

&amp;#x200B;

Any help would be awesome! I'm really quite lost, and quite new to PyTorch.",t2_41xvyd6,False,,0,False,Confused about why the model isn't predicting correctly,[],r/pytorch,False,6,,0,97.0,,False,t3_c6cndz,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/W8QKa1GyFFm_U5W7FPBQTx_h2hX4Tm3bMHoPjlIFakA.jpg,1561678393.0,,[],{},,,True,,1561706805.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;(I&amp;#39;m new to this community, so if this post violates any rules, I&amp;#39;m so sorry. Edit: I&amp;#39;m also really sorry about the formatting - I have no idea what is going on with it :( )&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I’m trying to write a model that will try to approximate the underlying function behind some data (I have a lot of x,y pairs).&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Here is the model architecture:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;class Net(torch.nn.Module):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def __init__(self):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`super(Net, self).__init__()`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`self.fc1 = Linear(1, 50)`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`self.fc2 = Linear(50, 1)`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def forward(self, x):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`sigmoid = torch.nn.Sigmoid()`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`x = self.fc1(x)`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`x = sigmoid(x)`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`x = self.fc2(x)`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`return x`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;model = Net()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The X values are in the numpy array X = [[366922500], [696530521], …], the Y values are in a numpy array “res” formatted res = [[0.0], [-652.2099712329232],…].&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;And here is how I am attempting to training it:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# parameters&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;num_epochs = 10&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;learning_rate = 0.1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# define the loss function&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;critereon = L1Loss()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# define the optimizer&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;optimizer = Rprop(model.parameters(), lr=learning_rate)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# store the predictions during training&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;predictions = []&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# create our training loop&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for epoch in range(num_epochs):&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`epoch_loss = 0`

`predictions = []`



`for ix in range(len(X)):`

    `y_pred = model(Variable(Tensor(X[ix])))`

    `# print(X[ix], res[ix], y_pred.item())`

    `predictions.append(y_pred.item())`



    `loss = critereon(y_pred, Variable(Tensor(res[ix])))`

    `epoch_loss += loss.data.item()`



    `optimizer.zero_grad()`

    `loss.backward()`

    `optimizer.step()`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;epoch_loss = epoch_loss/len(X)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;print(&amp;quot;Epoch: {} Loss: {}&amp;quot;.format(epoch, epoch_loss))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# print(&amp;quot;Epoch: {}&amp;quot;.format(epoch))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;plt.plot(X,predictions)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;plt.plot(X,res)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://plt.show""&gt;&lt;code&gt;plt.show&lt;/code&gt;&lt;/a&gt;&lt;code&gt;()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The final three lines of that snippet are so I can see how the predictions during the final training loop compare to the true Y values. Orange are the true values, blue are the predictions.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/8es5yhoqgz631.png?width=1308&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa5672dfe28c1fb7551a5e2d12341a06751bc48b""&gt;https://preview.redd.it/8es5yhoqgz631.png?width=1308&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa5672dfe28c1fb7551a5e2d12341a06751bc48b&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The loss values decrease over the course of training, and the predictions do improve after each round of training.&lt;/p&gt;

&lt;p&gt;Now, I am trying to predict using this model, doing the following:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;preds = []&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for idx in range(len(X)):&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`pred = model(Variable(Tensor(X[idx])))`

`# print(X[idx], res[idx], pred.item())`

`preds.append(pred.item())`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;plt.plot(X,preds)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;plt.plot(X,res)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://plt.show""&gt;&lt;code&gt;plt.show&lt;/code&gt;&lt;/a&gt;&lt;code&gt;()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;However, when I do this, the model just predicts one value for the entirety of the X values, resulting in this plot:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/udjjkw8xgz631.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bce88314eacbb609cf94f7cd2973be2bcbaf0bd""&gt;https://preview.redd.it/udjjkw8xgz631.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bce88314eacbb609cf94f7cd2973be2bcbaf0bd&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Does anyone have any idea why this is happening? During training, the predictions seems to be correct, but after training, the model just predicts one value for all the inputs. These are the same inputs it was trained on, and the same inputs for the first plot, so I have no idea what is happening.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Any help would be awesome! I&amp;#39;m really quite lost, and quite new to PyTorch.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c6cndz,True,,wchew23,,1,True,all_ads,False,[],False,,/r/pytorch/comments/c6cndz/confused_about_why_the_model_isnt_predicting/,all_ads,False,https://www.reddit.com/r/pytorch/comments/c6cndz/confused_about_why_the_model_isnt_predicting/,7135,1561678005.0,0,,False,,,,"{'udjjkw8xgz631': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 81, 'x': 108, 'u': 'https://preview.redd.it/udjjkw8xgz631.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7de7cc916473b7fa48f131040fd1ddda957dc06f'}, {'y': 162, 'x': 216, 'u': 'https://preview.redd.it/udjjkw8xgz631.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc3fc03be77441bea46514acbb89943c8ee18e64'}, {'y': 240, 'x': 320, 'u': 'https://preview.redd.it/udjjkw8xgz631.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=188d1f4fe1bd3e18c116cf1fc1622384f73b8023'}, {'y': 480, 'x': 640, 'u': 'https://preview.redd.it/udjjkw8xgz631.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd8e73d74057f2d7026371ef8fbc67095cbbd88d'}], 's': {'y': 480, 'x': 640, 'u': 'https://preview.redd.it/udjjkw8xgz631.png?width=640&amp;format=png&amp;auto=webp&amp;s=6bce88314eacbb609cf94f7cd2973be2bcbaf0bd'}, 'id': 'udjjkw8xgz631'}, '8es5yhoqgz631': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 75, 'x': 108, 'u': 'https://preview.redd.it/8es5yhoqgz631.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b9599f87bced206ac685eb973a22ae4c679e522'}, {'y': 150, 'x': 216, 'u': 'https://preview.redd.it/8es5yhoqgz631.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=64e1b6e679da9f0c4aadaeaaa7efc7ff93e6ca19'}, {'y': 223, 'x': 320, 'u': 'https://preview.redd.it/8es5yhoqgz631.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d0b23dbecb378f4ee7f03be5c2d03ca7b6be5a6'}, {'y': 447, 'x': 640, 'u': 'https://preview.redd.it/8es5yhoqgz631.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7bed834730d53d9a61026375bdaf47115b6d8d8f'}, {'y': 670, 'x': 960, 'u': 'https://preview.redd.it/8es5yhoqgz631.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b508e06213838514c828e42a7f718f6dd0af318'}, {'y': 754, 'x': 1080, 'u': 'https://preview.redd.it/8es5yhoqgz631.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3159bf8a8e4242f38ff1db73014952070cd765a0'}], 's': {'y': 914, 'x': 1308, 'u': 'https://preview.redd.it/8es5yhoqgz631.png?width=1308&amp;format=png&amp;auto=webp&amp;s=aa5672dfe28c1fb7551a5e2d12341a06751bc48b'}, 'id': '8es5yhoqgz631'}}",,,,
852,,pytorch,"After using this command in the Windows Command Prompt:

    &gt;pip3 install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp37-cp37m-win_amd64.whl

I get this error:

    ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\Users\\me\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\caffe2\\python\\serialized_test\\data\\operator_test\\collect_and_distribute_fpn_rpn_proposals_op_test.test_collect_and_dist.zip'

I looked through my directory and the 'collect\_and\_distribute\_fpn\_rpn\_proposals\_op\_test.test\_collect\_and\_dist.zip' does not exist anywhere on my computer.

Any help would be appreciated. StackOverflow's most common answer (not pytorch specific) seems to be to reinstall some library mentioned in the file path, but I don't know which since I am quite new to all this.",t2_gweirw3,False,,0,False,Issue installing PyTorch,[],r/pytorch,False,6,,0,,,False,t3_c6cllq,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1561706512.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;After using this command in the Windows Command Prompt:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;pip3 install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp37-cp37m-win_amd64.whl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I get this error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: &amp;#39;C:\\Users\\me\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\caffe2\\python\\serialized_test\\data\\operator_test\\collect_and_distribute_fpn_rpn_proposals_op_test.test_collect_and_dist.zip&amp;#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I looked through my directory and the &amp;#39;collect_and_distribute_fpn_rpn_proposals_op_test.test_collect_and_dist.zip&amp;#39; does not exist anywhere on my computer.&lt;/p&gt;

&lt;p&gt;Any help would be appreciated. StackOverflow&amp;#39;s most common answer (not pytorch specific) seems to be to reinstall some library mentioned in the file path, but I don&amp;#39;t know which since I am quite new to all this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c6cllq,True,,winky334,,9,True,all_ads,False,[],False,,/r/pytorch/comments/c6cllq/issue_installing_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/c6cllq/issue_installing_pytorch/,7135,1561677712.0,0,,False,,,,,,,,
853,,pytorch,,t2_namkp97,False,,0,False,"Multi-class classification using CNN over PyTorch, and the basics of CNN",[],r/pytorch,False,6,,0,90.0,,False,t3_c66qfy,False,dark,0.33,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/i2wfDW1x3yJgym6iDVFyaWAvhhOc9rA7s26tTtP2Q2s.jpg,False,,[],{},link,,False,,1561677297.0,text,6,,,text,medium.com,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/2MeZhaWgGw7mzJhOTL9iJUu_gv6NAHl2QWbOZN0riEA.jpg?auto=webp&amp;s=aec46c9a221df9460809b5bdccffc2a317a69c87', 'width': 1200, 'height': 773}, 'resolutions': [{'url': 'https://external-preview.redd.it/2MeZhaWgGw7mzJhOTL9iJUu_gv6NAHl2QWbOZN0riEA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6df8bd25f56f14fed9636f8fa335ba051d7ed5e3', 'width': 108, 'height': 69}, {'url': 'https://external-preview.redd.it/2MeZhaWgGw7mzJhOTL9iJUu_gv6NAHl2QWbOZN0riEA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b62e3e355ff416e8e576cbd814d526fd7276cf96', 'width': 216, 'height': 139}, {'url': 'https://external-preview.redd.it/2MeZhaWgGw7mzJhOTL9iJUu_gv6NAHl2QWbOZN0riEA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=29dfc413948aa0a4fed5af9a912bc951241e1aa1', 'width': 320, 'height': 206}, {'url': 'https://external-preview.redd.it/2MeZhaWgGw7mzJhOTL9iJUu_gv6NAHl2QWbOZN0riEA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb4adb80e3d70a188d8e33b2fa51e79ab6881956', 'width': 640, 'height': 412}, {'url': 'https://external-preview.redd.it/2MeZhaWgGw7mzJhOTL9iJUu_gv6NAHl2QWbOZN0riEA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2bee0d102a4f20a54a547e25e6cd71fcffc769c7', 'width': 960, 'height': 618}, {'url': 'https://external-preview.redd.it/2MeZhaWgGw7mzJhOTL9iJUu_gv6NAHl2QWbOZN0riEA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6b42e9c66637711a8f8d94b2bd94ec0397147f03', 'width': 1080, 'height': 695}], 'variants': {}, 'id': '1ZcdIameLLbeyYn43kR5yL7r2tlUXHnJusmLtJ4od6s'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c66qfy,True,,thevatsalsaglani,,0,True,all_ads,False,[],False,,/r/pytorch/comments/c66qfy/multiclass_classification_using_cnn_over_pytorch/,all_ads,False,https://medium.com/@thevatsalsaglani/multi-class-image-classification-using-cnn-over-pytorch-and-the-basics-of-cnn-fdf425a11dc0,7135,1561648497.0,0,,False,https://medium.com/@thevatsalsaglani/multi-class-image-classification-using-cnn-over-pytorch-and-the-basics-of-cnn-fdf425a11dc0,,,,,,,
854,,pytorch,,t2_2yrrce83,False,,0,False,"""Using Kubernetes for Machine Learning Frameworks"" with Arun Gupta (53min talk from GOTO Chicago 2019)",[],r/pytorch,False,6,,0,105.0,,False,t3_c5r5g6,False,dark,0.75,,public,2,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/VgLxcu18bbw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'GOTO 2019 • Using Kubernetes for Machine Learning Frameworks • Arun Gupta', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/VgLxcu18bbw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'GOTO Conferences', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/VgLxcu18bbw/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/GotoConferences'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/VgLxcu18bbw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/c5r5g6', 'height': 338}",,False,2,,False,https://b.thumbs.redditmedia.com/FzG-sBmqRrkrd1cCRJYGu0LnR0WTiYSkVDN0UNFdrgI.jpg,False,,[],{},rich:video,,False,,1561591465.0,text,6,,,text,youtu.be,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fa0y7a22BGqblOVki50foncCf9b8qT9tPYBy9yXQR7o.jpg?auto=webp&amp;s=69db31deb107debee52cf257beb3d39ac922f877', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/fa0y7a22BGqblOVki50foncCf9b8qT9tPYBy9yXQR7o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5f67d136f88d952474a2472470b0cd413734faf1', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/fa0y7a22BGqblOVki50foncCf9b8qT9tPYBy9yXQR7o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f75203b50d77724879a8a54f45b4ad82dbdc1f37', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/fa0y7a22BGqblOVki50foncCf9b8qT9tPYBy9yXQR7o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=14d13588c2965e01237426199c19732a0582e7c6', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'VANXxGkvsFvmxt6eSi8aqjunD2RUuqYSK2k2tg07liw'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c5r5g6,True,,goto-con,,1,True,all_ads,False,[],False,,/r/pytorch/comments/c5r5g6/using_kubernetes_for_machine_learning_frameworks/,all_ads,False,https://youtu.be/VgLxcu18bbw?list=PLEx5khR4g7PLIxNHQ5Ze0Mz6sAXA8vSPE,7135,1561562665.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'GOTO 2019 • Using Kubernetes for Machine Learning Frameworks • Arun Gupta', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/VgLxcu18bbw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'GOTO Conferences', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/VgLxcu18bbw/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/GotoConferences'}}",False,https://youtu.be/VgLxcu18bbw?list=PLEx5khR4g7PLIxNHQ5Ze0Mz6sAXA8vSPE,,,,,,,
855,,pytorch,,t2_cvc9f,False,,0,False,Mask R-CNN Instance Segmentation with PyTorch,[],r/pytorch,False,6,,0,78.0,,False,t3_c5cnr9,False,dark,0.8,,public,6,0,{},140.0,,False,[],,True,False,,{},,False,6,,False,https://b.thumbs.redditmedia.com/qwoBbhlEPZFzYfXCosXXUwiim968ddqsqMm5yH_mBJg.jpg,False,,[],{},image,,False,,1561515115.0,text,6,,,text,i.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/4ian31qvmj631.png?auto=webp&amp;s=d25a8ba8a37146e6638fe6c35e01df4d9b4cb3ed', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://preview.redd.it/4ian31qvmj631.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc3308a6e149e1c4ba18dc7303603e5116f0e35c', 'width': 108, 'height': 60}, {'url': 'https://preview.redd.it/4ian31qvmj631.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=384ea774722a5618359d75e8ede27655ca7f5f19', 'width': 216, 'height': 121}, {'url': 'https://preview.redd.it/4ian31qvmj631.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65be81e8b841cf1eca6f572f8e3eeb6a19d17fe0', 'width': 320, 'height': 180}, {'url': 'https://preview.redd.it/4ian31qvmj631.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9af45ee592f7bb4c5d00cb5b943183b2911292cb', 'width': 640, 'height': 360}, {'url': 'https://preview.redd.it/4ian31qvmj631.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e1519bc57352b716620617668b5e2a1b8f4ad10', 'width': 960, 'height': 540}, {'url': 'https://preview.redd.it/4ian31qvmj631.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f129ea47007915eae76aa01a7ebdb46f09ded2e', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'RhXXB_JQzIt4bEGWKsb4WzrFYuklScQgFdQsdkFmUe0'}], 'enabled': True}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c5cnr9,True,,spmallick,,1,True,all_ads,False,[],False,,/r/pytorch/comments/c5cnr9/mask_rcnn_instance_segmentation_with_pytorch/,all_ads,False,https://i.redd.it/4ian31qvmj631.png,7135,1561486315.0,0,,False,https://i.redd.it/4ian31qvmj631.png,,,,,,,
856,,pytorch,"I've got a pretty simple CNN model which runs into a huge performance drop when using 4 GPUs.  One GPU takes 4 minutes per epoch, two GPUs takes 2 minutes, but four GPUs suddenly jumps up to 10 minutes per epoch.

It seems like the issue has something to do with the memory usage.  For one and two GPUs, the memory usage is about 900M.  On four GPUs, each GPU loads up to 900M like usual, but as soon as the training loop gets going the memory usage jumps up to 7G and stays up there.

Has anyone encountered anything like this before?",t2_3f87u,False,,0,False,Performance drops when using 4 GPUs?,[],r/pytorch,False,6,,0,,,False,t3_c4frub,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,True,self,False,,[],{},,,True,,1561366436.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve got a pretty simple CNN model which runs into a huge performance drop when using 4 GPUs.  One GPU takes 4 minutes per epoch, two GPUs takes 2 minutes, but four GPUs suddenly jumps up to 10 minutes per epoch.&lt;/p&gt;

&lt;p&gt;It seems like the issue has something to do with the memory usage.  For one and two GPUs, the memory usage is about 900M.  On four GPUs, each GPU loads up to 900M like usual, but as soon as the training loop gets going the memory usage jumps up to 7G and stays up there.&lt;/p&gt;

&lt;p&gt;Has anyone encountered anything like this before?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c4frub,True,,unkz,,3,True,all_ads,False,[],False,,/r/pytorch/comments/c4frub/performance_drops_when_using_4_gpus/,all_ads,False,https://www.reddit.com/r/pytorch/comments/c4frub/performance_drops_when_using_4_gpus/,7135,1561337636.0,0,,False,,,,,,,,
857,,pytorch,,t2_tiqto,False,,0,False,Building a CIFAR classifier neural network with PyTorch,[],r/pytorch,False,6,,0,67.0,,False,t3_c3hizr,False,dark,0.6,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://a.thumbs.redditmedia.com/EqwX2fexzHslVBM3FoFQ-ea24Ty9Hj2AleJrNJhCdF4.jpg,False,,[],{},link,,False,,1561185221.0,text,6,,,text,blog.paperspace.com,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Y8noDyPQct8MwMjt1Uvhqy8fFFUile9eVgz5wdKc6Uk.jpg?auto=webp&amp;s=3dbe9fe61cdd0abe36521d41217bd3e79ac87c06', 'width': 1600, 'height': 776}, 'resolutions': [{'url': 'https://external-preview.redd.it/Y8noDyPQct8MwMjt1Uvhqy8fFFUile9eVgz5wdKc6Uk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb9d788147ba2a2e6b96475665f2ebc4ece4650a', 'width': 108, 'height': 52}, {'url': 'https://external-preview.redd.it/Y8noDyPQct8MwMjt1Uvhqy8fFFUile9eVgz5wdKc6Uk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=02f49bc7f0be85979b3d1d861ca22bdbf25e3faf', 'width': 216, 'height': 104}, {'url': 'https://external-preview.redd.it/Y8noDyPQct8MwMjt1Uvhqy8fFFUile9eVgz5wdKc6Uk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b2a99433cb5df3abbc32c228737c70f0f944dddc', 'width': 320, 'height': 155}, {'url': 'https://external-preview.redd.it/Y8noDyPQct8MwMjt1Uvhqy8fFFUile9eVgz5wdKc6Uk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f186f776676a913a89d3c568f3eeccf07b4eb5d3', 'width': 640, 'height': 310}, {'url': 'https://external-preview.redd.it/Y8noDyPQct8MwMjt1Uvhqy8fFFUile9eVgz5wdKc6Uk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d85b81e1b5628cbb640db80c5a2d8683fe6bcb88', 'width': 960, 'height': 465}, {'url': 'https://external-preview.redd.it/Y8noDyPQct8MwMjt1Uvhqy8fFFUile9eVgz5wdKc6Uk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4e36c7804e621efb629a5919cb822a75793c0dac', 'width': 1080, 'height': 523}], 'variants': {}, 'id': 'ONvEdVr9CbgLkas-pn4PAIyl94lm-yxUnTC2Jqm7z18'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c3hizr,True,,dkobran,,0,True,all_ads,False,[],False,,/r/pytorch/comments/c3hizr/building_a_cifar_classifier_neural_network_with/,all_ads,False,https://blog.paperspace.com/pytorch-101-building-neural-networks/,7135,1561156421.0,0,,False,https://blog.paperspace.com/pytorch-101-building-neural-networks/,,,,,,,
858,,pytorch,"Following this awesome [blog](http://jalammar.github.io/illustrated-transformer/?utm_source=share&amp;utm_medium=ios_app) I implemented multi-head attention on my own, and I just saw that pytorch has it implemented already. As I understand it from that blog, the Query Key, and Value vectors are computed using a linear layer for each. However, PyTorch requires the query, key and value vectors as inputs for the forward pass of its attention layer. It seems strange that PyTorch wouldn't just take the embedding and compute the Q, K, V vectors on the inside. Also when I compute them myself and feed them into the pytorch implementation, the model is 50% slower than using my own. These two facts make me think I am misunderstanding the Self Attention Layer. 

&amp;#x200B;

I looked in the source code and I think they do compute the Q,K,V vectors but I am not certain. Does anyone know for sure? 

Also Is my understanding of the self attention layer incorrect or is PyTorch's implementation really that much slower? 

&amp;#x200B;

Thanks",t2_ox1vv,False,,0,False,pytorch multi-head attention module,[],r/pytorch,False,6,,0,,,False,t3_c2u6g5,False,dark,1.0,,public,15,0,{},,,False,[],,False,False,,{},,False,15,,False,self,False,,[],{},self,,True,,1561053907.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Following this awesome &lt;a href=""http://jalammar.github.io/illustrated-transformer/?utm_source=share&amp;amp;utm_medium=ios_app""&gt;blog&lt;/a&gt; I implemented multi-head attention on my own, and I just saw that pytorch has it implemented already. As I understand it from that blog, the Query Key, and Value vectors are computed using a linear layer for each. However, PyTorch requires the query, key and value vectors as inputs for the forward pass of its attention layer. It seems strange that PyTorch wouldn&amp;#39;t just take the embedding and compute the Q, K, V vectors on the inside. Also when I compute them myself and feed them into the pytorch implementation, the model is 50% slower than using my own. These two facts make me think I am misunderstanding the Self Attention Layer. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I looked in the source code and I think they do compute the Q,K,V vectors but I am not certain. Does anyone know for sure? &lt;/p&gt;

&lt;p&gt;Also Is my understanding of the self attention layer incorrect or is PyTorch&amp;#39;s implementation really that much slower? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?auto=webp&amp;s=6f463f6bff513c6d017fa5538a00f4548a2dcfe1', 'width': 1436, 'height': 804}, 'resolutions': [{'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f8a7e804fa309d37c49f00a733d9224f0be49337', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b16204a4e79f791d02291fe76812185b9020ae5f', 'width': 216, 'height': 120}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4574a3af653fcbecd95c46b4b76706aef9b447a1', 'width': 320, 'height': 179}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=236b712e35df9668aa6ac8750ddd6b46b6ce24cf', 'width': 640, 'height': 358}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=78c4a9af8900e812f85c7fa529f53043e236922a', 'width': 960, 'height': 537}, {'url': 'https://external-preview.redd.it/_30wJpzbtsBAIQYThTEBp8e2067WVexM_vG2RbdZfOE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c049be5f511c9d47b872329f22723838d608ee3a', 'width': 1080, 'height': 604}], 'variants': {}, 'id': 'c1mFuTYSarvnl5bzqbH7vE9qZMX_5O-mA_uV6IM3pBM'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c2u6g5,True,,Stupidperson-,,2,True,all_ads,False,[],False,,/r/pytorch/comments/c2u6g5/pytorch_multihead_attention_module/,all_ads,False,https://www.reddit.com/r/pytorch/comments/c2u6g5/pytorch_multihead_attention_module/,7135,1561025107.0,0,,False,,,,,,,,
859,,pytorch,"    
    class rotation_kernel(nn.Module):
        '''
        Custom Kernel for rotational CNN
        Input:  in_c - #input channel
                out_c - #output channel
                stride
                padding 
        '''
        def __init__(self,in_c,out_c,stride,padding):
            super(rotation_kernel, self).__init__()
            self.in_c = in_c
            self.out_c= out_c
            self.stride = stride
            self.weight = Parameter(torch.Tensor(out_c,in_c, 3,3),requires_grad=True)
            self.padding = padding
    
        def forward(self,x):
            w = self.weight
            # returns a  4*4*(4*3*3) shape kernel (out channel, rotation angle, in channel, kernel H, kernel W)
            w = permute(w)
            out = []
            # print(self.pl[0])
            for out_ch, kernels in enumerate(w):
                xs = []
                for i in range(4):
                    xs.append(nn.functional.conv2d(x,kernels[i].unsqueeze(0),stride=self.stride, padding=1)) 
                xs = torch.stack(xs)
                xs = xs.permute(2,1,0,3,4)
                # max activation ==&gt; results in Batch_size*4*14*14
                out.append(torch.max(xs,dim=2).values)
            out = torch.stack(out)
            out = out.permute(2,1,0,3,4)
            return out,self.pl
        def backward(self,x):
            pass
    
    class rotational_cnn(nn.Module):
        def __init__(self):
            super(rotational_cnn,self).__init__()
            # convolution layer 1
            self.conv1 = nn.Conv2d(1,4,kernel_size=3, padding=(1,1),bias=False)
            self.conv2 = nn.Conv2d(4,4,kernel_size=3, padding=0,bias=False)
            self.pool1 = nn.MaxPool2d(kernel_size =2,stride=2)
            self.dropout1 = nn.Dropout(0.25)
    
            self.conv3 = nn.Conv2d(4,4,kernel_size=3, padding=(1,1), bias=False)
            self.conv4 = rotation_kernel(4,4,1,1)
            self.pool2 = nn.MaxPool2d(kernel_size =2, stride=2)
            self.dropout2 = nn.Dropout(0.25)
            self.dropout3 = nn.Dropout(0.5)
            
            self.fc = nn.Linear(144, 70)
            self.fc2 = nn.Linear(70,10)
            self.sm = Softmax(dim=1)
        def forward(self,x):
            x = self.conv1(x)
            x = self.dropout1(self.pool1(F.relu(self.conv2(F.relu(x)))))
    
            tmp = self.conv4(F.relu(self.conv3(x)))
            # print(tmp[1])
            # print(tmp[1].shape, torch.stack(tmp[2]).shape)
            x = self.dropout2(self.pool2(F.relu(tmp[0].squeeze(1))))
            # flatten
            x = x.reshape(x.shape[0],-1)
            
            x = self.fc2(self.dropout3(F.relu(self.fc(x))))
            x = self.sm(x)
            return x
    

Hi everyone. I am trying to implement a rotational invariant CNN. So I have created a nn.module layer named rotational\_kernel and added it on the last convolution layer. Apart from this one layer it is identical to the conventional CNN model. I have tested the conventional one out (by simply changing self.conv4 into conv2d) and it performs fine. However I am facing a problem where my parameter objects are not being changed over epoch. I was wondering if anyone can kindly help me with this. Thank you!",t2_2l4b9p2n,False,,0,False,Custom Kernel not training,[],r/pytorch,False,6,,0,,,False,t3_c2t4x7,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1561045439.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;pre&gt;&lt;code&gt;class rotation_kernel(nn.Module):
    &amp;#39;&amp;#39;&amp;#39;
    Custom Kernel for rotational CNN
    Input:  in_c - #input channel
            out_c - #output channel
            stride
            padding 
    &amp;#39;&amp;#39;&amp;#39;
    def __init__(self,in_c,out_c,stride,padding):
        super(rotation_kernel, self).__init__()
        self.in_c = in_c
        self.out_c= out_c
        self.stride = stride
        self.weight = Parameter(torch.Tensor(out_c,in_c, 3,3),requires_grad=True)
        self.padding = padding

    def forward(self,x):
        w = self.weight
        # returns a  4*4*(4*3*3) shape kernel (out channel, rotation angle, in channel, kernel H, kernel W)
        w = permute(w)
        out = []
        # print(self.pl[0])
        for out_ch, kernels in enumerate(w):
            xs = []
            for i in range(4):
                xs.append(nn.functional.conv2d(x,kernels[i].unsqueeze(0),stride=self.stride, padding=1)) 
            xs = torch.stack(xs)
            xs = xs.permute(2,1,0,3,4)
            # max activation ==&amp;gt; results in Batch_size*4*14*14
            out.append(torch.max(xs,dim=2).values)
        out = torch.stack(out)
        out = out.permute(2,1,0,3,4)
        return out,self.pl
    def backward(self,x):
        pass

class rotational_cnn(nn.Module):
    def __init__(self):
        super(rotational_cnn,self).__init__()
        # convolution layer 1
        self.conv1 = nn.Conv2d(1,4,kernel_size=3, padding=(1,1),bias=False)
        self.conv2 = nn.Conv2d(4,4,kernel_size=3, padding=0,bias=False)
        self.pool1 = nn.MaxPool2d(kernel_size =2,stride=2)
        self.dropout1 = nn.Dropout(0.25)

        self.conv3 = nn.Conv2d(4,4,kernel_size=3, padding=(1,1), bias=False)
        self.conv4 = rotation_kernel(4,4,1,1)
        self.pool2 = nn.MaxPool2d(kernel_size =2, stride=2)
        self.dropout2 = nn.Dropout(0.25)
        self.dropout3 = nn.Dropout(0.5)

        self.fc = nn.Linear(144, 70)
        self.fc2 = nn.Linear(70,10)
        self.sm = Softmax(dim=1)
    def forward(self,x):
        x = self.conv1(x)
        x = self.dropout1(self.pool1(F.relu(self.conv2(F.relu(x)))))

        tmp = self.conv4(F.relu(self.conv3(x)))
        # print(tmp[1])
        # print(tmp[1].shape, torch.stack(tmp[2]).shape)
        x = self.dropout2(self.pool2(F.relu(tmp[0].squeeze(1))))
        # flatten
        x = x.reshape(x.shape[0],-1)

        x = self.fc2(self.dropout3(F.relu(self.fc(x))))
        x = self.sm(x)
        return x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hi everyone. I am trying to implement a rotational invariant CNN. So I have created a nn.module layer named rotational_kernel and added it on the last convolution layer. Apart from this one layer it is identical to the conventional CNN model. I have tested the conventional one out (by simply changing self.conv4 into conv2d) and it performs fine. However I am facing a problem where my parameter objects are not being changed over epoch. I was wondering if anyone can kindly help me with this. Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c2t4x7,True,,sbhchan2,,2,True,all_ads,False,[],False,,/r/pytorch/comments/c2t4x7/custom_kernel_not_training/,all_ads,False,https://www.reddit.com/r/pytorch/comments/c2t4x7/custom_kernel_not_training/,7135,1561016639.0,0,,False,,,,,,,,
860,,pytorch,,t2_cvc9f,False,,0,False,Faster R-CNN Object Detection with PyTorch,[],r/pytorch,False,6,,0,93.0,,False,t3_c25m5s,False,dark,0.81,,public,10,0,{},140.0,,False,[],,True,False,,{},,False,10,,False,https://b.thumbs.redditmedia.com/MN1mpFNxp2RO3YZ4V5BV0d4O2ynyDh9GNFPFA2vzfJs.jpg,False,,[],{},image,,False,,1560910020.0,text,6,,,text,i.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/rcmjc7qln5531.gif?format=png8&amp;s=d0e6fc7e45079882fa56344a6e540cb0f3b1d6f1', 'width': 300, 'height': 200}, 'resolutions': [{'url': 'https://preview.redd.it/rcmjc7qln5531.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=39aae8249ecd434aa44ae31aa13ad066153a65b2', 'width': 108, 'height': 72}, {'url': 'https://preview.redd.it/rcmjc7qln5531.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=56afa0130aef89906185a79d8ed4984060eb3d2a', 'width': 216, 'height': 144}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/rcmjc7qln5531.gif?s=ada0554d7f314e14d7ab1297c3bb2dd765df730d', 'width': 300, 'height': 200}, 'resolutions': [{'url': 'https://preview.redd.it/rcmjc7qln5531.gif?width=108&amp;crop=smart&amp;s=b192060befa6cb393363509bf146770f2490bac1', 'width': 108, 'height': 72}, {'url': 'https://preview.redd.it/rcmjc7qln5531.gif?width=216&amp;crop=smart&amp;s=eb1f6a665477fbf218112734b5d933193883aed4', 'width': 216, 'height': 144}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/rcmjc7qln5531.gif?format=mp4&amp;s=71d02f780868c4aee95bdb0dc1e2028a3a33c052', 'width': 300, 'height': 200}, 'resolutions': [{'url': 'https://preview.redd.it/rcmjc7qln5531.gif?width=108&amp;format=mp4&amp;s=69e093a828ac1d09dec5bf5f6b68e0985467b78b', 'width': 108, 'height': 72}, {'url': 'https://preview.redd.it/rcmjc7qln5531.gif?width=216&amp;format=mp4&amp;s=b41024ac1a296d7f1e8da2b78fbb771c08ca78d9', 'width': 216, 'height': 144}]}}, 'id': '9nu1PUy5hSEkLsLu8xER3QXxEic-eg9YfkF-SElABkU'}], 'enabled': True}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c25m5s,True,,spmallick,,2,True,all_ads,False,[],False,,/r/pytorch/comments/c25m5s/faster_rcnn_object_detection_with_pytorch/,all_ads,False,https://i.redd.it/rcmjc7qln5531.gif,7135,1560881220.0,0,,False,https://i.redd.it/rcmjc7qln5531.gif,,,,,,,
861,,pytorch,,t2_tiqto,False,,0,False,PyTorch Basics: Understanding Autograd and Computation Graphs,[],r/pytorch,False,6,,0,89.0,,False,t3_c1r9eb,False,dark,0.93,,public,11,0,{},140.0,,False,[],,False,False,,{},,False,11,,False,https://b.thumbs.redditmedia.com/5dXtFUK1ONnwpL74iPBhIwRX7-Aodvz-Id-z2t6Omlk.jpg,False,,[],{},link,,False,,1560825394.0,text,6,,,text,blog.paperspace.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/cAKu-_u6JrhD5yiPxlMFGKqQwLoXIfR3gMe08JnfZ88.jpg?auto=webp&amp;s=80d88ebe6af10058ff869e0a9561c5efdbb11561', 'width': 897, 'height': 576}, 'resolutions': [{'url': 'https://external-preview.redd.it/cAKu-_u6JrhD5yiPxlMFGKqQwLoXIfR3gMe08JnfZ88.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=956bcd3b5c80f75a8be45903ebae6306eb20bd89', 'width': 108, 'height': 69}, {'url': 'https://external-preview.redd.it/cAKu-_u6JrhD5yiPxlMFGKqQwLoXIfR3gMe08JnfZ88.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=36cb153a4d28d83fd61de5f3c7acac6c2fe3b858', 'width': 216, 'height': 138}, {'url': 'https://external-preview.redd.it/cAKu-_u6JrhD5yiPxlMFGKqQwLoXIfR3gMe08JnfZ88.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e034474f890f735a0028f2fc399ca544f02ce899', 'width': 320, 'height': 205}, {'url': 'https://external-preview.redd.it/cAKu-_u6JrhD5yiPxlMFGKqQwLoXIfR3gMe08JnfZ88.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0ebc4697db05d1b6332dae48275031ec0758aba8', 'width': 640, 'height': 410}], 'variants': {}, 'id': 'U7QRn_wsjjsoPuqC5-cPLPbobpodjwn2bIfMJZXyhEU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c1r9eb,True,,dkobran,,1,True,all_ads,False,[],False,,/r/pytorch/comments/c1r9eb/pytorch_basics_understanding_autograd_and/,all_ads,False,https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/,7135,1560796594.0,0,,False,https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/,,,,,,,
862,,pytorch,,t2_202jsaz7,False,,0,False,pastiche: a PyTorch implementation of Neural Style Transfer,[],r/pytorch,False,6,,0,140.0,,False,t3_c1dxs6,False,dark,0.86,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/gjsMj5Bz5ZpBMTlq0AuMu0gEQ8LDSJjrXajWMa6Dd5U.jpg,False,,[],{},link,,False,,1560742609.0,text,6,,,text,github.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/-qMsrGJ70kY3zjKwKPiUBw1IBRx-7YXaIyYasTuOxZ8.jpg?auto=webp&amp;s=af6081dfd5372107de2e4c0520bd547925f85ed6', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/-qMsrGJ70kY3zjKwKPiUBw1IBRx-7YXaIyYasTuOxZ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb08b1edec137adafdd13c2f4c37cd2b4043e0d5', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/-qMsrGJ70kY3zjKwKPiUBw1IBRx-7YXaIyYasTuOxZ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb315606a0c02079bce46538362500fd7ca46fb2', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/-qMsrGJ70kY3zjKwKPiUBw1IBRx-7YXaIyYasTuOxZ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=13553dcf54ad9e9dfe253f2dbb2da5fa11c85f55', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'ihDcmUMjVIkpAMAPu_nymA-o0t7FsfAKBvyNDAtanxU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,c1dxs6,True,,dstein64,,0,True,all_ads,False,[],False,,/r/pytorch/comments/c1dxs6/pastiche_a_pytorch_implementation_of_neural_style/,all_ads,False,https://github.com/dstein64/pastiche,7135,1560713809.0,1,,False,https://github.com/dstein64/pastiche,,,,,,,
863,,pytorch,,t2_2fv4yodo,False,,0,False,New Facebook PyTorch Hub Facilitates Reproducibility Testing,[],r/pytorch,False,6,,0,78.0,,False,t3_bzeet7,False,dark,0.93,,public,13,0,{},140.0,,False,[],,False,False,,{},,False,13,,False,https://b.thumbs.redditmedia.com/GCbb1cDaq-Le60CQ-FV6qBnUyfS20647r5ZM6zA8gqc.jpg,False,,[],{},link,,False,,1560298250.0,text,6,,,text,medium.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/aCKNqoquKWBhg11DW_5g5nUA7PzuNMBTFiEccL0ztOk.jpg?auto=webp&amp;s=abde59e3f2fcd7c0560b58e2239541fa2071c7a3', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/aCKNqoquKWBhg11DW_5g5nUA7PzuNMBTFiEccL0ztOk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d0b2aeb36a58a49af16b838deda85fb8e660221', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/aCKNqoquKWBhg11DW_5g5nUA7PzuNMBTFiEccL0ztOk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9704d465b8599c685f9a4ab4530b9eca74f87697', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/aCKNqoquKWBhg11DW_5g5nUA7PzuNMBTFiEccL0ztOk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=95d70df7e0da88b5029d65f9acf1fbccda631012', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/aCKNqoquKWBhg11DW_5g5nUA7PzuNMBTFiEccL0ztOk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=68bec18815368b998f6e8493078f222a69e9f638', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/aCKNqoquKWBhg11DW_5g5nUA7PzuNMBTFiEccL0ztOk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f1105978a7c95f89685dbddbb347b82ad6967449', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/aCKNqoquKWBhg11DW_5g5nUA7PzuNMBTFiEccL0ztOk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=163cd9b6b03b2312b96b72a377e16c37d12a654e', 'width': 1080, 'height': 607}], 'variants': {}, 'id': '6Q-6D1ppRmrfuNOVGE8Bqq4xoGZRFEs2NJr1XFuH96s'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bzeet7,True,,Yuqing7,,0,True,all_ads,False,[],False,,/r/pytorch/comments/bzeet7/new_facebook_pytorch_hub_facilitates/,all_ads,False,https://medium.com/syncedreview/new-facebook-pytorch-hub-facilitates-reproducibility-testing-795b835537b1?postPublishedType=initial,7135,1560269450.0,0,,False,https://medium.com/syncedreview/new-facebook-pytorch-hub-facilitates-reproducibility-testing-795b835537b1?postPublishedType=initial,,,,,,,
864,,pytorch,"Please take a moment and have a look at the notebook \[here\]([https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character\_Level\_RNN\_Solution.ipynb](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character_Level_RNN_Solution.ipynb)). This is a simple tutorial concerning LSTMs taught at Udemy's Pytorch Course. 

There are two sections in this IPython notebook that confuses me greatly. 

1. Why is it necessary to use contiguous() when using an LSTM? 

and more importantly : 

2. Why doesn't the training procedure fail! because of the following code snippet : 

&amp;#x200B;

        counter = 0
        n_chars = len(net.chars)
        for e in range(epochs):
            # initialize hidden state
            h = net.init_hidden(batch_size)
            
            for x, y in get_batches(data, batch_size, seq_length):
                counter += 1
                
                # One-hot encode our data and make them Torch tensors
                x = one_hot_encode(x, n_chars)
                inputs, targets = torch.from_numpy(x), torch.from_numpy(y)
                
                if(train_on_gpu):
                    inputs, targets = inputs.cuda(), targets.cuda()
    
                # Creating new variables for the hidden state, otherwise
                # we'd backprop through the entire training history
                h = tuple([each.data for each in h])
    
                # zero accumulated gradients
                net.zero_grad()
                
                # get the output from the model
                output, h = net(inputs, h)
                print(f'output.shape: {output.shape}')
                print(f'y.shape :{targets.shape}')
                print(targets[0,:])
                # calculate the loss and perform backprop
                loss = criterion(output, targets.view(batch_size*seq_length).long())
                loss.backward()
                # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
                nn.utils.clip_grad_norm_(net.parameters(), clip)
                opt.step()
                
                # loss stats
                if counter % print_every == 0:

&amp;#x200B;

&amp;#x200B;

What I'm specifically referring to is this line: 

`loss = criterion(output, targets.view(batch_size*seq_length).long())`

&amp;#x200B;

basically, here the author is using a one-hot encoded output with the shape \`(batch, sequence\_length, features)\` with a normal not-one-hot encoded target tensor of shape \``(batchsize, sequence_length)`\`! 

why does it not fail? how is \``crossentropy`\` doing its job when the two tensors are not both one-hot  encoded?! 

if you go and one-hot encode the targets as well, you will face the error : 

`RuntimeError: multi-target not supported at C:/w/1/s/windows/pytorch/aten/src\THCUNN/generic/ClassNLLCriterion.cu:15`

&amp;#x200B;

The usage of contiguous() seems not to do any good and the only way to get this to work seems like this!

&amp;#x200B;

 also have a side question, what does   `weight = next(self.parameters()).data` mean? 

Why did t he author do  : 

&amp;#x200B;

    weight = next(self.parameters()).data
    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),
                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())

&amp;#x200B;

&amp;#x200B;

whats this [weight.new](https://weight.new)? how does he/she know what parameters to send? why did not the author simply use tensor.zeros() instead and do : 

&amp;#x200B;

    hidden_state = torch.zeros(num_layer*direction, batch_size, hidden_size).to(device)
    cellstate = torch.zeros_like(hidden_state).to(device)
    hiddenstates = (hidden_state,cellstate)

&amp;#x200B;

Can anyone please explain to me what is happening here? 

I greatly appreciate it.",t2_12rvgcg3,False,,0,False,How is this allowed in Pytorch and not fail?,[],r/pytorch,False,6,,0,,,False,t3_bzn0g1,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1560344882.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Please take a moment and have a look at the notebook [here](&lt;a href=""https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character_Level_RNN_Solution.ipynb""&gt;https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character_Level_RNN_Solution.ipynb&lt;/a&gt;). This is a simple tutorial concerning LSTMs taught at Udemy&amp;#39;s Pytorch Course. &lt;/p&gt;

&lt;p&gt;There are two sections in this IPython notebook that confuses me greatly. &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Why is it necessary to use contiguous() when using an LSTM? &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;and more importantly : &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Why doesn&amp;#39;t the training procedure fail! because of the following code snippet : &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    counter = 0
    n_chars = len(net.chars)
    for e in range(epochs):
        # initialize hidden state
        h = net.init_hidden(batch_size)

        for x, y in get_batches(data, batch_size, seq_length):
            counter += 1

            # One-hot encode our data and make them Torch tensors
            x = one_hot_encode(x, n_chars)
            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)

            if(train_on_gpu):
                inputs, targets = inputs.cuda(), targets.cuda()

            # Creating new variables for the hidden state, otherwise
            # we&amp;#39;d backprop through the entire training history
            h = tuple([each.data for each in h])

            # zero accumulated gradients
            net.zero_grad()

            # get the output from the model
            output, h = net(inputs, h)
            print(f&amp;#39;output.shape: {output.shape}&amp;#39;)
            print(f&amp;#39;y.shape :{targets.shape}&amp;#39;)
            print(targets[0,:])
            # calculate the loss and perform backprop
            loss = criterion(output, targets.view(batch_size*seq_length).long())
            loss.backward()
            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
            nn.utils.clip_grad_norm_(net.parameters(), clip)
            opt.step()

            # loss stats
            if counter % print_every == 0:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What I&amp;#39;m specifically referring to is this line: &lt;/p&gt;

&lt;p&gt;&lt;code&gt;loss = criterion(output, targets.view(batch_size*seq_length).long())&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;basically, here the author is using a one-hot encoded output with the shape `(batch, sequence_length, features)` with a normal not-one-hot encoded target tensor of shape `&lt;code&gt;(batchsize, sequence_length)&lt;/code&gt;`! &lt;/p&gt;

&lt;p&gt;why does it not fail? how is `&lt;code&gt;crossentropy&lt;/code&gt;` doing its job when the two tensors are not both one-hot  encoded?! &lt;/p&gt;

&lt;p&gt;if you go and one-hot encode the targets as well, you will face the error : &lt;/p&gt;

&lt;p&gt;&lt;code&gt;RuntimeError: multi-target not supported at C:/w/1/s/windows/pytorch/aten/src\THCUNN/generic/ClassNLLCriterion.cu:15&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The usage of contiguous() seems not to do any good and the only way to get this to work seems like this!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;also have a side question, what does   &lt;code&gt;weight = next(self.parameters()).data&lt;/code&gt; mean? &lt;/p&gt;

&lt;p&gt;Why did t he author do  : &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;weight = next(self.parameters()).data
hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),
                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;whats this &lt;a href=""https://weight.new""&gt;weight.new&lt;/a&gt;? how does he/she know what parameters to send? why did not the author simply use tensor.zeros() instead and do : &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hidden_state = torch.zeros(num_layer*direction, batch_size, hidden_size).to(device)
cellstate = torch.zeros_like(hidden_state).to(device)
hiddenstates = (hidden_state,cellstate)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Can anyone please explain to me what is happening here? &lt;/p&gt;

&lt;p&gt;I greatly appreciate it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/9JC-Qo39zVGD9059LId5rK1sDLgIgDQvUWRsqmA6Pis.jpg?auto=webp&amp;s=6980ee95fa6a13665ace9f819925419c1c91ccdd', 'width': 304, 'height': 304}, 'resolutions': [{'url': 'https://external-preview.redd.it/9JC-Qo39zVGD9059LId5rK1sDLgIgDQvUWRsqmA6Pis.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=29184423086e22b5ed4dbf3e7394f9e7bea7d718', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/9JC-Qo39zVGD9059LId5rK1sDLgIgDQvUWRsqmA6Pis.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d38aabc7ea59be6a0f5529b54935cbc1c7c40a42', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'XoTkAdUL5---23nN8tIruAtvSZLKDjnNkO4YzrAzPA0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bzn0g1,True,,MasterSama,,6,True,all_ads,False,[],False,,/r/pytorch/comments/bzn0g1/how_is_this_allowed_in_pytorch_and_not_fail/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bzn0g1/how_is_this_allowed_in_pytorch_and_not_fail/,7135,1560316082.0,0,,False,,,,,,,,
865,,pytorch,Is there a way to install PyTorch on a computer without an internet connection? I've tried to find any documentation on this but was not able to.,t2_f4t6i,False,,0,False,Install Pytorch offline,[],r/pytorch,False,6,,0,,,False,t3_bz9yft,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1560271261.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there a way to install PyTorch on a computer without an internet connection? I&amp;#39;ve tried to find any documentation on this but was not able to.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bz9yft,True,,PM_for_bad_advice,,4,True,all_ads,False,[],False,,/r/pytorch/comments/bz9yft/install_pytorch_offline/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bz9yft/install_pytorch_offline/,7135,1560242461.0,0,,False,,,,,,,,
866,,pytorch,"Hi, I want to implement a CNN using PyTorch to train a classifier for classification of NSFW images (posted on GitHub before). I am a PyTorch beginner and I just know how to use PyTorch to classify dogs and cats (the MNIST dataset). The NSFW images are way larger and seem way complicated than those simple small-dimension dogs and cats images. Any suggestions on how I can get started? (Or any existing CNN models that I can use?)",t2_124c4dxl,False,,0,False,Training NSFW Images using PyTorch and CNN,[],r/pytorch,False,6,,0,,,False,t3_bz8onj,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1560261071.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I want to implement a CNN using PyTorch to train a classifier for classification of NSFW images (posted on GitHub before). I am a PyTorch beginner and I just know how to use PyTorch to classify dogs and cats (the MNIST dataset). The NSFW images are way larger and seem way complicated than those simple small-dimension dogs and cats images. Any suggestions on how I can get started? (Or any existing CNN models that I can use?)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bz8onj,True,,oscar23433,,1,True,all_ads,False,[],False,,/r/pytorch/comments/bz8onj/training_nsfw_images_using_pytorch_and_cnn/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bz8onj/training_nsfw_images_using_pytorch_and_cnn/,7135,1560232271.0,0,,False,,,,,,,,
867,,pytorch,,t2_cvc9f,False,,0,False,Image Classification using Pre-Trained Models in PyTorch,[],r/pytorch,False,6,,0,86.0,,False,t3_byu5ve,False,dark,0.92,,public,10,0,{},140.0,,False,[],,True,False,,{},,False,10,,False,https://a.thumbs.redditmedia.com/JK1J4w0fM2myYTlc6CXlJwGC9z1is5TQLDdLVA7rK90.jpg,False,,[],{},image,,False,,1560174804.0,text,6,,,text,i.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/nssz0ne0xg331.gif?format=png8&amp;s=ed9178662b47ad449f8f4e1b8e72ff0111164715', 'width': 512, 'height': 317}, 'resolutions': [{'url': 'https://preview.redd.it/nssz0ne0xg331.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=210c478ed02ae61d301e2b7f9949314991f5cca9', 'width': 108, 'height': 66}, {'url': 'https://preview.redd.it/nssz0ne0xg331.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=cbd6c8d6939e93edf5600097835ef35de9e81287', 'width': 216, 'height': 133}, {'url': 'https://preview.redd.it/nssz0ne0xg331.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=94a82b7fc2f335c0ab77c2ba118e33cf2d8a55f8', 'width': 320, 'height': 198}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/nssz0ne0xg331.gif?s=60f40a076817549e72aabc1e583268065e163f9b', 'width': 512, 'height': 317}, 'resolutions': [{'url': 'https://preview.redd.it/nssz0ne0xg331.gif?width=108&amp;crop=smart&amp;s=1ef13597eac3dac33aabd95652fcb993d5a7b972', 'width': 108, 'height': 66}, {'url': 'https://preview.redd.it/nssz0ne0xg331.gif?width=216&amp;crop=smart&amp;s=75c90fab0c043caf25f7dbead8f2099df4095e5d', 'width': 216, 'height': 133}, {'url': 'https://preview.redd.it/nssz0ne0xg331.gif?width=320&amp;crop=smart&amp;s=6809da0aebb33d4829cded72a0f42b5847c6eeaa', 'width': 320, 'height': 198}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/nssz0ne0xg331.gif?format=mp4&amp;s=ee518db41b728d8b5d22649ce0128f3db3906150', 'width': 512, 'height': 317}, 'resolutions': [{'url': 'https://preview.redd.it/nssz0ne0xg331.gif?width=108&amp;format=mp4&amp;s=b5d19c3d8378a10a3d7a32cfd36d4343f3d5c4c4', 'width': 108, 'height': 66}, {'url': 'https://preview.redd.it/nssz0ne0xg331.gif?width=216&amp;format=mp4&amp;s=23afd34931378bbf64820442eb8c2486e849dfe1', 'width': 216, 'height': 133}, {'url': 'https://preview.redd.it/nssz0ne0xg331.gif?width=320&amp;format=mp4&amp;s=c86fbc35179cbe7c01c4651f891005b3ff6723e3', 'width': 320, 'height': 198}]}}, 'id': 'CzwtD3SxtzaUoRlRraTRzY0Zt5faN18dm9hMviiaUZA'}], 'enabled': True}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,byu5ve,True,,spmallick,,1,True,all_ads,False,[],False,,/r/pytorch/comments/byu5ve/image_classification_using_pretrained_models_in/,all_ads,False,https://i.redd.it/nssz0ne0xg331.gif,7135,1560146004.0,0,,False,https://i.redd.it/nssz0ne0xg331.gif,,,,,,,
868,,pytorch,,t2_117tcc,False,,0,False,U-Net implementation in PyTorch for FLAIR abnormality segmentation in brain MRI - data and weights,[],r/pytorch,False,6,,0,140.0,,False,t3_bypwsk,False,dark,0.84,,public,4,0,{},140.0,,False,[],,False,False,,{},,False,4,,False,https://b.thumbs.redditmedia.com/4sZVaPskWSHP6SXGP2KurvPXW8S3M4sfal7fUVZou6E.jpg,False,,[],{},link,,False,,1560147491.0,text,6,,,text,github.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/R7E5hovzDCojySZwRllzLqizHSGsaAYM5_IhV1D0yBA.jpg?auto=webp&amp;s=c4fb5f7a45e6799f0107156ada24610d1c53d655', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/R7E5hovzDCojySZwRllzLqizHSGsaAYM5_IhV1D0yBA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=64a91176e177a858ec17d4ddbc390c5d401b18b0', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/R7E5hovzDCojySZwRllzLqizHSGsaAYM5_IhV1D0yBA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=92ecec5a7920fd07198e95ce38a6e709faeba53b', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/R7E5hovzDCojySZwRllzLqizHSGsaAYM5_IhV1D0yBA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d2c65a929f48f93e264fb516042ad4011fe6bdb', 'width': 320, 'height': 320}], 'variants': {}, 'id': '9TY8lP3g--XtIhTcw3ub8zvOii4dHAX11Izh6JxvG7M'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bypwsk,True,,ketsok,,3,True,all_ads,False,[],False,,/r/pytorch/comments/bypwsk/unet_implementation_in_pytorch_for_flair/,all_ads,False,https://github.com/mateuszbuda/brain-segmentation-pytorch,7135,1560118691.0,0,,False,https://github.com/mateuszbuda/brain-segmentation-pytorch,,,,,,,
869,,pytorch,,t2_2jnzcz74,False,,0,False,Does anyone know when Google Colab would support PyTorch &amp; TPU?,[],r/pytorch,False,6,,0,,,False,t3_byc8oo,False,dark,0.84,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1560054765.0,text,6,,,text,self.pytorch,False,,,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,byc8oo,True,,codebeatz,,3,True,all_ads,False,[],False,,/r/pytorch/comments/byc8oo/does_anyone_know_when_google_colab_would_support/,all_ads,False,https://www.reddit.com/r/pytorch/comments/byc8oo/does_anyone_know_when_google_colab_would_support/,7135,1560025965.0,0,,False,,,,,,,,
870,,pytorch,,t2_cvc9f,False,,0,False,PyTorch for Beginners,[],r/pytorch,False,6,,0,78.0,,False,t3_by7v0g,False,dark,0.81,,public,10,0,{},140.0,,False,[],,True,False,,{},,False,10,,False,https://b.thumbs.redditmedia.com/-NTaYiuAqZtyOGUgrSyVBp4XqEEFBaBECk3p8Z_nmJI.jpg,False,,[],{},image,,False,,1560028506.0,text,6,,,text,i.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/8n8146x7u4331.png?auto=webp&amp;s=cc9ca53a337d034ab52940b56e0b88a97c727c04', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://preview.redd.it/8n8146x7u4331.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2bef30f53e2b761558ce592d16cd402789de5a10', 'width': 108, 'height': 60}, {'url': 'https://preview.redd.it/8n8146x7u4331.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bff12b9385847ed14797d9fe49c45806ceec41ea', 'width': 216, 'height': 121}, {'url': 'https://preview.redd.it/8n8146x7u4331.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ea00abd6f692b1f395a4a1adf8548fa3d61029b', 'width': 320, 'height': 180}, {'url': 'https://preview.redd.it/8n8146x7u4331.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1fb3789fedd9b47bfe177b83f34f328003415313', 'width': 640, 'height': 360}, {'url': 'https://preview.redd.it/8n8146x7u4331.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bb3640ca066b0fd3bc9f179cdc9320c28f6659b1', 'width': 960, 'height': 540}, {'url': 'https://preview.redd.it/8n8146x7u4331.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=faae39da33f55b1fc305bdd3aa518e3cd62abb67', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'jIqJwOd7Bp9lqTycfogYRbjVdzu1vF9kVhmAK2qap9U'}], 'enabled': True}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,by7v0g,True,,spmallick,,1,True,all_ads,False,[],False,,/r/pytorch/comments/by7v0g/pytorch_for_beginners/,all_ads,False,https://i.redd.it/8n8146x7u4331.png,7135,1559999706.0,0,,False,https://i.redd.it/8n8146x7u4331.png,,,,,,,
871,,pytorch,I want to implement an image captioning model. Please direct me to some resources or some other resources .,t2_2mu1wc6g,False,,0,False,PROJECT HELP,[],r/pytorch,False,6,,0,,,False,t3_by8lte,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1560033528.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to implement an image captioning model. Please direct me to some resources or some other resources .&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,by8lte,True,,naskamag,,4,True,all_ads,False,[],False,,/r/pytorch/comments/by8lte/project_help/,all_ads,False,https://www.reddit.com/r/pytorch/comments/by8lte/project_help/,7135,1560004728.0,0,,False,,,,,,,,
872,,pytorch,"So I'm trying to load a certain ONNX model in OpenCV (started in Python), but it crashes

It gives me this error when I try to convert that ONNX model to Tensorflow:  

    ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int64: 'Tensor(""Mul_3:0"", shape=(), dtype=int64)'

I suppose this is why it isn't working in OpenCV, is there any way to cast the int64 Tensor to a float32 one? I can try to modify the float32 &lt;-&gt; float16 conversion function so that it can also convert int64 to float16, but think this could be quite complex or impossible (I would have to move the weight up by 19 digits?).

**So I'm basically asking how I could cast at int64 to a float32 in an existing pre-trained Pytorch model, while getting the same result in the neural network (don't want to retrain it)**",t2_13ykeb,False,,0,False,Cast int64 Tensor to float32 in pre-trained model?,[],r/pytorch,False,6,,0,,,False,t3_bxlaci,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1559880217.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I&amp;#39;m trying to load a certain ONNX model in OpenCV (started in Python), but it crashes&lt;/p&gt;

&lt;p&gt;It gives me this error when I try to convert that ONNX model to Tensorflow:  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int64: &amp;#39;Tensor(&amp;quot;Mul_3:0&amp;quot;, shape=(), dtype=int64)&amp;#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I suppose this is why it isn&amp;#39;t working in OpenCV, is there any way to cast the int64 Tensor to a float32 one? I can try to modify the float32 &amp;lt;-&amp;gt; float16 conversion function so that it can also convert int64 to float16, but think this could be quite complex or impossible (I would have to move the weight up by 19 digits?).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;So I&amp;#39;m basically asking how I could cast at int64 to a float32 in an existing pre-trained Pytorch model, while getting the same result in the neural network (don&amp;#39;t want to retrain it)&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bxlaci,True,,iamalex_,,2,True,all_ads,False,[],False,,/r/pytorch/comments/bxlaci/cast_int64_tensor_to_float32_in_pretrained_model/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bxlaci/cast_int64_tensor_to_float32_in_pretrained_model/,7135,1559851417.0,0,,False,,,,,,,,
873,,pytorch,"Hello everyone,

I have a model that does a text generation on character level.

So far, I have been saving and loading the training models like this:

    torch.save(model, 'model')
    
    model = torch.load('model')
    model.eval()

and it has been fine.

Now I am trying to do the same thing using the state_dict because it is more convenient but the text being generated during inference looks like a complete random text without any meaning.

The way I save and load the state dict is:


    torch.save(model.state_dict(), 'model_rnn.pt')
    
    model = RNN(tokens=chars, hidden_size=hidden_size,  n_layers=n_layers, dropout_prob=dropout_prob).to(device)
    model.load_state_dict(torch.load('model_rnn.pt', map_location='cpu'))
    model.eval()


I know that setting the model for evaluation mode is important because dropout shouldn’t be used during eval and I have that.

Here is some example of the text when I save and load the whole model:


    Ти ми стои верност се појаваш
    сам си ми се проклето сега
    и пак да стави светот на порти

and here is an example with state_dict:

  
    о2Е’лС’кокоЊХсьоTьiпоМьВьоџоЊьоЏХ’к’TP2СДоСьоЊџоЊzп́’z’PоTџz’оT’Э=ДЦДP2K’лоШДzьоTьоШХ’лџокьzџШльС2Сџд


I know most of you can’t read Cyrillic but still it is easy to see the difference.

Does anyone know what might be the problem?",t2_b7l557y,False,,0,False,Different results during inference,[],r/pytorch,False,6,,0,,,False,t3_bxf4bs,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1559818572.0,,[],{},,,True,,1559845542.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;I have a model that does a text generation on character level.&lt;/p&gt;

&lt;p&gt;So far, I have been saving and loading the training models like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;torch.save(model, &amp;#39;model&amp;#39;)

model = torch.load(&amp;#39;model&amp;#39;)
model.eval()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and it has been fine.&lt;/p&gt;

&lt;p&gt;Now I am trying to do the same thing using the state_dict because it is more convenient but the text being generated during inference looks like a complete random text without any meaning.&lt;/p&gt;

&lt;p&gt;The way I save and load the state dict is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;torch.save(model.state_dict(), &amp;#39;model_rnn.pt&amp;#39;)

model = RNN(tokens=chars, hidden_size=hidden_size,  n_layers=n_layers, dropout_prob=dropout_prob).to(device)
model.load_state_dict(torch.load(&amp;#39;model_rnn.pt&amp;#39;, map_location=&amp;#39;cpu&amp;#39;))
model.eval()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I know that setting the model for evaluation mode is important because dropout shouldn’t be used during eval and I have that.&lt;/p&gt;

&lt;p&gt;Here is some example of the text when I save and load the whole model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Ти ми стои верност се појаваш
сам си ми се проклето сега
и пак да стави светот на порти
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and here is an example with state_dict:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;о2Е’лС’кокоЊХсьоTьiпоМьВьоџоЊьоЏХ’к’TP2СДоСьоЊџоЊzп́’z’PоTџz’оT’Э=ДЦДP2K’лоШДzьоTьоШХ’лџокьzџШльС2Сџд
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I know most of you can’t read Cyrillic but still it is easy to see the difference.&lt;/p&gt;

&lt;p&gt;Does anyone know what might be the problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bxf4bs,True,,tetrix994,,0,True,all_ads,False,[],False,,/r/pytorch/comments/bxf4bs/different_results_during_inference/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bxf4bs/different_results_during_inference/,7135,1559816742.0,0,,False,,,,,,,,
874,,pytorch,"I've installed pytorch by running `pip3 install torch torchvision`. But pytorch is not importing in jupyter notebook or in python interpreter.

&amp;#x200B;

```
---&gt; 79 from torch._C import *
     80 
     81 __all__ += [name for name in dir(_C)

ImportError: dlopen(/usr/local/lib/python3.6/site-packages/torch/_C.cpython-36m-darwin.so, 9): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: /usr/local/lib/python3.6/site-packages/torch/lib/libshm.dylib
  Reason: image not found

```
&amp;#x200B;

On pytorch github page [this issue](https://github.com/pytorch/pytorch/issues/4518) is mentioned but there is not any working solution yet.

Edit: Seems like there is bug in new version of pytorch for mac. I have installed previous version and this solved my issue.",t2_90mugpg,False,,0,False,"Error (from torch._C import *, image not found) while importing pytorch.",[],r/pytorch,False,6,,0,,,False,t3_bxealw,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1560059649.0,,[],{},self,,True,,1559838566.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve installed pytorch by running &lt;code&gt;pip3 install torch torchvision&lt;/code&gt;. But pytorch is not importing in jupyter notebook or in python interpreter.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;```
---&amp;gt; 79 from torch.&lt;em&gt;C import *
     80 
     81 __all&lt;/em&gt;_ += [name for name in dir(_C)&lt;/p&gt;

&lt;p&gt;ImportError: dlopen(/usr/local/lib/python3.6/site-packages/torch/_C.cpython-36m-darwin.so, 9): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib
  Referenced from: /usr/local/lib/python3.6/site-packages/torch/lib/libshm.dylib
  Reason: image not found&lt;/p&gt;

&lt;p&gt;```
&amp;#x200B;&lt;/p&gt;

&lt;p&gt;On pytorch github page &lt;a href=""https://github.com/pytorch/pytorch/issues/4518""&gt;this issue&lt;/a&gt; is mentioned but there is not any working solution yet.&lt;/p&gt;

&lt;p&gt;Edit: Seems like there is bug in new version of pytorch for mac. I have installed previous version and this solved my issue.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?auto=webp&amp;s=08c43430ff8793a25b767aa172468acb35be611a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4adad794ed3c0e2f76e2da992ae9eef8c074cae4', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5b3cb400fea60e0dfe87ebc5314f7db8016f8ab', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f149723b636ef3087022ddc1efa6ab53e3d4bc69', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_YPUxJ8AyYwgpO9r-tD7JUoQ-Nx8IlATLoEfmjW48iQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bxealw,True,,SuggestAnyName,,1,True,all_ads,False,[],False,,/r/pytorch/comments/bxealw/error_from_torch_c_import_image_not_found_while/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bxealw/error_from_torch_c_import_image_not_found_while/,7135,1559809766.0,0,,False,,,,,,,,
875,,pytorch,"I'm currently using PyCharm with the Anaconda plugin to use their great completion along with the convenience of Jupyter notebooks, but I'm starting to get tired of the program crashing at least 3 times per day. Just wondering - what does everyone use when doing NN experiments, and why? 

I've considered going all in and using emacs with the EIN plugin - completion has saved me so much time recently, and I don't think I could go back to pure notebooks.",,False,,0,False,What does your pytorch / nn dev / research workflow look like?,[],r/pytorch,False,6,,0,,,False,t3_bx2j69,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,,self,False,,,{},,,True,,1559769955.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently using PyCharm with the Anaconda plugin to use their great completion along with the convenience of Jupyter notebooks, but I&amp;#39;m starting to get tired of the program crashing at least 3 times per day. Just wondering - what does everyone use when doing NN experiments, and why? &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve considered going all in and using emacs with the EIN plugin - completion has saved me so much time recently, and I don&amp;#39;t think I could go back to pure notebooks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bx2j69,True,,[deleted],,5,True,all_ads,False,[],,dark,/r/pytorch/comments/bx2j69/what_does_your_pytorch_nn_dev_research_workflow/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bx2j69/what_does_your_pytorch_nn_dev_research_workflow/,7135,1559741155.0,0,,False,,,,,,,,
876,,pytorch,,t2_1ffz9tjt,False,,0,False,[AI application] Let your machine play Super Mario Bros!,[],r/pytorch,False,6,,0,131.0,,False,t3_bvj0px,False,dark,0.91,,public,16,0,{},140.0,,False,[],,True,False,,{},,False,16,,False,https://b.thumbs.redditmedia.com/0WMsnXAKBl4u8tpyGsdJF57izTtLqw_jfkjb-FRr4dk.jpg,False,,[],{},image,,False,,1559410035.0,text,6,,,text,i.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/tucx85xgrp131.gif?format=png8&amp;s=ff83e4e453277c79316ff9de497dbaebece9cb62', 'width': 256, 'height': 240}, 'resolutions': [{'url': 'https://preview.redd.it/tucx85xgrp131.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=acbe117460d754c57a09ebdf41cdda3f005e1f1c', 'width': 108, 'height': 101}, {'url': 'https://preview.redd.it/tucx85xgrp131.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=66736a03894b74a42fad1664169ca4f4b447f6bc', 'width': 216, 'height': 202}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/tucx85xgrp131.gif?s=2bc8cfb4eb0054da0872ab61083254f3787817c0', 'width': 256, 'height': 240}, 'resolutions': [{'url': 'https://preview.redd.it/tucx85xgrp131.gif?width=108&amp;crop=smart&amp;s=0b2bbcca104366b91b7774df371ef60bd9e569e1', 'width': 108, 'height': 101}, {'url': 'https://preview.redd.it/tucx85xgrp131.gif?width=216&amp;crop=smart&amp;s=c938c6ebcb537efbae7f42528e319f01f9f2d25d', 'width': 216, 'height': 202}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/tucx85xgrp131.gif?format=mp4&amp;s=6dfa96848bed992cb73ea6f9cb12c40bb7ce9d6c', 'width': 256, 'height': 240}, 'resolutions': [{'url': 'https://preview.redd.it/tucx85xgrp131.gif?width=108&amp;format=mp4&amp;s=4f873f92dd64c3929c5b28d7f1d210828ecfe7c2', 'width': 108, 'height': 101}, {'url': 'https://preview.redd.it/tucx85xgrp131.gif?width=216&amp;format=mp4&amp;s=8f761a3f94b57202749ff4d9d4c33eb6eb02e70c', 'width': 216, 'height': 202}]}}, 'id': 'byEpi2FCB4kVW2Z09c3v3TZ-fIh99OS2sbhSPJaoDGU'}], 'enabled': True}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bvj0px,True,,1991viet,,2,True,all_ads,False,[],False,,/r/pytorch/comments/bvj0px/ai_application_let_your_machine_play_super_mario/,all_ads,False,https://i.redd.it/tucx85xgrp131.gif,7135,1559381235.0,0,,False,https://i.redd.it/tucx85xgrp131.gif,,,,,,,
877,,pytorch,"I am trying to create a simple convolutional autoencoder, but I'm having a very hard time getting it to work! 

As far as I know, for every pooling operation in the encoder section, we should have a TransposedConv/Upsampling layer in the decoder! 

I have 4 Convolutional layers + pooling in the encoder section, therefore I created 4 ConvTranspose2d layers in the decoder! yet it complains about the size of the decoder that is 16 and not 28! 

This is the original code : 

    class ConvAutoEncoder(torch.nn.Module):
        def __init__(self, embedsize=4):
            super().__init__()
            
            # encoder 
            self.conv1 = torch.nn.Conv2d(1, 32, 3,padding=1)
            self.conv2 = torch.nn.Conv2d(32, 16, 3,padding=1)
            self.conv3 = torch.nn.Conv2d(16, 8, 3,padding=1)
            self.conv4 = torch.nn.Conv2d(8 , embedsize, 3,padding=1)
            self.pool = torch.nn.MaxPool2d(2,2)
            
            # decoder
            self.t_conv1 = torch.nn.ConvTranspose2d(embedsize, 8, 2, stride=2)
            self.t_conv2 = torch.nn.ConvTranspose2d(8, 16, 2, stride=2)
            self.t_conv3 = torch.nn.ConvTranspose2d(16, 32, 2, stride=2)
            self.t_conv4 = torch.nn.ConvTranspose2d(32, 1, 2, stride=2)
             
        def forward(self, x): 
            # encoder
            x = self.pool(F.relu(self.conv1(x)))
            x = self.pool(F.relu(self.conv2(x)))
            x = self.pool(F.relu(self.conv3(x)))
            x = self.pool(F.relu(self.conv4(x)))
            print(x.size())
            #decoder 
            x = F.relu(self.t_conv1(x))
            x = F.relu(self.t_conv2(x))
            x = F.relu(self.t_conv3(x))
            x = F.relu(self.t_conv4(x))
            # since our output needs to be an image
            # the values should be between 0,1 so we
            # use sigmoid here!
            x = F.sigmoid(self.t_conv4(x))
            print(x.size())
            return x 
    

Now the only way I can get this to run is to add a new upsampling layer and another convolutional layer at the end so that the sizes match! 

    class ConvAutoEncoder(torch.nn.Module):
        def __init__(self, embedsize=4):
            super().__init__()
            
            #encoder 
            self.conv1 = torch.nn.Conv2d(1, 32, 3,padding=1)
            self.conv2 = torch.nn.Conv2d(32, 16, 3,padding=1)
            self.conv3 = torch.nn.Conv2d(16, 8, 3,padding=1)
            self.conv4 = torch.nn.Conv2d(8 , embedsize, 3,padding=1)
            self.pool = torch.nn.MaxPool2d(2,2)
            
            # decoder
            self.t_conv1 = torch.nn.ConvTranspose2d(embedsize, 8, 2, stride=2)
            self.t_conv2 = torch.nn.ConvTranspose2d(8, 16, 2, stride=2)
            # an additional upsampling layer for making the image lager!
            self.t_conv3 = torch.nn.ConvTranspose2d(16, 16, 2, stride=2)
            self.t_conv4 = torch.nn.ConvTranspose2d(16, 32, 2, stride=2)
            self.t_conv5 = torch.nn.ConvTranspose2d(32, 1, 2, stride=2)
            # an additional conv layer for reducing the size! 
            self.conv5 = torch.nn.Conv2d(1,1,5,stride=1)
             
        def forward(self, x): 
            # encoder
            x = self.pool(F.relu(self.conv1(x)))
            x = self.pool(F.relu(self.conv2(x)))
            x = self.pool(F.relu(self.conv3(x)))
            x = self.pool(F.relu(self.conv4(x)))
    
            #decoder 
            x = F.relu(self.t_conv1(x))
            x = F.relu(self.t_conv2(x))
            x = F.relu(self.t_conv3(x))
            x = F.relu(self.t_conv4(x))
            # since our output needs to be an image
            # the values should be between 0,1 so we
            # use sigmoid here!
            x = F.sigmoid(self.conv5(self.t_conv5(x)))
            
            return x 

But why? What am I missing here? shouldnt the number of layers in encoder and decoder be the same? if so How do you do it without adding new layers ? 

Thanks in advance",t2_12rvgcg3,False,,0,False,Why doesnt my simple convolutional autoencoder work?,[],r/pytorch,False,6,,0,,,False,t3_bvi392,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1559400909.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to create a simple convolutional autoencoder, but I&amp;#39;m having a very hard time getting it to work! &lt;/p&gt;

&lt;p&gt;As far as I know, for every pooling operation in the encoder section, we should have a TransposedConv/Upsampling layer in the decoder! &lt;/p&gt;

&lt;p&gt;I have 4 Convolutional layers + pooling in the encoder section, therefore I created 4 ConvTranspose2d layers in the decoder! yet it complains about the size of the decoder that is 16 and not 28! &lt;/p&gt;

&lt;p&gt;This is the original code : &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class ConvAutoEncoder(torch.nn.Module):
    def __init__(self, embedsize=4):
        super().__init__()

        # encoder 
        self.conv1 = torch.nn.Conv2d(1, 32, 3,padding=1)
        self.conv2 = torch.nn.Conv2d(32, 16, 3,padding=1)
        self.conv3 = torch.nn.Conv2d(16, 8, 3,padding=1)
        self.conv4 = torch.nn.Conv2d(8 , embedsize, 3,padding=1)
        self.pool = torch.nn.MaxPool2d(2,2)

        # decoder
        self.t_conv1 = torch.nn.ConvTranspose2d(embedsize, 8, 2, stride=2)
        self.t_conv2 = torch.nn.ConvTranspose2d(8, 16, 2, stride=2)
        self.t_conv3 = torch.nn.ConvTranspose2d(16, 32, 2, stride=2)
        self.t_conv4 = torch.nn.ConvTranspose2d(32, 1, 2, stride=2)

    def forward(self, x): 
        # encoder
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = self.pool(F.relu(self.conv4(x)))
        print(x.size())
        #decoder 
        x = F.relu(self.t_conv1(x))
        x = F.relu(self.t_conv2(x))
        x = F.relu(self.t_conv3(x))
        x = F.relu(self.t_conv4(x))
        # since our output needs to be an image
        # the values should be between 0,1 so we
        # use sigmoid here!
        x = F.sigmoid(self.t_conv4(x))
        print(x.size())
        return x 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the only way I can get this to run is to add a new upsampling layer and another convolutional layer at the end so that the sizes match! &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class ConvAutoEncoder(torch.nn.Module):
    def __init__(self, embedsize=4):
        super().__init__()

        #encoder 
        self.conv1 = torch.nn.Conv2d(1, 32, 3,padding=1)
        self.conv2 = torch.nn.Conv2d(32, 16, 3,padding=1)
        self.conv3 = torch.nn.Conv2d(16, 8, 3,padding=1)
        self.conv4 = torch.nn.Conv2d(8 , embedsize, 3,padding=1)
        self.pool = torch.nn.MaxPool2d(2,2)

        # decoder
        self.t_conv1 = torch.nn.ConvTranspose2d(embedsize, 8, 2, stride=2)
        self.t_conv2 = torch.nn.ConvTranspose2d(8, 16, 2, stride=2)
        # an additional upsampling layer for making the image lager!
        self.t_conv3 = torch.nn.ConvTranspose2d(16, 16, 2, stride=2)
        self.t_conv4 = torch.nn.ConvTranspose2d(16, 32, 2, stride=2)
        self.t_conv5 = torch.nn.ConvTranspose2d(32, 1, 2, stride=2)
        # an additional conv layer for reducing the size! 
        self.conv5 = torch.nn.Conv2d(1,1,5,stride=1)

    def forward(self, x): 
        # encoder
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = self.pool(F.relu(self.conv4(x)))

        #decoder 
        x = F.relu(self.t_conv1(x))
        x = F.relu(self.t_conv2(x))
        x = F.relu(self.t_conv3(x))
        x = F.relu(self.t_conv4(x))
        # since our output needs to be an image
        # the values should be between 0,1 so we
        # use sigmoid here!
        x = F.sigmoid(self.conv5(self.t_conv5(x)))

        return x 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But why? What am I missing here? shouldnt the number of layers in encoder and decoder be the same? if so How do you do it without adding new layers ? &lt;/p&gt;

&lt;p&gt;Thanks in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bvi392,True,,MasterSama,,2,True,all_ads,False,[],False,,/r/pytorch/comments/bvi392/why_doesnt_my_simple_convolutional_autoencoder/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bvi392/why_doesnt_my_simple_convolutional_autoencoder/,7135,1559372109.0,0,,False,,,,,,,,
878,,pytorch,"[https://github.com/kakaobrain/torchgpipe](https://github.com/kakaobrain/torchgpipe)

Kakao Brain announces torchgpipe, an implementation of GPipe in PyTorch as a handy library.

GPipe is a scalable pipeline parallelism library published by Google Brain. It leverages the training of a giant model which requires much memory. For instance, Google trained AmoebaNet-B with 557M parameters over GPipe.",t2_8wh67,False,,0,False,"torchgpipe, A GPipe implementation in PyTorch",[],r/pytorch,False,6,,0,,,False,t3_bubkgb,False,dark,0.93,,public,11,0,{},,,False,[],,False,False,,{},,False,11,,False,self,False,,[],{},self,,True,,1559142956.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://github.com/kakaobrain/torchgpipe""&gt;https://github.com/kakaobrain/torchgpipe&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Kakao Brain announces torchgpipe, an implementation of GPipe in PyTorch as a handy library.&lt;/p&gt;

&lt;p&gt;GPipe is a scalable pipeline parallelism library published by Google Brain. It leverages the training of a giant model which requires much memory. For instance, Google trained AmoebaNet-B with 557M parameters over GPipe.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/gL2e-kJQqfg2R57Qiv8iuj_GyvNqG_Xn6MEgxDSR02c.jpg?auto=webp&amp;s=540afe9364e44b208abf82a50f7e2f00329c5b77', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/gL2e-kJQqfg2R57Qiv8iuj_GyvNqG_Xn6MEgxDSR02c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d4d4491116aed37e6dc3ce0715e59bf50b50f53', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/gL2e-kJQqfg2R57Qiv8iuj_GyvNqG_Xn6MEgxDSR02c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=996c2c6e4fa747c696d4860e5fa153204b435598', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/gL2e-kJQqfg2R57Qiv8iuj_GyvNqG_Xn6MEgxDSR02c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0dc0de2449eff76c97540bab00002386ffa819e', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'zWKPcfY1D1N0uzYay525PVuAaGJ32v8AqUb1yjN8dQg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bubkgb,True,,sublee,,0,True,all_ads,False,[],False,,/r/pytorch/comments/bubkgb/torchgpipe_a_gpipe_implementation_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bubkgb/torchgpipe_a_gpipe_implementation_in_pytorch/,7135,1559114156.0,0,,False,,,,,,,,
879,,pytorch,,t2_12i1bz,False,,0,False,"discounted ebooks about machine learning ""Artificial Intelligence by Packt"" (Humble Book Bundle), valid until 2019-05-27 18:00 UTC",[],r/pytorch,False,6,,0,73.0,,False,t3_bsyn23,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://a.thumbs.redditmedia.com/slg98FyNHxc7D-1cipqBtxhHPg3upYn_9QBIkWoGl78.jpg,False,,[],{},link,,False,,1558842165.0,text,6,,,text,humblebundle.com,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Q-Vm7zL4d3-WkKJ4psaFlX3tWmmCUYKYwRCapdjxBkI.jpg?auto=webp&amp;s=aca83f15151423cfb4e33d0298a17bc2b02cbd4d', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/Q-Vm7zL4d3-WkKJ4psaFlX3tWmmCUYKYwRCapdjxBkI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7a45eaeb2d897e55503b33ec43d7745dbce9afe9', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/Q-Vm7zL4d3-WkKJ4psaFlX3tWmmCUYKYwRCapdjxBkI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8833ceb62ddbea5e1bcfc7d94cc8af30c6a798ac', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/Q-Vm7zL4d3-WkKJ4psaFlX3tWmmCUYKYwRCapdjxBkI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fca971845facdb02f67974d616317555159055ce', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/Q-Vm7zL4d3-WkKJ4psaFlX3tWmmCUYKYwRCapdjxBkI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=93fb6d940db5c9378be27b70a5f17aa8eaceeb7f', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/Q-Vm7zL4d3-WkKJ4psaFlX3tWmmCUYKYwRCapdjxBkI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1fb84238a4f37426c160f3d290904434248ebc33', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/Q-Vm7zL4d3-WkKJ4psaFlX3tWmmCUYKYwRCapdjxBkI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ab02ae23e78bd57f67aced121d5bbd7f74cc671b', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'eqTOpjcFTXkVLFYOP45bU7FfXRejXkEi2G6OtcjALd8'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bsyn23,True,,101testing,,2,True,all_ads,False,[],False,,/r/pytorch/comments/bsyn23/discounted_ebooks_about_machine_learning/,all_ads,False,https://www.humblebundle.com/books/artificial-intelligence-deep-learning-books,7135,1558813365.0,0,,False,https://www.humblebundle.com/books/artificial-intelligence-deep-learning-books,,,,,,,
880,,pytorch,I am unable to find any good tutorials which help build dataset loaders with architecture like these https://github.com/pytorch/text/blob/master/torchtext/datasets/imdb.py,t2_15p51r,False,,0,False,Any recommended resources on how to extend torchtext for loading own datasets?,[],r/pytorch,False,6,,0,,,False,t3_bs8nso,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},self,,True,,1558676272.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am unable to find any good tutorials which help build dataset loaders with architecture like these &lt;a href=""https://github.com/pytorch/text/blob/master/torchtext/datasets/imdb.py""&gt;https://github.com/pytorch/text/blob/master/torchtext/datasets/imdb.py&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?auto=webp&amp;s=07ca976f2628a4aeaf3183ce72eec096643e840f', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=950ea7a0f777157bbd0b8745b20e06aa336dd923', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4153bf0d485e12346f9dd8c386a31de9512595a0', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=50e63f6ec38baa90825c802fb91a3cac2c8de8d3', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_YPUxJ8AyYwgpO9r-tD7JUoQ-Nx8IlATLoEfmjW48iQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bs8nso,True,,pascaltuna,,5,True,all_ads,False,[],False,,/r/pytorch/comments/bs8nso/any_recommended_resources_on_how_to_extend/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bs8nso/any_recommended_resources_on_how_to_extend/,7135,1558647472.0,0,,False,,,,,,,,
881,,pytorch,"When creating an \`nn.Module\` in Pytorch, I have seen both of them being used, but whats the difference between them and which one is  the correct way of doing it? 

Thanks a lot in advance",t2_12rvgcg3,False,,0,False,"Whats the difference between super().__init__() and super(Network, self).__init__() in pytorch?",[],r/pytorch,False,6,,0,,,False,t3_brhq55,False,dark,0.81,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1558511614.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When creating an `nn.Module` in Pytorch, I have seen both of them being used, but whats the difference between them and which one is  the correct way of doing it? &lt;/p&gt;

&lt;p&gt;Thanks a lot in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,brhq55,True,,MasterSama,,2,True,all_ads,False,[],False,,/r/pytorch/comments/brhq55/whats_the_difference_between_super_init_and/,all_ads,False,https://www.reddit.com/r/pytorch/comments/brhq55/whats_the_difference_between_super_init_and/,7135,1558482814.0,0,,False,,,,,,,,
882,,pytorch,"Facebook just open sourced a new framework called Pythia for multitask learning in vision and language domains. Pythia is built on top of PyTorch. Here are a few resources:

\- Blogpost: [https://code.fb.com/ai-research/pythia/](https://code.fb.com/ai-research/pythia/)

\- GitHub: [https://github.com/facebookresearch/pythia](https://github.com/facebookresearch/pythia)

&amp;#x200B;

They've been releasing a lot of these new PyTorch based frameworks recently. First BoTorch + Ax and now this.",t2_15r2w7zy,False,,0,False,Pythia: open-source framework for multimodal AI models,[],r/pytorch,False,6,,0,,,False,t3_brcecs,False,dark,1.0,,public,10,0,{},,,False,[],,False,False,,{},,False,10,,False,self,False,,[],{},self,,True,,1558484377.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Facebook just open sourced a new framework called Pythia for multitask learning in vision and language domains. Pythia is built on top of PyTorch. Here are a few resources:&lt;/p&gt;

&lt;p&gt;- Blogpost: &lt;a href=""https://code.fb.com/ai-research/pythia/""&gt;https://code.fb.com/ai-research/pythia/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;- GitHub: &lt;a href=""https://github.com/facebookresearch/pythia""&gt;https://github.com/facebookresearch/pythia&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;They&amp;#39;ve been releasing a lot of these new PyTorch based frameworks recently. First BoTorch + Ax and now this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Au_Gf8rACpaXCREPfgHIjEcsb4Icksdl3e0AZDUs5Jo.jpg?auto=webp&amp;s=4b42a09ced6125e036e36b244ef3e87ab4a71f1b', 'width': 2000, 'height': 1125}, 'resolutions': [{'url': 'https://external-preview.redd.it/Au_Gf8rACpaXCREPfgHIjEcsb4Icksdl3e0AZDUs5Jo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=925969347400912bd84bde0761ba3a5df922d8f1', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/Au_Gf8rACpaXCREPfgHIjEcsb4Icksdl3e0AZDUs5Jo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb268a326d7f94025cfd26481a920976396f397b', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/Au_Gf8rACpaXCREPfgHIjEcsb4Icksdl3e0AZDUs5Jo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aeefbd4fe44f41b853a841659720e19773f8084f', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/Au_Gf8rACpaXCREPfgHIjEcsb4Icksdl3e0AZDUs5Jo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a16e934229a434111f7ffe9bfa6463ebb69b2f10', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/Au_Gf8rACpaXCREPfgHIjEcsb4Icksdl3e0AZDUs5Jo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bb94801d8e597b541f5cd1a9e4f73b7e10c2a11a', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/Au_Gf8rACpaXCREPfgHIjEcsb4Icksdl3e0AZDUs5Jo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b13de8ae0638e454e224cdb716c0ebb74035fa89', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'Bomw2ryifdGsC_A0CB0X_JiWQgpPsj51iRtwImMrxr8'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,brcecs,True,,iyaja,,0,True,all_ads,False,[],False,,/r/pytorch/comments/brcecs/pythia_opensource_framework_for_multimodal_ai/,all_ads,False,https://www.reddit.com/r/pytorch/comments/brcecs/pythia_opensource_framework_for_multimodal_ai/,7135,1558455577.0,0,,False,,,,,,,,
883,,pytorch,"Hello,  
I have a question regarding the save/load procedures described here:  
[https://pytorch.org/tutorials/beginner/saving\_loading\_models.html](https://pytorch.org/tutorials/beginner/saving_loading_models.html)

I want to know if i am able to:  
\-Train a model on some data (lots of general domain data like movie reviews, product reviews, etc, that are associated with positive/negative classes)  
\-Save it  
\-Load the pre-trained model  
\-Train again with less data ( but task-specific like hate speech where the classes might be offensive/not-offensive ) and fine tune it by using a lower learning rate and try other techniques as well.

So, after I load my pre-trained model, can I start training again and modify it? With the procedures described in the link i mentioned above.

Thank you very much!",t2_2ytufbkp,False,,0,False,Can I save my own pre-trained models and fine tune them later?,[],r/pytorch,False,6,,0,,,False,t3_brjpjp,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1558523732.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;br/&gt;
I have a question regarding the save/load procedures described here:&lt;br/&gt;
&lt;a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html""&gt;https://pytorch.org/tutorials/beginner/saving_loading_models.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I want to know if i am able to:&lt;br/&gt;
-Train a model on some data (lots of general domain data like movie reviews, product reviews, etc, that are associated with positive/negative classes)&lt;br/&gt;
-Save it&lt;br/&gt;
-Load the pre-trained model&lt;br/&gt;
-Train again with less data ( but task-specific like hate speech where the classes might be offensive/not-offensive ) and fine tune it by using a lower learning rate and try other techniques as well.&lt;/p&gt;

&lt;p&gt;So, after I load my pre-trained model, can I start training again and modify it? With the procedures described in the link i mentioned above.&lt;/p&gt;

&lt;p&gt;Thank you very much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,brjpjp,True,,the_parallax_II,,2,True,all_ads,False,[],False,,/r/pytorch/comments/brjpjp/can_i_save_my_own_pretrained_models_and_fine_tune/,all_ads,False,https://www.reddit.com/r/pytorch/comments/brjpjp/can_i_save_my_own_pretrained_models_and_fine_tune/,7135,1558494932.0,0,,False,,,,,,,,
884,,pytorch,,t2_32wgy,False,,0,False,Optimizing Steering Car Paths with PyTorch,[],r/pytorch,False,6,,0,105.0,,False,t3_brbbrk,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://a.thumbs.redditmedia.com/0lzt3V39AS1FilRyzNekFIgBVpI0uqpTexkopdLUzB4.jpg,False,,[],{},link,,False,,1558479036.0,text,6,,,text,blog.benwiener.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/3JmBGel1ZMQi91Lr1Y-Qz1QQymd_-vRdlfcfO0D-7DU.jpg?auto=webp&amp;s=62b43d9a42a85c8520751662be74a53a4050e296', 'width': 640, 'height': 480}, 'resolutions': [{'url': 'https://external-preview.redd.it/3JmBGel1ZMQi91Lr1Y-Qz1QQymd_-vRdlfcfO0D-7DU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=653ac1c482df7231a4a38139ee7302b3c7d09d75', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/3JmBGel1ZMQi91Lr1Y-Qz1QQymd_-vRdlfcfO0D-7DU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=623935a21067dce4dcad6a16f9d9273fefcc9600', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/3JmBGel1ZMQi91Lr1Y-Qz1QQymd_-vRdlfcfO0D-7DU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9113195ea6a1214972ac0398f78d94e84e96a31', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/3JmBGel1ZMQi91Lr1Y-Qz1QQymd_-vRdlfcfO0D-7DU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e25330f69ea6c0e491e907512ada3bf6bf94dd3a', 'width': 640, 'height': 480}], 'variants': {}, 'id': '2ei5iK7Jpp0G8vVvGlG3CdSHzeGpxZpGhPamhaqEDl0'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,brbbrk,True,,ishmandoo,,5,True,all_ads,False,[],False,,/r/pytorch/comments/brbbrk/optimizing_steering_car_paths_with_pytorch/,all_ads,False,http://blog.benwiener.com/programming/2019/05/14/steering-car.html,7135,1558450236.0,0,,False,http://blog.benwiener.com/programming/2019/05/14/steering-car.html,,,,,,,
885,,pytorch,,t2_3t1b85je,False,,0,False,Self Driving Car with Jetson Nano and Pytorch,[],r/pytorch,False,6,,0,78.0,,False,t3_br2nt8,False,dark,0.91,,public,21,0,{},140.0,,False,[],"{'reddit_video': {'fallback_url': 'https://v.redd.it/v70lbko25gz21/DASH_1080?source=fallback', 'height': 1080, 'width': 1920, 'scrubber_media_url': 'https://v.redd.it/v70lbko25gz21/DASH_96', 'dash_url': 'https://v.redd.it/v70lbko25gz21/DASHPlaylist.mpd?a=1618044323%2CYmNmZWZhMzc3MTE5M2Y0MzBhYTJiYzU4MGFlYmIxMTYyNDRiYzQ5MDhmNmE0MGIzZTM5N2YzNjY2Y2U3NzRhMg%3D%3D&amp;v=1&amp;f=sd', 'duration': 41, 'hls_url': 'https://v.redd.it/v70lbko25gz21/HLSPlaylist.m3u8?a=1618044323%2CY2Q4NDIzZGNjMjIxNzJlNmVmYTlmYmUzOGIyMzM4YjEwODg2OTdkZDA2YzA3NTc5MjI0NzZmMzEwNjc0YTc4OA%3D%3D&amp;v=1&amp;f=sd', 'is_gif': False, 'transcoding_status': 'completed'}}",True,False,,{},,False,21,,True,https://a.thumbs.redditmedia.com/I_tjBURTtt361Ns6JVdgNbF7-udbHKCndUIsAwWVng8.jpg,False,,[],{},hosted:video,,False,,1558421936.0,text,6,,,text,v.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/FF1JGn9ABNc8aoywtv0AllhWuq1Im0ssavULqHp_yow.png?format=pjpg&amp;auto=webp&amp;s=47a12161a54d3010b13ca1e25bc85b0faecb6dd5', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://external-preview.redd.it/FF1JGn9ABNc8aoywtv0AllhWuq1Im0ssavULqHp_yow.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=22278708f5968a5906e2bced4e6e751ead93c4bc', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/FF1JGn9ABNc8aoywtv0AllhWuq1Im0ssavULqHp_yow.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7109bd1a4b1b5b6bbf38463746d9a91564f600df', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/FF1JGn9ABNc8aoywtv0AllhWuq1Im0ssavULqHp_yow.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d4cb9d7a8ba629b09b7fab938af0acc1b71dfee2', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/FF1JGn9ABNc8aoywtv0AllhWuq1Im0ssavULqHp_yow.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dc1f1538f8f7aaa5a3191f49c69f46584685e149', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/FF1JGn9ABNc8aoywtv0AllhWuq1Im0ssavULqHp_yow.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6051daf68af4f01e8288ae8f24cec27e818caf27', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/FF1JGn9ABNc8aoywtv0AllhWuq1Im0ssavULqHp_yow.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=aead91b902e95235830924fb96155ed4fa54847b', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'T3vr57kZ8OEB8IEa-to1yeC5TfWze7EAhQsb1hP9B1M'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,br2nt8,True,,thinking_computer,,3,True,all_ads,False,[],False,,/r/pytorch/comments/br2nt8/self_driving_car_with_jetson_nano_and_pytorch/,all_ads,False,https://v.redd.it/v70lbko25gz21,7135,1558393136.0,0,"{'reddit_video': {'fallback_url': 'https://v.redd.it/v70lbko25gz21/DASH_1080?source=fallback', 'height': 1080, 'width': 1920, 'scrubber_media_url': 'https://v.redd.it/v70lbko25gz21/DASH_96', 'dash_url': 'https://v.redd.it/v70lbko25gz21/DASHPlaylist.mpd?a=1618044323%2CYmNmZWZhMzc3MTE5M2Y0MzBhYTJiYzU4MGFlYmIxMTYyNDRiYzQ5MDhmNmE0MGIzZTM5N2YzNjY2Y2U3NzRhMg%3D%3D&amp;v=1&amp;f=sd', 'duration': 41, 'hls_url': 'https://v.redd.it/v70lbko25gz21/HLSPlaylist.m3u8?a=1618044323%2CY2Q4NDIzZGNjMjIxNzJlNmVmYTlmYmUzOGIyMzM4YjEwODg2OTdkZDA2YzA3NTc5MjI0NzZmMzEwNjc0YTc4OA%3D%3D&amp;v=1&amp;f=sd', 'is_gif': False, 'transcoding_status': 'completed'}}",True,https://v.redd.it/v70lbko25gz21,,,,,,,
886,,pytorch,,t2_1ffz9tjt,False,,0,False,[AI application] Let your machine teach itself to play flappy bird!,[],r/pytorch,False,6,,0,87.0,,False,t3_bnq33q,False,dark,0.79,,public,13,0,{},140.0,,False,[],,True,False,,{},,False,13,,False,https://a.thumbs.redditmedia.com/VA4o2a13ncRqmAIJnqdF5I4T8lXr7TWBP7tx06CGh-4.jpg,False,,[],{},image,,False,,1557703358.0,text,6,,,text,i.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/ezlh50znssx21.gif?format=png8&amp;s=fb0b33445199f80afec2a608303cb37a23fd4a56', 'width': 1440, 'height': 900}, 'resolutions': [{'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=167b49b18fa048c421705559814d635c2c2a5590', 'width': 108, 'height': 67}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=9511d88daa23e0b0088c61ce79b56bb8c7719a48', 'width': 216, 'height': 135}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=d01e06dbc9482a2ad00632021b042ad3a3fc3d01', 'width': 320, 'height': 200}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=11c0fd357f0c42a9ef2a04a9180c191d757be836', 'width': 640, 'height': 400}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=f3dc2c9bf90938017bc6582554c1a6c0832478b7', 'width': 960, 'height': 600}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=0ec04bf7712342af1db2e6a4e7cbbafc3edc615d', 'width': 1080, 'height': 675}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/ezlh50znssx21.gif?s=0c119e8a50f50b86983cea17a32dbee21abddc36', 'width': 1440, 'height': 900}, 'resolutions': [{'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=108&amp;crop=smart&amp;s=c9fbdee2513617c0e851cf3100d35fe8625a2fb7', 'width': 108, 'height': 67}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=216&amp;crop=smart&amp;s=4e4f1c9cd5179348b887f410a89e16fcfc500a09', 'width': 216, 'height': 135}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=320&amp;crop=smart&amp;s=231f8445263a3574ac1bce6e8712e7ec4f721f8e', 'width': 320, 'height': 200}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=640&amp;crop=smart&amp;s=d9e24f761ce870c89f1b98d4ca109d738fb8fae9', 'width': 640, 'height': 400}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=960&amp;crop=smart&amp;s=5e7d2bda5299cd54173c0c7f70cc9e2c8adaabd8', 'width': 960, 'height': 600}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=1080&amp;crop=smart&amp;s=da96b6fb4ab8cb9a72a4e2e2e0cf8e51cab12bc1', 'width': 1080, 'height': 675}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/ezlh50znssx21.gif?format=mp4&amp;s=149e92cdebd0333821cb92746ea96a0228783d71', 'width': 1440, 'height': 900}, 'resolutions': [{'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=108&amp;format=mp4&amp;s=b5c5ce386f462d5601f733d2f8ccaedff006efea', 'width': 108, 'height': 67}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=216&amp;format=mp4&amp;s=eca5cbcb3720a65bfdba26dfbee0915ec7398cc1', 'width': 216, 'height': 135}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=320&amp;format=mp4&amp;s=870c2f701d06b820815c454bdfd9bbc32fa3301d', 'width': 320, 'height': 200}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=640&amp;format=mp4&amp;s=e5198e19caee43410b6cbc1db90a771d80a85417', 'width': 640, 'height': 400}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=960&amp;format=mp4&amp;s=b07c3a13b505877014213ffdc8a5e9fd11f21b07', 'width': 960, 'height': 600}, {'url': 'https://preview.redd.it/ezlh50znssx21.gif?width=1080&amp;format=mp4&amp;s=8cfcb80d303dc9af673ab55399d3c153cbd1ec91', 'width': 1080, 'height': 675}]}}, 'id': 'OScJo6hYs2tWbZis7aG6jqebdVzjy-rokjohEHlpIfQ'}], 'enabled': True}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bnq33q,True,,1991viet,,3,True,all_ads,False,[],False,,/r/pytorch/comments/bnq33q/ai_application_let_your_machine_teach_itself_to/,all_ads,False,https://i.redd.it/ezlh50znssx21.gif,7135,1557674558.0,0,,False,https://i.redd.it/ezlh50znssx21.gif,,,,,,,
887,,pytorch,Is there any?,t2_2gjgfxkv,False,,0,False,Comprehensive Pytorch tutorial?,[],r/pytorch,False,6,,0,,,False,t3_bmmbhj,False,dark,0.91,,public,8,0,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},,,True,,1557450277.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there any?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bmmbhj,True,,gecicihesap17,,5,True,all_ads,False,[],False,,/r/pytorch/comments/bmmbhj/comprehensive_pytorch_tutorial/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bmmbhj/comprehensive_pytorch_tutorial/,7135,1557421477.0,0,,False,,,,,,,,
888,,pytorch,"This   repo aims to cover Pytorch details, Pytorch example implementations,   Pytorch sample codes, running Pytorch codes with Google Colab (with K80   GPU/CPU) in a nutshell.

**Tutorial Link:** [**https://github.com/omerbsezer/Fast-Pytorch**](https://github.com/omerbsezer/Fast-Pytorch)

## Table of Contents:

* 🔥[Fast Pytorch Tutorial](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchtutorial)  

   * [Pytorch Playground](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchplayground)  

      * 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/Pytorch_Playground.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/Pytorch_Playground.ipynb)
   * [Model (Neural Network Layers)](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#model)
   * [Optimizer](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#optimizer)
   * [Loss Functions](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#lossfunctions)
   * [Pooling Layers](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#poolinglayers)
   * [Non-linear activation functions](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#nonlinearactivation)
   * [Basic 2 Layer NN](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#example)
* 🔥[Fast Torchvision Tutorial](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#torchvisiontutorial)  

   * [ImageFolder](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#imagefolder)
   * [Transforms](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#transforms)
   * [Datasets](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#datasets)
   * [Models](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#torchvisionmodels)
   * [Utils](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#utils)
* 🔥[Pytorch with Google Colab](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchcolab)
* 🔥[Pytorch Example Implementations](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchexamples)  

   * [MLP](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#mlp)  

      * MLP 1 Class with Binary Cross Entropy (BCE) Loss: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_1class_BinaryCrossEntropyLoss.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_1class_BinaryCrossEntropyLoss.ipynb)
      * MLP 2 Classes with Cross Entropy Loss: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_2class_CrossEntropyLoss.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_2class_CrossEntropyLoss.ipynb)
      * MLP 3-Layer with MNIST Example: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_3layer_MNIST.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_3layer_MNIST.ipynb)
   * [CNN](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#cnn)  

      * CNN with MNIST Example: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/CNN_Mnist.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/CNN_Mnist.ipynb)
      * Improved CNN with MNIST Example: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/Improved_CNN_Mnist.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/Improved_CNN_Mnist.ipynb)
   * [CNN Visualization](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#cnnvisualization)  

      * CNN Visualization: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/CNN_Visualization.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/CNN_Visualization.ipynb)
   * [RNN](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#rnn)  

      * RNN Text Generation: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/RNN_word_embeddings.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/RNN_word_embeddings.ipynb)
   * [Transfer Learning](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#transferlearning)  

      * Transfer Learning Implementation: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/TransferLearning.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/TransferLearning.ipynb)
   * [DCGAN](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#dcgan)  

      * DCGAN Implementation: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/DCGAN.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/DCGAN.ipynb)
   * [ChatBot](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#chatbot)  

      * Chatbot Implementation: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/ChatBot.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/ChatBot.ipynb)
* 🔥[Pytorch Sample Codes](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchsamplecodes)

**Extra:** Reinforcement Learning Tutorial:

[**https://github.com/omerbsezer/Reinforcement\_learning\_tutorial\_with\_demo**](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo)",t2_muiope,False,,0,False,"Fast-Pytorch with Google Colab: Pytorch Tutorial, Pytorch Implementations",[],r/pytorch,False,6,,0,,,False,t3_bm2jit,False,dark,0.9,,public,13,0,{},,,False,[],,False,False,,{},,False,13,,False,self,False,,[],{},self,,True,,1557331420.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This   repo aims to cover Pytorch details, Pytorch example implementations,   Pytorch sample codes, running Pytorch codes with Google Colab (with K80   GPU/CPU) in a nutshell.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tutorial Link:&lt;/strong&gt; &lt;a href=""https://github.com/omerbsezer/Fast-Pytorch""&gt;&lt;strong&gt;https://github.com/omerbsezer/Fast-Pytorch&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Table of Contents:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;🔥&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchtutorial""&gt;Fast Pytorch Tutorial&lt;/a&gt;  &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchplayground""&gt;Pytorch Playground&lt;/a&gt;&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;📗&lt;a href=""https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/Pytorch_Playground.ipynb""&gt;[Colab]&lt;/a&gt;, 📓&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/Pytorch_Playground.ipynb""&gt;[Notebook]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#model""&gt;Model (Neural Network Layers)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#optimizer""&gt;Optimizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#lossfunctions""&gt;Loss Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#poolinglayers""&gt;Pooling Layers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#nonlinearactivation""&gt;Non-linear activation functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#example""&gt;Basic 2 Layer NN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;🔥&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#torchvisiontutorial""&gt;Fast Torchvision Tutorial&lt;/a&gt;  &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#imagefolder""&gt;ImageFolder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#transforms""&gt;Transforms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#datasets""&gt;Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#torchvisionmodels""&gt;Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#utils""&gt;Utils&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;🔥&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchcolab""&gt;Pytorch with Google Colab&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;🔥&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchexamples""&gt;Pytorch Example Implementations&lt;/a&gt;  &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#mlp""&gt;MLP&lt;/a&gt;&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;MLP 1 Class with Binary Cross Entropy (BCE) Loss: 📗&lt;a href=""https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_1class_BinaryCrossEntropyLoss.ipynb""&gt;[Colab]&lt;/a&gt;, 📓&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_1class_BinaryCrossEntropyLoss.ipynb""&gt;[Notebook]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MLP 2 Classes with Cross Entropy Loss: 📗&lt;a href=""https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_2class_CrossEntropyLoss.ipynb""&gt;[Colab]&lt;/a&gt;, 📓&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_2class_CrossEntropyLoss.ipynb""&gt;[Notebook]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MLP 3-Layer with MNIST Example: 📗&lt;a href=""https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_3layer_MNIST.ipynb""&gt;[Colab]&lt;/a&gt;, 📓&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_3layer_MNIST.ipynb""&gt;[Notebook]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#cnn""&gt;CNN&lt;/a&gt;&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;CNN with MNIST Example: 📗&lt;a href=""https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/CNN_Mnist.ipynb""&gt;[Colab]&lt;/a&gt;, 📓&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/CNN_Mnist.ipynb""&gt;[Notebook]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improved CNN with MNIST Example: 📗&lt;a href=""https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/Improved_CNN_Mnist.ipynb""&gt;[Colab]&lt;/a&gt;, 📓&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/Improved_CNN_Mnist.ipynb""&gt;[Notebook]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#cnnvisualization""&gt;CNN Visualization&lt;/a&gt;&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;CNN Visualization: 📗&lt;a href=""https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/CNN_Visualization.ipynb""&gt;[Colab]&lt;/a&gt;, 📓&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/CNN_Visualization.ipynb""&gt;[Notebook]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#rnn""&gt;RNN&lt;/a&gt;&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;RNN Text Generation: 📗&lt;a href=""https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/RNN_word_embeddings.ipynb""&gt;[Colab]&lt;/a&gt;, 📓&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/RNN_word_embeddings.ipynb""&gt;[Notebook]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#transferlearning""&gt;Transfer Learning&lt;/a&gt;&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;Transfer Learning Implementation: 📗&lt;a href=""https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/TransferLearning.ipynb""&gt;[Colab]&lt;/a&gt;, 📓&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/TransferLearning.ipynb""&gt;[Notebook]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#dcgan""&gt;DCGAN&lt;/a&gt;&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;DCGAN Implementation: 📗&lt;a href=""https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/DCGAN.ipynb""&gt;[Colab]&lt;/a&gt;, 📓&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/DCGAN.ipynb""&gt;[Notebook]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#chatbot""&gt;ChatBot&lt;/a&gt;&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;Chatbot Implementation: 📗&lt;a href=""https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/ChatBot.ipynb""&gt;[Colab]&lt;/a&gt;, 📓&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/ChatBot.ipynb""&gt;[Notebook]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;🔥&lt;a href=""https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchsamplecodes""&gt;Pytorch Sample Codes&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Extra:&lt;/strong&gt; Reinforcement Learning Tutorial:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo""&gt;&lt;strong&gt;https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/aC5C2VYwZJmGWnXeD_A-c6WbBMybdYyto2flMszwPI4.jpg?auto=webp&amp;s=6f4b4b3791dbc70f55b4185d35d9d1eb1d564a73', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/aC5C2VYwZJmGWnXeD_A-c6WbBMybdYyto2flMszwPI4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ca9eb7f3468fbffaf73147062776412ed7ed81f8', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/aC5C2VYwZJmGWnXeD_A-c6WbBMybdYyto2flMszwPI4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5161e29d34dd55c7b78bd4a6bb1937bb07f08f40', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/aC5C2VYwZJmGWnXeD_A-c6WbBMybdYyto2flMszwPI4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a764a423d3d2373ea90527ef210eb74c5b9b001', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'j3QvWpNt_maNemFJpvHKJkCmXVIHfBl2PC3sO3E6r3A'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bm2jit,True,,obsezer,,0,True,all_ads,False,[],False,,/r/pytorch/comments/bm2jit/fastpytorch_with_google_colab_pytorch_tutorial/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bm2jit/fastpytorch_with_google_colab_pytorch_tutorial/,7135,1557302620.0,0,,False,,,,,,,,
889,,pytorch,"This [tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py) says that the LSTM expects inputs that are 3D tensors, where the first axis is the sequence, the second is the minibatch, and the third indexes elements of the input. What does ""indexes elements of the input"" actually mean? Is it referring to indexing them by time-step? Also, say I had a LSTM, where I pass in 54 values per time-step, would the dimensions of the input be 54 x 1 x number of timesteps?",t2_ym026,False,,0,False,Question about inputs into an LSTM in Pytorch,[],r/pytorch,False,6,,0,,,False,t3_bl7m51,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1557146428.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This &lt;a href=""https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py""&gt;tutorial&lt;/a&gt; says that the LSTM expects inputs that are 3D tensors, where the first axis is the sequence, the second is the minibatch, and the third indexes elements of the input. What does &amp;quot;indexes elements of the input&amp;quot; actually mean? Is it referring to indexing them by time-step? Also, say I had a LSTM, where I pass in 54 values per time-step, would the dimensions of the input be 54 x 1 x number of timesteps?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bl7m51,True,,alexhuhcya,,2,True,all_ads,False,[],False,,/r/pytorch/comments/bl7m51/question_about_inputs_into_an_lstm_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bl7m51/question_about_inputs_into_an_lstm_in_pytorch/,7135,1557117628.0,0,,False,,,,,,,,
890,,pytorch," 

embeds = self.embedding(x)

lstm\_out, hidden = self.lstm(embeds, hidden)

\# stack up lstm outputs

lstm\_out = lstm\_out.contiguous().view(-1, self.hidden\_dim)

\# dropout and fully-connected layer

out = self.dropout(lstm\_out)

out = self.fc(out)

\# sigmoid function

sig\_out = self.sig(out)

\# reshape to be batch\_size first

sig\_out = sig\_out.view(batch\_size, -1)

sig\_out = sig\_out\[:, -1\] # get last batch of label

Why do we stack the lstm outputs ? what does that even mean ? Can that be replaced by [torch.cat](https://torch.cat/) ? And please if you can tell me where can learn pytorch efficiently rather than the website documentation",t2_35jifly3,False,,0,False,Confusion in Pytorch RNN and LSTM code,[],r/pytorch,False,6,,0,,,False,t3_bkjeiy,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1556990670.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;embeds = self.embedding(x)&lt;/p&gt;

&lt;p&gt;lstm_out, hidden = self.lstm(embeds, hidden)&lt;/p&gt;

&lt;p&gt;# stack up lstm outputs&lt;/p&gt;

&lt;p&gt;lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)&lt;/p&gt;

&lt;p&gt;# dropout and fully-connected layer&lt;/p&gt;

&lt;p&gt;out = self.dropout(lstm_out)&lt;/p&gt;

&lt;p&gt;out = self.fc(out)&lt;/p&gt;

&lt;p&gt;# sigmoid function&lt;/p&gt;

&lt;p&gt;sig_out = self.sig(out)&lt;/p&gt;

&lt;p&gt;# reshape to be batch_size first&lt;/p&gt;

&lt;p&gt;sig_out = sig_out.view(batch_size, -1)&lt;/p&gt;

&lt;p&gt;sig_out = sig_out[:, -1] # get last batch of label&lt;/p&gt;

&lt;p&gt;Why do we stack the lstm outputs ? what does that even mean ? Can that be replaced by &lt;a href=""https://torch.cat/""&gt;torch.cat&lt;/a&gt; ? And please if you can tell me where can learn pytorch efficiently rather than the website documentation&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bkjeiy,True,,bikanation,,5,True,all_ads,False,[],False,,/r/pytorch/comments/bkjeiy/confusion_in_pytorch_rnn_and_lstm_code/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bkjeiy/confusion_in_pytorch_rnn_and_lstm_code/,7135,1556961870.0,0,,False,,,,,,,,
891,,pytorch,So i am starting a new DL project in PyTorch and i was asking if there was an already approved template for PyTorch projects so my code doesn't end messed up like the others before him.,t2_1259yeec,False,,0,False,PyTorch Project Template ?,[],r/pytorch,False,6,,0,,,False,t3_bkdus4,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1556948433.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So i am starting a new DL project in PyTorch and i was asking if there was an already approved template for PyTorch projects so my code doesn&amp;#39;t end messed up like the others before him.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bkdus4,True,,MohamedRashad,,3,True,all_ads,False,[],False,,/r/pytorch/comments/bkdus4/pytorch_project_template/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bkdus4/pytorch_project_template/,7135,1556919633.0,0,,False,,,,,,,,
892,,pytorch,"I am trying to use the maskrcnn benchmark on my own dataset — with 6 classes , none of which are in COCO tor an object detection.

Already have bounding boxes collected, what are the next steps?",t2_vbttm,False,,0,False,A good tutorial to format your dataset CoCo style for MaskRCNN,[],r/pytorch,False,6,,0,,,False,t3_bkf68x,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1556956317.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to use the maskrcnn benchmark on my own dataset — with 6 classes , none of which are in COCO tor an object detection.&lt;/p&gt;

&lt;p&gt;Already have bounding boxes collected, what are the next steps?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bkf68x,True,,spoiltForChoice,,1,True,all_ads,False,[],False,,/r/pytorch/comments/bkf68x/a_good_tutorial_to_format_your_dataset_coco_style/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bkf68x/a_good_tutorial_to_format_your_dataset_coco_style/,7135,1556927517.0,0,,False,,,,,,,,
893,,pytorch,My data is formatted in such fashion: Class / folder / folder data. I'm using a time-series LSTM to iterate over the folder data. How would I use torchvision.datasets.DatasetFolder to load the data in?,t2_ym026,False,,0,False,Question about data loading,[],r/pytorch,False,6,,0,,,False,t3_bk5grd,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1556895349.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My data is formatted in such fashion: Class / folder / folder data. I&amp;#39;m using a time-series LSTM to iterate over the folder data. How would I use torchvision.datasets.DatasetFolder to load the data in?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bk5grd,True,,alexhuhcya,,0,True,all_ads,False,[],False,,/r/pytorch/comments/bk5grd/question_about_data_loading/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bk5grd/question_about_data_loading/,7135,1556866549.0,0,,False,,,,,,,,
894,,pytorch," Still reeling from a string of damaging news reports accusing it of misinformation, data abuse and violent content; Facebook is hoping its recent AI R&amp;D efforts can help it climb out of the mess it’s in.

On the second day of its annual F8 developer conference, executives from the social media giant framed AI as a weapon in Facebook’s battle against objectionable content. “**Our goal is to reduce the prevalence by taking action on violent content proactively with few minutes**,” said Facebook CTO Mike Schroepfer.

Link: [https://medium.com/syncedreview/ai-facebook-f8-self-supervision-fairness-inclusivity-and-pytorch-1-1-3951ce45872a](https://medium.com/syncedreview/ai-facebook-f8-self-supervision-fairness-inclusivity-and-pytorch-1-1-3951ce45872a)",t2_2fv4yodo,False,,0,False,"AI @Facebook F8 | Self-Supervision, Fairness, Inclusivity and PyTorch 1.1",[],r/pytorch,False,6,,0,,,False,t3_bjxl18,False,dark,0.71,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1556846492.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Still reeling from a string of damaging news reports accusing it of misinformation, data abuse and violent content; Facebook is hoping its recent AI R&amp;amp;D efforts can help it climb out of the mess it’s in.&lt;/p&gt;

&lt;p&gt;On the second day of its annual F8 developer conference, executives from the social media giant framed AI as a weapon in Facebook’s battle against objectionable content. “&lt;strong&gt;Our goal is to reduce the prevalence by taking action on violent content proactively with few minutes&lt;/strong&gt;,” said Facebook CTO Mike Schroepfer.&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=""https://medium.com/syncedreview/ai-facebook-f8-self-supervision-fairness-inclusivity-and-pytorch-1-1-3951ce45872a""&gt;https://medium.com/syncedreview/ai-facebook-f8-self-supervision-fairness-inclusivity-and-pytorch-1-1-3951ce45872a&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/TGGENA7YrSTr36BL1JeeTKug2MP6sRVtGwDtL1b_pjA.jpg?auto=webp&amp;s=b32f40493c76a5fbefeae7bc1854c03431c1a4bd', 'width': 1200, 'height': 847}, 'resolutions': [{'url': 'https://external-preview.redd.it/TGGENA7YrSTr36BL1JeeTKug2MP6sRVtGwDtL1b_pjA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc421baeff19f22002d01d504ee9fd841f81024a', 'width': 108, 'height': 76}, {'url': 'https://external-preview.redd.it/TGGENA7YrSTr36BL1JeeTKug2MP6sRVtGwDtL1b_pjA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5df453a2e2cac80b601e52808dc1d12e0774b06c', 'width': 216, 'height': 152}, {'url': 'https://external-preview.redd.it/TGGENA7YrSTr36BL1JeeTKug2MP6sRVtGwDtL1b_pjA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=740c2c64b970c2b2dc90bed6c1072e269473c5a6', 'width': 320, 'height': 225}, {'url': 'https://external-preview.redd.it/TGGENA7YrSTr36BL1JeeTKug2MP6sRVtGwDtL1b_pjA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe01055641f2cb08a764201a57b8594a741214e0', 'width': 640, 'height': 451}, {'url': 'https://external-preview.redd.it/TGGENA7YrSTr36BL1JeeTKug2MP6sRVtGwDtL1b_pjA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ace43a8f996ee86a6b78487dac601e7691dee0c', 'width': 960, 'height': 677}, {'url': 'https://external-preview.redd.it/TGGENA7YrSTr36BL1JeeTKug2MP6sRVtGwDtL1b_pjA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9109c90e38195c73fe14d708861bbea05bbc69d6', 'width': 1080, 'height': 762}], 'variants': {}, 'id': 'RIhz-KtBM22AiHZE1n-lUXwKFLzJJ8OUcpk5v17So5s'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bjxl18,True,,Yuqing7,,0,True,all_ads,False,[],False,,/r/pytorch/comments/bjxl18/ai_facebook_f8_selfsupervision_fairness/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bjxl18/ai_facebook_f8_selfsupervision_fairness/,7135,1556817692.0,0,,False,,,,,,,,
895,,pytorch,"Hi, I've developed a pytorch convnet, but I have to implement it in an existent C# code?

Does someone knows how can I do it? Thanks!",t2_3b7uazps,False,,0,False,Run a pytorch model in C#,[],r/pytorch,False,6,,0,,,False,t3_bjvbb4,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1556834640.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;ve developed a pytorch convnet, but I have to implement it in an existent C# code?&lt;/p&gt;

&lt;p&gt;Does someone knows how can I do it? Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bjvbb4,True,,drr21,,7,True,all_ads,False,[],False,,/r/pytorch/comments/bjvbb4/run_a_pytorch_model_in_c/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bjvbb4/run_a_pytorch_model_in_c/,7135,1556805840.0,0,,False,,,,,,,,
896,,pytorch,,t2_15r2w7zy,False,,0,False,"[D] How to Build OpenAI's GPT-2: ""The AI That's Too Dangerous to Release""",[],r/pytorch,False,6,,0,73.0,,False,t3_bj0hv5,False,dark,1.0,,public,4,0,{},140.0,,False,[],,False,False,,{},,False,4,,False,https://a.thumbs.redditmedia.com/lc_AooQNb_5lhmFwdus211lAZqcG2TBnOAtWxrjgoF0.jpg,False,,[],{},link,,False,,1556637952.0,text,6,,,text,self.MachineLearning,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/P46PbPj0-Ush3N_KQhQC2DLHEOG3t1olAtWp8RLHux4.jpg?auto=webp&amp;s=4568e8614dbe46f38b5e9203c906a8fefb358176', 'width': 2000, 'height': 3000}, 'resolutions': [{'url': 'https://external-preview.redd.it/P46PbPj0-Ush3N_KQhQC2DLHEOG3t1olAtWp8RLHux4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=96f8442edc5da4d31eddae1eb84fb1694bd5baf9', 'width': 108, 'height': 162}, {'url': 'https://external-preview.redd.it/P46PbPj0-Ush3N_KQhQC2DLHEOG3t1olAtWp8RLHux4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=27cdc957a269faf04bc775e84a9a5dd5e3e681b4', 'width': 216, 'height': 324}, {'url': 'https://external-preview.redd.it/P46PbPj0-Ush3N_KQhQC2DLHEOG3t1olAtWp8RLHux4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3fa08c835f08c52d32e4278e4060b9302e43abd7', 'width': 320, 'height': 480}, {'url': 'https://external-preview.redd.it/P46PbPj0-Ush3N_KQhQC2DLHEOG3t1olAtWp8RLHux4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9140f6ffe89679a8fda23be49ebbe14f0e1eecbd', 'width': 640, 'height': 960}, {'url': 'https://external-preview.redd.it/P46PbPj0-Ush3N_KQhQC2DLHEOG3t1olAtWp8RLHux4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5730464347955f4fe4de2320ab28419e26d4ca52', 'width': 960, 'height': 1440}, {'url': 'https://external-preview.redd.it/P46PbPj0-Ush3N_KQhQC2DLHEOG3t1olAtWp8RLHux4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1378a8a4c355133dc3f0ea2d081ba5ed842865cd', 'width': 1080, 'height': 1620}], 'variants': {}, 'id': 'MWR0DWZvZf-yyoLyG2UZ0_IEXjlqNLCWJ0kEaqF5QwA'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bj0hv5,True,,iyaja,,1,True,all_ads,False,[],False,,/r/pytorch/comments/bj0hv5/d_how_to_build_openais_gpt2_the_ai_thats_too/,all_ads,False,https://www.reddit.com/r/MachineLearning/comments/bj0dsa/d_how_to_build_openais_gpt2_the_ai_thats_too/,7135,1556609152.0,0,,False,https://www.reddit.com/r/MachineLearning/comments/bj0dsa/d_how_to_build_openais_gpt2_the_ai_thats_too/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'Hi everyone. I wrote [an article about OpenAI\'s GPT-2 language model](https://blog.floydhub.com/gpt2/), which recently got published on [the FloydHub blog](https://blog.floydhub.com). In it, I explain most of the NLP breakthroughs that led to the creation of what media outlets are referring to as ""the AI that\'s too dangerous to release."" You can read the article [here](https://blog.floydhub.com/gpt2/).\n\n&amp;#x200B;\n\nLike in my [previous article](https://blog.nanonets.com/hyperparameter-optimization/), I included a jupyter notebook that can be run with just a few clicks, so that you see an actual, live, demo running in real time. The demo includes a pretrained GPT-2 ([courtesy of hugging face](https://github.com/huggingface/pytorch-pretrained-BERT)) that can generate text based on a prompt that you provide.\n\n&amp;#x200B;\n\nI\'ve stayed away from any opinions on the decision not to release the full model since I feel that there\'s been plenty of that on twitter already. Here, I aim to provide a clear and straightforward explanation of how exactly the algorithm works, so that you can make your own informed decisions.\n\n&amp;#x200B;\n\nHere\'s a link to the article: [https://blog.floydhub.com/gpt2/](https://blog.floydhub.com/gpt2/)', 'author_fullname': 't2_15r2w7zy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] How to Build OpenAI\'s GPT-2: ""The AI That\'s Too Dangerous to Release""', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_bj0dsa', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 69, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 69, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1556636955.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone. I wrote &lt;a href=""https://blog.floydhub.com/gpt2/""&gt;an article about OpenAI&amp;#39;s GPT-2 language model&lt;/a&gt;, which recently got published on &lt;a href=""https://blog.floydhub.com""&gt;the FloydHub blog&lt;/a&gt;. In it, I explain most of the NLP breakthroughs that led to the creation of what media outlets are referring to as &amp;quot;the AI that&amp;#39;s too dangerous to release.&amp;quot; You can read the article &lt;a href=""https://blog.floydhub.com/gpt2/""&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Like in my &lt;a href=""https://blog.nanonets.com/hyperparameter-optimization/""&gt;previous article&lt;/a&gt;, I included a jupyter notebook that can be run with just a few clicks, so that you see an actual, live, demo running in real time. The demo includes a pretrained GPT-2 (&lt;a href=""https://github.com/huggingface/pytorch-pretrained-BERT""&gt;courtesy of hugging face&lt;/a&gt;) that can generate text based on a prompt that you provide.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve stayed away from any opinions on the decision not to release the full model since I feel that there&amp;#39;s been plenty of that on twitter already. Here, I aim to provide a clear and straightforward explanation of how exactly the algorithm works, so that you can make your own informed decisions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a link to the article: &lt;a href=""https://blog.floydhub.com/gpt2/""&gt;https://blog.floydhub.com/gpt2/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/P46PbPj0-Ush3N_KQhQC2DLHEOG3t1olAtWp8RLHux4.jpg?auto=webp&amp;s=4568e8614dbe46f38b5e9203c906a8fefb358176', 'width': 2000, 'height': 3000}, 'resolutions': [{'url': 'https://external-preview.redd.it/P46PbPj0-Ush3N_KQhQC2DLHEOG3t1olAtWp8RLHux4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=96f8442edc5da4d31eddae1eb84fb1694bd5baf9', 'width': 108, 'height': 162}, {'url': 'https://external-preview.redd.it/P46PbPj0-Ush3N_KQhQC2DLHEOG3t1olAtWp8RLHux4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=27cdc957a269faf04bc775e84a9a5dd5e3e681b4', 'width': 216, 'height': 324}, {'url': 'https://external-preview.redd.it/P46PbPj0-Ush3N_KQhQC2DLHEOG3t1olAtWp8RLHux4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3fa08c835f08c52d32e4278e4060b9302e43abd7', 'width': 320, 'height': 480}, {'url': 'https://external-preview.redd.it/P46PbPj0-Ush3N_KQhQC2DLHEOG3t1olAtWp8RLHux4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9140f6ffe89679a8fda23be49ebbe14f0e1eecbd', 'width': 640, 'height': 960}, {'url': 'https://external-preview.redd.it/P46PbPj0-Ush3N_KQhQC2DLHEOG3t1olAtWp8RLHux4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5730464347955f4fe4de2320ab28419e26d4ca52', 'width': 960, 'height': 1440}, {'url': 'https://external-preview.redd.it/P46PbPj0-Ush3N_KQhQC2DLHEOG3t1olAtWp8RLHux4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1378a8a4c355133dc3f0ea2d081ba5ed842865cd', 'width': 1080, 'height': 1620}], 'variants': {}, 'id': 'MWR0DWZvZf-yyoLyG2UZ0_IEXjlqNLCWJ0kEaqF5QwA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'bj0dsa', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'iyaja', 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/bj0dsa/d_how_to_build_openais_gpt2_the_ai_thats_too/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/bj0dsa/d_how_to_build_openais_gpt2_the_ai_thats_too/', 'subreddit_subscribers': 1740779, 'created_utc': 1556608155.0, 'num_crossposts': 5, 'media': None, 'is_video': False}]",t3_bj0dsa,,,,,
897,,pytorch,"I'm looking for a library which can generate images with text which I can use to train my neural net. I found some github projects but I can not use them directly as seem to lack some capabilities. But maybe I missed some cool project?

&amp;#x200B;

What I am looking for:

* I'd like to emulate printed text (e.g. scanned or medium-quality smartphone pictures). No need for really complicated ""scene text recognition"" like in the ""French Street Name Signs"" dataset (or Robust Reading Competition/ICDAR).
* ability to specify range of skew angle
* ability to use specific fonts (+ font-size)
* ability to generate texts according to a specific pattern
* need orientated bounding boxes (ideally character + word boxes). Maybe I'll try something like Mask-RCNN in the future so I imaging polygons might be even better.

&amp;#x200B;

What I found:

* [SynthText](https://github.com/ankush-me/SynthText) \- seems to be focused on incidental scene text, my target domain is much simpler
* [TextRecognitionDataGenerator](https://github.com/Belval/TextRecognitionDataGenerator) \- almost there but no bounding boxes, missing text patterns (also a bit hack-ish, needs some code organization)

&amp;#x200B;

If you know some other resources which might be interesting you could really help me saving a few days of work :-)",t2_12i1bz,False,,0,False,looking for a library to generate images with text (building a training corpus for text detection),[],r/pytorch,False,6,,0,,,False,t3_bj0sdq,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1556640578.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for a library which can generate images with text which I can use to train my neural net. I found some github projects but I can not use them directly as seem to lack some capabilities. But maybe I missed some cool project?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What I am looking for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I&amp;#39;d like to emulate printed text (e.g. scanned or medium-quality smartphone pictures). No need for really complicated &amp;quot;scene text recognition&amp;quot; like in the &amp;quot;French Street Name Signs&amp;quot; dataset (or Robust Reading Competition/ICDAR).&lt;/li&gt;
&lt;li&gt;ability to specify range of skew angle&lt;/li&gt;
&lt;li&gt;ability to use specific fonts (+ font-size)&lt;/li&gt;
&lt;li&gt;ability to generate texts according to a specific pattern&lt;/li&gt;
&lt;li&gt;need orientated bounding boxes (ideally character + word boxes). Maybe I&amp;#39;ll try something like Mask-RCNN in the future so I imaging polygons might be even better.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What I found:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/ankush-me/SynthText""&gt;SynthText&lt;/a&gt; - seems to be focused on incidental scene text, my target domain is much simpler&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/Belval/TextRecognitionDataGenerator""&gt;TextRecognitionDataGenerator&lt;/a&gt; - almost there but no bounding boxes, missing text patterns (also a bit hack-ish, needs some code organization)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;If you know some other resources which might be interesting you could really help me saving a few days of work :-)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/19smSixEqnXrRrUkwgjiQFF2plnneVIgscJBsYeaMHA.jpg?auto=webp&amp;s=22b8b5744344fad645b771094ed43ece8d1eff1a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/19smSixEqnXrRrUkwgjiQFF2plnneVIgscJBsYeaMHA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=db2cbc1f044dfbf785682cfce6fad1eb954e3750', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/19smSixEqnXrRrUkwgjiQFF2plnneVIgscJBsYeaMHA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d96eb0798f6971bf3e3c0c9c71ba84c73758a123', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/19smSixEqnXrRrUkwgjiQFF2plnneVIgscJBsYeaMHA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=649c1830c74915ffafbfdbe6978134aacb4bb7a7', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'YiRlqzTUTp2s66hy46Dc0hfLLXsbDT6eTjZ7m3ULu8s'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bj0sdq,True,,101testing,,3,True,all_ads,False,[],False,,/r/pytorch/comments/bj0sdq/looking_for_a_library_to_generate_images_with/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bj0sdq/looking_for_a_library_to_generate_images_with/,7135,1556611778.0,0,,False,,,,,,,,
898,,pytorch,"I know I can calculate a VJP using the backward function. Is there any way I can right multiply the Jacobian by a vector? I'm trying to apply [this trick](https://j-towns.github.io/2017/06/12/A-new-trick.html) but as far as I can tell, it won't work because the first backward and the resulting VJP is not part of the computational graph and doesn't have `requires_grad=True`. I've been running into a lot of optimization problems that require a JVP, so does anyone know how I can achieve this?

&amp;#x200B;

I should add that in the past I have calculated the Jacobian explicitly by calling backward on every element of my network's output and that I'm trying to avoid having to use that solution.",t2_q6jthgl,False,,0,False,Calculating a JVP in PyTorch?,[],r/pytorch,False,6,,0,,,False,t3_bii8hi,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1556498984.0,,[],{},self,,True,,1556527434.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know I can calculate a VJP using the backward function. Is there any way I can right multiply the Jacobian by a vector? I&amp;#39;m trying to apply &lt;a href=""https://j-towns.github.io/2017/06/12/A-new-trick.html""&gt;this trick&lt;/a&gt; but as far as I can tell, it won&amp;#39;t work because the first backward and the resulting VJP is not part of the computational graph and doesn&amp;#39;t have &lt;code&gt;requires_grad=True&lt;/code&gt;. I&amp;#39;ve been running into a lot of optimization problems that require a JVP, so does anyone know how I can achieve this?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I should add that in the past I have calculated the Jacobian explicitly by calling backward on every element of my network&amp;#39;s output and that I&amp;#39;m trying to avoid having to use that solution.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AXlmV4nV-fUqo1eb1P5-Sk-riM8o89xnT2W9cRk7-oM.jpg?auto=webp&amp;s=054284c97861bea86379cce6859e432ce20e350e', 'width': 979, 'height': 597}, 'resolutions': [{'url': 'https://external-preview.redd.it/AXlmV4nV-fUqo1eb1P5-Sk-riM8o89xnT2W9cRk7-oM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4e707749a17d0b1688604493c919093ef7ddabce', 'width': 108, 'height': 65}, {'url': 'https://external-preview.redd.it/AXlmV4nV-fUqo1eb1P5-Sk-riM8o89xnT2W9cRk7-oM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd1c495399db49b6fc29980ca955826588f84c1d', 'width': 216, 'height': 131}, {'url': 'https://external-preview.redd.it/AXlmV4nV-fUqo1eb1P5-Sk-riM8o89xnT2W9cRk7-oM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2787d173f6464929efd10c3cf1913620cf78d9a2', 'width': 320, 'height': 195}, {'url': 'https://external-preview.redd.it/AXlmV4nV-fUqo1eb1P5-Sk-riM8o89xnT2W9cRk7-oM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=013618fec9104f655a968f62a2cbd3198c96e1d1', 'width': 640, 'height': 390}, {'url': 'https://external-preview.redd.it/AXlmV4nV-fUqo1eb1P5-Sk-riM8o89xnT2W9cRk7-oM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7983f71745a7c442673f8f87cdbb966e86bfa8ca', 'width': 960, 'height': 585}], 'variants': {}, 'id': '_lIcAcnDtxAs1RKxML1mJMGqEfDUov67FzX5aZjAPeg'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bii8hi,True,,vandelet_industries,,1,True,all_ads,False,[],False,,/r/pytorch/comments/bii8hi/calculating_a_jvp_in_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bii8hi/calculating_a_jvp_in_pytorch/,7135,1556498634.0,0,,False,,,,,,,,
899,,pytorch,"So I have a pre-trained model named model.pth. I'm trying to convert it to ONNX by doing the following:

\- Load it from model.pth.

\- Provide dummy input.

\- Export to ONNX.

I was told that I need to subclass torch.nn.Module to load a model properly. But if I subclass it, I have to implement \_\_init\_\_ and forward methods linked to parameters, which there are like 100 of in the model... So it gets really complicated.

Can't I load a model in Python from .pth file without subclassing nn.Module? And then just export it to ONNX?",t2_1izxujva,False,,0,False,Converting PyTorch model to ONNX?,[],r/pytorch,False,6,,0,,,False,t3_bi91x5,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1556467652.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I have a pre-trained model named model.pth. I&amp;#39;m trying to convert it to ONNX by doing the following:&lt;/p&gt;

&lt;p&gt;- Load it from model.pth.&lt;/p&gt;

&lt;p&gt;- Provide dummy input.&lt;/p&gt;

&lt;p&gt;- Export to ONNX.&lt;/p&gt;

&lt;p&gt;I was told that I need to subclass torch.nn.Module to load a model properly. But if I subclass it, I have to implement __init__ and forward methods linked to parameters, which there are like 100 of in the model... So it gets really complicated.&lt;/p&gt;

&lt;p&gt;Can&amp;#39;t I load a model in Python from .pth file without subclassing nn.Module? And then just export it to ONNX?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bi91x5,True,,sidyakinian,,11,True,all_ads,False,[],False,,/r/pytorch/comments/bi91x5/converting_pytorch_model_to_onnx/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bi91x5/converting_pytorch_model_to_onnx/,7135,1556438852.0,0,,False,,,,,,,,
900,,pytorch,,,False,,0,False,Gluon to PyTorch model converter (with code generation),[],r/pytorch,False,6,,0,140.0,,False,t3_bemnwn,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,,https://b.thumbs.redditmedia.com/7kqAGTWL81v8cBA0LmpsmoRKMwPmE9K_B4pNQ2WPP0Y.jpg,False,,,{},link,,False,,1555628961.0,text,6,,,,github.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/RbRm8ZXPUpDfAAmYzZqOwZhjeFlRZPjybmIMPuU2Zns.jpg?auto=webp&amp;s=bb5dd084108f90122f82f5c8aebc804dd3263252', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/RbRm8ZXPUpDfAAmYzZqOwZhjeFlRZPjybmIMPuU2Zns.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5284e1cbc69aa19ce11655096c1c8c0f73e839b6', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/RbRm8ZXPUpDfAAmYzZqOwZhjeFlRZPjybmIMPuU2Zns.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=57b41e2d539ec20c5b0b0ccff73449ddd19bca6c', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/RbRm8ZXPUpDfAAmYzZqOwZhjeFlRZPjybmIMPuU2Zns.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e8cf5df39b3b011298707cbf4dea2f9e8e3fd9c9', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'ANy652y36HFSFbR1375Elvl8ovMfyA7ygH4nyPCUe84'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bemnwn,True,,[deleted],,0,True,all_ads,False,[],,dark,/r/pytorch/comments/bemnwn/gluon_to_pytorch_model_converter_with_code/,all_ads,False,https://github.com/nerox8664/gluon2pytorch,7135,1555600161.0,0,,False,https://github.com/nerox8664/gluon2pytorch,,,,,,,
901,,pytorch,"Hi There,

I am attempting to apply Expectation Maximisation to a `torch.nn.Module` and I am having a bit of trouble getting it to work. Without having to spell out too much has anyone attempted this and been successful? 

Here's an example:
   
    class RNN(torch.nn.Module):
        def __init__(self):
            super(RNN, self).__init__()
            self.w1 = torch.nn.Linear(2, 1024)
            self.w2 = torch.nn.Linear(1024, 256)
            self.out = torch.nn.Linear(256, 1)
            self.internal_var = torch.nn.Parameter(torch.Tensor([0.0]))
    ...

The idea is to iteratively run and expectation step which would consist modifying `self.internal_var` for one backwards pass then a maximisation step which would consist of modifying `w1`, `w2` &amp; `out` through another backwards pass for each epoch. I assume this would mean 2 separate optimizer functions, one for expectation and one for maximisation. Does anyone know how I can freeze one parameter and modify the others and visa versa? 

Any help would be greatly appreciated. Also let me know if you need any clarification S.O. was no help and no one replied on the forum. So reddit your my only hope",t2_1m55y4m7,False,,0,False,Expectation Maximisation,[],r/pytorch,False,6,,0,,,False,t3_bdvzjo,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1555461040.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi There,&lt;/p&gt;

&lt;p&gt;I am attempting to apply Expectation Maximisation to a &lt;code&gt;torch.nn.Module&lt;/code&gt; and I am having a bit of trouble getting it to work. Without having to spell out too much has anyone attempted this and been successful? &lt;/p&gt;

&lt;p&gt;Here&amp;#39;s an example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class RNN(torch.nn.Module):
    def __init__(self):
        super(RNN, self).__init__()
        self.w1 = torch.nn.Linear(2, 1024)
        self.w2 = torch.nn.Linear(1024, 256)
        self.out = torch.nn.Linear(256, 1)
        self.internal_var = torch.nn.Parameter(torch.Tensor([0.0]))
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The idea is to iteratively run and expectation step which would consist modifying &lt;code&gt;self.internal_var&lt;/code&gt; for one backwards pass then a maximisation step which would consist of modifying &lt;code&gt;w1&lt;/code&gt;, &lt;code&gt;w2&lt;/code&gt; &amp;amp; &lt;code&gt;out&lt;/code&gt; through another backwards pass for each epoch. I assume this would mean 2 separate optimizer functions, one for expectation and one for maximisation. Does anyone know how I can freeze one parameter and modify the others and visa versa? &lt;/p&gt;

&lt;p&gt;Any help would be greatly appreciated. Also let me know if you need any clarification S.O. was no help and no one replied on the forum. So reddit your my only hope&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bdvzjo,True,,MachinaDoctrina,,2,True,all_ads,False,[],False,,/r/pytorch/comments/bdvzjo/expectation_maximisation/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bdvzjo/expectation_maximisation/,7135,1555432240.0,0,,False,,,,,,,,
902,,pytorch,,t2_v7ywm,False,,0,False,Fast Neural Style Transfer in Pytorch,[],r/pytorch,False,6,,0,140.0,,False,t3_bcyeu3,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/7vcVAoRAL3aXVLv56W2oDQeBX6GgyjOaFPrncY1vcPY.jpg,False,,[],{},link,,False,,1555238706.0,text,6,,,text,github.com,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/S51cOBZDSGOXqDYPlZtBbVJ_AZ1S-vbWdTwsuecZrX4.jpg?auto=webp&amp;s=2cd6366b28776992afb0e629ce167ca908c24d75', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/S51cOBZDSGOXqDYPlZtBbVJ_AZ1S-vbWdTwsuecZrX4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf8e66373d13a219cd4dbd30c007c093978fba93', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/S51cOBZDSGOXqDYPlZtBbVJ_AZ1S-vbWdTwsuecZrX4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc99150b35837dde40dfb37b85f5aad77c2889b7', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/S51cOBZDSGOXqDYPlZtBbVJ_AZ1S-vbWdTwsuecZrX4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ea6c137d63605cde60c714b24a5c4ab112b56a5', 'width': 320, 'height': 320}], 'variants': {}, 'id': '2SwWNVGHMZ5qnXSxir4Z5rZFT8qk_67KCbV0vAYr4lU'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bcyeu3,True,,treguess,,1,True,all_ads,False,[],False,,/r/pytorch/comments/bcyeu3/fast_neural_style_transfer_in_pytorch/,all_ads,False,https://github.com/eriklindernoren/Fast-Neural-Style-Transfer,7135,1555209906.0,1,,False,https://github.com/eriklindernoren/Fast-Neural-Style-Transfer,,,,,,,
903,,pytorch,"I am interested in creating a custom multilabel dataset class. I am reading the data from a csv file. How can this be implemented?

&amp;#x200B;

[Code](https://preview.redd.it/kbv0yi7kr2s21.png?width=925&amp;format=png&amp;auto=webp&amp;s=241a491f646cefd11eaa5af74cc967f7403c3e96)

&amp;#x200B;

[CSV Format](https://preview.redd.it/px67wzp1r2s21.png?width=1052&amp;format=png&amp;auto=webp&amp;s=60a3a4a98c0c7ff44db15ed71b7d90731c544c46)",t2_a85xbdx,False,,0,False,Custom MultiLabel Dataset,[],r/pytorch,False,6,,0,83.0,,False,t3_bctkfg,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://a.thumbs.redditmedia.com/OftYLrbuqPN8wZNpYrh3AaQZ9PIiB2xUYfVJEN7FbH4.jpg,False,,[],{},,,True,,1555208992.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am interested in creating a custom multilabel dataset class. I am reading the data from a csv file. How can this be implemented?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/kbv0yi7kr2s21.png?width=925&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=241a491f646cefd11eaa5af74cc967f7403c3e96""&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/px67wzp1r2s21.png?width=1052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=60a3a4a98c0c7ff44db15ed71b7d90731c544c46""&gt;CSV Format&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bctkfg,True,,deltaArch,,0,True,all_ads,False,[],False,,/r/pytorch/comments/bctkfg/custom_multilabel_dataset/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bctkfg/custom_multilabel_dataset/,7135,1555180192.0,0,,False,,,,"{'px67wzp1r2s21': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 48, 'x': 108, 'u': 'https://preview.redd.it/px67wzp1r2s21.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=16dbd7058e274e673c5bafa07afffdf398237d94'}, {'y': 97, 'x': 216, 'u': 'https://preview.redd.it/px67wzp1r2s21.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ccf819fa98180f2a8812ea58a91f1e98833b73aa'}, {'y': 144, 'x': 320, 'u': 'https://preview.redd.it/px67wzp1r2s21.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=39c2d818dc689a9fd5022ecf1534cb7bf3156205'}, {'y': 289, 'x': 640, 'u': 'https://preview.redd.it/px67wzp1r2s21.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a65f11e89c8816a7d100cb765f142ae2d5715a93'}, {'y': 434, 'x': 960, 'u': 'https://preview.redd.it/px67wzp1r2s21.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6edf6f07a6dbfb8937458b1761ee817fa43ab3a5'}], 's': {'y': 476, 'x': 1052, 'u': 'https://preview.redd.it/px67wzp1r2s21.png?width=1052&amp;format=png&amp;auto=webp&amp;s=60a3a4a98c0c7ff44db15ed71b7d90731c544c46'}, 'id': 'px67wzp1r2s21'}, 'kbv0yi7kr2s21': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 64, 'x': 108, 'u': 'https://preview.redd.it/kbv0yi7kr2s21.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=556ead40a22f37eea59ef2a8d1f3e598b6e7e278'}, {'y': 129, 'x': 216, 'u': 'https://preview.redd.it/kbv0yi7kr2s21.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=badfd3c7f081f2431e7343da7e06b6fb3796722e'}, {'y': 191, 'x': 320, 'u': 'https://preview.redd.it/kbv0yi7kr2s21.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=58650c391efba459b0a88350eaf23e3584f6ce97'}, {'y': 383, 'x': 640, 'u': 'https://preview.redd.it/kbv0yi7kr2s21.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e38a513fe640ea9dabb4ae6f6bc929addaaf66d9'}], 's': {'y': 554, 'x': 925, 'u': 'https://preview.redd.it/kbv0yi7kr2s21.png?width=925&amp;format=png&amp;auto=webp&amp;s=241a491f646cefd11eaa5af74cc967f7403c3e96'}, 'id': 'kbv0yi7kr2s21'}}",,,,
904,,pytorch,,t2_xag6c,False,,0,False,[x-post] PyTorch implementation best practices,[],r/pytorch,False,6,,0,73.0,,False,t3_bcoy6x,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://a.thumbs.redditmedia.com/rC5GxrugEiKsZn13iqA_ItE_RUmDNW46kJcY8HK11W0.jpg,False,,[],{},link,,False,,1555177856.0,text,6,,,text,self.MachineLearning,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/kNlcFT0idpCsXfQaxqBPylYtHiaLocG4my91Fz7h15A.jpg?auto=webp&amp;s=253cfb00b49a96ca47c36bf3a95d1a23d53cef8b', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/kNlcFT0idpCsXfQaxqBPylYtHiaLocG4my91Fz7h15A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04ac62901463e1d3b3186de466f92fe3780f5b0c', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/kNlcFT0idpCsXfQaxqBPylYtHiaLocG4my91Fz7h15A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9c30f694cb58e4d9cf24d4053f333d3efca7fbb3', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/kNlcFT0idpCsXfQaxqBPylYtHiaLocG4my91Fz7h15A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2837fd81224fb5d722beabd99fdd17e64552ea7', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'Rvy4m6BgiqQ56dVfSo-mhff3627svEU9lfsI7j9I1a4'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bcoy6x,True,,floodvalve,,6,True,all_ads,False,[],False,,/r/pytorch/comments/bcoy6x/xpost_pytorch_implementation_best_practices/,all_ads,False,https://www.reddit.com/r/MachineLearning/comments/bcfyo2/d_pytorch_implementation_best_practices/,7135,1555149056.0,0,,False,https://www.reddit.com/r/MachineLearning/comments/bcfyo2/d_pytorch_implementation_best_practices/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': ""Hi r/MachineLearning! Let's discuss PyTorch best practices.\n\n\nI recently finished a PyTorch [**re-implementation**](https://github.com/joel-huang/zeroshot-capsnet-pytorch) (with help from various sources) for the paper [Zero-shot User Intent Detection via Capsule Neural Networks](https://arxiv.org/abs/1809.00385), which originally had Python 2 code for TensorFlow.\n\n\nI'd like to request perhaps a critique on the code I've written so far (it's not perfect, yet!) and any suggestions if there are best practices specifically in PyTorch, for implementing directly from research papers as well as converting them from other frameworks.\n\n\nSome thoughts I had while programming (feel free to raise more!):\n\n1. I've been implementing a Dataset class and custom batch functions for every dataset I've been working with. Is this the PyTorch best practice?\n\n2. Where is the optimal place to shift `Tensors` to `.cuda()`? I've been doing this in the training loop, just before feeding it into the model.\n\n3. How to manage the use of both `numpy` and `torch`, seeing as PyTorch aims to reinvent many of the basic operations in `numpy`?\n\n\nIf you're a fellow PyTorch user/contributor please share a little!"", 'author_fullname': 't2_xag6c', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] PyTorch implementation best practices', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_bcfyo2', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 47, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 47, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': 1555091625.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1555117359.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi &lt;a href=""/r/MachineLearning""&gt;r/MachineLearning&lt;/a&gt;! Let&amp;#39;s discuss PyTorch best practices.&lt;/p&gt;\n\n&lt;p&gt;I recently finished a PyTorch &lt;a href=""https://github.com/joel-huang/zeroshot-capsnet-pytorch""&gt;&lt;strong&gt;re-implementation&lt;/strong&gt;&lt;/a&gt; (with help from various sources) for the paper &lt;a href=""https://arxiv.org/abs/1809.00385""&gt;Zero-shot User Intent Detection via Capsule Neural Networks&lt;/a&gt;, which originally had Python 2 code for TensorFlow.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to request perhaps a critique on the code I&amp;#39;ve written so far (it&amp;#39;s not perfect, yet!) and any suggestions if there are best practices specifically in PyTorch, for implementing directly from research papers as well as converting them from other frameworks.&lt;/p&gt;\n\n&lt;p&gt;Some thoughts I had while programming (feel free to raise more!):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I&amp;#39;ve been implementing a Dataset class and custom batch functions for every dataset I&amp;#39;ve been working with. Is this the PyTorch best practice?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Where is the optimal place to shift &lt;code&gt;Tensors&lt;/code&gt; to &lt;code&gt;.cuda()&lt;/code&gt;? I&amp;#39;ve been doing this in the training loop, just before feeding it into the model.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How to manage the use of both &lt;code&gt;numpy&lt;/code&gt; and &lt;code&gt;torch&lt;/code&gt;, seeing as PyTorch aims to reinvent many of the basic operations in &lt;code&gt;numpy&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;If you&amp;#39;re a fellow PyTorch user/contributor please share a little!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/kNlcFT0idpCsXfQaxqBPylYtHiaLocG4my91Fz7h15A.jpg?auto=webp&amp;s=253cfb00b49a96ca47c36bf3a95d1a23d53cef8b', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/kNlcFT0idpCsXfQaxqBPylYtHiaLocG4my91Fz7h15A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04ac62901463e1d3b3186de466f92fe3780f5b0c', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/kNlcFT0idpCsXfQaxqBPylYtHiaLocG4my91Fz7h15A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9c30f694cb58e4d9cf24d4053f333d3efca7fbb3', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/kNlcFT0idpCsXfQaxqBPylYtHiaLocG4my91Fz7h15A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2837fd81224fb5d722beabd99fdd17e64552ea7', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'Rvy4m6BgiqQ56dVfSo-mhff3627svEU9lfsI7j9I1a4'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'bcfyo2', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'floodvalve', 'discussion_type': None, 'num_comments': 15, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/bcfyo2/d_pytorch_implementation_best_practices/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/bcfyo2/d_pytorch_implementation_best_practices/', 'subreddit_subscribers': 1740778, 'created_utc': 1555088559.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_bcfyo2,,,,,
905,,pytorch,"I don't have a degree in physics, so I don't know much about tensors.

And I have just started with deep learning.

After reading through the docs, my impression is that PyTorch actually implements good-and-old matrix-and-vector linear algebra, and in addition,

1) names n-d arrays as tensors, which is correct mathematically

2) has some elementary operations on n-d arrays, which demand no knowledge of (mathematical) tensors and may not necessarily be a part of tensor algebra , such as torch.cat(), torch.chunk().

It seems that tensor algebra is not actually implemented. The closest thing I have found is torch.tensordot() which I am not sure what tensor algebra operation(s) it corresponds to.

Am I missing something? Or is it just PyTorch? Or are deep learning frameworks all like that?",t2_3kriau6q,False,,0,False,Does PyTorch actually use tensors (as in tensor algebra)?,[],r/pytorch,False,6,,0,,,False,t3_bc9npj,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1555045412.0,,[],{},,,True,,1555073395.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I don&amp;#39;t have a degree in physics, so I don&amp;#39;t know much about tensors.&lt;/p&gt;

&lt;p&gt;And I have just started with deep learning.&lt;/p&gt;

&lt;p&gt;After reading through the docs, my impression is that PyTorch actually implements good-and-old matrix-and-vector linear algebra, and in addition,&lt;/p&gt;

&lt;p&gt;1) names n-d arrays as tensors, which is correct mathematically&lt;/p&gt;

&lt;p&gt;2) has some elementary operations on n-d arrays, which demand no knowledge of (mathematical) tensors and may not necessarily be a part of tensor algebra , such as torch.cat(), torch.chunk().&lt;/p&gt;

&lt;p&gt;It seems that tensor algebra is not actually implemented. The closest thing I have found is torch.tensordot() which I am not sure what tensor algebra operation(s) it corresponds to.&lt;/p&gt;

&lt;p&gt;Am I missing something? Or is it just PyTorch? Or are deep learning frameworks all like that?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bc9npj,True,,justinecarolin,,3,True,all_ads,False,[],False,,/r/pytorch/comments/bc9npj/does_pytorch_actually_use_tensors_as_in_tensor/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bc9npj/does_pytorch_actually_use_tensors_as_in_tensor/,7135,1555044595.0,0,,False,,,,,,,,
906,,pytorch,,t2_tpwter9,False,,0,False,Facebook AI Open-Sources PyTorch-BigGraph Tool for ‘Extremely Large’ Graphs,[],r/pytorch,False,6,,0,73.0,,False,t3_bb9kro,False,dark,1.0,,public,14,0,{},140.0,,False,[],,False,False,,{},,False,14,,False,https://b.thumbs.redditmedia.com/n2Ity6s-3f_YwlDuhntdbb0oHuv5clX7vcfXSVNK5Qk.jpg,False,,[],{},link,,False,,1554854013.0,text,6,,,text,medium.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vH3Hq2VghEUpa5-uJt6NG26s6uHe5MoIVG1ta6RJjWQ.jpg?auto=webp&amp;s=2d0c8ea19744f43c87b6b684a615ce06e2045554', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/vH3Hq2VghEUpa5-uJt6NG26s6uHe5MoIVG1ta6RJjWQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3708a23c218f64e88d3212a08d69f41b1cf437b2', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/vH3Hq2VghEUpa5-uJt6NG26s6uHe5MoIVG1ta6RJjWQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7410e9f663f68a8d74aa06ad4f7de2265b9535e6', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/vH3Hq2VghEUpa5-uJt6NG26s6uHe5MoIVG1ta6RJjWQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c607e9994d1bdfe3dd278c5b273a68a8046fa6b', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/vH3Hq2VghEUpa5-uJt6NG26s6uHe5MoIVG1ta6RJjWQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d8ddf089de63e3328b3c765e159f885ff66ba042', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/vH3Hq2VghEUpa5-uJt6NG26s6uHe5MoIVG1ta6RJjWQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e9e7a64b241fef25ec95d6f303382056c3ae1275', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/vH3Hq2VghEUpa5-uJt6NG26s6uHe5MoIVG1ta6RJjWQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0d07c766f7eab06116a34e46ba43f538ce2d657', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'd0fH9HZLNvEz7pj3trqGs5Z3TrotlIlyWL-sPaVkzno'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bb9kro,True,,gwen0927,,0,True,all_ads,False,[],False,,/r/pytorch/comments/bb9kro/facebook_ai_opensources_pytorchbiggraph_tool_for/,all_ads,False,https://medium.com/syncedreview/facebook-ai-open-sources-pytorch-biggraph-tool-for-extremely-large-graphs-3b741b270693,7135,1554825213.0,0,,False,https://medium.com/syncedreview/facebook-ai-open-sources-pytorch-biggraph-tool-for-extremely-large-graphs-3b741b270693,,,,,,,
907,,pytorch,,t2_shfmi,False,,0,False,[P] Ignite - High-level library to help with training neural networks in PyTorch,[],r/pytorch,False,6,,0,73.0,,False,t3_bbc238,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/0avSQLIiQmmupXQMkjEiB5cgVVypx2XVkwmLjmQZx0g.jpg,False,,[],{},link,,False,,1554866226.0,text,6,,,text,self.MachineLearning,False,,,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?auto=webp&amp;s=07ca976f2628a4aeaf3183ce72eec096643e840f', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=950ea7a0f777157bbd0b8745b20e06aa336dd923', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4153bf0d485e12346f9dd8c386a31de9512595a0', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=50e63f6ec38baa90825c802fb91a3cac2c8de8d3', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_YPUxJ8AyYwgpO9r-tD7JUoQ-Nx8IlATLoEfmjW48iQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bbc238,True,,pie_oh_my_,,9,True,all_ads,False,[],False,,/r/pytorch/comments/bbc238/p_ignite_highlevel_library_to_help_with_training/,all_ads,False,https://www.reddit.com/r/MachineLearning/comments/bbbxaw/p_ignite_highlevel_library_to_help_with_training/,7135,1554837426.0,0,,False,https://www.reddit.com/r/MachineLearning/comments/bbbxaw/p_ignite_highlevel_library_to_help_with_training/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'We are happy to announce the v0.2.0 release of *ignite*.\n\nLink to our github - https://github.com/pytorch/ignite\n\nLink to docs - https://pytorch.org/ignite/index.html\n\nHere is a high level description of changes from the last release:\n\n* Added multilabel option for Accuracy, Precision, Recall.\n* Removed deprecated BinaryAccuracy and CategoricalAccuracy, in favor of Accuracy.\n* Added ConfusionMatrix, IoU, mIoU.\n* Operation on Metrics including PyTorch operators and indexing. \n* Added a regression module with a variety of metrics.\n* Added loggers for Tensorboard, Visdom, Polyaxon. \n* Improved ProgressBar with notebook support. \n* Added Parameter Schedulers and a variety of Learning Rate Schedulers. \n* Improved docs, added FAQ section.\n\nrelease notes - https://github.com/pytorch/ignite/releases/tag/v0.2.0\n\n\nWe would like to thank our community and all our contributors for the issues, PRs and support. We welcome contributions! \n\nHappy training!', 'author_fullname': 't2_shfmi', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[P] Ignite - High-level library to help with training neural networks in PyTorch', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'four', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_bbbxaw', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': 1554837514.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1554865597.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We are happy to announce the v0.2.0 release of &lt;em&gt;ignite&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;Link to our github - &lt;a href=""https://github.com/pytorch/ignite""&gt;https://github.com/pytorch/ignite&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Link to docs - &lt;a href=""https://pytorch.org/ignite/index.html""&gt;https://pytorch.org/ignite/index.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here is a high level description of changes from the last release:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Added multilabel option for Accuracy, Precision, Recall.&lt;/li&gt;\n&lt;li&gt;Removed deprecated BinaryAccuracy and CategoricalAccuracy, in favor of Accuracy.&lt;/li&gt;\n&lt;li&gt;Added ConfusionMatrix, IoU, mIoU.&lt;/li&gt;\n&lt;li&gt;Operation on Metrics including PyTorch operators and indexing. &lt;/li&gt;\n&lt;li&gt;Added a regression module with a variety of metrics.&lt;/li&gt;\n&lt;li&gt;Added loggers for Tensorboard, Visdom, Polyaxon. &lt;/li&gt;\n&lt;li&gt;Improved ProgressBar with notebook support. &lt;/li&gt;\n&lt;li&gt;Added Parameter Schedulers and a variety of Learning Rate Schedulers. &lt;/li&gt;\n&lt;li&gt;Improved docs, added FAQ section.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;release notes - &lt;a href=""https://github.com/pytorch/ignite/releases/tag/v0.2.0""&gt;https://github.com/pytorch/ignite/releases/tag/v0.2.0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We would like to thank our community and all our contributors for the issues, PRs and support. We welcome contributions! &lt;/p&gt;\n\n&lt;p&gt;Happy training!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?auto=webp&amp;s=07ca976f2628a4aeaf3183ce72eec096643e840f', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=950ea7a0f777157bbd0b8745b20e06aa336dd923', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4153bf0d485e12346f9dd8c386a31de9512595a0', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=50e63f6ec38baa90825c802fb91a3cac2c8de8d3', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_YPUxJ8AyYwgpO9r-tD7JUoQ-Nx8IlATLoEfmjW48iQ'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'bbbxaw', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'pie_oh_my_', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/bbbxaw/p_ignite_highlevel_library_to_help_with_training/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/bbbxaw/p_ignite_highlevel_library_to_help_with_training/', 'subreddit_subscribers': 1740778, 'created_utc': 1554836797.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_bbbxaw,,,,,
908,,pytorch,"Hi, I have an rtx graphic card and I would like to make use of the tensor cores in order to speed up the training process and forward process.

I've used nvidia apex amp for mixed precision training, although I don't know if correctly. Do I have to say model.half() or it's not necessary?

Although I'm very interested in forward pass in fp16 since I'm working in an application which requieres the maximum prediction speed possible. How can I do it? Is there any good guide?",t2_3b7uazps,False,,0,False,Guide to fp16 forward and mixed precision training,[],r/pytorch,False,6,,0,,,False,t3_bawqiz,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1554774083.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I have an rtx graphic card and I would like to make use of the tensor cores in order to speed up the training process and forward process.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve used nvidia apex amp for mixed precision training, although I don&amp;#39;t know if correctly. Do I have to say model.half() or it&amp;#39;s not necessary?&lt;/p&gt;

&lt;p&gt;Although I&amp;#39;m very interested in forward pass in fp16 since I&amp;#39;m working in an application which requieres the maximum prediction speed possible. How can I do it? Is there any good guide?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bawqiz,True,,drr21,,4,True,all_ads,False,[],False,,/r/pytorch/comments/bawqiz/guide_to_fp16_forward_and_mixed_precision_training/,all_ads,False,https://www.reddit.com/r/pytorch/comments/bawqiz/guide_to_fp16_forward_and_mixed_precision_training/,7135,1554745283.0,0,,False,,,,,,,,
909,,pytorch,,t2_1zmygjcv,False,,0,False,Implemented in pytorch!,[],r/pytorch,False,6,,0,78.0,,False,t3_bah0qi,False,dark,0.64,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/3g9N58kMrxhtS0zzB2qATA8HIWcybyOMRemL5Q--zZg.jpg,False,,[],{},link,,False,,1554677654.0,text,6,,,text,self.MediaSynthesis,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Nx2Ss864fk5s3TBvKc82qR_cQ1BWMIQpBURj9i4A_Vo.jpg?auto=webp&amp;s=20025e8a87a961aeafdb0a46d85b05367362c4e4', 'width': 1280, 'height': 720}, 'resolutions': [{'url': 'https://external-preview.redd.it/Nx2Ss864fk5s3TBvKc82qR_cQ1BWMIQpBURj9i4A_Vo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99978759c532e2183de1ff9dd211e1ef96565471', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/Nx2Ss864fk5s3TBvKc82qR_cQ1BWMIQpBURj9i4A_Vo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c51d273937f499bba29f31e9d3c14ff2ecefd10f', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/Nx2Ss864fk5s3TBvKc82qR_cQ1BWMIQpBURj9i4A_Vo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e4d23b10f146699d53bc9ef539381eecf2a813a', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/Nx2Ss864fk5s3TBvKc82qR_cQ1BWMIQpBURj9i4A_Vo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=894c1db1e52110e4bcf36a3840fbd0f6a7071907', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/Nx2Ss864fk5s3TBvKc82qR_cQ1BWMIQpBURj9i4A_Vo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=061a25deb7082934ae9735f33fb66adf17942c73', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/Nx2Ss864fk5s3TBvKc82qR_cQ1BWMIQpBURj9i4A_Vo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e2d8940e50507f4108fef4d7386c676d7612caeb', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'bw7Bik4le3s6_yIBzS6wUIU58H6r0Ltf3rle2dikMPQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,bah0qi,True,,soumya6097,,2,False,all_ads,False,[],False,,/r/pytorch/comments/bah0qi/implemented_in_pytorch/,all_ads,False,https://www.reddit.com/r/MediaSynthesis/comments/ba7k9c/icface_ai_that_can_make_an_face_image_lively/,7135,1554648854.0,0,,False,https://www.reddit.com/r/MediaSynthesis/comments/ba7k9c/icface_ai_that_can_make_an_face_image_lively/,"[{'approved_at_utc': None, 'subreddit': 'MediaSynthesis', 'selftext': '&amp;#x200B;\n\n**Please watch this video to see the ICface in action**: [**https://lnkd.in/gHkmPcS**](https://lnkd.in/gHkmPcS)\n\nPlease follow our project page for updates on implementation: [**https://lnkd.in/gusAyjb**](https://lnkd.in/gusAyjb)\n\nPlease see our paper for more details: [**https://lnkd.in/gFpGgyS**](https://lnkd.in/gFpGgyS)\n\n&amp;#x200B;\n\n[ICface](https://reddit.com/link/ba7k9c/video/iygy52ziwoq21/player)', 'author_fullname': 't2_1zmygjcv', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '""ICface"", AI that can make an face image lively', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MediaSynthesis', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 78, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'iygy52ziwoq21': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/ba7k9c/asset/iygy52ziwoq21/DASHPlaylist.mpd?a=1618044327%2CYTZkZDZkZjJiNDcwODhkNjE3NjdmZWE5NTE0OWM4NmE2NGY3MDMwY2ZjNTMyODI3YTE4ZjA5MTgzZTZlZjgwMA%3D%3D&amp;v=1&amp;f=sd', 'x': 1920, 'y': 1080, 'hlsUrl': 'https://v.redd.it/link/ba7k9c/asset/iygy52ziwoq21/HLSPlaylist.m3u8?a=1618044327%2COGI0MDNhZDU0YjIxNDk4ZjljNmQxNDI4ODU4OTM2NWIyZjkxMGZmYWJlMzcyODc5ZWZlMjNlMDZiNmU1ZGJjZA%3D%3D&amp;v=1&amp;f=sd', 'id': 'iygy52ziwoq21', 'isGif': False}}, 'name': 't3_ba7k9c', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 14, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Media Manipulation', 'can_mod_post': False, 'score': 14, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/3g9N58kMrxhtS0zzB2qATA8HIWcybyOMRemL5Q--zZg.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1554605366.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MediaSynthesis', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Please watch this video to see the ICface in action&lt;/strong&gt;: &lt;a href=""https://lnkd.in/gHkmPcS""&gt;&lt;strong&gt;https://lnkd.in/gHkmPcS&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Please follow our project page for updates on implementation: &lt;a href=""https://lnkd.in/gusAyjb""&gt;&lt;strong&gt;https://lnkd.in/gusAyjb&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Please see our paper for more details: &lt;a href=""https://lnkd.in/gFpGgyS""&gt;&lt;strong&gt;https://lnkd.in/gFpGgyS&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/ba7k9c/video/iygy52ziwoq21/player""&gt;ICface&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/Nx2Ss864fk5s3TBvKc82qR_cQ1BWMIQpBURj9i4A_Vo.jpg?auto=webp&amp;s=20025e8a87a961aeafdb0a46d85b05367362c4e4', 'width': 1280, 'height': 720}, 'resolutions': [{'url': 'https://external-preview.redd.it/Nx2Ss864fk5s3TBvKc82qR_cQ1BWMIQpBURj9i4A_Vo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99978759c532e2183de1ff9dd211e1ef96565471', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/Nx2Ss864fk5s3TBvKc82qR_cQ1BWMIQpBURj9i4A_Vo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c51d273937f499bba29f31e9d3c14ff2ecefd10f', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/Nx2Ss864fk5s3TBvKc82qR_cQ1BWMIQpBURj9i4A_Vo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e4d23b10f146699d53bc9ef539381eecf2a813a', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/Nx2Ss864fk5s3TBvKc82qR_cQ1BWMIQpBURj9i4A_Vo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=894c1db1e52110e4bcf36a3840fbd0f6a7071907', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/Nx2Ss864fk5s3TBvKc82qR_cQ1BWMIQpBURj9i4A_Vo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=061a25deb7082934ae9735f33fb66adf17942c73', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/Nx2Ss864fk5s3TBvKc82qR_cQ1BWMIQpBURj9i4A_Vo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e2d8940e50507f4108fef4d7386c676d7612caeb', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'bw7Bik4le3s6_yIBzS6wUIU58H6r0Ltf3rle2dikMPQ'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '862bc40e-31ff-11e9-9892-0e01d6f93842', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_f2bje', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ba7k9c', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'soumya6097', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MediaSynthesis/comments/ba7k9c/icface_ai_that_can_make_an_face_image_lively/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MediaSynthesis/comments/ba7k9c/icface_ai_that_can_make_an_face_image_lively/', 'subreddit_subscribers': 22032, 'created_utc': 1554576566.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_ba7k9c,,,,,
910,,pytorch,Any clue how I can apply batch normalization in an LSTM cell? ,t2_7tb2j,False,,0,False,Batch normalization on nn.LSTM,[],r/pytorch,False,6,,0,,,False,t3_b9rs60,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1554502857.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Any clue how I can apply batch normalization in an LSTM cell? &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,b9rs60,True,,gevezex,,5,True,all_ads,False,[],False,,/r/pytorch/comments/b9rs60/batch_normalization_on_nnlstm/,all_ads,False,https://www.reddit.com/r/pytorch/comments/b9rs60/batch_normalization_on_nnlstm/,7135,1554474057.0,0,,False,,,,,,,,
911,,pytorch,"The command ‘import torch’ causes the build error: ’ModuleNotFoundError: No module named ‘torch’’.

&amp;#x200B;

But when I run the command in the terminal, pytorch works just fine (Python 3.7.X).",t2_m1dh5,False,,0,False,Why does importing Pytorch in Sublime Text cause a ModuleNotFoundError?,[],r/pytorch,False,6,,0,,,False,t3_b8wr30,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1554319806.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The command ‘import torch’ causes the build error: ’ModuleNotFoundError: No module named ‘torch’’.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;But when I run the command in the terminal, pytorch works just fine (Python 3.7.X).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,b8wr30,True,,2stefan2000,,3,True,all_ads,False,[],False,,/r/pytorch/comments/b8wr30/why_does_importing_pytorch_in_sublime_text_cause/,all_ads,False,https://www.reddit.com/r/pytorch/comments/b8wr30/why_does_importing_pytorch_in_sublime_text_cause/,7135,1554291006.0,0,,False,,,,,,,,True
912,,pytorch,"Hi guys, as I've been working on my PhD in AI at NYU, I've been developing a rapid research library to use best practices. I couldn't find anything where it was quick to iterate on, but with enough control for researchers to modify what they wanted. 

Looking for contributors to help clean it up!

[https://github.com/williamFalcon/pytorch-lightning](https://github.com/williamFalcon/pytorch-lightning)",t2_zxptg,False,,0,False,Keras for AI researchers using Pytorch,[],r/pytorch,False,6,,0,,,False,t3_b84z3r,False,dark,0.88,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},self,,True,,1554166478.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys, as I&amp;#39;ve been working on my PhD in AI at NYU, I&amp;#39;ve been developing a rapid research library to use best practices. I couldn&amp;#39;t find anything where it was quick to iterate on, but with enough control for researchers to modify what they wanted. &lt;/p&gt;

&lt;p&gt;Looking for contributors to help clean it up!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/williamFalcon/pytorch-lightning""&gt;https://github.com/williamFalcon/pytorch-lightning&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/s8-_oq9__t_DegeOyVzslyO-XMm-fcbx6H5-9RvlC2E.jpg?auto=webp&amp;s=4aa783e30b54c272eb6b96d98c98afac2e974eca', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/s8-_oq9__t_DegeOyVzslyO-XMm-fcbx6H5-9RvlC2E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c126e78d44d7a2f06c8542306160698b40f9f93c', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/s8-_oq9__t_DegeOyVzslyO-XMm-fcbx6H5-9RvlC2E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fbc4772057ee310f6d24a6bafdee44413aca3c58', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/s8-_oq9__t_DegeOyVzslyO-XMm-fcbx6H5-9RvlC2E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e99ca02a1a2cd3e32a8c13ef4cf38fbec5611ecd', 'width': 320, 'height': 320}], 'variants': {}, 'id': '5qWKboejbVQcsI-Lw_9AKNca-2c4Lu6KkKQaIsgC7Yk'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,b84z3r,True,,waf04,,2,False,all_ads,False,[],False,,/r/pytorch/comments/b84z3r/keras_for_ai_researchers_using_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/b84z3r/keras_for_ai_researchers_using_pytorch/,7135,1554137678.0,0,,False,,,,,,,,
913,,pytorch,"torch (1.0.1.post2) (via pip3)

Spyder3 (3.3.3)

Python 3.5.2 64-bit 

Qt 5.12.2 

PyQt5 5.12.1

Linux 4.4.0-143-generic 

IPython 7.4.0

PyQtWebEngine (5.12.1)

jupyter-client (5.2.4)

jupyter-core (4.4.0)

spyder-kernels (0.4.2)

Ubuntu16.04

&amp;#x200B;

Error occurs the second time I try to run a program that imports torch or something from torch.

I tried reinstalling, upgrading, downgrading nothing seems to help. Programs that don't import torch run fine.

Thank you in advance.

&amp;#x200B;",t2_cjsqo,False,,0,False,"""RuntimeError: method 'detach' already has a docstring"" Can someone help?",[],r/pytorch,False,6,,0,,,False,t3_b825eu,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1554152900.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;torch (1.0.1.post2) (via pip3)&lt;/p&gt;

&lt;p&gt;Spyder3 (3.3.3)&lt;/p&gt;

&lt;p&gt;Python 3.5.2 64-bit &lt;/p&gt;

&lt;p&gt;Qt 5.12.2 &lt;/p&gt;

&lt;p&gt;PyQt5 5.12.1&lt;/p&gt;

&lt;p&gt;Linux 4.4.0-143-generic &lt;/p&gt;

&lt;p&gt;IPython 7.4.0&lt;/p&gt;

&lt;p&gt;PyQtWebEngine (5.12.1)&lt;/p&gt;

&lt;p&gt;jupyter-client (5.2.4)&lt;/p&gt;

&lt;p&gt;jupyter-core (4.4.0)&lt;/p&gt;

&lt;p&gt;spyder-kernels (0.4.2)&lt;/p&gt;

&lt;p&gt;Ubuntu16.04&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Error occurs the second time I try to run a program that imports torch or something from torch.&lt;/p&gt;

&lt;p&gt;I tried reinstalling, upgrading, downgrading nothing seems to help. Programs that don&amp;#39;t import torch run fine.&lt;/p&gt;

&lt;p&gt;Thank you in advance.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,b825eu,True,,ReasonablyBadass,,4,True,all_ads,False,[],False,,/r/pytorch/comments/b825eu/runtimeerror_method_detach_already_has_a/,all_ads,False,https://www.reddit.com/r/pytorch/comments/b825eu/runtimeerror_method_detach_already_has_a/,7135,1554124100.0,0,,False,,,,,,,,
914,,pytorch,"To get a feeling for the PyTorch C++ frontend I'm trying to convert a few existing scripts. Thinking about slicing, in Python one might write:

    a = torch.rand(10,6)
    print(a[None,:8:2,1::3].shape)
    
    &gt;&gt;&gt; torch.Size([1, 4, 2])

The closest equivalent in the C++ frontend that I can come up with is

    torch::Tensor a = torch::rand({10,6});
    auto b = a.slice(0,0,8,2).slice(1,1,-1,3).unsqueeze(0);
    std::cout &lt;&lt; b.sizes() &lt;&lt; std::endl;

    &gt;&gt;&gt; [1,4,2]

Is there a shorthand for this?",t2_3y5il,False,,0,False,Tensor slicing in C++ Frontend?,[],r/pytorch,False,6,,0,,,False,t3_b7ndpu,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1554037523.0,,[],{},,,True,,1554065130.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;To get a feeling for the PyTorch C++ frontend I&amp;#39;m trying to convert a few existing scripts. Thinking about slicing, in Python one might write:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a = torch.rand(10,6)
print(a[None,:8:2,1::3].shape)

&amp;gt;&amp;gt;&amp;gt; torch.Size([1, 4, 2])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The closest equivalent in the C++ frontend that I can come up with is&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;torch::Tensor a = torch::rand({10,6});
auto b = a.slice(0,0,8,2).slice(1,1,-1,3).unsqueeze(0);
std::cout &amp;lt;&amp;lt; b.sizes() &amp;lt;&amp;lt; std::endl;

&amp;gt;&amp;gt;&amp;gt; [1,4,2]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Is there a shorthand for this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,b7ndpu,True,,Squirrl,,1,True,all_ads,False,[],False,,/r/pytorch/comments/b7ndpu/tensor_slicing_in_c_frontend/,all_ads,False,https://www.reddit.com/r/pytorch/comments/b7ndpu/tensor_slicing_in_c_frontend/,7135,1554036330.0,0,,False,,,,,,,,
915,,pytorch,"Hello everyone!

I do multiCLASS segmentation on pytorch and I am having trouble creating a custom `\`Dataset`\` for this. I use Unet for segmentation and this model accepts the input of the `input - image target - mask`. How to make my  \``Dataset`\` so that would split the data into 4 classes, now in `__get_item__` I return the dictionary \``{'image': image, 'target': mask, 'class': class}`\`. But I think that this approach is not correct. Somebody can tell me how to do it properly. Thanks for the help!",t2_290ymnhy,False,,0,False,How to make custom dataset for multiclass segmentation?,[],r/pytorch,False,6,,0,,,False,t3_b6i1xk,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1553804152.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;

&lt;p&gt;I do multiCLASS segmentation on pytorch and I am having trouble creating a custom &lt;code&gt;\&lt;/code&gt;Dataset&lt;code&gt;\&lt;/code&gt; for this. I use Unet for segmentation and this model accepts the input of the &lt;code&gt;input - image target - mask&lt;/code&gt;. How to make my  `&lt;code&gt;Dataset&lt;/code&gt;` so that would split the data into 4 classes, now in &lt;code&gt;__get_item__&lt;/code&gt; I return the dictionary `&lt;code&gt;{&amp;#39;image&amp;#39;: image, &amp;#39;target&amp;#39;: mask, &amp;#39;class&amp;#39;: class}&lt;/code&gt;`. But I think that this approach is not correct. Somebody can tell me how to do it properly. Thanks for the help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,b6i1xk,True,,Sterben712,,1,True,all_ads,False,[],False,,/r/pytorch/comments/b6i1xk/how_to_make_custom_dataset_for_multiclass/,all_ads,False,https://www.reddit.com/r/pytorch/comments/b6i1xk/how_to_make_custom_dataset_for_multiclass/,7135,1553775352.0,0,,False,,,,,,,,
916,,pytorch,"Hi, 

I'm trying to install PyTorch on computer (Windows 10 OS). 

I am using the following command in the windows command line:

 conda install pytorch-cpu torchvision-cpu -c pytorch 

&amp;#x200B;

Unfortunately I get the following error:

PackagesNotFoundError: The following packages are not available from current channels:

  \- pytorch-cpu

  \- torchvision-cpu

  \- pytorch-cpu\[version='&gt;=1.0.0'\]

 

Followed by a list of current channels.

Has anyone run into this error? Unfortunately searching google has failed me in this case ",t2_13haee,False,,0,False,Trying to isntall PyTorch on Windows 10,[],r/pytorch,False,6,,0,,,False,t3_b59cvb,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1553541277.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to install PyTorch on computer (Windows 10 OS). &lt;/p&gt;

&lt;p&gt;I am using the following command in the windows command line:&lt;/p&gt;

&lt;p&gt;conda install pytorch-cpu torchvision-cpu -c pytorch &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Unfortunately I get the following error:&lt;/p&gt;

&lt;p&gt;PackagesNotFoundError: The following packages are not available from current channels:&lt;/p&gt;

&lt;p&gt;- pytorch-cpu&lt;/p&gt;

&lt;p&gt;- torchvision-cpu&lt;/p&gt;

&lt;p&gt;- pytorch-cpu[version=&amp;#39;&amp;gt;=1.0.0&amp;#39;]&lt;/p&gt;

&lt;p&gt;Followed by a list of current channels.&lt;/p&gt;

&lt;p&gt;Has anyone run into this error? Unfortunately searching google has failed me in this case &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,b59cvb,True,,Jokowski,,7,True,all_ads,False,[],False,,/r/pytorch/comments/b59cvb/trying_to_isntall_pytorch_on_windows_10/,all_ads,False,https://www.reddit.com/r/pytorch/comments/b59cvb/trying_to_isntall_pytorch_on_windows_10/,7135,1553512477.0,0,,False,,,,,,,,
917,,pytorch,"Apparently AI has little to do with the actual implementation but more of the logical thought driving the rule-based System.

&amp;#x200B;

As of right now, Python is the die hard language, everything else is experimental for most of the big API's.  Since, I have gone through a rigorous degree and learned C++ as my first language, I would rather do a bottom up approach.  Would this appropriate with the tools we have today?  Would I be stuck in the theory of neural nets or would this provide greater intuition for fine tuning a training model?

Ultimately, I need advice on a independent research course and on what level should I spend my time in the beginning.  I am sure that AI or data driven processes are what interest me so I am deciding to pursue AI first.  My implementation for my proposal is.

&amp;#x200B;

What API or library?

Tflow or Pytorch

&amp;#x200B;

should I focus on visual analysis or could it be soundwaves?

I found that Pytorch might be better suited for sound.

&amp;#x200B;

What is more valuable in industry?  

The math and stats based intuition or strict technology application.

&amp;#x200B;

I appreciate any and all comments.",t2_2juyu8uw,False,,0,False,I am a Math major with programming experience,[],r/pytorch,False,6,,0,,,False,t3_b1nu4l,False,dark,0.38,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1552734466.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Apparently AI has little to do with the actual implementation but more of the logical thought driving the rule-based System.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;As of right now, Python is the die hard language, everything else is experimental for most of the big API&amp;#39;s.  Since, I have gone through a rigorous degree and learned C++ as my first language, I would rather do a bottom up approach.  Would this appropriate with the tools we have today?  Would I be stuck in the theory of neural nets or would this provide greater intuition for fine tuning a training model?&lt;/p&gt;

&lt;p&gt;Ultimately, I need advice on a independent research course and on what level should I spend my time in the beginning.  I am sure that AI or data driven processes are what interest me so I am deciding to pursue AI first.  My implementation for my proposal is.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What API or library?&lt;/p&gt;

&lt;p&gt;Tflow or Pytorch&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;should I focus on visual analysis or could it be soundwaves?&lt;/p&gt;

&lt;p&gt;I found that Pytorch might be better suited for sound.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What is more valuable in industry?  &lt;/p&gt;

&lt;p&gt;The math and stats based intuition or strict technology application.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I appreciate any and all comments.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,b1nu4l,True,,knickerBockerJones,,9,True,all_ads,False,[],False,,/r/pytorch/comments/b1nu4l/i_am_a_math_major_with_programming_experience/,all_ads,False,https://www.reddit.com/r/pytorch/comments/b1nu4l/i_am_a_math_major_with_programming_experience/,7135,1552705666.0,0,,False,,,,,,,,
918,,pytorch,"Is there any guide where it is explained how to use it properly?

&amp;#x200B;

Thanks!

&amp;#x200B;

&amp;#x200B;",t2_3b7uazps,False,,0,False,How to use mixed precision training (FP16+FP32) in pytorch,[],r/pytorch,False,6,,0,,,False,t3_b1fd34,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1552687555.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there any guide where it is explained how to use it properly?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,b1fd34,True,,drr21,,1,True,all_ads,False,[],False,,/r/pytorch/comments/b1fd34/how_to_use_mixed_precision_training_fp16fp32_in/,all_ads,False,https://www.reddit.com/r/pytorch/comments/b1fd34/how_to_use_mixed_precision_training_fp16fp32_in/,7135,1552658755.0,0,,False,,,,,,,,
919,,pytorch,"Hi everybody,   
Honestly i don't understand how to work with PyTorch.

&amp;#x200B;

**My question is:**   
I have already wrote all my scripts, and i would like them to run on my GPU (I have some data mining job that would take approx. 80 days to run).  I used no pytorch module or what so ever, only the basic stuff. 

I would'nt mind to show my script (screen sharing/skype) with one of you and if you think it is possible to make it work, I am willing to pay you for your time (teaching me how to make it work)

&amp;#x200B;

Thanks, 

feel free to DM me / answer here / ask for clarification  
",t2_l56l0g5,False,,0,False,Noob question (Possible $$$ Reward),[],r/pytorch,False,6,,0,,,False,t3_b157i6,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1552623475.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everybody,&lt;br/&gt;
Honestly i don&amp;#39;t understand how to work with PyTorch.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;My question is:&lt;/strong&gt;&lt;br/&gt;
I have already wrote all my scripts, and i would like them to run on my GPU (I have some data mining job that would take approx. 80 days to run).  I used no pytorch module or what so ever, only the basic stuff. &lt;/p&gt;

&lt;p&gt;I would&amp;#39;nt mind to show my script (screen sharing/skype) with one of you and if you think it is possible to make it work, I am willing to pay you for your time (teaching me how to make it work)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks, &lt;/p&gt;

&lt;p&gt;feel free to DM me / answer here / ask for clarification  &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,b157i6,True,,alex_von11,,7,True,all_ads,False,[],False,,/r/pytorch/comments/b157i6/noob_question_possible_reward/,all_ads,False,https://www.reddit.com/r/pytorch/comments/b157i6/noob_question_possible_reward/,7135,1552594675.0,0,,False,,,,,,,,
920,,pytorch,"When you wish to not update (freeze) parts of the network, the recommended solution is to set requires\_grad = False, \*\*and/or (please confirm?)\*\* not send the parameters you wish to freeze to the optimizer input.

&amp;#x200B;

I would like to clarify that the requires\_grad = False simply avoids unnecessary computation, update, and storage of gradients at those nodes and does not create subgraphs which saves memory.

&amp;#x200B;

**However, the parameters with requires\_grad = False will still contain a grad\_fn (if it has one) so that in the backward pass, the gradient from that node is still technically still calculated passed backward and the chain rule is still maintained and used for the parameters with requires\_grad = True?**

&amp;#x200B;

So if we have layers L1 -&gt; L2 -&gt; LOSS and want to freeze L2, the gradient of LOSS wrt L1 requires the gradient of LOSS wrt L2 as per chain rule. My confusion is that as the name implies, requires\_grad = False on L2 will not compute the gradient, but we need it to update L1, no?

&amp;#x200B;

Please confirm :) Thank you",t2_ej36r,False,,0,False,Understanding of requires_grad = False,[],r/pytorch,False,6,,0,,,False,t3_b0qbnc,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1552534895.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When you wish to not update (freeze) parts of the network, the recommended solution is to set requires_grad = False, **and/or (please confirm?)** not send the parameters you wish to freeze to the optimizer input.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I would like to clarify that the requires_grad = False simply avoids unnecessary computation, update, and storage of gradients at those nodes and does not create subgraphs which saves memory.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;However, the parameters with requires_grad = False will still contain a grad_fn (if it has one) so that in the backward pass, the gradient from that node is still technically still calculated passed backward and the chain rule is still maintained and used for the parameters with requires_grad = True?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So if we have layers L1 -&amp;gt; L2 -&amp;gt; LOSS and want to freeze L2, the gradient of LOSS wrt L1 requires the gradient of LOSS wrt L2 as per chain rule. My confusion is that as the name implies, requires_grad = False on L2 will not compute the gradient, but we need it to update L1, no?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Please confirm :) Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,b0qbnc,True,,likethevegetable,,1,True,all_ads,False,[],False,,/r/pytorch/comments/b0qbnc/understanding_of_requires_grad_false/,all_ads,False,https://www.reddit.com/r/pytorch/comments/b0qbnc/understanding_of_requires_grad_false/,7135,1552506095.0,0,,False,,,,,,,,
921,,pytorch,"On the pytorch github, I found a repository of examples ([https://github.com/pytorch/examples](https://github.com/pytorch/examples)) and within those examples I found how to train a dcgan ([https://github.com/pytorch/examples/tree/master/dcgan](https://github.com/pytorch/examples/tree/master/dcgan)).

I git cloned the examples repository to an environment I created using Anaconda in my terminal. I then opened the dcgan example in a jupyter notebook and am trying to run the code. [main.py](https://main.py) shows I am supposed to clarify whether I am importing the provided lsun dataset (which I have downloaded) or my own folder. Ideally I could train the gan on a folder of images I have collected, but I want to run the lsun first to see if I can even get this to work. How can I specify that I am trying to plug in the lsun folder into the dcgan?

The two required lines are what I'm having the most trouble with. I realize '--dataset' is asking me to specify whether it's lsun, folder of my own, etc., and '--dataroot' is asking me where to retrieve that dataset, but I don't know the syntax on how to specify these two things :(

Below is the code and further below is a video of where I get stuck. Thanks so much for any help you could provide!

    from __future__ import print_function
    import argparse
    import os
    import random
    import torch
    import torch.nn as nn
    import torch.nn.parallel
    import torch.backends.cudnn as cudnn
    import torch.optim as optim
    import torch.utils.data
    import torchvision.datasets as dset
    import torchvision.transforms as transforms
    import torchvision.utils as vutils
    
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', required=True, help='cifar10 | lsun | mnist |imagenet | folder | lfw | fake')
    parser.add_argument('--dataroot', required=True, help='path to dataset')
    parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)
    parser.add_argument('--batchSize', type=int, default=64, help='input batch size')
    parser.add_argument('--imageSize', type=int, default=64, help='the height / width of the input image to network')
    parser.add_argument('--nz', type=int, default=100, help='size of the latent z vector')
    parser.add_argument('--ngf', type=int, default=64)
    parser.add_argument('--ndf', type=int, default=64)
    parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')
    parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')
    parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')
    parser.add_argument('--cuda', action='store_true', help='enables cuda')
    parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')
    parser.add_argument('--netG', default='', help=""path to netG (to continue training)"")
    parser.add_argument('--netD', default='', help=""path to netD (to continue training)"")
    parser.add_argument('--outf', default='.', help='folder to output images and model checkpoints')
    parser.add_argument('--manualSeed', type=int, help='manual seed')

&amp;#x200B;

https://reddit.com/link/b0hv02/video/4ng3ghg7ftl21/player",t2_xysk9,False,,0,False,Sorry to be a noob – How to link a dataset folder within the pytorch dcgan example,[],r/pytorch,False,6,,0,140.0,,False,t3_b0hv02,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://a.thumbs.redditmedia.com/vvIp_c6Tk9gKsKERuWB2kRlMUoYDhTWMIJQBcrOOiQ4.jpg,False,,[],{},self,,True,,1552481079.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;On the pytorch github, I found a repository of examples (&lt;a href=""https://github.com/pytorch/examples""&gt;https://github.com/pytorch/examples&lt;/a&gt;) and within those examples I found how to train a dcgan (&lt;a href=""https://github.com/pytorch/examples/tree/master/dcgan""&gt;https://github.com/pytorch/examples/tree/master/dcgan&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;I git cloned the examples repository to an environment I created using Anaconda in my terminal. I then opened the dcgan example in a jupyter notebook and am trying to run the code. &lt;a href=""https://main.py""&gt;main.py&lt;/a&gt; shows I am supposed to clarify whether I am importing the provided lsun dataset (which I have downloaded) or my own folder. Ideally I could train the gan on a folder of images I have collected, but I want to run the lsun first to see if I can even get this to work. How can I specify that I am trying to plug in the lsun folder into the dcgan?&lt;/p&gt;

&lt;p&gt;The two required lines are what I&amp;#39;m having the most trouble with. I realize &amp;#39;--dataset&amp;#39; is asking me to specify whether it&amp;#39;s lsun, folder of my own, etc., and &amp;#39;--dataroot&amp;#39; is asking me where to retrieve that dataset, but I don&amp;#39;t know the syntax on how to specify these two things :(&lt;/p&gt;

&lt;p&gt;Below is the code and further below is a video of where I get stuck. Thanks so much for any help you could provide!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from __future__ import print_function
import argparse
import os
import random
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils


parser = argparse.ArgumentParser()
parser.add_argument(&amp;#39;--dataset&amp;#39;, required=True, help=&amp;#39;cifar10 | lsun | mnist |imagenet | folder | lfw | fake&amp;#39;)
parser.add_argument(&amp;#39;--dataroot&amp;#39;, required=True, help=&amp;#39;path to dataset&amp;#39;)
parser.add_argument(&amp;#39;--workers&amp;#39;, type=int, help=&amp;#39;number of data loading workers&amp;#39;, default=2)
parser.add_argument(&amp;#39;--batchSize&amp;#39;, type=int, default=64, help=&amp;#39;input batch size&amp;#39;)
parser.add_argument(&amp;#39;--imageSize&amp;#39;, type=int, default=64, help=&amp;#39;the height / width of the input image to network&amp;#39;)
parser.add_argument(&amp;#39;--nz&amp;#39;, type=int, default=100, help=&amp;#39;size of the latent z vector&amp;#39;)
parser.add_argument(&amp;#39;--ngf&amp;#39;, type=int, default=64)
parser.add_argument(&amp;#39;--ndf&amp;#39;, type=int, default=64)
parser.add_argument(&amp;#39;--niter&amp;#39;, type=int, default=25, help=&amp;#39;number of epochs to train for&amp;#39;)
parser.add_argument(&amp;#39;--lr&amp;#39;, type=float, default=0.0002, help=&amp;#39;learning rate, default=0.0002&amp;#39;)
parser.add_argument(&amp;#39;--beta1&amp;#39;, type=float, default=0.5, help=&amp;#39;beta1 for adam. default=0.5&amp;#39;)
parser.add_argument(&amp;#39;--cuda&amp;#39;, action=&amp;#39;store_true&amp;#39;, help=&amp;#39;enables cuda&amp;#39;)
parser.add_argument(&amp;#39;--ngpu&amp;#39;, type=int, default=1, help=&amp;#39;number of GPUs to use&amp;#39;)
parser.add_argument(&amp;#39;--netG&amp;#39;, default=&amp;#39;&amp;#39;, help=&amp;quot;path to netG (to continue training)&amp;quot;)
parser.add_argument(&amp;#39;--netD&amp;#39;, default=&amp;#39;&amp;#39;, help=&amp;quot;path to netD (to continue training)&amp;quot;)
parser.add_argument(&amp;#39;--outf&amp;#39;, default=&amp;#39;.&amp;#39;, help=&amp;#39;folder to output images and model checkpoints&amp;#39;)
parser.add_argument(&amp;#39;--manualSeed&amp;#39;, type=int, help=&amp;#39;manual seed&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://reddit.com/link/b0hv02/video/4ng3ghg7ftl21/player""&gt;https://reddit.com/link/b0hv02/video/4ng3ghg7ftl21/player&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?auto=webp&amp;s=07ca976f2628a4aeaf3183ce72eec096643e840f', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=950ea7a0f777157bbd0b8745b20e06aa336dd923', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4153bf0d485e12346f9dd8c386a31de9512595a0', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/K4k7NYRy6-yozwRAemDUp5dCrNR46kezue_S8_a32ko.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=50e63f6ec38baa90825c802fb91a3cac2c8de8d3', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_YPUxJ8AyYwgpO9r-tD7JUoQ-Nx8IlATLoEfmjW48iQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,b0hv02,True,,optimysticman,,0,True,all_ads,False,[],False,,/r/pytorch/comments/b0hv02/sorry_to_be_a_noob_how_to_link_a_dataset_folder/,all_ads,False,https://www.reddit.com/r/pytorch/comments/b0hv02/sorry_to_be_a_noob_how_to_link_a_dataset_folder/,7135,1552452279.0,0,,False,,,,"{'4ng3ghg7ftl21': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/b0hv02/asset/4ng3ghg7ftl21/DASHPlaylist.mpd?a=1618044327%2CYzRiM2QwMWY2MTVkMzE3ZTg1NjM4ZGQ1ZTNiYzkzMGVhYmNmMmMwN2I3ZDMyM2ZmMDI4ZTYyYmM0MGI4M2M1Nw%3D%3D&amp;v=1&amp;f=sd', 'x': 1728, 'y': 1080, 'hlsUrl': 'https://v.redd.it/link/b0hv02/asset/4ng3ghg7ftl21/HLSPlaylist.m3u8?a=1618044327%2CMGNlZjNjNGE0Y2YwMjczNTZjNzdlODdkYzNlN2Q4ZDYyNWMwYmVjNDg4YTdkZTJiN2JhYTg4MmZmODI1ZTgyNw%3D%3D&amp;v=1&amp;f=sd', 'id': '4ng3ghg7ftl21', 'isGif': False}}",,,,
922,,pytorch,"Hey all,

&amp;#x200B;

Conv3D changes behaviour above a certain batch size. MWE:

`import torch`   
`import torch.nn as nn`    
`conv1=nn.Conv3d(1, 10, kernel_size=(3, 3, 10), stride=(1, 1, 1), padding=(1, 1, 0)).cuda()`    
`x = torch.randn(24, 1, 100, 375, 128).cuda()`   
`print(conv1(x))`    
`x = torch.randn(25, 1, 100, 375, 128).cuda()`   
`print(conv1(x))`

&amp;#x200B;

The first print succeeds but the seconds says illegal memory access error.

I made a ticket on github, and I would like to ask if you can reproduce the error, and if you have any ideas.

[https://github.com/pytorch/pytorch/issues/17930](https://github.com/pytorch/pytorch/issues/17930)",t2_10he9o,False,,0,False,Strange behaviour for Conv3D after certain batch size,[],r/pytorch,False,6,,0,,,False,t3_b0jmt3,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1552495116.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Conv3D changes behaviour above a certain batch size. MWE:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;import torch&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;import torch.nn as nn&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;conv1=nn.Conv3d(1, 10, kernel_size=(3, 3, 10), stride=(1, 1, 1), padding=(1, 1, 0)).cuda()&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;x = torch.randn(24, 1, 100, 375, 128).cuda()&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;print(conv1(x))&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;x = torch.randn(25, 1, 100, 375, 128).cuda()&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;print(conv1(x))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The first print succeeds but the seconds says illegal memory access error.&lt;/p&gt;

&lt;p&gt;I made a ticket on github, and I would like to ask if you can reproduce the error, and if you have any ideas.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/pytorch/pytorch/issues/17930""&gt;https://github.com/pytorch/pytorch/issues/17930&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AXf84XvErragFEZCCcDWh5OdkbOJ7c3R5pR4DiqzkrU.jpg?auto=webp&amp;s=5178f8befddcde0b6f229a4b72652bb58e48cfda', 'width': 420, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/AXf84XvErragFEZCCcDWh5OdkbOJ7c3R5pR4DiqzkrU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3599bd1d2dbdae95b3336b7caddca00352c69d24', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/AXf84XvErragFEZCCcDWh5OdkbOJ7c3R5pR4DiqzkrU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=65f35c6bbc0d8e28f6b31c0de7330b26e360e912', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/AXf84XvErragFEZCCcDWh5OdkbOJ7c3R5pR4DiqzkrU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aea276bf291e2bcedbd995374b24b0b4740e866e', 'width': 320, 'height': 320}], 'variants': {}, 'id': '3I4HVgxIMYjwRe4-BAwBNZpdeOa33O0CgZVSOfjSsFM'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,b0jmt3,True,,paland3,,0,True,all_ads,False,[],False,,/r/pytorch/comments/b0jmt3/strange_behaviour_for_conv3d_after_certain_batch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/b0jmt3/strange_behaviour_for_conv3d_after_certain_batch/,7135,1552466316.0,0,,False,,,,,,,,
923,,pytorch,"Trying to bring Ignite into my training pipeline, but I've come across some issues with it, any help would be appreciated.

Not sure if I'm doing something wrong, but I tried to use Ignite with a basic LSTM. It kept throwing ""RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed"", caused by: 

    def forward(self, sentence):
            num_timesteps = sentence.size()[1]
            lstm_out, self.hidden = self.lstm(
                sentence.view(num_timesteps, 1, -1), self.hidden)
            score_space = self.hidden2output(lstm_out.view(num_timesteps, -1))
            scores = torch.sigmoid(score_space).view(score_space.size()[1], score_space.size()[0])
    
            return scores   

This was based on the tutorial code for Pytorch, and works with the training loop made from vanilla Pytorch.

I fixed the error by changing it to:

    def forward(self, sentence):
            num_timesteps = sentence.size()[1]
            lstm_out, hidden_temp = self.lstm(
                sentence.view(num_timesteps, 1, -1), (self.hidden, self.cell))
            self.hidden = hidden_temp[0]
            self.cell = hidden_temp[1]
            self.hidden = self.hidden.detach()
            self.cell = self.cell.detach()
            score_space = self.hidden2output(lstm_out.view(num_timesteps, -1))
            scores = torch.sigmoid(score_space).view(score_space.size()[1], score_space.size()[0])
    
            return scores

Is this intended behaviour, and does my fix negatively impact something I haven't taken into account?

And also, I find that it takes over 3x as many epochs to converge on a single sample using Ignite vs. my own training loop, could someone shed light on that behaviour? 

Thanks!",t2_a28u6,False,,0,False,Ignite doesn't repackage LSTM hidden properly? And slower to converge than hand-made training loop?,[],r/pytorch,False,6,,0,,,False,t3_azwfky,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1552355637.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Trying to bring Ignite into my training pipeline, but I&amp;#39;ve come across some issues with it, any help would be appreciated.&lt;/p&gt;

&lt;p&gt;Not sure if I&amp;#39;m doing something wrong, but I tried to use Ignite with a basic LSTM. It kept throwing &amp;quot;RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed&amp;quot;, caused by: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def forward(self, sentence):
        num_timesteps = sentence.size()[1]
        lstm_out, self.hidden = self.lstm(
            sentence.view(num_timesteps, 1, -1), self.hidden)
        score_space = self.hidden2output(lstm_out.view(num_timesteps, -1))
        scores = torch.sigmoid(score_space).view(score_space.size()[1], score_space.size()[0])

        return scores   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This was based on the tutorial code for Pytorch, and works with the training loop made from vanilla Pytorch.&lt;/p&gt;

&lt;p&gt;I fixed the error by changing it to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def forward(self, sentence):
        num_timesteps = sentence.size()[1]
        lstm_out, hidden_temp = self.lstm(
            sentence.view(num_timesteps, 1, -1), (self.hidden, self.cell))
        self.hidden = hidden_temp[0]
        self.cell = hidden_temp[1]
        self.hidden = self.hidden.detach()
        self.cell = self.cell.detach()
        score_space = self.hidden2output(lstm_out.view(num_timesteps, -1))
        scores = torch.sigmoid(score_space).view(score_space.size()[1], score_space.size()[0])

        return scores
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Is this intended behaviour, and does my fix negatively impact something I haven&amp;#39;t taken into account?&lt;/p&gt;

&lt;p&gt;And also, I find that it takes over 3x as many epochs to converge on a single sample using Ignite vs. my own training loop, could someone shed light on that behaviour? &lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,azwfky,True,,Linooney,,7,True,all_ads,False,[],False,,/r/pytorch/comments/azwfky/ignite_doesnt_repackage_lstm_hidden_properly_and/,all_ads,False,https://www.reddit.com/r/pytorch/comments/azwfky/ignite_doesnt_repackage_lstm_hidden_properly_and/,7135,1552326837.0,0,,False,,,,,,,,
924,,pytorch,"I've been developing in PyTorch for about a year and mostly all my utils are written in numpy in numpy. 

I always build DataLoaders and I'll load a sample, do a lot of processing on it, resizing, filtering, data augmentation, and so on, which ends up requiring a fair amount of code. Once the sample is ready, I convert it to a Tensor just before returning it. Then, during validation or inference, I'll convert the output back to numpy almost immediately and do all my analysis using more utility code.

This works well, but with a recent project I'd love to do a lot of this processing on the GPU. There are a few libraries to do this, like CuPy and Numba. However, I'm thinking perhaps I can just work directly on PyTorch tensors every step of the way.

Is it bad practice or inefficient? Is PyTorch too limited? 
",t2_5d6f3,False,,0,False,Is it bad practice to use PyTorch for utility code?,[],r/pytorch,False,6,,0,,,False,t3_az4h8q,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1552173506.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been developing in PyTorch for about a year and mostly all my utils are written in numpy in numpy. &lt;/p&gt;

&lt;p&gt;I always build DataLoaders and I&amp;#39;ll load a sample, do a lot of processing on it, resizing, filtering, data augmentation, and so on, which ends up requiring a fair amount of code. Once the sample is ready, I convert it to a Tensor just before returning it. Then, during validation or inference, I&amp;#39;ll convert the output back to numpy almost immediately and do all my analysis using more utility code.&lt;/p&gt;

&lt;p&gt;This works well, but with a recent project I&amp;#39;d love to do a lot of this processing on the GPU. There are a few libraries to do this, like CuPy and Numba. However, I&amp;#39;m thinking perhaps I can just work directly on PyTorch tensors every step of the way.&lt;/p&gt;

&lt;p&gt;Is it bad practice or inefficient? Is PyTorch too limited? &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,az4h8q,True,,hivesteel,,3,True,all_ads,False,[],False,,/r/pytorch/comments/az4h8q/is_it_bad_practice_to_use_pytorch_for_utility_code/,all_ads,False,https://www.reddit.com/r/pytorch/comments/az4h8q/is_it_bad_practice_to_use_pytorch_for_utility_code/,7135,1552144706.0,0,,False,,,,,,,,
925,,pytorch,"Is it possible to have a dot product of two column vectors in pytorch?

This code obviously doesn't work.

    vec_1 = torch.Tensor([[3],[5]])
    vec_2 = torch.Tensor([[2],[6]])
    
    vec_dot = torch.dot(vec_1,vec_2)
    
    print(vec_dot)

I am used to making dot product of two column vectors from algebra, how can I make it to work?",t2_4cltnh8,False,,0,False,How can I have a dot product of COLUMN vectors?,[],r/pytorch,False,6,,0,,,False,t3_aybbsk,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1551984485.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is it possible to have a dot product of two column vectors in pytorch?&lt;/p&gt;

&lt;p&gt;This code obviously doesn&amp;#39;t work.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vec_1 = torch.Tensor([[3],[5]])
vec_2 = torch.Tensor([[2],[6]])

vec_dot = torch.dot(vec_1,vec_2)

print(vec_dot)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I am used to making dot product of two column vectors from algebra, how can I make it to work?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,aybbsk,True,,IDontHaveNicknameToo,,1,True,all_ads,False,[],False,,/r/pytorch/comments/aybbsk/how_can_i_have_a_dot_product_of_column_vectors/,all_ads,False,https://www.reddit.com/r/pytorch/comments/aybbsk/how_can_i_have_a_dot_product_of_column_vectors/,7135,1551955685.0,0,,False,,,,,,,,
926,,pytorch,"Hi, I'm dealing with LSTM autoencoder from trajectories. I have trajectories of various length.

I sort them by length, so that the variance inside batches is reduced. How can I pad them only up to the longest length in the batch ?

So far when I try to do it after using the DataLoader I end up with exceptions as they aren't of same size.

Thanks !",t2_124aub,False,,0,False,Help: How to pad in a batch,[],r/pytorch,False,6,,0,,,False,t3_axyqsp,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1551907214.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m dealing with LSTM autoencoder from trajectories. I have trajectories of various length.&lt;/p&gt;

&lt;p&gt;I sort them by length, so that the variance inside batches is reduced. How can I pad them only up to the longest length in the batch ?&lt;/p&gt;

&lt;p&gt;So far when I try to do it after using the DataLoader I end up with exceptions as they aren&amp;#39;t of same size.&lt;/p&gt;

&lt;p&gt;Thanks !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,axyqsp,True,,kptn_spoutnovitch,,1,True,all_ads,False,[],False,,/r/pytorch/comments/axyqsp/help_how_to_pad_in_a_batch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/axyqsp/help_how_to_pad_in_a_batch/,7135,1551878414.0,0,,False,,,,,,,,
927,,pytorch,"I am sorry for probably asking a noob question but I couldn't find any answer. Without CUDA code worked prefectly but now i am having TypeError: tensor() missing 1 required positional arguments: ""data"" when i try to make empty CUDA tensor and concatenate to it. How can I do it? Here's the [code](https://pastebin.com/AVWbwQAh)(all of the tensors used are defined before in the code and that's not the problem with them)",t2_4cltnh8,False,,0,False,Hot to concatenate to CUDA tensor?,[],r/pytorch,False,6,,0,,,False,t3_axqn1s,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1551850965.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am sorry for probably asking a noob question but I couldn&amp;#39;t find any answer. Without CUDA code worked prefectly but now i am having TypeError: tensor() missing 1 required positional arguments: &amp;quot;data&amp;quot; when i try to make empty CUDA tensor and concatenate to it. How can I do it? Here&amp;#39;s the &lt;a href=""https://pastebin.com/AVWbwQAh""&gt;code&lt;/a&gt;(all of the tensors used are defined before in the code and that&amp;#39;s not the problem with them)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/apn0jimBDcu7v5NrQv4_AjqWqHWQMnATxOAbLbDgyQw.jpg?auto=webp&amp;s=fe7abb3ae0c8be0d3d9905987e5f8442ae661e3c', 'width': 250, 'height': 250}, 'resolutions': [{'url': 'https://external-preview.redd.it/apn0jimBDcu7v5NrQv4_AjqWqHWQMnATxOAbLbDgyQw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5367be8e7095cf04498ec9ba39774b325c032522', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/apn0jimBDcu7v5NrQv4_AjqWqHWQMnATxOAbLbDgyQw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe4baf0e8c438821877b890882970151a18df315', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'tEFaKdpbTuSBBWpWQ-kmQ1l_KwNUpQtPpUtOwmLiL-A'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,axqn1s,True,,IDontHaveNicknameToo,,13,True,all_ads,False,[],False,,/r/pytorch/comments/axqn1s/hot_to_concatenate_to_cuda_tensor/,all_ads,False,https://www.reddit.com/r/pytorch/comments/axqn1s/hot_to_concatenate_to_cuda_tensor/,7135,1551822165.0,0,,False,,,,,,,,
928,,pytorch,"I am using Adam optimiser to optimise a list of lists, where each inner list consists of `[x_n, y_n]`. I need to implement a rule during optimisation such that `x &gt;= y`. I have no idea how to begin with this, apart from setting a simple correction after the optimisation step that sets `x = y` if it drops below `y`. Is this the best way of doing it or is there something I should look into? Any help is appreciated.",,False,,0,False,Help: how to set constraints on optimisers?,[],r/pytorch,False,6,,0,,,False,t3_ax927s,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,,self,False,,,{},,,True,,1551746016.0,text,6,,,,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am using Adam optimiser to optimise a list of lists, where each inner list consists of &lt;code&gt;[x_n, y_n]&lt;/code&gt;. I need to implement a rule during optimisation such that &lt;code&gt;x &amp;gt;= y&lt;/code&gt;. I have no idea how to begin with this, apart from setting a simple correction after the optimisation step that sets &lt;code&gt;x = y&lt;/code&gt; if it drops below &lt;code&gt;y&lt;/code&gt;. Is this the best way of doing it or is there something I should look into? Any help is appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ax927s,True,,[deleted],,6,True,all_ads,False,[],,dark,/r/pytorch/comments/ax927s/help_how_to_set_constraints_on_optimisers/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ax927s/help_how_to_set_constraints_on_optimisers/,7135,1551717216.0,0,,False,,,,,,,,
929,,pytorch,,t2_ch9a8a,False,,0,False,"Tutorial: Image Classification using Logistic Regression in PyTorch - Part 3 of ""PyTorch: Zero to GANs""",[],r/pytorch,False,6,,0,140.0,,False,t3_ax466n,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/sIWSMs4UPRBQWIUIhwZ2YJCKHczIBi0suaIW8iECQ8I.jpg,False,,[],{},link,,False,,1551711413.0,text,6,,,text,medium.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/lBy57ka_UPEDHg3hPQ0d_Ugkw6kXwekRYcOYEa68ms4.jpg?auto=webp&amp;s=6048c6aa56e8d07431ec31c26513cc43aba4fd83', 'width': 339, 'height': 339}, 'resolutions': [{'url': 'https://external-preview.redd.it/lBy57ka_UPEDHg3hPQ0d_Ugkw6kXwekRYcOYEa68ms4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8fb0340ad7325319a7fa1cdb8bd92c2272f7c14c', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/lBy57ka_UPEDHg3hPQ0d_Ugkw6kXwekRYcOYEa68ms4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1838ebd93966a8bfa6b2ddea7277f79a4cfdca17', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/lBy57ka_UPEDHg3hPQ0d_Ugkw6kXwekRYcOYEa68ms4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ffbf15eb7ca7d59a1547dabd3bc404b7e27104c9', 'width': 320, 'height': 320}], 'variants': {}, 'id': '7RW7aNr5Gdzadxjr8T_3T8HHNAM1DsL1HXG2pr2iFM4'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ax466n,True,,appeasydesign,,0,True,all_ads,False,[],False,,/r/pytorch/comments/ax466n/tutorial_image_classification_using_logistic/,all_ads,False,https://medium.com/jovian-io/image-classification-using-logistic-regression-in-pytorch-ebb96cc9eb79,7135,1551682613.0,0,,False,https://medium.com/jovian-io/image-classification-using-logistic-regression-in-pytorch-ebb96cc9eb79,"[{'approved_at_utc': None, 'subreddit': 'learnmachinelearning', 'selftext': '', 'author_fullname': 't2_mjlci', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Tutorial: Image Classification using Logistic Regression in PyTorch - Part 3 of ""PyTorch: Zero to GANs""', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/learnmachinelearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_awtv0t', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 70, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 70, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/sIWSMs4UPRBQWIUIhwZ2YJCKHczIBi0suaIW8iECQ8I.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1551647395.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'medium.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://medium.com/jovian-io/image-classification-using-logistic-regression-in-pytorch-ebb96cc9eb79', 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/lBy57ka_UPEDHg3hPQ0d_Ugkw6kXwekRYcOYEa68ms4.jpg?auto=webp&amp;s=6048c6aa56e8d07431ec31c26513cc43aba4fd83', 'width': 339, 'height': 339}, 'resolutions': [{'url': 'https://external-preview.redd.it/lBy57ka_UPEDHg3hPQ0d_Ugkw6kXwekRYcOYEa68ms4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8fb0340ad7325319a7fa1cdb8bd92c2272f7c14c', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/lBy57ka_UPEDHg3hPQ0d_Ugkw6kXwekRYcOYEa68ms4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1838ebd93966a8bfa6b2ddea7277f79a4cfdca17', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/lBy57ka_UPEDHg3hPQ0d_Ugkw6kXwekRYcOYEa68ms4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ffbf15eb7ca7d59a1547dabd3bc404b7e27104c9', 'width': 320, 'height': 320}], 'variants': {}, 'id': '7RW7aNr5Gdzadxjr8T_3T8HHNAM1DsL1HXG2pr2iFM4'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3cqa1', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'awtv0t', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'aakashns', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/learnmachinelearning/comments/awtv0t/tutorial_image_classification_using_logistic/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://medium.com/jovian-io/image-classification-using-logistic-regression-in-pytorch-ebb96cc9eb79', 'subreddit_subscribers': 217921, 'created_utc': 1551618595.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_awtv0t,,,,,
930,,pytorch,"Hello everyone,

I just started learning PyTorch and thought a good learning exercise would be implementing a neural network, without using torch.nn, just to practice tensor operations.

However, I'm running into problems when updating the weights, because .backward only calculates the gradients on the first iteration, and then all grads become None even though .requires_grad is True.

I *think* this is because when I update the weights for the first time, they stop being a leave of the graph, so I wrapped my update block with ''with torch.no_grad'', as such:

with torch.no_grad:
   w1 = w1 - learning_rate*w1.grad

But this resulted in the same error!
What left me totally confused is that when I replaced that line by

 w1 - = learning_rate*w1.grad

It seemed to work just fine.

Hopefully I'm not posting this in the wrong place. Would really appreciate some help, because if I don't understand the fundamentals then I'll have no chance in the future.

Thanks",t2_8xg3h,False,,0,False,Problem implementing a Neural Network from scratch using PyTorch,[],r/pytorch,False,6,,0,,,False,t3_ax5h2f,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1551722401.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;I just started learning PyTorch and thought a good learning exercise would be implementing a neural network, without using torch.nn, just to practice tensor operations.&lt;/p&gt;

&lt;p&gt;However, I&amp;#39;m running into problems when updating the weights, because .backward only calculates the gradients on the first iteration, and then all grads become None even though .requires_grad is True.&lt;/p&gt;

&lt;p&gt;I &lt;em&gt;think&lt;/em&gt; this is because when I update the weights for the first time, they stop being a leave of the graph, so I wrapped my update block with &amp;#39;&amp;#39;with torch.no_grad&amp;#39;&amp;#39;, as such:&lt;/p&gt;

&lt;p&gt;with torch.no_grad:
   w1 = w1 - learning_rate*w1.grad&lt;/p&gt;

&lt;p&gt;But this resulted in the same error!
What left me totally confused is that when I replaced that line by&lt;/p&gt;

&lt;p&gt;w1 - = learning_rate*w1.grad&lt;/p&gt;

&lt;p&gt;It seemed to work just fine.&lt;/p&gt;

&lt;p&gt;Hopefully I&amp;#39;m not posting this in the wrong place. Would really appreciate some help, because if I don&amp;#39;t understand the fundamentals then I&amp;#39;ll have no chance in the future.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ax5h2f,True,,Fredbull,,6,True,all_ads,False,[],False,,/r/pytorch/comments/ax5h2f/problem_implementing_a_neural_network_from/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ax5h2f/problem_implementing_a_neural_network_from/,7135,1551693601.0,0,,False,,,,,,,,
931,,pytorch,Guys can you suggest me a book to learn PyTorch from the beginning to advance level?,t2_1w0dbmc0,False,,0,False,Need a book on PyTorch,[],r/pytorch,False,6,,0,,,False,t3_awy2aw,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1551673148.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Guys can you suggest me a book to learn PyTorch from the beginning to advance level?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,awy2aw,True,,SalmanHaydar,,8,True,all_ads,False,[],False,,/r/pytorch/comments/awy2aw/need_a_book_on_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/awy2aw/need_a_book_on_pytorch/,7135,1551644348.0,0,,False,,,,,,,,
932,,pytorch,"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/nithin/Git/Datasets/MNIST/raw/train-images-idx3-ubyte.gz

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-4-481efeb3c63f&gt; in &lt;module&gt;
      2                      train=True,
      3                      transform=transforms.ToTensor(),
----&gt; 4                      download=True)
      5 test_dataset = dsets.MNIST(root='/home/nithin/Git/Datasets',
      6                      train=False,

~/.conda/envs/ml/lib/python3.7/site-packages/torchvision/datasets/mnist.py in __init__(self, root, train, transform, target_transform, download)
     66 
     67         if download:
---&gt; 68             self.download()
     69 
     70         if not self._check_exists():

~/.conda/envs/ml/lib/python3.7/site-packages/torchvision/datasets/mnist.py in download(self)
    141             filename = url.rpartition('/')[2]
    142             file_path = os.path.join(self.raw_folder, filename)
--&gt; 143             download_url(url, root=self.raw_folder, filename=filename, md5=None)
    144             self.extract_gzip(gzip_path=file_path, remove_finished=True)
    145 

~/.conda/envs/ml/lib/python3.7/site-packages/torchvision/datasets/utils.py in download_url(url, root, filename, md5)
     71             urllib.request.urlretrieve(
     72                 url, fpath,
---&gt; 73                 reporthook=gen_bar_updater(tqdm())
     74             )
     75         except OSError:

TypeError: __init__() missing 1 required positional argument: 'total'

",t2_399gjnu3,False,,0,False,Problem with torchvision when loading datasets. The new update broke it.,[],r/pytorch,False,6,,0,,,False,t3_aw2o2h,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1551459165.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Downloading &lt;a href=""http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz""&gt;http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz&lt;/a&gt; to /home/nithin/Git/Datasets/MNIST/raw/train-images-idx3-ubyte.gz&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;TypeError                                 Traceback (most recent call last)
&amp;lt;ipython-input-4-481efeb3c63f&amp;gt; in &amp;lt;module&amp;gt;
      2                      train=True,
      3                      transform=transforms.ToTensor(),
----&amp;gt; 4                      download=True)
      5 test_dataset = dsets.MNIST(root=&amp;#39;/home/nithin/Git/Datasets&amp;#39;,
      6                      train=False,&lt;/p&gt;

&lt;p&gt;~/.conda/envs/ml/lib/python3.7/site-packages/torchvision/datasets/mnist.py in &lt;strong&gt;init&lt;/strong&gt;(self, root, train, transform, target_transform, download)
     66 
     67         if download:
---&amp;gt; 68             self.download()
     69 
     70         if not self._check_exists():&lt;/p&gt;

&lt;p&gt;~/.conda/envs/ml/lib/python3.7/site-packages/torchvision/datasets/mnist.py in download(self)
    141             filename = url.rpartition(&amp;#39;/&amp;#39;)[2]
    142             file_path = os.path.join(self.raw_folder, filename)
--&amp;gt; 143             download_url(url, root=self.raw_folder, filename=filename, md5=None)
    144             self.extract_gzip(gzip_path=file_path, remove_finished=True)
    145 &lt;/p&gt;

&lt;p&gt;~/.conda/envs/ml/lib/python3.7/site-packages/torchvision/datasets/utils.py in download_url(url, root, filename, md5)
     71             urllib.request.urlretrieve(
     72                 url, fpath,
---&amp;gt; 73                 reporthook=gen_bar_updater(tqdm())
     74             )
     75         except OSError:&lt;/p&gt;

&lt;p&gt;TypeError: &lt;strong&gt;init&lt;/strong&gt;() missing 1 required positional argument: &amp;#39;total&amp;#39;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,aw2o2h,True,,nfjlanlsad,,1,True,all_ads,False,[],False,,/r/pytorch/comments/aw2o2h/problem_with_torchvision_when_loading_datasets/,all_ads,False,https://www.reddit.com/r/pytorch/comments/aw2o2h/problem_with_torchvision_when_loading_datasets/,7135,1551430365.0,0,,False,,,,,,,,
933,,pytorch,"I have working tf with cuda-tookit 10 installed.  
But where I'm going to install pytorch :  
(base) C:\\Users\\Dima&gt;conda install pytorch torchvision  -c pytorch

Solving environment: done

&amp;#x200B;

\## Package Plan ##

&amp;#x200B;

  environment location: C:\\Users\\Dima\\Miniconda3

&amp;#x200B;

  added / updated specs:

\- pytorch

\- torchvision

&amp;#x200B;

&amp;#x200B;

The following packages will be downloaded:

&amp;#x200B;

package                    |            build

\---------------------------|-----------------

jpeg-9b                    |       hb83a4c4\_2         313 KB

conda-4.6.7                |           py37\_0         1.7 MB

mkl-2019.1                 |              144       158.3 MB

intel-openmp-2019.1        |              144         1.7 MB

pytorch-1.0.1              |py3.7\_cuda100\_cudnn7\_1       464.9 MB  pytorch

mkl\_random-1.0.2           |   py37h343c172\_0         328 KB

xz-5.2.4                   |       h2fa13f4\_4         812 KB

icc\_rt-2019.0.0            |       h0cc432a\_1         9.4 MB

libtiff-4.0.10             |       hb898794\_2         1.1 MB

olefile-0.46               |           py37\_0          49 KB

torchvision-0.2.1          |             py\_2          37 KB  pytorch

mkl\_fft-1.0.10             |   py37h14836fe\_0         135 KB

libpng-1.6.36              |       h2a8f88b\_0         550 KB

ninja-1.8.2                |   py37he980bc4\_1         109 KB

freetype-2.9.1             |       ha9979f8\_1         470 KB

ca-certificates-2019.1.23  |                0         158 KB

zstd-1.3.7                 |       h508b16e\_0         536 KB

numpy-base-1.15.4          |   py37hc3f5095\_0         3.9 MB

cudatoolkit-10.0.130       |                0       371.0 MB

numpy-1.15.4               |   py37h19fb1c0\_0          47 KB

tk-8.6.8                   |       hfa6e2cd\_0         3.8 MB

blas-1.0                   |              mkl           6 KB

zlib-1.2.11                |       h62dcd97\_3         128 KB

openssl-1.1.1b             |       he774522\_0         5.8 MB

pillow-5.4.1               |   py37hdc69c19\_0         686 KB

\------------------------------------------------------------

Total:        1.00 GB

&amp;#x200B;

The following NEW packages will be INSTALLED:

&amp;#x200B;

blas:            1.0-mkl

cudatoolkit:     10.0.130-0

freetype:        2.9.1-ha9979f8\_1

icc\_rt:          2019.0.0-h0cc432a\_1

intel-openmp:    2019.1-144

jpeg:            9b-hb83a4c4\_2

libpng:          1.6.36-h2a8f88b\_0

libtiff:         4.0.10-hb898794\_2

mkl:             2019.1-144

mkl\_fft:         1.0.10-py37h14836fe\_0

mkl\_random:      1.0.2-py37h343c172\_0

ninja:           1.8.2-py37he980bc4\_1

numpy:           1.15.4-py37h19fb1c0\_0

numpy-base:      1.15.4-py37hc3f5095\_0

olefile:         0.46-py37\_0

pillow:          5.4.1-py37hdc69c19\_0

pytorch:         1.0.1-py3.7\_cuda100\_cudnn7\_1 pytorch

tk:              8.6.8-hfa6e2cd\_0

torchvision:     0.2.1-py\_2                   pytorch

xz:              5.2.4-h2fa13f4\_4

zlib:            1.2.11-h62dcd97\_3

zstd:            1.3.7-h508b16e\_0

&amp;#x200B;

The following packages will be UPDATED:

&amp;#x200B;

ca-certificates: 2018.03.07-0                         --&gt; 2019.1.23-0

conda:           4.5.12-py37\_0                        --&gt; 4.6.7-py37\_0

openssl:         1.1.1a-he774522\_0                    --&gt; 1.1.1b-he774522\_0

&amp;#x200B;

Proceed (\[y\]/n)?

&amp;#x200B;

cudatoolkit-10.0.130       |                0       371.0 MB  
This is a massive huge of space

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",t2_zog9s,False,,0,False,Why pytorch want to install cuda-tookit again?,[],r/pytorch,False,6,,0,,,False,t3_avgjyp,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1551320444.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have working tf with cuda-tookit 10 installed.&lt;br/&gt;
But where I&amp;#39;m going to install pytorch :&lt;br/&gt;
(base) C:\Users\Dima&amp;gt;conda install pytorch torchvision  -c pytorch&lt;/p&gt;

&lt;p&gt;Solving environment: done&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;## Package Plan ##&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;environment location: C:\Users\Dima\Miniconda3&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;added / updated specs:&lt;/p&gt;

&lt;p&gt;- pytorch&lt;/p&gt;

&lt;p&gt;- torchvision&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The following packages will be downloaded:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;package                    |            build&lt;/p&gt;

&lt;p&gt;---------------------------|-----------------&lt;/p&gt;

&lt;p&gt;jpeg-9b                    |       hb83a4c4_2         313 KB&lt;/p&gt;

&lt;p&gt;conda-4.6.7                |           py37_0         1.7 MB&lt;/p&gt;

&lt;p&gt;mkl-2019.1                 |              144       158.3 MB&lt;/p&gt;

&lt;p&gt;intel-openmp-2019.1        |              144         1.7 MB&lt;/p&gt;

&lt;p&gt;pytorch-1.0.1              |py3.7_cuda100_cudnn7_1       464.9 MB  pytorch&lt;/p&gt;

&lt;p&gt;mkl_random-1.0.2           |   py37h343c172_0         328 KB&lt;/p&gt;

&lt;p&gt;xz-5.2.4                   |       h2fa13f4_4         812 KB&lt;/p&gt;

&lt;p&gt;icc_rt-2019.0.0            |       h0cc432a_1         9.4 MB&lt;/p&gt;

&lt;p&gt;libtiff-4.0.10             |       hb898794_2         1.1 MB&lt;/p&gt;

&lt;p&gt;olefile-0.46               |           py37_0          49 KB&lt;/p&gt;

&lt;p&gt;torchvision-0.2.1          |             py_2          37 KB  pytorch&lt;/p&gt;

&lt;p&gt;mkl_fft-1.0.10             |   py37h14836fe_0         135 KB&lt;/p&gt;

&lt;p&gt;libpng-1.6.36              |       h2a8f88b_0         550 KB&lt;/p&gt;

&lt;p&gt;ninja-1.8.2                |   py37he980bc4_1         109 KB&lt;/p&gt;

&lt;p&gt;freetype-2.9.1             |       ha9979f8_1         470 KB&lt;/p&gt;

&lt;p&gt;ca-certificates-2019.1.23  |                0         158 KB&lt;/p&gt;

&lt;p&gt;zstd-1.3.7                 |       h508b16e_0         536 KB&lt;/p&gt;

&lt;p&gt;numpy-base-1.15.4          |   py37hc3f5095_0         3.9 MB&lt;/p&gt;

&lt;p&gt;cudatoolkit-10.0.130       |                0       371.0 MB&lt;/p&gt;

&lt;p&gt;numpy-1.15.4               |   py37h19fb1c0_0          47 KB&lt;/p&gt;

&lt;p&gt;tk-8.6.8                   |       hfa6e2cd_0         3.8 MB&lt;/p&gt;

&lt;p&gt;blas-1.0                   |              mkl           6 KB&lt;/p&gt;

&lt;p&gt;zlib-1.2.11                |       h62dcd97_3         128 KB&lt;/p&gt;

&lt;p&gt;openssl-1.1.1b             |       he774522_0         5.8 MB&lt;/p&gt;

&lt;p&gt;pillow-5.4.1               |   py37hdc69c19_0         686 KB&lt;/p&gt;

&lt;p&gt;------------------------------------------------------------&lt;/p&gt;

&lt;p&gt;Total:        1.00 GB&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The following NEW packages will be INSTALLED:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;blas:            1.0-mkl&lt;/p&gt;

&lt;p&gt;cudatoolkit:     10.0.130-0&lt;/p&gt;

&lt;p&gt;freetype:        2.9.1-ha9979f8_1&lt;/p&gt;

&lt;p&gt;icc_rt:          2019.0.0-h0cc432a_1&lt;/p&gt;

&lt;p&gt;intel-openmp:    2019.1-144&lt;/p&gt;

&lt;p&gt;jpeg:            9b-hb83a4c4_2&lt;/p&gt;

&lt;p&gt;libpng:          1.6.36-h2a8f88b_0&lt;/p&gt;

&lt;p&gt;libtiff:         4.0.10-hb898794_2&lt;/p&gt;

&lt;p&gt;mkl:             2019.1-144&lt;/p&gt;

&lt;p&gt;mkl_fft:         1.0.10-py37h14836fe_0&lt;/p&gt;

&lt;p&gt;mkl_random:      1.0.2-py37h343c172_0&lt;/p&gt;

&lt;p&gt;ninja:           1.8.2-py37he980bc4_1&lt;/p&gt;

&lt;p&gt;numpy:           1.15.4-py37h19fb1c0_0&lt;/p&gt;

&lt;p&gt;numpy-base:      1.15.4-py37hc3f5095_0&lt;/p&gt;

&lt;p&gt;olefile:         0.46-py37_0&lt;/p&gt;

&lt;p&gt;pillow:          5.4.1-py37hdc69c19_0&lt;/p&gt;

&lt;p&gt;pytorch:         1.0.1-py3.7_cuda100_cudnn7_1 pytorch&lt;/p&gt;

&lt;p&gt;tk:              8.6.8-hfa6e2cd_0&lt;/p&gt;

&lt;p&gt;torchvision:     0.2.1-py_2                   pytorch&lt;/p&gt;

&lt;p&gt;xz:              5.2.4-h2fa13f4_4&lt;/p&gt;

&lt;p&gt;zlib:            1.2.11-h62dcd97_3&lt;/p&gt;

&lt;p&gt;zstd:            1.3.7-h508b16e_0&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The following packages will be UPDATED:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;ca-certificates: 2018.03.07-0                         --&amp;gt; 2019.1.23-0&lt;/p&gt;

&lt;p&gt;conda:           4.5.12-py37_0                        --&amp;gt; 4.6.7-py37_0&lt;/p&gt;

&lt;p&gt;openssl:         1.1.1a-he774522_0                    --&amp;gt; 1.1.1b-he774522_0&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Proceed ([y]/n)?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;cudatoolkit-10.0.130       |                0       371.0 MB&lt;br/&gt;
This is a massive huge of space&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,avgjyp,True,,dimkoss11,,1,True,all_ads,False,[],False,,/r/pytorch/comments/avgjyp/why_pytorch_want_to_install_cudatookit_again/,all_ads,False,https://www.reddit.com/r/pytorch/comments/avgjyp/why_pytorch_want_to_install_cudatookit_again/,7135,1551291644.0,0,,False,,,,,,,,
934,,pytorch,"Hi,
I am using a encoder decoder model to using lstm with attention to predict a tiime series.
The encoder takes the source as input(a random time series)  which returns the hidden state, the cell state and context vector as output.
The context vector is fed as initial hidden states to the decoder and the target value is fed as sequence(target value is the actual time series)

After training the model the source (the random time series) fits according to the target pretty accurately.

My question here now is that
how can i use this trained model to predict future values where i give a random time series as input.
The problem here is that i dont have a target value to feed in the decoder.",t2_7bl92sf,False,,0,False,Need Help with a encoder-decoder architecture for time series prediction.,[],r/pytorch,False,6,,0,,,False,t3_auaqit,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1551061249.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,
I am using a encoder decoder model to using lstm with attention to predict a tiime series.
The encoder takes the source as input(a random time series)  which returns the hidden state, the cell state and context vector as output.
The context vector is fed as initial hidden states to the decoder and the target value is fed as sequence(target value is the actual time series)&lt;/p&gt;

&lt;p&gt;After training the model the source (the random time series) fits according to the target pretty accurately.&lt;/p&gt;

&lt;p&gt;My question here now is that
how can i use this trained model to predict future values where i give a random time series as input.
The problem here is that i dont have a target value to feed in the decoder.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,auaqit,True,,shamssahal,,4,True,all_ads,False,[],False,,/r/pytorch/comments/auaqit/need_help_with_a_encoderdecoder_architecture_for/,all_ads,False,https://www.reddit.com/r/pytorch/comments/auaqit/need_help_with_a_encoderdecoder_architecture_for/,7135,1551032449.0,0,,False,,,,,,,,
935,,pytorch,"I own 4 1080tis that I've recently began using for deep learning on Pytorch. However, I can't seem to make sense of how to parallelize models across my GPUs - was wondering if anyone has any example code for doing this? Can't for the life of me figure out how to do this.",t2_7mzxe,False,,0,False,nn.DataParallel example code?,[],r/pytorch,False,6,,0,,,False,t3_atnive,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1550901896.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I own 4 1080tis that I&amp;#39;ve recently began using for deep learning on Pytorch. However, I can&amp;#39;t seem to make sense of how to parallelize models across my GPUs - was wondering if anyone has any example code for doing this? Can&amp;#39;t for the life of me figure out how to do this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,atnive,True,,yibaiveintiocho,,3,True,all_ads,False,[],False,,/r/pytorch/comments/atnive/nndataparallel_example_code/,all_ads,False,https://www.reddit.com/r/pytorch/comments/atnive/nndataparallel_example_code/,7135,1550873096.0,0,,False,,,,,,,,
936,,pytorch,,t2_ch9a8a,False,,0,False,Part 2 of PyTorch: Zero to GANs - Linear Regression explained from scratch with PyTorch,[],r/pytorch,False,6,,0,93.0,,False,t3_atevr8,False,dark,0.91,,public,8,0,{},140.0,,False,[],,False,False,,{},,False,8,,False,https://b.thumbs.redditmedia.com/8bsvX9R9UzBpoj0sgr2zQ4IThkEu2Y10cDU0BjkaL_M.jpg,False,,[],{},link,,False,,1550851362.0,text,6,,,text,medium.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/HYEdHD_rwkGhfwnai4v16Z5WY-83PcKqZCqXwRYu2o4.jpg?auto=webp&amp;s=2bf0d88c6375522cb7029a35664abbe1815a2c38', 'width': 1024, 'height': 682}, 'resolutions': [{'url': 'https://external-preview.redd.it/HYEdHD_rwkGhfwnai4v16Z5WY-83PcKqZCqXwRYu2o4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7deada9b249ed86d88bbdc08a3ae00af56013c20', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/HYEdHD_rwkGhfwnai4v16Z5WY-83PcKqZCqXwRYu2o4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb2a2088635599550f7772ee1f619ba698d9d299', 'width': 216, 'height': 143}, {'url': 'https://external-preview.redd.it/HYEdHD_rwkGhfwnai4v16Z5WY-83PcKqZCqXwRYu2o4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d18019ed17b21b602cf83d565dd4bf108b17972', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/HYEdHD_rwkGhfwnai4v16Z5WY-83PcKqZCqXwRYu2o4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=522cd8b45c11e6ce848b2520757067e701b2fd23', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/HYEdHD_rwkGhfwnai4v16Z5WY-83PcKqZCqXwRYu2o4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e4436cc0c835a9602d9411eae053712af4f307ef', 'width': 960, 'height': 639}], 'variants': {}, 'id': '5fqpV7P9FjFQKWoQMpabx7Z15ftX1GXaubwYSao9lzI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,atevr8,True,,appeasydesign,,0,True,all_ads,False,[],False,,/r/pytorch/comments/atevr8/part_2_of_pytorch_zero_to_gans_linear_regression/,all_ads,False,https://medium.com/jovian-io/linear-regression-with-pytorch-3dde91d60b50,7135,1550822562.0,0,,False,https://medium.com/jovian-io/linear-regression-with-pytorch-3dde91d60b50,"[{'approved_at_utc': None, 'subreddit': 'learnmachinelearning', 'selftext': '', 'author_fullname': 't2_mjlci', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Part 2 of PyTorch: Zero to GANs - Linear Regression explained from scratch with PyTorch', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/learnmachinelearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 93, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_at59eu', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 8, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/8bsvX9R9UzBpoj0sgr2zQ4IThkEu2Y10cDU0BjkaL_M.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1550796163.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'medium.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://medium.com/jovian-io/linear-regression-with-pytorch-3dde91d60b50', 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/HYEdHD_rwkGhfwnai4v16Z5WY-83PcKqZCqXwRYu2o4.jpg?auto=webp&amp;s=2bf0d88c6375522cb7029a35664abbe1815a2c38', 'width': 1024, 'height': 682}, 'resolutions': [{'url': 'https://external-preview.redd.it/HYEdHD_rwkGhfwnai4v16Z5WY-83PcKqZCqXwRYu2o4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7deada9b249ed86d88bbdc08a3ae00af56013c20', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/HYEdHD_rwkGhfwnai4v16Z5WY-83PcKqZCqXwRYu2o4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb2a2088635599550f7772ee1f619ba698d9d299', 'width': 216, 'height': 143}, {'url': 'https://external-preview.redd.it/HYEdHD_rwkGhfwnai4v16Z5WY-83PcKqZCqXwRYu2o4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d18019ed17b21b602cf83d565dd4bf108b17972', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/HYEdHD_rwkGhfwnai4v16Z5WY-83PcKqZCqXwRYu2o4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=522cd8b45c11e6ce848b2520757067e701b2fd23', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/HYEdHD_rwkGhfwnai4v16Z5WY-83PcKqZCqXwRYu2o4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e4436cc0c835a9602d9411eae053712af4f307ef', 'width': 960, 'height': 639}], 'variants': {}, 'id': '5fqpV7P9FjFQKWoQMpabx7Z15ftX1GXaubwYSao9lzI'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3cqa1', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'at59eu', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'aakashns', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/learnmachinelearning/comments/at59eu/part_2_of_pytorch_zero_to_gans_linear_regression/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://medium.com/jovian-io/linear-regression-with-pytorch-3dde91d60b50', 'subreddit_subscribers': 217921, 'created_utc': 1550767363.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_at59eu,,,,,
937,,pytorch,,t2_1ffz9tjt,False,,0,False,My implementation of YOLO - You only look once (ver 2) for object detection tasks. Source code: https://github.com/vietnguyen91/Yolo-v2-pytorch,[],r/pytorch,False,6,,0,78.0,,False,t3_at34bu,False,dark,0.54,,public,2,0,{},140.0,,False,[],"{'reddit_video': {'fallback_url': 'https://v.redd.it/kdwwll7uaxh21/DASH_1080?source=fallback', 'height': 1080, 'width': 1920, 'scrubber_media_url': 'https://v.redd.it/kdwwll7uaxh21/DASH_240', 'dash_url': 'https://v.redd.it/kdwwll7uaxh21/DASHPlaylist.mpd?a=1618044336%2CMTQyMjFmZjhlNzk5OTdkNGJkZjBiNTlmNDQzYTdmOGJjNDdlY2Y3MjNkMzI2YTlhYWY2MTQ4MzliMDU4N2QxMA%3D%3D&amp;v=1&amp;f=sd', 'duration': 49, 'hls_url': 'https://v.redd.it/kdwwll7uaxh21/HLSPlaylist.m3u8?a=1618044336%2CN2I1ODhiZTk5YWI1YTFkZDI3NDU2MmI5ZGVkMjY2ZmNmYzEwNzcyMzlmZWQxNmM2ZDQxMDdkMzYyYTc2YTQzNg%3D%3D&amp;v=1&amp;f=sd', 'is_gif': False, 'transcoding_status': 'completed'}}",True,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/QTkKwdAO0faz85Sl-rQ0PiarTEaj5_Y1VZGoGEgIRyY.jpg,False,,[],{},hosted:video,,False,,1550784388.0,text,6,,,text,v.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/g7v-61w5PhkIwRpjBwOexzngCG79HtqfO00cZL6IMSg.png?format=pjpg&amp;auto=webp&amp;s=89b77bcae18676dea435dae0e7145bafd6e2e88a', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://external-preview.redd.it/g7v-61w5PhkIwRpjBwOexzngCG79HtqfO00cZL6IMSg.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=04aa8a4d127f494a876e7ed4169382684b1cd9d7', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/g7v-61w5PhkIwRpjBwOexzngCG79HtqfO00cZL6IMSg.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=72909397b7b2241b008542f74fd60cba0e665a0f', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/g7v-61w5PhkIwRpjBwOexzngCG79HtqfO00cZL6IMSg.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=08954ad6ad5acdab554f150e09decfd26d334393', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/g7v-61w5PhkIwRpjBwOexzngCG79HtqfO00cZL6IMSg.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8c1c348538d872709068a88e0b4754ebd6b6c86f', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/g7v-61w5PhkIwRpjBwOexzngCG79HtqfO00cZL6IMSg.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5f8c47f9d8e79f3389ba58791570cbfa033c6537', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/g7v-61w5PhkIwRpjBwOexzngCG79HtqfO00cZL6IMSg.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8fa968233808e9fb257fa026728b7d2ba480c88c', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'Yc_DCg1UWYraOeMxS-azOe7QsM0abKW7SHoe-Jq2uCI'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,at34bu,True,,1991viet,,1,True,all_ads,False,[],False,,/r/pytorch/comments/at34bu/my_implementation_of_yolo_you_only_look_once_ver/,all_ads,False,https://v.redd.it/kdwwll7uaxh21,7135,1550755588.0,0,"{'reddit_video': {'fallback_url': 'https://v.redd.it/kdwwll7uaxh21/DASH_1080?source=fallback', 'height': 1080, 'width': 1920, 'scrubber_media_url': 'https://v.redd.it/kdwwll7uaxh21/DASH_240', 'dash_url': 'https://v.redd.it/kdwwll7uaxh21/DASHPlaylist.mpd?a=1618044336%2CMTQyMjFmZjhlNzk5OTdkNGJkZjBiNTlmNDQzYTdmOGJjNDdlY2Y3MjNkMzI2YTlhYWY2MTQ4MzliMDU4N2QxMA%3D%3D&amp;v=1&amp;f=sd', 'duration': 49, 'hls_url': 'https://v.redd.it/kdwwll7uaxh21/HLSPlaylist.m3u8?a=1618044336%2CN2I1ODhiZTk5YWI1YTFkZDI3NDU2MmI5ZGVkMjY2ZmNmYzEwNzcyMzlmZWQxNmM2ZDQxMDdkMzYyYTc2YTQzNg%3D%3D&amp;v=1&amp;f=sd', 'is_gif': False, 'transcoding_status': 'completed'}}",True,https://v.redd.it/kdwwll7uaxh21,,,,,,,
938,,pytorch,,t2_2fv4yodo,False,,0,False,"Hugging Face Releases Pytorch-BERT, Pretrained Models and More",[],r/pytorch,False,6,,0,51.0,,False,t3_assf0k,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/qilposXAmRwx46vsTKS9LoCzNEgPy3jGg97ameEgstA.jpg,False,,[],{},link,,False,,1550719175.0,text,6,,,text,medium.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fexP1Ss9nds-7hVrTbnFeh1FKY82DAzXcD1VkVGK3Go.jpg?auto=webp&amp;s=e9b76ab34dfdd11124185b0af541de5054da16da', 'width': 738, 'height': 271}, 'resolutions': [{'url': 'https://external-preview.redd.it/fexP1Ss9nds-7hVrTbnFeh1FKY82DAzXcD1VkVGK3Go.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b22e9870a001a384e81097aeaa64f56da02d1767', 'width': 108, 'height': 39}, {'url': 'https://external-preview.redd.it/fexP1Ss9nds-7hVrTbnFeh1FKY82DAzXcD1VkVGK3Go.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f3330f677f44b3100abd96a7b0975946694c873', 'width': 216, 'height': 79}, {'url': 'https://external-preview.redd.it/fexP1Ss9nds-7hVrTbnFeh1FKY82DAzXcD1VkVGK3Go.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3294f8fcc1e68cd7e16f8308bfbf424b1fde22f9', 'width': 320, 'height': 117}, {'url': 'https://external-preview.redd.it/fexP1Ss9nds-7hVrTbnFeh1FKY82DAzXcD1VkVGK3Go.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ad81c05c64ad7cbca6d63c743d5c36fedca3b8cf', 'width': 640, 'height': 235}], 'variants': {}, 'id': 'pAEyrfeh1AeGXXBOWpq3bRBn4ytNCvbAM3uhI-t_pEQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,assf0k,True,,Yuqing7,,0,True,all_ads,False,[],False,,/r/pytorch/comments/assf0k/hugging_face_releases_pytorchbert_pretrained/,all_ads,False,https://medium.com/syncedreview/hugging-face-releases-pytorch-bert-pretrained-models-and-more-b8a7839e7730,7135,1550690375.0,0,,False,https://medium.com/syncedreview/hugging-face-releases-pytorch-bert-pretrained-models-and-more-b8a7839e7730,,,,,,,
939,,pytorch,"I have a project in which an NN outputs drawing commands.  Although the drawing requirements are pretty crude at this point I am having a hard time finding anything suitable that can run cross platform (for experimentation on my Mac and later training on Linux or Win machines).

1. PyAgg doesn't support python 3 yet
2. I can't get PyCairo to work in Anaconda on my Mac for some reason.  I've also read that it's slow.
3. I've used NanoVG before and was pleased to see a python wrapper and I got this working on my Mac.  But I was unable to get this working on Windows.  (Looks like I may be able to with more work manually compiling libraries, but... ug).
4. PyOpenGL This sounds like it would be supported well enough but I'd prefer a higher level drawing API than OpenGL if I can find it :)
5. I can just render things myself directly to the tensors with appropriate operations, but this would be very slow and as with #4 I'd rather not re-invent the wheel here if I can avoid it.

I'm considering writing a rendering server in Java and talking to it over a local socket.  I'm guessing that I can get that to perform well enough and at least then I'll have a full, high quality drawing API available everywhere.

Am I missing other options?  I really wasn't expecting ""draw some stuff to an image"" to be the stumbling block in this project :)

thanks.

&amp;#x200B;",t2_4bwhg,False,,0,False,Trying to find a drawing API for use with pytorch project...,[],r/pytorch,False,6,,0,,,False,t3_as6vvc,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1550554684.0,,[],{},,,True,,1550582879.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a project in which an NN outputs drawing commands.  Although the drawing requirements are pretty crude at this point I am having a hard time finding anything suitable that can run cross platform (for experimentation on my Mac and later training on Linux or Win machines).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;PyAgg doesn&amp;#39;t support python 3 yet&lt;/li&gt;
&lt;li&gt;I can&amp;#39;t get PyCairo to work in Anaconda on my Mac for some reason.  I&amp;#39;ve also read that it&amp;#39;s slow.&lt;/li&gt;
&lt;li&gt;I&amp;#39;ve used NanoVG before and was pleased to see a python wrapper and I got this working on my Mac.  But I was unable to get this working on Windows.  (Looks like I may be able to with more work manually compiling libraries, but... ug).&lt;/li&gt;
&lt;li&gt;PyOpenGL This sounds like it would be supported well enough but I&amp;#39;d prefer a higher level drawing API than OpenGL if I can find it :)&lt;/li&gt;
&lt;li&gt;I can just render things myself directly to the tensors with appropriate operations, but this would be very slow and as with #4 I&amp;#39;d rather not re-invent the wheel here if I can avoid it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I&amp;#39;m considering writing a rendering server in Java and talking to it over a local socket.  I&amp;#39;m guessing that I can get that to perform well enough and at least then I&amp;#39;ll have a full, high quality drawing API available everywhere.&lt;/p&gt;

&lt;p&gt;Am I missing other options?  I really wasn&amp;#39;t expecting &amp;quot;draw some stuff to an image&amp;quot; to be the stumbling block in this project :)&lt;/p&gt;

&lt;p&gt;thanks.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,False,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,as6vvc,True,,patniemeyer,,3,True,all_ads,False,[],False,,/r/pytorch/comments/as6vvc/trying_to_find_a_drawing_api_for_use_with_pytorch/,all_ads,False,https://www.reddit.com/r/pytorch/comments/as6vvc/trying_to_find_a_drawing_api_for_use_with_pytorch/,7135,1550554079.0,0,,False,,,,,,,,
940,,pytorch,"my GPU (gtx 760 cuda capa. version 3.0), only works on pytorch version 0.3.1. but I want to get it to work on the newest pytorch version 1.0.0

the issue is they removed support for older GPUs in the new pytorch if I install from the source and change some files will pytorch 1.0.0 work with my 760?",t2_2t11jhi4,False,,0,False,can U get newest version of pytorch on old GPU?,[],r/pytorch,False,6,,0,,,False,t3_ardzpl,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1550385698.0,text,6,,,text,self.pytorch,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;my GPU (gtx 760 cuda capa. version 3.0), only works on pytorch version 0.3.1. but I want to get it to work on the newest pytorch version 1.0.0&lt;/p&gt;

&lt;p&gt;the issue is they removed support for older GPUs in the new pytorch if I install from the source and change some files will pytorch 1.0.0 work with my 760?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,True,True,False,False,False,,[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ardzpl,True,,AceOfSamuel,,6,True,all_ads,False,[],False,,/r/pytorch/comments/ardzpl/can_u_get_newest_version_of_pytorch_on_old_gpu/,all_ads,False,https://www.reddit.com/r/pytorch/comments/ardzpl/can_u_get_newest_version_of_pytorch_on_old_gpu/,7135,1550356898.0,0,,False,,,,,,,,
941,,pytorch,,t2_1ffz9tjt,False,,0,False,"Here is my pytorch implementation of the model described in the paper DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs (https://arxiv.org/pdf/1606.00915.pdf) Source code: https://github.com/vietnguyen91/Deeplab-pytorch",[],r/pytorch,False,6,,0,104.0,,False,t3_ar6ys8,False,dark,1.0,,public,9,0,{},140.0,,False,[],,True,False,,{},,False,9,,False,https://b.thumbs.redditmedia.com/QrhhoseH1et1rp5mr_uple-bv3SN6eHxokp7CpDB8mk.jpg,False,,[],{},image,,False,,1550334801.0,text,6,,,text,i.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/7wifwj666wg21.jpg?auto=webp&amp;s=3b1e75948ae7d53c11d9dc01e78c03f4a62ca254', 'width': 668, 'height': 500}, 'resolutions': [{'url': 'https://preview.redd.it/7wifwj666wg21.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c68820a22a30150e4328c5c355ffc682bb28f779', 'width': 108, 'height': 80}, {'url': 'https://preview.redd.it/7wifwj666wg21.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8e8e31aaebc9be15c6fe0f40b55078fc8e7f843', 'width': 216, 'height': 161}, {'url': 'https://preview.redd.it/7wifwj666wg21.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6da0945f7d9b3097d70326f9d32bbb4d6306de7c', 'width': 320, 'height': 239}, {'url': 'https://preview.redd.it/7wifwj666wg21.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=48085a4af7e8aa130f23b14eb637bacdfb79763b', 'width': 640, 'height': 479}], 'variants': {}, 'id': '4g697F-YLZ21NTMBhWqrqgV5B7BGVSGwEK866h_VJWo'}], 'enabled': True}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ar6ys8,True,,1991viet,,3,True,all_ads,False,[],False,,/r/pytorch/comments/ar6ys8/here_is_my_pytorch_implementation_of_the_model/,all_ads,False,https://i.redd.it/7wifwj666wg21.jpg,7135,1550306001.0,0,,False,https://i.redd.it/7wifwj666wg21.jpg,,,,,,,
942,,pytorch,,t2_mjlci,False,,0,False,"""PyTorch: Zero to GANs"" - A series of coding-focused tutorials on Deep Learning with PyTorch",[],r/pytorch,False,6,,0,58.0,,False,t3_aq62cf,False,dark,0.9,,public,7,0,{},140.0,,False,[],,False,False,,{},,False,7,,False,https://b.thumbs.redditmedia.com/S7212znWmwdPAqNyEtPDJox7y0sq2CEARQaEdQlBIyw.jpg,False,,[],{},link,,False,,1550089583.0,text,6,,,text,medium.com,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/gnrTzA5fOQS1Lyxgt6Lt43cH_uon13z8KM458oAqkPw.jpg?auto=webp&amp;s=f0b45fbb4631b4856da020f061ef1044a0f4ce78', 'width': 1200, 'height': 503}, 'resolutions': [{'url': 'https://external-preview.redd.it/gnrTzA5fOQS1Lyxgt6Lt43cH_uon13z8KM458oAqkPw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf99b90ac5896172caf80f8111aff0b291eaa8a6', 'width': 108, 'height': 45}, {'url': 'https://external-preview.redd.it/gnrTzA5fOQS1Lyxgt6Lt43cH_uon13z8KM458oAqkPw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4acd7609bab5a97f766ca042ad0236e062bf17f3', 'width': 216, 'height': 90}, {'url': 'https://external-preview.redd.it/gnrTzA5fOQS1Lyxgt6Lt43cH_uon13z8KM458oAqkPw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6672bb0e6b5a058b1a897e545e9cc35dbc04aa1', 'width': 320, 'height': 134}, {'url': 'https://external-preview.redd.it/gnrTzA5fOQS1Lyxgt6Lt43cH_uon13z8KM458oAqkPw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=51bad694b9f2dcc204e46dc5d71f14d1ac25fe11', 'width': 640, 'height': 268}, {'url': 'https://external-preview.redd.it/gnrTzA5fOQS1Lyxgt6Lt43cH_uon13z8KM458oAqkPw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1758fb781b051594b6bede6521b212c4effe76ac', 'width': 960, 'height': 402}, {'url': 'https://external-preview.redd.it/gnrTzA5fOQS1Lyxgt6Lt43cH_uon13z8KM458oAqkPw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0f01749b06c9321399d2b868df157787bf28c2ad', 'width': 1080, 'height': 452}], 'variants': {}, 'id': 'VFwl7u_klUx2ELMMUzaw3AO501GtHJA8-PQvDvFNoEQ'}], 'enabled': False}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,aq62cf,True,,aakashns,,3,True,all_ads,False,[],False,,/r/pytorch/comments/aq62cf/pytorch_zero_to_gans_a_series_of_codingfocused/,all_ads,False,https://medium.com/jovian-io/pytorch-basics-tensors-and-gradients-eb2f6e8a6eee,7135,1550060783.0,0,,False,https://medium.com/jovian-io/pytorch-basics-tensors-and-gradients-eb2f6e8a6eee,,,,,,,
943,,pytorch,,t2_1ffz9tjt,False,,0,False,My implementation of 3 NLP models for text classification in Pytorch and Tensorflow,[],r/pytorch,False,6,,0,78.0,,False,t3_ap93g7,False,dark,0.87,,public,11,0,{},140.0,,False,[],,True,False,,{},,False,11,,False,https://b.thumbs.redditmedia.com/xz_mBo3724KyzCYzEFPKzWmh-HevAj3v15qZUjeKdRQ.jpg,False,,[],{},image,,False,,1549865429.0,text,6,,,text,i.redd.it,False,,,,,,True,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/3avbe74wctf21.gif?format=png8&amp;s=10f80be42f07bee4d5353a36529b3182c4a498ae', 'width': 1600, 'height': 900}, 'resolutions': [{'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=f54664b8e7b3ceae2cae773f3eebbb9eacde144e', 'width': 108, 'height': 60}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=b069f5e62960f4d41f850f32b891e0f424b99dde', 'width': 216, 'height': 121}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=51b5f6bc79c0fc4bc69038f5c020c73ffb37736c', 'width': 320, 'height': 180}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=6925889c50e11a46bc8ce44fae401adc2caabfb7', 'width': 640, 'height': 360}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=4f69714b4e824b8091ac885b9a6ddc1e93cfdb91', 'width': 960, 'height': 540}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=42d5f700e51b6400af68d403d9d117e74ad12f6c', 'width': 1080, 'height': 607}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/3avbe74wctf21.gif?s=416a36b1115eac1acfecddf0538b6f232cb5c966', 'width': 1600, 'height': 900}, 'resolutions': [{'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=108&amp;crop=smart&amp;s=cd146cf7fa27b0342be1a4f4ea94ad2879562e76', 'width': 108, 'height': 60}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=216&amp;crop=smart&amp;s=5ea8e32f0a8c32fec36d64a325cfdb111726c1fe', 'width': 216, 'height': 121}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=320&amp;crop=smart&amp;s=ed3e8dfbf6ebb1cd930ed503644cf8106bc68b4e', 'width': 320, 'height': 180}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=640&amp;crop=smart&amp;s=fea1d889e020875adb6bc2eeb823e499b03a1ace', 'width': 640, 'height': 360}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=960&amp;crop=smart&amp;s=72d35750cfa30e15d1cdee12162de53119bd8dcc', 'width': 960, 'height': 540}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=1080&amp;crop=smart&amp;s=5452a52eafc40be665d3c3aebeacc163c1dd17be', 'width': 1080, 'height': 607}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/3avbe74wctf21.gif?format=mp4&amp;s=4a6f5c024ddae401003b03fcee1d9abb716e374a', 'width': 1600, 'height': 900}, 'resolutions': [{'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=108&amp;format=mp4&amp;s=4023b89def6377afba3f756e5493870fa0cb551a', 'width': 108, 'height': 60}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=216&amp;format=mp4&amp;s=876bac02a0d4093084e598968a8ad29a8476a00d', 'width': 216, 'height': 121}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=320&amp;format=mp4&amp;s=e07b93abdafe188769b67b60441d0ea30c5d61d3', 'width': 320, 'height': 180}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=640&amp;format=mp4&amp;s=a0548ca5dc731370dd3f4b708a080dd54bd84d73', 'width': 640, 'height': 360}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=960&amp;format=mp4&amp;s=86385942a1a683a2a5ed9b5998724bb54e1b61d4', 'width': 960, 'height': 540}, {'url': 'https://preview.redd.it/3avbe74wctf21.gif?width=1080&amp;format=mp4&amp;s=e94a6c260ac8e3c9bb45c1c6aa7d29198e99ced1', 'width': 1080, 'height': 607}]}}, 'id': 'zwoH2RAbgH0GtBJi4UIKk3He12HbcKG2QAYJM6vGuVc'}], 'enabled': True}",[],[],False,False,False,False,,[],False,,,,t5_3gbwg,,,,ap93g7,True,,1991viet,,7,True,all_ads,False,[],False,,/r/pytorch/comments/ap93g7/my_implementation_of_3_nlp_models_for_text/,all_ads,False,https://i.redd.it/3avbe74wctf21.gif,7135,1549836629.0,0,,False,https://i.redd.it/3avbe74wctf21.gif,,,,,,,
